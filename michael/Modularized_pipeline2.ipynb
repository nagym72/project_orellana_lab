{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be60346-a603-46d9-a77a-d05e88d76a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdb\n",
    "import requests\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict, OrderedDict\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil\n",
    "import statistics\n",
    "from datetime import date\n",
    "#import hail as hl\n",
    "import glob\n",
    "import time\n",
    "import pytrimal\n",
    "# Import from installed package\n",
    "#from pypdb.clients.pdb.pdb_client import *\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import pandas as pd\n",
    "#import plotly as px\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO, Align, PDB, Seq, AlignIO\n",
    "from Bio.PDB import PDBParser, PDBIO, Select, MMCIFParser, Structure, Chain, Atom\n",
    "from Bio.PDB import Model as Bio_Model\n",
    "from Bio.PDB import Chain as Bio_chain\n",
    "from Bio.SeqIO import PirIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align import substitution_matrices\n",
    "#from Bio import pairwise2\n",
    "from io import StringIO\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "from modeller.parallel import job, local_slave\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import logging\n",
    "import subprocess\n",
    "import shlex\n",
    "from subprocess import PIPE, run\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup  #required later to download SIFT files.\n",
    "import atomium\n",
    "from itertools import compress\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.gridspec as gridspe\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from bravado.client import SwaggerClient\n",
    "from pycanal import Canal\n",
    "#import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from threading import Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from packman import molecule\n",
    "from packman.apps import predict_hinge\n",
    "\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "#logging.getLogger(\"requests\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd73c953-08ee-4ab2-ae2f-b9e660a0e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadPipe:\n",
    "    '''Class object containing the download function that will download all pdbs \n",
    "    which we need for downstream analysis of a particular uniprot ID'''\n",
    "\n",
    "    def __init__(self, templates, work_dir, script_dir, seq_id=None, download_type=\"pdb\"):\n",
    "        self.work_dir = work_dir # Storage of seq identity useful later for temp selection.\n",
    "        self.script_dir = script_dir #here we store all scripts\n",
    "        self.seq_id = seq_id\n",
    "        self.download_type = download_type # Download PDB or also mmCIF (currently only PDB)\n",
    "        self.pdbs_to_download = templates \n",
    "        # The bash script location which will download the pdbs. \n",
    "        self.download_script = os.path.join(script_dir, \"batch_download_modified.sh\") #modify for script location\n",
    "        self.download_tmp = os.path.join(work_dir, \"pdb_list.csv\") # The location for the temporary file that is required for the download_script as input.\n",
    "        self.log_dir = os.path.join(work_dir, \"log_files\")\n",
    "        # The list of chains that will be used later to fetch correct structures.\n",
    "        self.chain_seqid_dict = self._setup_download_list()\n",
    "        self.temp_seqid_dict = {template: seq_id for template, seq_id in zip(self.pdbs_to_download, self.seq_id)}\n",
    "        # We store also meta info as a json dict\n",
    "        self.meta_dict = None\n",
    "        #we store high resolution structures as a list if the user wants to separate based on resolution.\n",
    "        self.high_resolution = None\n",
    "        # set a flag that stops redownloading.\n",
    "        self.already_downloaded = None\n",
    "        # collect conservation\n",
    "        self.conservation_df = None\n",
    "        #filtered structures based on meta resolution\n",
    "        self.filtered_structures = None\n",
    "        #store shifts.\n",
    "        self.shift_dict = None\n",
    "            \n",
    "    def paralellized_download(self):\n",
    "        '''\n",
    "        This function is going to call _download_files n times to parallelize download. \n",
    "        It is going to pass the function call itself **_download_file**,\n",
    "        self.download_tmp (the location of the tmp file which is pdb_id comma separated), \n",
    "        p (an additional parameter specifying that \n",
    "        we want to download pdbs, and self.work_dir(the current work dir)\n",
    "        '''\n",
    "        \n",
    "        self.already_downloaded = self._check_for_pdbs_present()\n",
    "        # ThreadPoolExecutor\n",
    "        if self.already_downloaded == False:\n",
    "            print(\"we start downloading now:\")\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                # Submit your tasks to the executor.\n",
    "                futures_pdb = [executor.submit(self._download_files, self.download_tmp, 'p', self.work_dir)]\n",
    "                # Optionally, you can use as_completed to wait for and retrieve completed results.\n",
    "                for future in as_completed(futures_pdb):\n",
    "                    result = future.result()\n",
    "            self.already_downloaded = True\n",
    "        else:\n",
    "            print(\"we already have pdbs from the templates downloaded\")\n",
    "    \n",
    "    def _setup_download_list(self):\n",
    "        '''Helper function to setup the list of comma-separated pdb\n",
    "        ids for the download_files function'''\n",
    "        chain_seqid_dict = defaultdict(list)\n",
    "        for pdb, seq_id in zip(self.pdbs_to_download, self.seq_id):\n",
    "            pdb_4_digit_id = pdb[:4] # e.g 4CFR\n",
    "            chain = pdb[-1] # e.g A\n",
    "            chain_seqid_dict[pdb_4_digit_id].append((chain, seq_id)) #map chains and seq id to pdb id\n",
    "        # We only want to download pdb files once. \n",
    "        # No reason to download a PDB-file 4 times just because we need chain [A, B, C, D]\n",
    "        unique_pdbs = chain_seqid_dict.keys() # Keys : PDB-IDs, Vals: Chains, seq_id\n",
    "        # Create download_files input list\n",
    "        with open(self.download_tmp, \"w\") as pdb_tar:\n",
    "            pdb_tar.write(\",\".join(unique_pdbs))\n",
    "            \n",
    "            \n",
    "        #return dict {key: pdb_id = [(chain, seq_id)]}\n",
    "        return chain_seqid_dict\n",
    "    \n",
    "    def _download_files(self, download_tmp, download_type, path)->list:\n",
    "        \"\"\"This helper function runs inside paralellized_download \n",
    "        and will be used to get the PDB files that we require for downstream analysis.\"\"\"\n",
    "        results = []\n",
    "        # Input for subprocess\n",
    "        bash_curl_cmd = f\"{self.download_script} -f {download_tmp} -o {path} -{download_type}\"\n",
    "        # split into list \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "        \n",
    "        try:\n",
    "            # Run subprocess\n",
    "            result = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            # Append result output to results\n",
    "            results.append(result.stdout.split(\"\\n\")[:-1])  # Skip the last empty element\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error downloading: {e}\")\n",
    "\n",
    "        return results    \n",
    "\n",
    "    def _check_for_pdbs_present(self):\n",
    "        '''\n",
    "        Could be good to improve so that if we miss SOME structures we fetch them as well and download ONLY those.\n",
    "        For those structures we also need seqid per chain and then also update the seqid_chain dict for the whole directory after\n",
    "        successful download.\n",
    "        Currently we only check if pdbs are present and if yes we dont download anything further.\n",
    "        '''\n",
    "        pdbs_to_retrieve = {f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")}  # Use a set for efficient lookups\n",
    "        template_codes = {f[:4] for f in self.pdbs_to_download}  # Convert list to set for efficient intersection operation\n",
    "\n",
    "        # Check for any overlap between the two sets\n",
    "        overlap = pdbs_to_retrieve.intersection(template_codes)\n",
    "\n",
    "        print(f\"This is overlap in the directory: {overlap}\")\n",
    "        # Return 1 if there is an overlap, else 0\n",
    "        return True if overlap else False\n",
    "\n",
    "    \n",
    "    def retrieve_meta(self, dict_location=None, human_readable=True)->dict:\n",
    "        '''\n",
    "        We also want to store meta information about resolution etc.\n",
    "        This function takes each pdb file and retrieves the following information:\n",
    "        - Title\n",
    "        - Keywords\n",
    "        - PDBcode\n",
    "        - Authors\n",
    "        - Deposition date\n",
    "        - Technique\n",
    "        - Resolution\n",
    "        - R_value : If crystallography else None\n",
    "        - R_free : If crystallographe else None\n",
    "        - Classification\n",
    "        - Organism\n",
    "        - Expression System\n",
    "        - Number of amino acids in the asymmetric unit\n",
    "        - Mass of amino acids in the asymmetric unit (Da)\n",
    "        - Number of amino acids in the biological unit\n",
    "        - Mass of amino acids in the biological unit (Da)\n",
    "        '''\n",
    "        \n",
    "        json_file_path = os.path.join(self.log_dir, 'meta_dictionary.json')\n",
    "\n",
    "        for path in [json_file_path, dict_location]: #check first the supposed location alternatively the user supplied location.\n",
    "            if path and os.path.exists(path):\n",
    "                try:\n",
    "                    with open(path, 'r') as json_fh:\n",
    "                        self.meta_dict = json.load(json_fh)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {path}: {e}\")\n",
    "                \n",
    "        #little helper function to deal with date data\n",
    "        def _date_encoder(obj):\n",
    "            if isinstance(obj, date):\n",
    "                return obj.isoformat()  # Convert date to ISO format\n",
    "\n",
    "        #grab all PDB files which contain the meta information.\n",
    "        pdbs_to_retrieve = [f for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "        #here we store info about ALL pdbs.\n",
    "        meta_dictionary = dict()\n",
    "        \n",
    "        for pdbs in pdbs_to_retrieve:\n",
    "            if len(pdbs) == 8: #lets exclude preprocessed pdbs that are longer or shorter.\n",
    "                sub_dict = dict()\n",
    "                pdb_code = pdbs[:4]\n",
    "                try:\n",
    "                    fullp = os.path.join(self.work_dir, pdbs)\n",
    "                    pdb = atomium.open(fullp)\n",
    "                    sub_dict[\"title\"] = pdb.title\n",
    "                    sub_dict[\"key_words\"] = pdb.keywords\n",
    "                    sub_dict[\"code\"] = pdb.code\n",
    "                    sub_dict[\"authors\"] = pdb.authors\n",
    "                    #sub_dict[\"deposition_date\"] = pdb.deposition_date.isoformat()  #isoformat because it is a time object\n",
    "                    sub_dict[\"technique\"] = pdb.technique\n",
    "                    sub_dict[\"resolution\"] = pdb.resolution\n",
    "                    sub_dict[\"r_val\"] = pdb.rvalue\n",
    "                    sub_dict[\"r_free\"] = pdb.rfree\n",
    "                    sub_dict[\"classification\"] = pdb.classification\n",
    "                    sub_dict[\"organism\"] = pdb.source_organism\n",
    "                    sub_dict[\"expression_system\"] = pdb.expression_system\n",
    "                    sub_dict['number_of_residues_asymmetric_unit'] = len(pdb.model.residues())\n",
    "                    sub_dict['mass_dalton_asymetric_unit'] = f\"{pdb.model.mass:.2f}\" \n",
    "                    try:\n",
    "                        assembly = pdb.generate_assembly(1) #build the biological assembly \n",
    "                        sub_dict['number_of_residues_biological_unit'] = len(assembly.residues())\n",
    "                        sub_dict['mass_dalton_biological_unit'] = f\"{assembly.mass:.2f}\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"We could not build the assembly for: {pdb_code}\")\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"We had an error with file: {pdb_code}\")\n",
    "                # store meta info and return\n",
    "                meta_dictionary[pdb_code] = sub_dict\n",
    "\n",
    "\n",
    "        #lets store meta info as json dict\n",
    "        self.meta_dict = meta_dictionary\n",
    "        \n",
    "        # Code block to store meta info as a txt file.\n",
    "        self._save_meta_dict(self.meta_dict, human_readable=human_readable)\n",
    "\n",
    "\n",
    "    def _save_meta_dict(self, meta_dictionary, human_readable=True):\n",
    "        '''Helper function to store meta info as a txt file.'''\n",
    "        #check if log file dir exists, else make it.\n",
    "        \n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        #lets store the dict in json to read it in for later useage.\n",
    "        json_file_path = os.path.join(self.log_dir, 'meta_dictionary.json')\n",
    "        #convert defaultdict to normal dict.\n",
    "        \n",
    "        with open(json_file_path, 'w') as json_fh:\n",
    "            json.dump(meta_dictionary, json_fh, indent=4, default=str)  # Use default=str to handle non-serializable objects\n",
    "\n",
    "    \n",
    "    def conservation(self, uniprot_id):\n",
    "        '''Gets 3 different types of Conservation:\n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "\n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        \n",
    "        mmseq_fasta_result = self._mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=self.work_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = self._get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "        self.conservation_df = conserv_df\n",
    "\n",
    "        conserv_df.to_csv(f\"{self.log_dir}/conservation_df.csv\")\n",
    "        \n",
    "    def _mmseq_multi_fasta(self, uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "        \"\"\"\n",
    "        uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "        outdir: location where result files will be stored.\n",
    "        sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "        filter_msa = 0 default. if 1 hits are stricter.\n",
    "        query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "        \"\"\"\n",
    "\n",
    "        #we blast with this fasta as query.\n",
    "        trgt_fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "        #Make outdir for all required files.\n",
    "        #we need to write it out to file.\n",
    "        with open(f\"{self.work_dir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "            fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "            fasta_out.write(trgt_fasta_seq)\n",
    "\n",
    "        #fetch pre downloaded database from a parent folder.\n",
    "        msa_file = None\n",
    "        new_location = None\n",
    "        try:\n",
    "            DB_storage_location = f\"{work_dir}\"\n",
    "            #shutil.copy(previous_path, savepath)\n",
    "            bash_curl_cmd = f\"mmseqs createdb {self.work_dir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\" \n",
    "            bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "            #run first cmd which setups query database based on our input fasta file\n",
    "            result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"    \n",
    "            bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "            #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "            result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "            bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "            bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "            result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #delete last line.. required.\n",
    "            sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'        \n",
    "            bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "            #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "                result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            # Specify the path to your MSA file\n",
    "            msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            #transfer the meta file to another location and delete useless files.\n",
    "            # we need to delete : all uniprot* files. \n",
    "            # all query*. All result* \n",
    "            new_location = f\"{self.work_dir}/{uniprot_id}.fasta\"\n",
    "            shutil.copy(msa_file, new_location)\n",
    "            #remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        #we want the path to msa_file for downstream analysis.\n",
    "        return new_location\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "        '''\n",
    "        Helper function to grab the sequence \n",
    "        based on the Uniprot ID\n",
    "        '''\n",
    "        fields = \"sequence\"\n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        seq = \"\"\n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "            i += 1\n",
    "        return seq\n",
    "\n",
    "    def _get_conservation(self, path_to_msa:str):    \n",
    "        '''\n",
    "        Helper function to compute 3 different types of conservation.\n",
    "        \n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "        canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "        result_cons = canal.analysis(method=\"all\")\n",
    "        return result_cons\n",
    "\n",
    "    def _get_url(self, url):\n",
    "        '''Helper function that uses requests for Downloads.'''\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit() \n",
    "        return response\n",
    "    \n",
    "    def setup_cutoff(self, cutoff=10, apply_filter=False):\n",
    "        '''If we want to setup a resolution cutoff filter for further downstream analysis, \n",
    "        this function helps with it.'''\n",
    "        # If there is no meta dict we cant proceed and filter based on resolution.\n",
    "        if self.meta_dict:\n",
    "            #here we store the pdb codes that we keep\n",
    "            pdbs_to_keep = []\n",
    "            #Now lets parse through the whole meta dict and fetch the cutoffs for structures.\n",
    "            for _, single_pdbs in self.meta_dict.items():\n",
    "                if single_pdbs['resolution'] <= cutoff:\n",
    "                    pdbs_to_keep.append(single_pdbs['code'].lower()) #normalize to lower in order to have uniform list members.   \n",
    "                    \n",
    "            self.filtered_structures = pdbs_to_keep\n",
    "            #now if we directly want to apply the filter to remove files that dont match our criteria.\n",
    "            if apply_filter:\n",
    "                #check for union between files and kept structures.\n",
    "                pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "                #lets fetch the intersect between the 2 sets which corresponds to the pdbs we want to keep.\n",
    "                common_pdb = set(pdbs_to_retrieve) & set(pdbs_to_keep) #intersection\n",
    "                intersect_lst = list(common_pdb)\n",
    "                self.filtered_structures = intersect_lst\n",
    "                if self.chain_seqid_dict:\n",
    "                    #now we need to update the chain_dict as well:\n",
    "                    filtered_dict = {pdb: v for pdb, v in self.chain_seqid_dict.items() if pdb[:4] in self.filtered_structures}\n",
    "                    self.filtered_structures = filtered_dict\n",
    "                    \n",
    "        else:\n",
    "            print(\"We have no meta dict to implement a cutoff\")\n",
    "            #In this case we take all.\n",
    "            pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\") and len(f) == 8] #exclude non original files. Only store pdb + _ + chains.\n",
    "            self.filtered_structures = pdbs_to_retrieve\n",
    "\n",
    "    def parallel_shift_calculation(self):\n",
    "        '''Here we compute the shift according to uniprot or authors\n",
    "        in order to be in line with UNIPROT numbering which is crucial for later renumbering.'''\n",
    "        \n",
    "        pdbs_to_retrieve = [f[0:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]  \n",
    "        pdbs_to_retrieve = set(pdbs_to_retrieve) & set(x[:4] for x in self.oligodict.keys()) #here we check the first 4 which is pdb code\n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        shift_dict = defaultdict()\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            calculate_shift_bound = partial(self._calculate_shift)\n",
    "            tasks = ((link_path, pdb) for pdb in pdbs_to_retrieve)\n",
    "            # Map the bound function to the arguments in parallel\n",
    "            results = executor.map(calculate_shift_bound, tasks)\n",
    "            for result in results:\n",
    "                for keys, vals in result.items():\n",
    "                    shift_dict[keys] = vals\n",
    "                    \n",
    "        self.shifts = shift_dict\n",
    "\n",
    "    def _calculate_shift(self, args):\n",
    "        '''\n",
    "        Helper function to compute the shift.\n",
    "        Args: link_path to UNIPROT page and the pdb path.\n",
    "        '''\n",
    "        link_path, pdb = args\n",
    "        shift_dict = defaultdict()\n",
    "        searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "        resp = self._get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                for mapping in uniprot_info['mappings']:\n",
    "                    chain_id = mapping['chain_id']\n",
    "                    unp_start = mapping['unp_start']\n",
    "                    unp_end = mapping['unp_end']\n",
    "                    author_start = mapping['start']['author_residue_number']\n",
    "                    author_end = mapping['end']['author_residue_number']\n",
    "    \n",
    "                    if author_start is None:\n",
    "                        author_start = unp_start\n",
    "                    if author_end is None:\n",
    "                        author_end = unp_end\n",
    "                    shift_start = unp_start - author_start\n",
    "                    shift_end = unp_end - author_end\n",
    "                    shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "                    \n",
    "        self.shift_dict = shift_dict\n",
    "        return shift_dict\n",
    "\n",
    "    \n",
    "    def parallel_renumbering(self):\n",
    "        '''\n",
    "        Helper function to do parallelized renumbering.\n",
    "        If already renumbered, don't do it again.\n",
    "        '''\n",
    "        if self.renumbered:\n",
    "            print(\"You already renumbered your structures based on shift.\")\n",
    "            return  # Exit the function early\n",
    "\n",
    "        if not self.shifts:\n",
    "            print(\"You first need to obtain shifts which will be used as reference in order to start renumbering.\\nCall .parallel_shift_calculation() first.\")\n",
    "            return  # Exit the function if no shifts are available\n",
    "\n",
    "        # At this point, we know renumbering has not been done and shifts are available\n",
    "        relevant_files = self.chain_seq_dict.keys()\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Using partial to create a function with fixed parameters (shift_dict, path)\n",
    "            renumber_structure_partial = partial(self._renumber_structure, shift_dict=self.shifts, path=self.work_dir)\n",
    "            # Map the renumbering function to each relevant file in parallel\n",
    "            executor.map(renumber_structure_partial, relevant_files)\n",
    "\n",
    "        self.renumbered = True\n",
    "\n",
    "    \n",
    "    def _renumber_structure(self, files, shift_dict, path):\n",
    "        '''Function that is going to apply pdb_shiftres_by_chain.py to each pdb file that is shifted.\n",
    "        Will apply renumbering to ALL structures if you did not set a cutoff previously and applied filter. \n",
    "        If filter applied for resolution will only renumber those structures that are left after filtering.'''\n",
    "        for keys, vals in shift_dict.items():\n",
    "            #dont renumber if there is not shift\n",
    "            if files == keys[0:4] and vals != str(0):\n",
    "                chain = keys[-1]\n",
    "                shift = int(vals)\n",
    "                filepath = f\"{self.work_dir}/{files}.pdb\"\n",
    "                # Should we really shift by shift + 1??? or just shift?\n",
    "                bash_cmd = f\"python {self.script_dir}/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "            \n",
    "                with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                    result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, universal_newlines=True)\n",
    "                    # Now replace the original one with the temp file.\n",
    "                    os.replace(f\"{filepath}_tmp\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "67fa273b-3fd6-41b8-8997-0cc7a72da73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is overlap in the directory: {'7nnj', '5ltu', '3mcf', '2duk', '7tn4'}\n",
      "we already have pdbs from the templates downloaded\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf74f3d-61e6-42f7-a1d5-5f5f20eaa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBBuilder:\n",
    "    '''Class to build biological units from asymmetric units'''\n",
    "    def __init__(self, work_dir, structures, remove_intermediates=False, main_protein_sequence=None, logging=True):\n",
    "        self.work_dir = work_dir\n",
    "        #Here we store the structures that need to be built.\n",
    "        self.structures = structures\n",
    "        #we store intermediate files per default.\n",
    "        self.remove_intermediates = remove_intermediates\n",
    "        #we set oligo to None but store later the oligodict\n",
    "        self.oligodict = None\n",
    "        # dict to store the potential ranges.\n",
    "        self.range_dict = None\n",
    "        # logging purpose\n",
    "        self.log_dir = os.path.join(work_dir, \"log_files\")\n",
    "        # enable logging.\n",
    "        self.logging = logging\n",
    "        #main_prot_seq\n",
    "        self.main_protein_sequence = main_protein_sequence\n",
    "        # result dict for established domains\n",
    "        self.established_domain_dict = None\n",
    "        # top templates found for each range\n",
    "        self.top_templates = None\n",
    "\n",
    "        self.assemblied_structures = None\n",
    "\n",
    "    def build_assembly(self):\n",
    "        '''\n",
    "        This function takes all pdbs in self.structures that were passed to class initialization:\n",
    "\n",
    "        structure:\n",
    "        self.structures -> dict\n",
    "        keys: pdb_id_4_digit\n",
    "        values: tuple (chain_id, seq_identity)\n",
    "\n",
    "        example: {'5ltu': [('A', 0.96), ('B', 0.96)]}\n",
    "\n",
    "        We rechain all pdbs in order to have uniformized chain labels for downstream processing.\n",
    "        Then, we build biological assemblies based on the asymmetric unit deposition and caputure the oligomeric status.\n",
    "        We return then an updated self.oligodict that has the following structure:\n",
    "\n",
    "        key: PDB_ID_CHAIN(s)\n",
    "        values: tuple(chain, seq_id)\n",
    "        example: {'5ltu_AB.pdb': [('A', 0.96), ('B', 0.96)]}\n",
    "        This corresponds to the fully assembled structure.\n",
    "        \n",
    "        '''\n",
    "        # These files need to be opened, rechained and assemblies built.\n",
    "\n",
    "        full_pdb_paths = [os.path.join(self.work_dir, f\"{file}.pdb\") for file in self.structures]\n",
    "        oligostates = defaultdict(str)\n",
    "        #this letterdict is used for rechaining.\n",
    "        letterdict = {i: chr(65 + i) for i in range(26)}\n",
    "        #changed this here from threadpool to process pool\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Define your processing function, partially applied with gene_name and main_protein_seq\n",
    "            process_func = partial(self._process_pdb, letterdict=letterdict)\n",
    "            results = executor.map(process_func, full_pdb_paths)\n",
    "            for result in results:\n",
    "                #print(f\"this is result: {result}\")\n",
    "                oligostates.update(result)\n",
    "\n",
    "        #beautifully chaotic.... basically we update... 4 digit ID + chains i.e 4 means ABCD 2 means AB 1 means A, and concatenate it with .pdb to get 4rt4_ABCD.pdb\n",
    "        new_oligostates = {f\"{k[0:4]}_{''.join(letterdict[i] for i in range(v))}.pdb\": v for k, v in oligostates.items()}\n",
    "\n",
    "        #lets update the dict now with info about seq id.\n",
    "        self._update_oligostates_with_sequence_identities(new_oligostates)\n",
    "\n",
    "    def _update_oligostates_with_sequence_identities(self, dict_to_update):\n",
    "        '''\n",
    "        This function helps to bring back the sequence identity information to the new oligodict\n",
    "        For those chains that wont end up in the biological unit, we drop the information.\n",
    "        '''\n",
    "        updated_oligostates = {}\n",
    "        for pdb_chain_file, oligostate in dict_to_update.items():\n",
    "            pdb_id = pdb_chain_file.split('_')[0]  # Extract the PDB ID from the filename\n",
    "            chain_ids = pdb_chain_file.split('_')[1].rstrip('.pdb')  # Extract chain IDs\n",
    "            # Initialize a list to hold sequence identities for each chain\n",
    "            seq_identities = []\n",
    "            \n",
    "            if pdb_id in self.structures:\n",
    "                # Iterate through each chain and its corresponding sequence identity in the structures\n",
    "                for chain_id, seq_identity in self.structures[pdb_id]:\n",
    "                    # If the current chain is part of the oligostate (i.e., it's in the chain_ids string), add its sequence identity\n",
    "                    if chain_id in chain_ids:\n",
    "                        seq_identities.append((chain_id, seq_identity))\n",
    "            \n",
    "            # Update the dictionary with the new value format\n",
    "            updated_oligostates[pdb_chain_file] = (seq_identities)\n",
    "        \n",
    "        # Update the class attribute\n",
    "        self.oligodict = updated_oligostates\n",
    "    \n",
    "    def _process_pdb(self, path:str,letterdict:dict)->dict:\n",
    "        #helper function to split between nmr and xray / cryoem\n",
    "        try:\n",
    "            pdb_file_name = os.path.basename(path)\n",
    "            if len(pdb_file_name) != 8:\n",
    "                #this means its already processed previously because its not 5DUK_A.pdb instead of 5DUK.pdb\n",
    "                oligostate = len(pdb_file_name[5:-4]) # this is the oligostate e.g ABC = 3 \n",
    "                return {pdb_file_name: oligostate}\n",
    "                \n",
    "            pdb_file = atomium.open(path)\n",
    "            model_len = len(pdb_file.models)\n",
    "            \n",
    "            if model_len > 5:  # if multiple models => NMR OR CRYO-EM?\n",
    "                return {pdb_file_name: self._NMR_ensemble(path=path, letterdict=letterdict)}\n",
    "            else:\n",
    "                return {pdb_file_name: self._non_NMR_structures(path=path, letterdict=letterdict)}\n",
    "\n",
    "            #now we remove the original file.\n",
    "            os.remove(path)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(\"process pdb did not work\")\n",
    "            print(error)\n",
    "            return {}\n",
    "    \n",
    "    #helper function for XRAY and CRYO-EM ensembles.\n",
    "    def _non_NMR_structures(self, path:str, letterdict:dict):\n",
    "    \n",
    "        \"\"\"This function takes in the the pdb file that is xray or cryoem and rechains each chain. \n",
    "        Additionally, we merge the new labelled chains into a merged_pdb file for further use.\"\"\"\n",
    "    \n",
    "        #store base dir.\n",
    "        base_dir = os.path.dirname(path)\n",
    "        pdb_name = os.path.basename(path)[:4]\n",
    "        #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95.pdb\n",
    "        pdb_file = atomium.open(path)\n",
    "        assemblies = [pdb_file.generate_assembly(n + 1) for n in range(len(pdb_file.assemblies))]\n",
    "        #we take the first one(this is the biological unit built from the asymmetric unit)\n",
    "        assembly = assemblies[0]\n",
    "        #for logger purpose\n",
    "        #assembly_info = f\"{pdb_name=}, {assemblies=}\"\n",
    "        #self._logger(assembly_info)        \n",
    "        #tuple containing chain ID and LEN of each chain.\n",
    "        seq_chains = [(chain.id, len(chain.sequence)) for chain in assembly.chains()]\n",
    "        sorted_lens = sorted(seq_chains, key= lambda x: x[1], reverse=True) #reverse = true :largest first.\n",
    "        accepted_chains = []  #this will be used to store and evalute oligomeric state.\n",
    "        min_accepted_length = float('inf')   # Minimum length of accepted chains init as pos inf.\n",
    "\n",
    "        for chain, length in seq_chains:\n",
    "            #for each chain and length.\n",
    "            if not accepted_chains or length > 0.8 * min_accepted_length:\n",
    "                #we accept it if either we have no other chain so far OR length > 80% of the chain we have so far.\n",
    "                accepted_chains.append(chain)\n",
    "                #first chain will be accepted and then will be the standard for the next chains to follow.\n",
    "                min_accepted_length = min(min_accepted_length, length)\n",
    "\n",
    "        oligostate = len(accepted_chains)  #this excludes small peptides ect from being mistaken as oligomers.\n",
    "        \"\"\"Part 2. We investigate oligomeric state.\"\"\"\n",
    "        accepted_chains_set = set(accepted_chains)\n",
    "        oligomeric_status = None\n",
    "        if len(accepted_chains_set) != len(accepted_chains):\n",
    "            #this means we have a homo-oligomer!\n",
    "            #e.g A vs A A A .. len(1) != len(0)\n",
    "            #hetero-mers are not caught here.. A B C == A B C == len(3)\n",
    "            oligomeric_status = \"homo_oligomer\"\n",
    "        \n",
    "        elif len(accepted_chains) == 1:\n",
    "            #this means we deal with a monomer.\n",
    "            oligomeric_status = \"monomer\"\n",
    "        \n",
    "        elif len(accepted_chains) > 1 and len(accepted_chains) == len(accepted_chains_set):\n",
    "            #this means its a mixed heteromer. becaue len(1) > AND set == list aka no redundancy ergo heteromer.\n",
    "            oligomeric_status = \"hetero_oligomer\"\n",
    "\n",
    "        \"\"\"Part 3: We follow through and now save individual chains + send them to proper rechaining. \"\"\"    \n",
    "        path_list = []\n",
    "\n",
    "        for idx, chain in enumerate(assembly.chains()):\n",
    "            chain_label = chain.id\n",
    "            if chain_label in accepted_chains_set:\n",
    "                path_to_pdb = f\"{self.work_dir}/{pdb_name}_{idx}.pdb\"\n",
    "                #save it here.\n",
    "                path_list.append(path_to_pdb)\n",
    "                #and also save the structure in its wrong chain state first.\n",
    "                chain.save(path_to_pdb)\n",
    "        \n",
    "            path_list = sorted(path_list, key=lambda x: int(x[-5]))\n",
    "    \n",
    "        \"\"\"Part 4: We now deal with all kind of oligomers, and also save all single chains in the procedure.\n",
    "        Normal monomers are also simply saved and rechained. Everything according to a general schema for efficient\n",
    "        downstream processing.\"\"\"\n",
    "    \n",
    "        self._merge_pdb_chains(path_list, pdb_name=pdb_name, oligomeric_status=oligomeric_status, \n",
    "                      letterdict=letterdict, accepted_chains = accepted_chains, accepted_chains_set=accepted_chains_set)\n",
    "        \n",
    "        #we return the oligostate of this file and merge it into dict as return value.\n",
    "        return oligostate\n",
    "\n",
    "    #helper function for NMR ensembles.\n",
    "    def _NMR_ensemble(self, path:str, letterdict:dict):\n",
    "\n",
    "        \"\"\"This function takes in the NMR ensemble and \n",
    "        splits each state into a respective PDB file.\"\"\"\n",
    "    \n",
    "        #open the pdb file\n",
    "        print(f\"we currently open with atomium: {path}\")\n",
    "        pdb_name = os.path.basename(path)[:4] #4 digit ID\n",
    "        base_dir = os.path.dirname(path) #base dir name\n",
    "        pdb_file = atomium.open(path)\n",
    "        oligostate = 1 #default initialize\n",
    "        path_list = []\n",
    "    \n",
    "        for i, model in enumerate(pdb1.models):\n",
    "            #now iterate through each model and its respective chains.\n",
    "            chain_len = len([x.id for x in model.chains()]) > 1 # True if multiple chains.\n",
    "            #if larger than 1 : we need to merge.\n",
    "            chain_paths = []\n",
    "            new_chains = []\n",
    "            \n",
    "            for j, chain in enumerate(model.chains()): #enumerate because there are NMR models with MULTIPLE CHAINS\n",
    "                #here we save the structure. as number.. we need to check the chain id.\n",
    "                new_chain = chain.copy(id=letterdict[j]) #new chain ID.\n",
    "                #this effectively rechained the chain.\n",
    "                save_location = f\"{base_dir}/{pdb_name}_{i}_{letterdict[j]}.pdb\"\n",
    "                new_chain.save(save_location) # e.g 4ND5_0_A.pdb 4ND5_1_A etc..\n",
    "                  \n",
    "                if chain_len:\n",
    "                    chain_paths.append(save_location)\n",
    "                    new_chains.append(new_chain.id)\n",
    "        \n",
    "            #if done: check if there are multiple chains. if yes. merge.\n",
    "            if chain_len:\n",
    "                oligostate = len(new_chains)\n",
    "                #we merge the chains.\n",
    "                save_nmr_oligomer = f\"{base_dir}/{pdb_name}_{i}_{''.join(new_chains)}.pdb\"\n",
    "                merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(chain_paths)}\"\n",
    "                merge_command_rdy = merge_command.split()\n",
    "                merge_output_file = f\"{save_nmr_oligomer}_tmp.pdb\"  #tmp\n",
    "    \n",
    "                with open(merge_output_file, \"w\") as fh_out:\n",
    "                    result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "                # Run tidy on the merged PDB\n",
    "                tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "                tidy_command_rdy = tidy_command.split()\n",
    "                tidy_output_file = f\"{save_nmr_oligomer}.pdb\"\n",
    "    \n",
    "                with open(tidy_output_file, \"w\") as fh_out2:\n",
    "                    results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "                #we remove tmp intermediate files.\n",
    "                if self.remove_intermediates:\n",
    "                    print(\"we remove\", merge_output_file)\n",
    "                    os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "\n",
    "        return oligostate\n",
    "    \n",
    "    def _merge_pdb_chains(self, path_list:list, pdb_name:str, oligomeric_status:str, letterdict:dict,\n",
    "                     accepted_chains:list, accepted_chains_set:set):\n",
    "\n",
    "        if oligomeric_status == \"homo_oligomer\":\n",
    "            self._pure_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "            \n",
    "        elif oligomeric_status == \"hetero_oligomer\":\n",
    "            self._mixed_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name,\n",
    "                                  accepted_chains=accepted_chains, accepted_chains_set=accepted_chains_set)\n",
    "            \n",
    "        elif oligomeric_status == \"monomer\":\n",
    "            self._monomeric_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "            \n",
    "        else:\n",
    "            print(f\"There was an issue with: {oligomeric_status=}\")\n",
    "            \n",
    "        return\n",
    "\n",
    "    def _pure_oligomer_rechaining(self, path_list:list, letterdict:dict, pdb_name:str): \n",
    "        #path_list = ['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_0.pdb',\n",
    "        # '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_1.pdb']\n",
    "        directory = os.path.dirname(path_list[0]) #this does not change. so no reason to constantly evaluate it in the loop\n",
    "        #store path to chains here.\n",
    "        lst_to_merge_paths = []\n",
    "        #store lists here for later merge.\n",
    "        chain_lst = []\n",
    "\n",
    "        for path_to_pdb in path_list:\n",
    "            \n",
    "            filename = os.path.basename(path_to_pdb) #same as above.\n",
    "            new_chain_digit = filename[5]  # Get the single chain id (e.g., '0')\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            individual_structure = parser.get_structure(\"default\", path_to_pdb)\n",
    "            new_chain = letterdict[int(new_chain_digit)]\n",
    "\n",
    "            for models in individual_structure:\n",
    "                for chain in models:\n",
    "                    if chain.id == new_chain:\n",
    "                        #then we simply save it under its original chain.\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        chain_lst.append(chain.id)\n",
    "                        io.save(save_location)\n",
    "                    else:\n",
    "                        chain.id = new_chain\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        chain_lst.append(chain.id)\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        io.save(save_location)\n",
    "            \n",
    "            if self.remove_intermediates:\n",
    "                os.remove(path_to_pdb) #we no longer need the original file\n",
    "\n",
    "        #prepare subprocess for pdb_merge.py\n",
    "        merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "        merge_command_rdy = merge_command.split()\n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}_tmp.pdb\"  #tmp\n",
    "    \n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        # prepare subprocess for pdb_tidy.py\n",
    "        tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}.pdb\"\n",
    "    \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "        #lets remove artifacts if the user wishes\n",
    "        if self.remove_intermediates:\n",
    "            for artifacts in lst_to_merge_paths:\n",
    "                os.remove(artifacts)\n",
    "\n",
    "    def _mixed_oligomer_rechaining(self, accepted_chains:list,\n",
    "                               accepted_chains_set:set,\n",
    "                               path_list:list,\n",
    "                               letterdict:dict, pdb_name:str):\n",
    "\n",
    "        seen_chains = sorted(accepted_chains, reverse=False)\n",
    "        chain_seq_len = len(seen_chains) #e.g 6\n",
    "        shift = len(accepted_chains_set) # e.g 3\n",
    "        blocksize = chain_seq_len // shift # e.g 2\n",
    "        block_count = int(chain_seq_len/blocksize) \n",
    "        '''\n",
    "        # A A B B C C becomes A D B E C F\n",
    "        # B B C C becomes A C B D \n",
    "        # i = 1\n",
    "        # A D \n",
    "        # block 1 2 3 for A A B B C C \n",
    "        # 0 2 1 3\n",
    "        '''\n",
    "        j = 0\n",
    "        new_chain_seq = []\n",
    "        lst_to_merge_paths = []\n",
    "    \n",
    "        for blocks in range(0, block_count):\n",
    "            for i in range(0, blocksize):\n",
    "                '''\n",
    "                # first iteration A D\n",
    "                # second iteration B E\n",
    "                # third iteration C F\n",
    "                '''\n",
    "                new_chain = letterdict[blocks+i*shift]\n",
    "                new_chain_seq.append(new_chain)\n",
    "                path_to_pdb = path_list[j]\n",
    "                directory = os.path.dirname(path_to_pdb)\n",
    "                j += 1\n",
    "                parser = PDBParser(QUIET=True)\n",
    "                prot_name = f\"default\"\n",
    "                #open the correct pdb and rechain it.\n",
    "                structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "                \n",
    "                for models in structure_template:\n",
    "                    for chain in models:\n",
    "                        if chain.id != new_chain:\n",
    "                            chain.id = new_chain\n",
    "                \n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        #print(f\"This is single chain save inside oligomer rechain: {directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        io.save(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "\n",
    "                #remove intermediate file\n",
    "                os.remove(path_to_pdb)\n",
    "\n",
    "        # Prepare subprocess for pdb_merge.py\n",
    "        merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "        merge_command_rdy = merge_command.split()\n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}_tmp.pdb\"  #tmp\n",
    "        \n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        # Prepare subprocess for pdb_tidy.py\n",
    "        tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}.pdb\"\n",
    "        \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.  \n",
    "        #lets remove artifacts if the user wishes\n",
    "        if self.remove_intermediates:\n",
    "            for artifacts in lst_to_merge_paths:\n",
    "                os.remove(artifacts)\n",
    "    \n",
    "    def _monomeric_rechaining(self, path_list:list,\n",
    "                          letterdict:dict,\n",
    "                          pdb_name:str):\n",
    "\n",
    "        #path_list=['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2q9p_0.pdb'], \n",
    "        #letterdict={0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K',\n",
    "        #11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21:\n",
    "        #'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}, \n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = \"default\"\n",
    "        #We only have 1 path in this list.\n",
    "        pdb = path_list[0]\n",
    "        # Open the correct PDB and rechain it.\n",
    "        dir_name = os.path.dirname(pdb)\n",
    "        structure_template = parser.get_structure(prot_name, pdb) \n",
    "        # Get the new chain ID\n",
    "        new_chain = \"A\"  #always... A\n",
    "        for model in structure_template:\n",
    "            for original_chain in model:\n",
    "                if original_chain.id != new_chain:\n",
    "                    original_chain.id = new_chain\n",
    "        save_path = os.path.join(dir_name, f\"{pdb_name}_{new_chain}.pdb\")\n",
    "        #print(f\"This is save path: {save_path=}\")\n",
    "        # Save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure_template)\n",
    "        #print(\"we save now:\")\n",
    "        io.save(save_path)\n",
    "        if self.remove_intermediates:\n",
    "            #print(\"we remove\", pdb)\n",
    "            os.remove(pdb)\n",
    "    \n",
    "    def create_domain_boundaries(self, save=True):\n",
    "        '''\n",
    "        Process PDB data by getting real ranges and then splitting domains.\n",
    "        This is the single entry point for users to execute the workflow.\n",
    "        '''\n",
    "        \n",
    "        # Step 1: Get Real Ranges\n",
    "        self.range_dict = self._get_real_ranges()\n",
    "\n",
    "        # Step 2: Split Domains\n",
    "        # Ensure that range_dict is not None or handle the case if it is\n",
    "        if self.range_dict is not None:\n",
    "            self.established_domain_dict = self._split_domains_pdb(self.range_dict)\n",
    "            #logging\n",
    "            if logging:\n",
    "                outp = os.path.join(self.log_dir, \"established_domains.json\")\n",
    "                converted_dict = self._convert_keys_to_string(self.established_domain_dict)\n",
    "                self._logging(outp, converted_dict)\n",
    "            \n",
    "        else:\n",
    "            print(\"Range dictionary is empty. Cannot proceed with splitting domains.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _get_real_ranges(self):\n",
    "        '''\n",
    "        Here we try to find out if the protein of interest is a multi-domain or single domain protein.\n",
    "        If there are many structures that consist of multiple different ranges that might or might not overlap,\n",
    "        we will end up with N domains corresponding to K ranges where K can be larger than N (if the domain itself has many different ranges)\n",
    "        '''\n",
    "        real_range_dict = defaultdict()\n",
    "        pdbs = [os.path.join(self.work_dir, k) for k in self.oligodict]\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = list(executor.map(self._get_range_parallelized, pdbs))\n",
    "        # Process the results\n",
    "        for hit in results:\n",
    "            for pdb_path, chains, min_num, max_num in hit:\n",
    "                real_range_dict[pdb_path] = (min_num, max_num)\n",
    "\n",
    "        return real_range_dict\n",
    "    \n",
    "    def _get_range_parallelized(self, pdb_path:str):\n",
    "        \"\"\"\n",
    "        This function gets called by get_real_ranges and leverages parallelization \n",
    "        to speed up domain boundary computations\n",
    "        \"\"\"\n",
    "        hits = []\n",
    "        try:\n",
    "            parser = PDBParser()\n",
    "            structure = parser.get_structure(\"none\", pdb_path)\n",
    "            chains_residues = []\n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    chain_id = chain.id\n",
    "                    # Filter the residues within the chain\n",
    "                    residues_in_chain = [res for res in chain if res.get_id()[0] == \" \"]\n",
    "                    result = [res.get_id()[1] for res in residues_in_chain]\n",
    "                    chains_residues.append((chain_id, result))\n",
    "\n",
    "            #now we have all chains and their ids\n",
    "            if len(chains_residues) == 1: #means its monomer \n",
    "                for chains, resids in chains_residues:\n",
    "                    min_num, max_num = min(resids), max(resids)\n",
    "                    #print(pdb_path, min_num, max_num)\n",
    "                    hits.append((pdb_path, chains, min_num, max_num))\n",
    "                \n",
    "            elif len(chains_residues) > 1: # oligomer.\n",
    "                #real chains is in the name of the pdb.\n",
    "                #'/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/\n",
    "                #NUDT4B/merged_cleaned_files/3i7u_ABCD.pdb'\n",
    "                chain_name = os.path.basename(pdb_path)[5:-4]  #this should be ABCD in the case above\n",
    "                min_max_list = []\n",
    "                for chains, resids in chains_residues:\n",
    "                    chains += chains\n",
    "                    min_num, max_num = min(resids), max(resids)\n",
    "                    min_max_list.append((min_num, max_num))\n",
    "\n",
    "                lowest_start = min([x[0] for x in min_max_list])\n",
    "                highest_stop = max([x[1] for x in min_max_list])\n",
    "            \n",
    "                hits.append((pdb_path, chain_name, lowest_start, highest_stop))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        return hits\n",
    "\n",
    "    def _split_domains_pdb(self, range_dict):\n",
    "        '''\n",
    "        Function to split domains based on the ranges obtained in the previous step.\n",
    "        '''\n",
    "        # lets first do a quick merge\n",
    "        merged_dict = self._merge_paths_within_interval(range_dict)\n",
    "        #print(f\"This is merged_dict_length: {len(merged_dict.values())}\")\n",
    "        #now we start iterative merging of overlaps.\n",
    "        domain_dict = self._make_groups_iter_pdb(merged_dict, num_of_iterations=0 , break_point=10)\n",
    "        #print(f\"This is domain_dict_length after make_groups_iter: {len(domain_dict.values())}\")\n",
    "        cleaned_dict = defaultdict()\n",
    "    \n",
    "        for ranges, twisted_path_lists in domain_dict.items():\n",
    "            #now we need to flatten domain_dict\n",
    "            flattened_data = self._flatten_nested_lists(twisted_path_lists)\n",
    "            #print(flattened_data)\n",
    "            # Use a set to remove duplicates\n",
    "            unique_flattened_data = set(flattened_data)\n",
    "            unique_flattened_list = list(unique_flattened_data)\n",
    "            #this works.\n",
    "            cleaned_dict[ranges] = unique_flattened_list\n",
    "\n",
    "        prot_length = 10 #len(main_prot_seq) test purpose\n",
    "\n",
    "        #lets try this first.\n",
    "        if self.main_protein_sequence: # this case we can handle by fetching the uniprot id and the associated structure.\n",
    "            if len(self.main_protein_sequence) > 300:\n",
    "                tolerance = 0.3 * prot_length\n",
    "            else:\n",
    "                tolerance = 80\n",
    "        else:\n",
    "            tolerance = 80\n",
    "            \n",
    "        #print(f\"This is cleaned_dict_length before merge_overlapping_int: {len(cleaned_dict.values())}\")\n",
    "        domain_dict = self._merge_overlapping_intervals(cleaned_dict, tolerance=tolerance)\n",
    "    \n",
    "        #print(f\"This is cleaned_dict_length after merge_overlapping_int: {len(domain_dict.values())}\")\n",
    "        return domain_dict\n",
    "\n",
    "    def _make_groups_iter_pdb(self, merged_dict:dict, num_of_iterations, break_point):\n",
    "        #print(f\"This is iteration : {num_of_iterations}\")\n",
    "        while True:\n",
    "            if num_of_iterations > break_point:\n",
    "                 #that means we cant progress.\n",
    "                 break\n",
    "            #print(f\"iteration number: {num_of_iterations}\")\n",
    "            if len(merged_dict) == 1:\n",
    "                return merged_dict\n",
    "    \n",
    "            num_of_iterations += 1\n",
    "            ranges_1 = []\n",
    "            ranges_2 = []\n",
    "    \n",
    "            #attach key ranges to both lists\n",
    "            for keys, vals in merged_dict.items():\n",
    "                ranges_1.append(keys)\n",
    "                ranges_2.append(keys)\n",
    "        \n",
    "            union_dict = defaultdict(list)        \n",
    "            \n",
    "            for range_to_check in ranges_1:  #lets parse through the ranges.\n",
    "                union_new = False  #default false for start.\n",
    "                for range_to_compare in ranges_2: #we compare our hit against all potential ranges.\n",
    "                    if range_to_check != range_to_compare:  #means they are different.\n",
    "                        \n",
    "                        range_to_check_paths = merged_dict[range_to_check]\n",
    "                        range_to_comp_paths = merged_dict[range_to_compare]\n",
    "                        \n",
    "                        range_abs_check = np.abs(int(range_to_check[1])-int(range_to_check[0]))\n",
    "                        range_abs_compare = np.abs(int(range_to_compare[1])-int(range_to_compare[0]))\n",
    "    \n",
    "                        intersect_between_both = self._get_intersect(range_to_check, range_to_compare)# gets abs length of intersect.\n",
    "                        #range abs check and range abs comp are integers. \n",
    "                        #intersect is int and corresponds to length between intersection of both.\n",
    "                        #cond 1: if the intersection between the 2 ranges is LESS than the absolute length of range to check and \n",
    "                        #the intersection is also GREATER than 80% of the first interval.\n",
    "                        condition_1 = intersect_between_both <= range_abs_check and intersect_between_both >= 0.8* range_abs_check  \n",
    "                        #cond 2: if the intersection between the 2 ranges is LESS than the absolute length of range to compare and \n",
    "                        #the intersection is also GREATER than 80% of the 2nd interval.\n",
    "                        condition_2 = intersect_between_both <= range_abs_compare and intersect_between_both >= 0.8* range_abs_compare\n",
    "                        #this means the intersect is less than the original range and the intersect is also larger than 80% of the orignal range.\n",
    "                        #as well as the same for the second range to compare.\n",
    "                        if condition_1 and condition_2:\n",
    "                            union_new = self._merge_into_union(range_to_check, range_to_compare)\n",
    "    \n",
    "                if union_new:\n",
    "                    if union_new in union_dict:\n",
    "                        union_dict[union_new].append(range_to_check_paths + range_to_comp_paths)\n",
    "                    else:\n",
    "                        union_dict[union_new] = range_to_check_paths + range_to_comp_paths\n",
    "    \n",
    "                else:\n",
    "                    union_dict[range_to_check].extend(range_to_check_paths)\n",
    "                \n",
    "            merged_dict = union_dict\n",
    "            merged_dict = self._make_groups_iter_pdb(merged_dict, num_of_iterations, break_point)\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "    def _get_intersect(self, range_to_check, range_to_compare):\n",
    "\n",
    "        # range_to_check = (start, stop)\n",
    "        # range_to_compare = (start, stop)\n",
    "        start_check = int(range_to_check[0])\n",
    "        stop_check = int(range_to_check[1])\n",
    "    \n",
    "        start_comp = int(range_to_compare[0])\n",
    "        stop_comp = int(range_to_compare[1])\n",
    "        \n",
    "        full_range_to_check = set([x for x in range(start_check, stop_check+1)])\n",
    "        full_range_to_compare = set([x for x in range(start_comp, stop_comp+1)])\n",
    "    \n",
    "        #now lets grab set for both\n",
    "        intersection = full_range_to_check.intersection(full_range_to_compare)\n",
    "        #we are interested in the length of the intersect\n",
    "        return len(intersection)\n",
    "\n",
    "    def _merge_dicts_range_seqid(self, range_dict, seqid_dict):\n",
    "        merged_dict = {}\n",
    "\n",
    "        #     #{'5ltu_AB.pdb': [('A', 0.96), ('B', 0.96)], is seqid_dict here.\n",
    "        '''\n",
    "        range_dict={(1, 151): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/7nnj_A.pdb', 'A'), \n",
    "        ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/7tn4_A.pdb', 'A'),\n",
    "        ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/3mcf_A.pdb', 'A'),\n",
    "        ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/2duk_A.pdb', 'A'),\n",
    "        ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/5ltu_AB.pdb', 'AB')]}, \n",
    "        seqid_dict={'5ltu_AB.pdb': [('A', 0.96), ('B', 0.96)], '7nnj_A.pdb': [('A', 0.956)], \n",
    "        '2duk_A.pdb': [('A', 0.967)], '3mcf_A.pdb': [('A', 0.952)], '7tn4_A.pdb': [('A', 0.787)]}\n",
    "        '''\n",
    "\n",
    "        for range_key, pdb_entries in range_dict.items():\n",
    "            merged_list = []\n",
    "            for pdb_path, chain_group in pdb_entries:\n",
    "                pdb_file = os.path.basename(pdb_path)  # Extracts '5ltu_AB.pdb' from the full path\n",
    "                # Check if the PDB file is in the seqid_dict\n",
    "                if pdb_file in seqid_dict:\n",
    "                    # Extract sequence identities for the relevant chains\n",
    "                    for chain_id, seq_id in seqid_dict[pdb_file]:\n",
    "                        if chain_id in chain_group:\n",
    "                            # Append the full path for consistency with your range_dict structure\n",
    "                            merged_list.append((pdb_path, (chain_id, seq_id)))\n",
    "                else:\n",
    "                    print(f\"Warning: No seqid found for {pdb_file}\")\n",
    "            merged_dict[range_key] = merged_list\n",
    "\n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _merge_into_union(self, range_to_check, range_to_compare):\n",
    "\n",
    "        start_check = int(range_to_check[0])\n",
    "        stop_check = int(range_to_check[1])\n",
    "    \n",
    "        start_comp = int(range_to_compare[0])\n",
    "        stop_comp = int(range_to_compare[1])\n",
    "    \n",
    "        full_range_to_check = set([x for x in range(start_check, stop_check+1)])\n",
    "        full_range_to_compare = set([x for x in range(start_comp, stop_comp+1)])\n",
    "    \n",
    "        union_merge = full_range_to_check.union(full_range_to_compare)\n",
    "        \n",
    "        sorted_union = sorted(union_merge)\n",
    "        result = (sorted_union[0], sorted_union[-1])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def _merge_overlapping_intervals(self, path_interval_dict, tolerance):\n",
    "\n",
    "        #input: key: ranges values = list of paths.\n",
    "        intervals = list(path_interval_dict.keys())\n",
    "    \n",
    "        #[(5, 500), (1, 507), (3, 507), (1, 535), (8, 500), (4, 507)]\n",
    "        #print(f\"these are the intervals we deal with in merging: {intervals}\")\n",
    "        merged_intervals = self._merge_intervals(intervals, tolerance)\n",
    "    \n",
    "        merged_dict = {}\n",
    "        #print(f\"this is path_interval_dict : {path_interval_dict}\")\n",
    "        for merged_interval in merged_intervals:\n",
    "            merged_paths = []\n",
    "                \n",
    "            for interval, path_chain_list in path_interval_dict.items():\n",
    "                if self._is_within_tolerance(merged_interval, interval, tolerance):\n",
    "                    #print(f\"{path_chain_list=}\")\n",
    "                    for path, chain in path_chain_list: # list of lists consisting of tuples a (path / chain)\n",
    "                        merged_paths.append((path, chain))\n",
    "            merged_dict[merged_interval] = merged_paths\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _merge_intervals(self, intervals, tolerance):\n",
    "\n",
    "        merged = [intervals[0]]  # Initialize with the first interval\n",
    "        for start, end in intervals[1:]:\n",
    "            merged_interval = None\n",
    "    \n",
    "            for i, (merged_start, merged_end) in enumerate(merged):\n",
    "                if abs(start - merged_start) <= tolerance and abs(end - merged_end) <= tolerance:\n",
    "                    # Merge the interval into the existing one\n",
    "                    merged_interval = (min(start, merged_start), max(end, merged_end))\n",
    "                    merged[i] = merged_interval\n",
    "                    break\n",
    "    \n",
    "            if merged_interval is None:\n",
    "                # No suitable merged interval found, create a new one\n",
    "                merged.append((start, end))\n",
    "    \n",
    "        return merged\n",
    "\n",
    "\n",
    "    def _merge_paths_within_interval(self, path_start_stop_dict):\n",
    "        \n",
    "        merged_dict = {}  # Create a new dictionary to store merged paths\n",
    "        #print(f\"{path_start_stop_dict=}\")\n",
    "        for pdb_path, (start, stop) in path_start_stop_dict.items():\n",
    "            pdb_location, chain = pdb_path, os.path.basename(pdb_path)[5:-4]  #location + chain\n",
    "            #print(f\"{pdb_location=}, {chain=}\")\n",
    "            interval = (start, stop)\n",
    "            \n",
    "            if interval in merged_dict:\n",
    "                merged_dict[interval].append((pdb_path, chain))\n",
    "            else:\n",
    "                merged_dict[interval] = [(pdb_path, chain)]\n",
    "            \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _flatten_nested_lists(self, lst):\n",
    "        flattened = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(self._flatten_nested_lists(item))  #recursive call\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "\n",
    "    def _is_within_tolerance(self, interval1, interval2, tolerance):\n",
    "    \n",
    "        # Calculate the differences in starts and stops for both intervals\n",
    "        start_diff = abs(interval1[0] - interval2[0])\n",
    "        stop_diff = abs(interval1[1] - interval2[1])\n",
    "        # Check if both differences are within the tolerance\n",
    "        return start_diff <= tolerance and stop_diff <= tolerance\n",
    "\n",
    "\n",
    "    def _save_human_readable(self, save=False):\n",
    "        #helper function to save the result in human readable format.\n",
    "        with open(os.path.join(self.log_dir, \"domain_boundaries.txt\"), \"w\") as db_out:\n",
    "            for keys, vals in self.established_domain_dict.items():\n",
    "                db_out.write(str(keys))\n",
    "                db_out.write(\":\")\n",
    "                db_out.write(\"\\n\")\n",
    "                for pdb in vals:\n",
    "                    pdb_name = os.path.basename(pdb[0])[:4]\n",
    "                    pdb_chain = pdb[1]\n",
    "                    db_out.write(pdb_name)\n",
    "                    db_out.write(\"_\")\n",
    "                    db_out.write(pdb_chain)\n",
    "                    db_out.write(\"\\n\")\n",
    "\n",
    "\n",
    "    #{'5ltu_AB.pdb': [('A', 0.96), ('B', 0.96)], is oligodict\n",
    "\n",
    "    def get_oligostates(self, num_most_common_oligostates=2):\n",
    "        # First, we split into groups based on boundaries\n",
    "        domains_grouped_dict = self.established_domain_dict\n",
    "        # lets merge seq id and paths so we can handle them later\n",
    "        domains_merge_dict = self._merge_dicts_range_seqid(domains_grouped_dict, self.oligodict)\n",
    "\n",
    "        grouped_dict = self._split_and_group_by_chain(domains_merge_dict)\n",
    "\n",
    "        # Initialize a dictionary to store the Counters for each range\n",
    "        most_common_oligostates = {}\n",
    "\n",
    "        # Initialize a dictionary to store the oligostate paths for each range\n",
    "        oligostate_paths = {}\n",
    "\n",
    "        for ranges, vals in grouped_dict.items():\n",
    "            oligostate_counter = Counter()\n",
    "\n",
    "            # Count occurrences for each oligostate\n",
    "            for oligostate, entries in vals.items():\n",
    "                oligostate_counter[oligostate] += len(entries)\n",
    "\n",
    "            # Get the top oligostates up to the number specified by num_most_common_oligostates\n",
    "            top_oligostates = oligostate_counter.most_common(num_most_common_oligostates)\n",
    "\n",
    "            # Store the top oligostates and their counts\n",
    "            most_common_oligostates[ranges] = top_oligostates\n",
    "\n",
    "            # Store all paths associated with the top oligostates\n",
    "            oligostate_paths[ranges] = [entry for oligostate, count in top_oligostates for entry in vals[oligostate]]\n",
    "\n",
    "        self.most_common_filtered_oligostates = most_common_oligostates\n",
    "        self.oligostates_filtered_paths = oligostate_paths\n",
    "\n",
    "        # Display the major oligostates and their entries\n",
    "        if self.logging:\n",
    "            outp = os.path.join(self.log_dir, \"filtered_oligostates.json\")\n",
    "            converted_dict =self._convert_keys_to_string(self.most_common_filtered_oligostates)\n",
    "            self._logging(outp, dict_to_write=converted_dict)\n",
    "\n",
    "        # Optionally, return the major oligostates and their entries if needed for further processing\n",
    "        #return most_common_oligostates, oligostate_paths\n",
    "\n",
    "        #lets fetch now for suitable templates for each of these states.\n",
    "        self._process_templates()\n",
    "\n",
    "    \n",
    "    def _process_templates(self):\n",
    "        #first we split into groups based on boundaries\n",
    "        if self.oligostates_filtered_paths:\n",
    "            domains_grouped_dict = self.oligostates_filtered_paths\n",
    "        else:\n",
    "            print(\"we have no self.oligostates_filtered_paths\")\n",
    "            return\n",
    "\n",
    "        sorted_oligos_by_range = defaultdict()\n",
    "        \n",
    "        self.top_templates = defaultdict()\n",
    "        \n",
    "        for keys, vals in self.oligostates_filtered_paths.items():\n",
    "            oligostate_dict = defaultdict(list)\n",
    "\n",
    "            for (path, seqid) in vals:\n",
    "                oligostate = len(os.path.basename(path)[5:-4])  # Extract oligostate\n",
    "                oligostate_dict[oligostate].append((path, seqid))\n",
    "\n",
    "            for oligostate in oligostate_dict:\n",
    "                # Sort the list of tuples for each oligostate by seq_id\n",
    "                oligostate_dict[oligostate] = sorted(oligostate_dict[oligostate], key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "            sorted_oligostate_dict = OrderedDict(sorted(oligostate_dict.items()))\n",
    "\n",
    "            formatted_key = \"-\".join([str(keys[0]), str(keys[1])])\n",
    "            sorted_oligos_by_range[formatted_key] = sorted_oligostate_dict\n",
    "            self.assemblied_structures = sorted_oligos_by_range\n",
    "\n",
    "            return\n",
    "            \n",
    "            # Store the first hit of each oligostate for each range\n",
    "            for oligostate, templates in sorted_oligostate_dict.items():\n",
    "                # Initialize a dictionary for 'keys' if it does not exist\n",
    "                if keys not in self.top_templates:\n",
    "                    self.top_templates[keys] = {}\n",
    "                    self.top_templates[keys][oligostate] = templates[0]  # Take the first template\n",
    "                    \n",
    "        if self.logging:\n",
    "            outp = os.path.join(self.log_dir, \"templates_oligos.json\")\n",
    "            converted_dict = self._convert_keys_to_string(self.templates_for_oligos)\n",
    "            self._logging(outp, converted_dict)\n",
    "\n",
    "    def _split_and_group_by_chain(self, input_dict):\n",
    "        grouped_dict = {}\n",
    "        for interval, pdb_list in input_dict.items():\n",
    "            # Initialize a sub-dictionary for each interval\n",
    "            grouped_dict[interval] = {}\n",
    "\n",
    "            for pdb_path, seq_id in pdb_list:\n",
    "                chain_length = len(os.path.basename(pdb_path)[5:-4])\n",
    "                \n",
    "                if chain_length not in grouped_dict[interval]:\n",
    "                    grouped_dict[interval][chain_length] = []\n",
    "\n",
    "                grouped_dict[interval][chain_length].append((pdb_path, seq_id))\n",
    "        return grouped_dict\n",
    "\n",
    "    \n",
    "    def _logging(self, outp, dict_to_write):\n",
    "        '''Logger to keep track of changes and get infos about potential debugging'''\n",
    "        with open(outp, \"w\") as json_fh:\n",
    "            json.dump(dict_to_write, json_fh, indent=4, default=str)\n",
    "  \n",
    "    def _convert_keys_to_string(self, dictionary):\n",
    "        \"\"\"Converts all the keys of the given dictionary to strings.\"\"\"\n",
    "        converted_dict = {}\n",
    "        for key, value in dictionary.items():\n",
    "            # Convert the key to a string, you might want to customize this formatting\n",
    "            str_key = '_'.join(map(str, key)) if isinstance(key, tuple) else str(key)\n",
    "            converted_dict[str_key] = value\n",
    "        return converted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd96d68-c29d-4ae2-9206-df2e3f8f2787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is overlap in the directory: {'2duk', '7nnj', '7tn4', '3mcf', '5ltu'}\n",
      "we already have pdbs from the templates downloaded\n"
     ]
    }
   ],
   "source": [
    "templates = ['5ltu_A', '5ltu_B', '7nnj_A', '7nnj_B', '2duk_A', '2duk_B', '3mcf_A', '3mcf_B', '7tn4_A', '2q9p_A', '2fvv_A', '6pck_A', '6pcl_A', '6wo7_A', '6wo8_A', '6wo9_A', '6woa_A', '6wob_A', '6woc_A', '6wod_A', '6woe_A', '6wof_A', '6wog_A', '6woh_A', '6woi_A', '7aut_A', '7aui_A', '7auk_A', '7aul_A', '7aum_A', '7aun_A', '7auo_A', '7aup_A', '7auq_A', '7aur_A', '7aus_A', '7auu_A', '7auj_A', '3h95_A', '3i7u_A', '3i7u_B', '3i7u_C', '3i7u_D', '3i7v_A', '3i7v_B', '4hfq_A', '4hfq_B']\n",
    "seq_id = [0.96, 0.96, 0.956, 0.956, 0.967, 0.967, 0.952, 0.952, 0.787, 0.787, 0.787, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.4, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.419, 0.388, 0.295, 0.438, 0.438, 0.438, 0.438, 0.438, 0.438, 0.355, 0.355]\n",
    "query_start = [1, 1, 7, 7, 9, 9, 18, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 29, 30, 30, 30, 30, 30, 30, 31, 31]\n",
    "query_end = [181, 181, 147, 147, 147, 147, 145, 145, 157, 157, 157, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 85, 91, 128, 82, 82, 82, 82, 82, 82, 97, 97]\n",
    "temp_start = [1, 1, 1, 1, 1, 1, 2, 2, 1, 23, 23, 1, 1, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 36, 13, 13, 13, 13, 13, 13, 78, 78]\n",
    "temp_end = [180, 180, 140, 140, 138, 138, 129, 129, 156, 178, 178, 148, 148, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 102, 108, 139, 63, 63, 63, 63, 63, 63, 148, 148]\n",
    "work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline\"\n",
    "script_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\"\n",
    "# in the near future: supply here with a uniprot id! and then fetch from there the conservation\n",
    "\n",
    "#DownloadPipe\n",
    "Downloader = DownloadPipe(templates=templates[0:9], work_dir=work_dir, seq_id=seq_id[0:9], script_dir=script_dir)\n",
    "Downloader.paralellized_download()\n",
    "Downloader.retrieve_meta() \n",
    "Downloader.setup_cutoff(cutoff=6, apply_filter=True)\n",
    "\n",
    "struct = Downloader.filtered_structures\n",
    "\n",
    "#PDB_builder block\n",
    "PDB_Builder = PDBBuilder(work_dir=work_dir, structures=struct, remove_intermediates=True) #structures that are filtered\n",
    "PDB_Builder.build_assembly()\n",
    "PDB_Builder.create_domain_boundaries()\n",
    "PDB_Builder.get_oligostates(num_most_common_oligostates=2)\n",
    "assemblied_structures = PDB_Builder.assemblied_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdd1f46-a9c7-4798-a790-1c28b216fa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'1-151': OrderedDict([(1,\n",
       "                           [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/2duk_A.pdb',\n",
       "                             ('A', 0.967)),\n",
       "                            ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/7nnj_A.pdb',\n",
       "                             ('A', 0.956)),\n",
       "                            ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/3mcf_A.pdb',\n",
       "                             ('A', 0.952)),\n",
       "                            ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/7tn4_A.pdb',\n",
       "                             ('A', 0.787))]),\n",
       "                          (2,\n",
       "                           [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/5ltu_AB.pdb',\n",
       "                             ('B', 0.96)),\n",
       "                            ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/5ltu_AB.pdb',\n",
       "                             ('A', 0.96))])])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblied_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79806070-db56-4620-97be-9006d5686910",
   "metadata": {},
   "source": [
    "# CONTINUE HERE. \n",
    "Next step will be USAligner overwork and integration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9ab30756-8286-4eff-b69f-68411c6b169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file saved as '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/log_files/msa_seq_dict.json'.\n"
     ]
    }
   ],
   "source": [
    "USAligner = USAlign(work_dir=work_dir, script_dir=os.path.join(work_dir, \"scripts\"), structure_dict=assemblied_structures, \n",
    "                   cluster_min_identity=0.4,\n",
    "                   num_strucs_per_cluster=10)\n",
    "\n",
    "USAligner.USAlign_run()\n",
    "USAligner.filter_results(tm_cutoff=0.7,rmsd_min_cutoff=0, rmsd_max_cutoff=10, log_file=True)\n",
    "USAligner.setup_oligo_directories()\n",
    "#USAligner.oligo_split_dictv\n",
    "#result_dict = USAligner.result_dict\n",
    "USAligner.multisequence_alignment(run_all_potential_oligos=True, log_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27216d9a-79c0-4299-b1cc-53079c9aabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys, vals in USAligner.msa_seqs.items():\n",
    "    print(keys)\n",
    "    for pdb_code, seq in vals.items():\n",
    "        print(pdb_code, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c54a8682-7d4c-4d3d-b766-a332247fb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USAlign:\n",
    "    '''Explain class USAlign'''\n",
    "    def __init__(self, work_dir,\n",
    "                 structure_dict,\n",
    "                 script_dir=None,\n",
    "                 top_templates=None,\n",
    "                 cluster_min_identity=None, \n",
    "                 num_strucs_per_cluster=None,\n",
    "                 logging=True):\n",
    "\n",
    "        #current work dir\n",
    "        self.work_dir = work_dir\n",
    "        #location of all scripts required for USAlign\n",
    "        self.script_dir = script_dir\n",
    "        #template dict: key: range   value: dict : key: oligostate, value: tuple (list of pdbs, tuple(Chain, Seq_ID))\n",
    "        self.structure_dict = structure_dict\n",
    "        #this parameter sets a cutoff for structures to be included in the final ensemble.\n",
    "        self.cluster_min_identity = cluster_min_identity\n",
    "        #additional control to select how oligoclusters to be included for each range.\n",
    "        self.num_strucs_per_cluster = num_strucs_per_cluster\n",
    "        #\n",
    "        self.logging = logging\n",
    "        self.log_dir = os.path.join(work_dir, \"log_files\")\n",
    "        #input normalized \n",
    "        self.oligo_split_dict = None\n",
    "        self.normalized_dict = None\n",
    "        self.filtered_cleaned_dict = None\n",
    "        self.filtered_dict_after_usalign = None\n",
    "        self.msa_seqs = None\n",
    "        self.list_of_sizes_per_range_and_cluster_cleaned = None\n",
    "        self.filtered_identity_clusters = None\n",
    "        self.filtered_injector_dict = None\n",
    "        self.list_of_sizes_per_cluster = None\n",
    "        self.results_usalign = None\n",
    "        self.result_dict = None\n",
    "        self.multiseq_alignment = None\n",
    "        self.pca_ready_ensemble = None\n",
    "    \n",
    "    def USAlign_run(self):\n",
    "        '''Function call that runs USAlign.\n",
    "        - First we convert our input into the correct input format.\n",
    "        - If specified, we put constraints on the retrieved ensembles. \n",
    "        (minimum_template_identity:0.9, top_oligo_clusters=3)\n",
    "        Means we only accept clusters that have at least a 0.9 or higher Seq identity in their cluster and top oligo_clusters=3 means\n",
    "        the top 3 oligostates only.'''\n",
    "        \n",
    "        #this function converts the input to a dict with a value list.\n",
    "        #self._convert_input_to_value_list()\n",
    "\n",
    "        #If specified we further filter the subclusters that we find for each oligomeric state based on sequence identity.\n",
    "        \n",
    "        self.normalized_dict = self._restructure_input()\n",
    "\n",
    "        if self.cluster_min_identity:\n",
    "            #function to filter subclusters based on the passed threshold.\n",
    "            self.filtered_identity_clusters = self._filter_subclusters()\n",
    "        else:\n",
    "            self.filtered_identity_clusters = self.normalized_dict\n",
    "\n",
    "        #lets run USAlign\n",
    "        self.filtered_cleaned_dict = self._prep_clusters_USAlign()\n",
    "\n",
    "        #first dict is containing a dict: outer_key: range, inner key: oligostate val: path, tm_score, rmsd after alignment.\n",
    "        #second dict templates is a dict: key: outer_key: range, inner key: oligostate, val: path\n",
    "        self.results_usalign = self._parallelized_execution_USAlign()\n",
    "\n",
    "    \n",
    "    def _restructure_input(self):\n",
    "\n",
    "        \"\"\"Input needs to be normalized\"\"\"\n",
    "\n",
    "        oligomer_mapping = {\n",
    "            \"1\": \"monomer\", \"2\": \"dimer\", \"3\": \"trimer\", \"4\": \"tetramer\",\n",
    "            \"5\": \"pentamer\", \"6\": \"hexamer\", \"7\": \"heptamer\", \"8\": \"oktamer\",\n",
    "            \"9\": \"nonamer\", \"10\": \"decamer\", \"11\": \"undecamer\", \"12\": \"dodecamer\",\n",
    "            \"13\": \"tridecamer\", \"14\": \"tetradecamer\", \"15\": \"pentadecamer\",\n",
    "            \"16\": \"hexadecamer\", \"17\": \"heptadecamer\", \"18\": \"oktadecamer\",\n",
    "            \"19\": \"nonadecamer\", \"20\": \"eicosamer\", \"21\": \"eicosameundamer\",\n",
    "            \"22\": \"eicosadodamer\", \"23\": \"eicosatrimer\", \"24\": \"eicosatetramer\"\n",
    "        }\n",
    "\n",
    "        filtered_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        for range_key, oligostate_dict in self.structure_dict.items():\n",
    "            for oligostate_key, structures in oligostate_dict.items():\n",
    "                # Convert oligostate_key to string and fetch oligomer name, defaulting to \"X-mer\" if not found\n",
    "                oligomer_name = oligomer_mapping.get(str(oligostate_key), \"X-mer\")\n",
    "                for path, (chain, seq_ID) in structures:\n",
    "                    filtered_dict[range_key][oligomer_name].append((path, chain, seq_ID))\n",
    "\n",
    "        filtered_dict = self._flatten_dict(filtered_dict)\n",
    "        # this filtered_dict flattened structure has keys: start-stop-oligomer values: list of tuples (path, chain, seqid)\n",
    "        return filtered_dict\n",
    "\n",
    "\n",
    "    def _flatten_dict(self, nested_dict):\n",
    "        flat_dict = {}\n",
    "        for outer_key, inner_dict in nested_dict.items():\n",
    "            for inner_key, value_list in inner_dict.items():\n",
    "                combined_key = f\"{outer_key}-{inner_key}\"\n",
    "                flat_dict[combined_key] = value_list\n",
    "\n",
    "        return flat_dict\n",
    "\n",
    "    \n",
    "    def _parallelized_execution_USAlign(self):\n",
    "        \n",
    "        usalign_results = defaultdict(list)\n",
    "\n",
    "        #defaultdict(<class 'list'>, {'1-151-monomer': [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/2duk_A.pdb', 'A', 0.967)\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            for range_oligo, path_chain_seqid_lst in self.filtered_cleaned_dict.items():\n",
    "                range = range_oligo.split(\"-\")[:2]\n",
    "                oligo_state = range_oligo.split(\"-\")[2]\n",
    "                #print(f\"{range=}, {oligo_state=}\")\n",
    "                if len(path_chain_seqid_lst) > 1:\n",
    "                        template = path_chain_seqid_lst[0][0]\n",
    "                        queries = [item[0] for item in path_chain_seqid_lst[1:]] \n",
    "                        # Initialize the futures list for this oligo_state\n",
    "                        futures = []\n",
    "                        if oligo_state == 1:  # Monomeric\n",
    "                            for query in queries:\n",
    "                                future = executor.submit(self._run_usalign_monomer, template, query)\n",
    "                                futures.append(future)\n",
    "                        else:  # Oligomeric\n",
    "                            for query in queries:\n",
    "                                future = executor.submit(self._run_usalign_oligomer, template, query)\n",
    "                                futures.append(future)\n",
    "\n",
    "                    \n",
    "                        # Collecting results for each oligo_state\n",
    "                        oligo_state_results = []\n",
    "                        for future in as_completed(futures):\n",
    "                            try:\n",
    "                                result = future.result()\n",
    "                                if result:\n",
    "                                    oligo_state_results.append(result)\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                continue\n",
    "                                \n",
    "                        # Store results in the dictionary\n",
    "                        usalign_results[range_oligo] = (template, oligo_state_results)\n",
    "                        #print(f\"{range_oligo}: {usalign_results[range_oligo]=}\")\n",
    "\n",
    "        # Print or process range_results as needed\n",
    "        return usalign_results\n",
    "\n",
    "    \n",
    "    def _filter_subclusters(self):\n",
    "         \n",
    "        #print(self.cleaned_input)\n",
    "        filtered_dict = defaultdict(list)\n",
    "        # {'1-151-monomer': [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/2duk_A.pdb', 'A', 0.967)\n",
    "        for range_key_oligomer, path_chain_seqid_lst in self.normalized_dict.items():\n",
    "            if path_chain_seqid_lst:\n",
    "                for path, chain, seq_id in path_chain_seqid_lst: \n",
    "                        if seq_id > self.cluster_min_identity:\n",
    "                            filtered_dict[range_key_oligomer].append((path, chain, seq_id))\n",
    "                    \n",
    "        # Removing any range keys that no longer have any oligostates after filtering\n",
    "        filtered_dict = {k: v for k, v in filtered_dict.items() if v}\n",
    "        return filtered_dict\n",
    "\n",
    "    def _run_for_all_oligos(self):\n",
    "\n",
    "        dir_path_pdb_codes = defaultdict(list)\n",
    "\n",
    "        for dir_path, pdb_template_pdb_query_tuple in self.oligo_split_dict.items():\n",
    "            if pdb_template_pdb_query_tuple:\n",
    "                # Extracts all query PDB codes using list comprehension\n",
    "                queries = [os.path.basename(f)[:-4] for f in pdb_template_pdb_query_tuple[1]]\n",
    "\n",
    "                # Extract the PDB code from the template\n",
    "                template_code = os.path.basename(pdb_template_pdb_query_tuple[0])[:-4]\n",
    "\n",
    "                # Combine the template code with the query codes\n",
    "                pdb_codes = queries + [template_code]  # Append the template code to the list of query codes\n",
    "\n",
    "                # Store the combined list in the dictionary keyed by directory path\n",
    "                dir_path_pdb_codes[dir_path] = pdb_codes\n",
    "\n",
    "        # Print the resulting dictionary after the loop\n",
    "        return dir_path_pdb_codes\n",
    "    \n",
    "\n",
    "    def multisequence_alignment(self, directory=None, run_all_potential_oligos=False, log_file=True):\n",
    "\n",
    "        dir_path_pdb_codes = self._run_multiseq_alignment(directory=directory, run_all_potential_oligos=run_all_potential_oligos)\n",
    "\n",
    "        #print(f\"{dir_path_pdb_codes}\")\n",
    "        #lets check if not None.\n",
    "        if dir_path_pdb_codes and run_all_potential_oligos:\n",
    "            fasta_paths, fasta_cleared_paths = self._execute_msa_all_dirs(dir_path_pdb_codes)\n",
    "        else:\n",
    "            #implement for later use if someone wants only 1 directory.\n",
    "            #self._execute_msa_single_dir(dir_path_pdb_codes)\n",
    "            pass\n",
    "\n",
    "        sequences = self._read_msa_output_converter(fasta_paths)\n",
    "\n",
    "        #here we store all MSA for all oligos\n",
    "        merged_dict_all_oligos_msa = defaultdict(dict)\n",
    "        #this deals with monomeric cases\n",
    "        self._merge_both_oligo_dicts(merged_dict_all_oligos_msa, sequences)\n",
    "        #this deals with the oligomeric cases from mustang\n",
    "        self._merge_both_oligo_dicts(merged_dict_all_oligos_msa, fasta_cleared_paths)\n",
    "\n",
    "        #lets store it in msa_seqs\n",
    "        self.msa_seqs = merged_dict_all_oligos_msa\n",
    "\n",
    "        #now lets also logg the results.\n",
    "        if log_file:\n",
    "            self._log_results(merged_dict_all_oligos_msa)\n",
    "        \n",
    "\n",
    "\n",
    "    def _merge_both_oligo_dicts(self, dict1, dict2):\n",
    "        for path, pdb_dict in dict2.items():\n",
    "            if path in dict1:\n",
    "                # Update the existing dictionary with new PDB ID and sequence pairs\n",
    "                dict1[path].update(pdb_dict)\n",
    "            else:\n",
    "                # Add new directory path as a key with its PDB ID and sequence pairs\n",
    "                dict1[path] = pdb_dict\n",
    "\n",
    "    def _convert_defaultdict_to_dict(self, d):\n",
    "        \"\"\"Recursively convert a nested defaultdict to a standard dictionary.\"\"\"\n",
    "        if isinstance(d, defaultdict):\n",
    "            # Convert defaultdict to dict and recurse for nested structures\n",
    "            return {k: self._convert_defaultdict_to_dict(v) for k, v in d.items()}\n",
    "        elif isinstance(d, dict):\n",
    "            # Recurse for standard dict\n",
    "            return {k: self._convert_defaultdict_to_dict(v) for k, v in d.items()}\n",
    "        else:\n",
    "            # Return value as is (end of recursion)\n",
    "            return d \n",
    "\n",
    "    def _log_results(self, d, log_file_name=\"msa_seq_dict.json\"):\n",
    "        \n",
    "        \"\"\"Store the converted dictionary in a JSON log file.\"\"\"\n",
    "        converted_dict = self._convert_defaultdict_to_dict(d)\n",
    "        \n",
    "        # Ensure the log directory exists\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        \n",
    "        # Define the full path for the log file\n",
    "        filename = os.path.join(self.log_dir, log_file_name)\n",
    "        \n",
    "        # Write the converted dictionary to the JSON file\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(converted_dict, f, indent=4)\n",
    "        print(f\"Log file saved as '{filename}'.\")\n",
    "\n",
    "    \n",
    "    def _execute_msa_all_dirs(self, dir_path_pdb_codes):\n",
    "\n",
    "        fasta_path_dict = defaultdict()\n",
    "        fasta_clear_dict = defaultdict()\n",
    "        \n",
    "        for dir_to_check, path_lst in dir_path_pdb_codes.items():\n",
    "            \n",
    "            pdb_list = os.path.join(dir_to_check, \"pdb_list.txt\")\n",
    "\n",
    "            oligo_state = dir_to_check.split(os.sep)[-2]  # monomer, oligomer etc.\n",
    "            #dir_to_check='/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-151'\n",
    "            \n",
    "            with open(pdb_list, \"w\") as pdb_fh:\n",
    "                for pdbs in path_lst:\n",
    "                    pdb_fh.write(pdbs)\n",
    "                    pdb_fh.write(\"\\n\")\n",
    "\n",
    "            if oligo_state == \"monomer\":\n",
    "            \n",
    "                bash_tm_and_rmsd_calc = f\"{self.script_dir}/USalign -dir {dir_to_check}/ {pdb_list} -suffix .pdb -mm 4 -outfmt 1\"\n",
    "                bash_command = bash_tm_and_rmsd_calc.split()\n",
    "                #print(bash_command)\n",
    "                try:\n",
    "                    fasta_path = os.path.join(dir_to_check, \"multiseq_fasta_output.txt\")\n",
    "                    with open(fasta_path, \"w\") as fh_results:\n",
    "                        result = run(bash_command, stdout=fh_results, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "                    fasta_path_dict[dir_to_check] = fasta_path\n",
    "                    \n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "            else:\n",
    "                \"oligomer!\"\n",
    "                \"\"\"\n",
    "                HERE we should run mustang... \n",
    "                first delete all TER statements besides the last one. then run mustang.\"\"\"\n",
    "\n",
    "                #this function removes all TER statements between chains to make the room for mustang.\n",
    "                self._prep_for_mustang(dir_to_check)\n",
    "                \n",
    "                all_files = [f for f in os.listdir(dir_to_check) if os.path.isfile(os.path.join(dir_to_check, f))]\n",
    "                # Filter out files that end with '.pdb'\n",
    "                pdbs = [f for f in all_files if f.endswith(\".pdb\") and not f.startswith(\"multiseq\")]\n",
    "                path_set_lst = list(set(pdbs))\n",
    "                paths_abs = [os.path.join(dir_to_check, f) for f in path_set_lst]\n",
    "                #print(f\"{paths_abs=}\")\n",
    "\n",
    "                #works but careful. first argument supplied to -i is the reference where ALL structures will be superposed against This should ideally be the highest seq id\n",
    "                #needs to be implemented in the future maybe to specifically start with highest seq id structure as 1rst argument passed to mustang. \n",
    "                fasta_path = os.path.join(dir_to_check, \"multiseq_fasta_output\")\n",
    "                bash_mustang = f\"{self.script_dir}/mustang -i {' '.join(paths_abs)} -F fasta -o {fasta_path}\"\n",
    "                bash_command = bash_mustang.split()\n",
    "                #print(f\"{bash_command=}\")\n",
    "                try:\n",
    "                    result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "                    fpath_results = f\"{fasta_path}.afasta\"\n",
    "                    #lets read it in\n",
    "                    fasta_clear_dict[dir_to_check] = self._read_fasta_to_dict_mustang(fpath_results)\n",
    "                \n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "\n",
    "        return fasta_path_dict, fasta_clear_dict\n",
    "\n",
    "    def _prep_for_mustang(self, dir_to_check):\n",
    "        for filename in os.listdir(dir_to_check):\n",
    "            if filename.endswith(\".pdb\"):  # Check if the file is a PDB file\n",
    "                filepath = os.path.join(dir_to_check, filename)\n",
    "                with open(filepath, 'r') as file:\n",
    "                    lines = file.readlines()\n",
    "                    \n",
    "                # Remove all TER lines\n",
    "                #this seems to work!\n",
    "                new_lines = [line for line in lines if line.strip()[:3] != \"TER\"]\n",
    "                # Add TER at the end\n",
    "                new_lines.append(\"TER\\n\")\n",
    "\n",
    "                # Write the modified lines to the same file to overwrite or to a new file\n",
    "                with open(filepath, 'w') as file:  # Overwrite the same file\n",
    "                    file.writelines(new_lines)\n",
    "                # Optionally, you can write to a new file if you want to keep the original files unchanged\n",
    "\n",
    "    \n",
    "    def _read_fasta_to_dict_mustang(self, file_path):\n",
    "        sequences = {}  # Initialize an empty dictionary to store sequences\n",
    "        current_id = None  # Keep track of the current sequence ID\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()  # Remove any leading/trailing whitespace\n",
    "                if line.startswith('>'):  # Check if the line is an ID line\n",
    "                    current_id = line[1:]  # Remove the '>' and set as the current ID\n",
    "                    sequences[current_id] = ''  # Initialize an empty sequence for this ID\n",
    "                else:\n",
    "                    sequences[current_id] += line\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    \n",
    "    def _read_msa_output_converter(self, fasta_path_dict):\n",
    "        \n",
    "        sequence_dict = defaultdict()\n",
    "\n",
    "        #print(f\"{fasta_path_dict=}\")\n",
    "\n",
    "        for dir_to_fasta, fastas in fasta_path_dict.items():\n",
    "            if os.path.exists(fastas):\n",
    "                sequences = defaultdict()\n",
    "                with open(fastas, \"r\") as fh_inp:\n",
    "                    for line in fh_inp:\n",
    "                        # Ignore lines that start with \"#\" or empty lines\n",
    "                        if not line.startswith(\">\") and not line.startswith(\"#\") and line.strip():\n",
    "                            # The previous line contained the PDB ID, process it here\n",
    "                            # Ensure there's a current PDB ID to process and sequence data exists\n",
    "                            if current_pdb_id and line.strip():\n",
    "                                sequences[current_pdb_id] = line.strip()\n",
    "                        elif line.startswith(\">\"):\n",
    "                            # Extract the PDB ID, remove \":A\" part, and strip whitespace\n",
    "                            current_pdb_id = line.split()[0][1:].split(':')[0].strip()\n",
    "                        else:\n",
    "                            # Reset current PDB ID for non-sequence lines or new entries\n",
    "                            current_pdb_id = None\n",
    "            \n",
    "                #self.multiseq_alignment = sequences\n",
    "                #print(f\"{sequences=}\")\n",
    "                #self._write_fasta(sequences, outdir=directory)\n",
    "                # Print the sequences dictionary\n",
    "                #for pdb_id, sequence in sequences.items():\n",
    "                #    print(f\"{pdb_id}: {sequence}\")\n",
    "                sequence_dict[dir_to_fasta] = sequences\n",
    "            \n",
    "            else:\n",
    "                print(f\"File {fasta_path} does not exist.\")\n",
    "        \n",
    "        return sequence_dict\n",
    "    \n",
    "\n",
    "    def _run_multiseq_alignment(self, directory=None, run_all_potential_oligos=False):\n",
    "        \"\"\"This function takes as input a pdb directory and will perform structure based superposition. \n",
    "        The results will be stored in a dict.\"\"\"\n",
    "        #grab all pdbs from the directory.\n",
    "        if run_all_potential_oligos:\n",
    "            if self.oligo_split_dict:\n",
    "                dir_path_pdb_codes = self._run_for_all_oligos()\n",
    "                return dir_path_pdb_codes # Ensure that the function exits after this operation\n",
    "            else:\n",
    "                print(\"Cannot run all potential oligos without oligo_split_dict!\")\n",
    "                return  # Exit the function if oligo_split_dict is not available\n",
    "\n",
    "        # This part of the code will only be reached if run_all_potential_oligos is False\n",
    "        if directory:\n",
    "            self._run_for_single_directory(directory)  # Assuming oligomer is relevant here\n",
    "        else:\n",
    "            print(\"Specify a directory for MSA computation.\")\n",
    "        \n",
    "    def prepare_for_PCA(self, directory, msa_alignment,cutting=\"strict\"):\n",
    "        #now we leverage the seq alignment from USAlign.\n",
    "        '''\n",
    "        Cutting based on a structure based alignment.\n",
    "        Here we offer two options: strict cutting or intelligent cutting.\n",
    "        \n",
    "        - strict cutting:\n",
    "        We directly take the ensemble, check for gaps in aligned positions \n",
    "        and REMOVE the atoms of a column IF there is a gap in 1 position.\n",
    "\n",
    "        - intelligent cutting: \n",
    "        We first try to find structures in the ensemble that introduce many gaps.    \n",
    "        We proceed by removal of those structures, rerunning the alignment and then continue with normal strict cutting.\n",
    "        This can significantly improve the number of retained positions in the final PCA, but comes at the cost of dropping potentially\n",
    "        relevant structures.\n",
    "        '''\n",
    "\n",
    "        # Directory to store results\n",
    "        save_dir = os.path.join(directory, \"trimmed_strucs\")\n",
    "        \n",
    "        if cutting == \"strict\":\n",
    "            #print(\"we move into strict cutting\")\n",
    "            residues_to_keep = self._hard_trimmer(msa_alignment)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.mkdir(save_dir)\n",
    "            #this will be used to run PCA.\n",
    "            self.pca_ready_ensemble = save_dir\n",
    "            self._select_proper_residues_after_trim(residues_to_keep, save_dir)\n",
    "\n",
    "        elif cutting == \"intelligent\":\n",
    "            #these strucs need to be removed.\n",
    "            strucs_to_remove = self._soft_trimmer(msa_alignment)\n",
    "            #and then we rerun usalign.\n",
    "            #we remove the strucs that are found to introduce many gaps.\n",
    "            self._hard_trimmer(msa_alignment)\n",
    "\n",
    "    def _soft_trimmer(self, result_fasta:str):\n",
    "        '''Explain soft trimmer'''\n",
    "        gap_counter = defaultdict()\n",
    "        ali = pytrimal.Alignment.load(result_fasta)\n",
    "        trimmer = pytrimal.AutomaticTrimmer(\"gappyout\")\n",
    "        trimmed = trimmer.trim(ali)\n",
    "        for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "            structure_name = name.decode().rjust(6)\n",
    "            gap_count = seq.count(\"-\")\n",
    "            gap_counter[structure_name] = gap_count\n",
    "\n",
    "        gap_values = list(gap_counter.values())\n",
    "        mean_gap_count = statistics.mean(gap_values)\n",
    "        median_gap_count = statistics.median(gap_values)\n",
    "\n",
    "        high_gap_count_sequence_ids = [seq_id for seq_id, gap_count in gap_counter.items() if gap_count > mean_gap_count]\n",
    "        print(high_gap_count_sequence_ids)\n",
    "        return high_gap_count_sequence_ids\n",
    "    \n",
    "\n",
    "    def _hard_trimmer(self, result_fasta:str):\n",
    "        '''Explain hard timmer'''\n",
    "        keep_res_dict = defaultdict()\n",
    "        ali = pytrimal.Alignment.load(result_fasta)\n",
    "        #trimm all gaps unless this leaves 10% of the original alignment.\n",
    "        trimmer = pytrimal.ManualTrimmer(gap_threshold=1) #changed to 80% now as test.\n",
    "        trimmed = trimmer.trim(ali)\n",
    "        for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "            keep_res_dict[name.decode().rjust(6)] = seq\n",
    "\n",
    "        return keep_res_dict\n",
    "\n",
    "    def _select_proper_residues_after_trim(self, keep_residue_dict:dict, save_dir:dir):\n",
    "        '''Helper function that takes the proper residues after trimming.'''\n",
    "        for pdb, seq in keep_residue_dict.items():\n",
    "            #seq in keep_residue_dict is extracted from hard trimmer.\n",
    "            full_p = os.path.join(self.work_dir, pdb)\n",
    "            result_seq = self._grab_sequence_from_struc(full_p)\n",
    "            positions_to_keep = self._find_positions(result_seq[0], seq)\n",
    "            #print(f\"this is seq to keep: {seq}, this is full_seq from structure: {result_seq}\")\n",
    "            #print(f\"this is our savepath for cutted struc: {full_p}\")\n",
    "            self._select_trimmed_positions(path_to_pdb=full_p, list_to_keep=positions_to_keep, save_dir=save_dir)\n",
    "\n",
    "    def _select_trimmed_positions(self, path_to_pdb:str, list_to_keep:str, save_dir):\n",
    "        '''Helper function that select only specific residues based on list_to_keep.'''\n",
    "        class CAlphaTrimmedPositions(Select):\n",
    "            def __init__(self, list_to_keep_shifts, *args):\n",
    "                super().__init__(*args)\n",
    "                self.list_to_keep_shifts = list_to_keep_shifts\n",
    "            #overload accept_residue inherited from Select with this conditional return\n",
    "            def accept_atom(self, atom):\n",
    "                return 1 if atom.id == \"CA\" else 0\n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] in self.list_to_keep_shifts else 0    \n",
    "\n",
    "        #lets make a new dir where we store the trimmed strucs.\n",
    "        pdb_name = os.path.basename(path_to_pdb)\n",
    "        new_location = os.path.join(save_dir, pdb_name) #here we save\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, path_to_pdb)\n",
    "        shift = [x.get_id()[1] for x in structure.get_residues()][0] #first residue number = shift\n",
    "        list_to_keep_shifted = [x+shift-1 for x in list_to_keep] #correct for shift. lets try with shift\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(new_location, CAlphaTrimmedPositions(list_to_keep_shifts=list_to_keep_shifted))\n",
    "    \n",
    "    def _find_positions(self, full_sequence, partial_sequence):\n",
    "        i = 0  # Index for full_sequence\n",
    "        j = 0  # Index for partial_sequence\n",
    "        found_positions = []\n",
    "        while i < len(full_sequence) and j < len(partial_sequence):\n",
    "            if partial_sequence[j] == '-':  # Skip gap positions in partial_sequence\n",
    "                j += 1\n",
    "            elif partial_sequence[j] == full_sequence[i]:  # Match found\n",
    "                found_positions.append(i)\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1  # Advance in full_sequence if no match\n",
    "        found_pos = [x + 1 for x in found_positions]  # Adjust for 1-based indexing\n",
    "        return found_pos\n",
    "\n",
    "    def _grab_sequence_from_struc(self, pdb):\n",
    "        '''Helper function that fetches sequence based on the structure'''\n",
    "        lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                        ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                        ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                        ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "        \n",
    "        canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        pdb_name = os.path.basename(pdb)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, pdb)\n",
    "        struc_seq = [canonical_aas[x.get_resname()] for x in structure.get_residues()]\n",
    "        struc_seq = \"\".join(struc_seq)\n",
    "        return (struc_seq, pdb_name)\n",
    "        \n",
    "    def _write_fasta(self, sequence_dict:dict, outdir=None):\n",
    "\n",
    "        if outdir is None:\n",
    "            outdir = self.work_dir\n",
    "            \n",
    "        outfile = os.path.join(outdir, \"multiseq_fasta.fa\")\n",
    "        #Here we store our output as fastas for downstream analysis.\n",
    "        seq_records = []\n",
    "        for pdb_id, seq in sequence_dict.items():\n",
    "            seq_record = SeqRecord(Seq.Seq(seq), id=pdb_id, description=\"\")\n",
    "            seq_records.append(seq_record)\n",
    "\n",
    "        with open(outfile, \"w\") as output_handle:\n",
    "            SeqIO.write(seq_records, output_handle, \"fasta\")\n",
    "        \n",
    "    \n",
    "    def filter_results(self, tm_cutoff=0.5, rmsd_min_cutoff=0.1, rmsd_max_cutoff=20, log_file=True):\n",
    "        # Check if results are available\n",
    "        filtered_result_dict = defaultdict()\n",
    "        \n",
    "        if self.results_usalign:\n",
    "            for range_oligos, usalign_outp in self.results_usalign.items():\n",
    "                # The template is the first element\n",
    "                template = usalign_outp[0]\n",
    "                # The rest of the elements are lists of tuples (path, tm, rmsd)\n",
    "                list_of_tuples = usalign_outp[1]  # Assuming the second element is the list of tuples\n",
    "    \n",
    "                filtered_results = []\n",
    "                for path_tm_rmsd in list_of_tuples:\n",
    "                    #print(f\"Processing tuple: {path_tm_rmsd}\")\n",
    "                    # Unpack the tuple\n",
    "                    path, tm, rmsd = path_tm_rmsd\n",
    "    \n",
    "                    # Convert tm and rmsd to floats and apply filters\n",
    "                    try:\n",
    "                        tm = float(tm)\n",
    "                        rmsd = float(rmsd)\n",
    "                        if tm > tm_cutoff and rmsd_min_cutoff <= rmsd <= rmsd_max_cutoff:\n",
    "                            filtered_results.append((path, tm, rmsd))\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error converting TM or RMSD to float: {e}\")\n",
    "                        continue  # Skip to the next tuple if there's an error\n",
    "    \n",
    "                # Update the result dict if there are any filtered results\n",
    "                if filtered_results:\n",
    "                    filtered_result_dict[range_oligos] = (template, filtered_results)\n",
    "    \n",
    "            # Optionally log the filtered results\n",
    "            if log_file:\n",
    "                self._logger_us_results(filtered_result_dict)\n",
    "    \n",
    "            # Update the instance variable with the filtered results\n",
    "            self.filtered_dict_after_usalign = filtered_result_dict\n",
    "    \n",
    "        return filtered_result_dict\n",
    "\n",
    "\n",
    "\n",
    "    #helper function that either passes filtered or unfiltered results downstream\n",
    "    def setup_oligo_directories(self):\n",
    "        #setup directories from here.\n",
    "        if self.filtered_dict_after_usalign:\n",
    "            self._organize_and_shuffle(self.filtered_dict_after_usalign)\n",
    "            #self._separate_oligostates(self.filtered_dict_after_usalign)\n",
    "        else:\n",
    "            self._organize_and_shuffle(self.results_usalign)\n",
    "            #self._separate_oligostates(self.results_usalign)\n",
    "            \n",
    "    \n",
    "    def _logger_us_results(self, result_dict:dict):\n",
    "\n",
    "        regular_dict = dict(result_dict) # convert to normal dict\n",
    "        with open(os.path.join(self.log_dir, \"filtered_usalign_log.json\"), \"w\") as json_file:\n",
    "            json.dump(regular_dict, json_file, indent=4)\n",
    "        \n",
    "    def _prep_clusters_USAlign(self):\n",
    "\n",
    "        filtered_cleaned_dict = defaultdict(list)\n",
    "\n",
    "        for range_oligostate, path_chain_seqid_lst in self.filtered_identity_clusters.items():\n",
    "            # Remove duplicates and sort by seqid in descending order\n",
    "            unique_tuples = set(path_chain_seqid_lst)\n",
    "            sorted_tuples = sorted(unique_tuples, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "            # Retain only the top 'num_strucs_per_cluster' entries\n",
    "            top_sorted_tuples = sorted_tuples[:self.num_strucs_per_cluster]\n",
    "\n",
    "            # Store the size (number of structures) for the 'range_oligostate'\n",
    "            filtered_cleaned_dict[range_oligostate] = top_sorted_tuples\n",
    "    \n",
    "        # Convert the default dicts to regular dicts for the final result\n",
    "        return filtered_cleaned_dict\n",
    "\n",
    "    \n",
    "    #def _parallelized_execution_USAlign(self):\n",
    "        \n",
    "        #print(self.num_top_clusters_per_range)\n",
    "        #self._get_top_clusters_per_range()\n",
    "        # comment\n",
    "        #self._extract_files_per_cluster()\n",
    "        #now we want to send this dict into a function that does something\n",
    "        #self.results = self._run_usalign_singles() \n",
    "\n",
    "    def _run_usalign_monomer(self, template, path_struc_1):\n",
    "    \n",
    "        bash_tm_and_rmsd_calc = f\"{self.work_dir}/USalign {path_struc_1} {template} -outfmt 2 -mol prot\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        print(bash_command)\n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            tm_2, rmsd = self._get_tm_scores_and_rmsd(result.stdout)\n",
    "        \n",
    "            # TM-score=0.5, it corresponds to a P-value of 5.5 × 10−7  taken from \n",
    "            #figure 3 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2913670/ \n",
    "        \n",
    "            if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "            else:\n",
    "                print(f\"we remove {path_struc_1} because of tm: {tm_2} and rmsd: {rmsd}\")\n",
    "                #os.remove(path_struc_1)\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "    def _run_usalign_oligomer(self, template, path_struc_1):\n",
    "\n",
    "        # -outfmt 2 : tsv outfile, \n",
    "        # -mol prot : only consider protein\n",
    "        # -ter 0:align all chains from all models (recommended for aligning biological assemblies, i.e. biounits)\n",
    "        # -mm 1: alignment of two multi-chain oligomeric structures\n",
    "    \n",
    "        bash_tm_and_rmsd_calc = f\"{self.work_dir}/USalign {path_struc_1} {template} -outfmt 2 -mol prot -ter 0 -mm 1\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        #print(bash_command)\n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            tm_2, rmsd = self._get_tm_scores_and_rmsd(result.stdout)\n",
    "            if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "            else:\n",
    "                print(f\"we remove {path_struc_1} because of tm: {tm_2} and rmsd: {rmsd}\")\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def _get_tm_scores_and_rmsd(self, results:str):\n",
    "        \"\"\" helper function to retrieve tm scores and rmsd\n",
    "        We are only interested in TM_2 and RMSD.\n",
    "        IF RMSD is HIGH and TM_2 HIGH that means we have a conformer.\n",
    "        IF RMSD is LOW and TM_2 HIGH that means we have the same structure in the same conformer\n",
    "        IF RMSD is HIGH and TM_2 LOW that means the structures are not related.\"\"\"\n",
    "    \n",
    "        \"\"\"['#PDBchain1', 'PDBchain2', 'TM1', 'TM2', 'RMSD', 'ID1', 'ID2', 'IDali', 'L1', 'L2', 'Lali\\n/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/2duk_A.pdb:A', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/5ltu_A.pdb:A', \n",
    "        '0.8833', '0.9565', '0.94', '0.920', '1.000', '1.000', '138', '127', '127\\n']\"\"\"\n",
    "    \n",
    "        res_list = results.split(\"\\t\")\n",
    "        #this one is from the mobile protein\n",
    "        tm_1 = res_list[12]\n",
    "        #this one belongs to the target protein (the one we superimpose the mobile protein onto)\n",
    "        tm_2 = res_list[13]\n",
    "        #rmsd used to judge cutoff for trashing structures.\n",
    "        rmsd = res_list[14]\n",
    "        #we return tm_2 and rmsd\n",
    "        return ((tm_2, rmsd))\n",
    "    \n",
    "\n",
    "    def _extract_files_per_cluster(self):\n",
    "        \n",
    "        files_per_cluster = {}\n",
    "        for range_key, clusters in self.list_of_sizes_per_cluster.items():\n",
    "            # Check if the range key exists in the filtered_injector_dict\n",
    "            if range_key in self.filtered_injector_dict:\n",
    "                files_per_cluster[range_key] = {}\n",
    "                for cluster_id in clusters.keys():\n",
    "                    # Fetch the file paths for each cluster id\n",
    "                    files_per_cluster[range_key][cluster_id] = self.filtered_injector_dict[range_key][cluster_id]\n",
    "\n",
    "        self.filtered_injector_dict = files_per_cluster\n",
    "\n",
    "    def _get_top_clusters_per_range(self):\n",
    "    \n",
    "        top_clusters = {}\n",
    "        for range_key, clusters in self.list_of_sizes_per_cluster.items():\n",
    "            # Sort clusters based on size, and in case of a tie, use larger key\n",
    "            sorted_clusters = sorted(clusters.items(), key=lambda x: (-x[1], -x[0]))\n",
    "            # Take the top n clusters, where n is specified by num_top_clusters_per_range\n",
    "            top_n_clusters = dict(sorted_clusters[:self.num_top_clusters_per_range])\n",
    "            top_clusters[range_key] = top_n_clusters\n",
    "\n",
    "        self.list_of_sizes_per_cluster = top_clusters\n",
    "        \n",
    "\n",
    "    def _organize_and_shuffle(self, dict_to_process):\n",
    "    \n",
    "        dir_info_dict = defaultdict(tuple)\n",
    "\n",
    "        for range_oligostate, (template_path, pdb_details) in dict_to_process.items():\n",
    "            # Extract range and oligostate from the key\n",
    "            range_key, oligostate = range_oligostate.rsplit('-', 1)\n",
    "    \n",
    "            # Define paths for oligostate and range directories\n",
    "            oligo_dir = os.path.join(work_dir, oligostate)\n",
    "            range_dir = os.path.join(oligo_dir, range_key)\n",
    "    \n",
    "            # Create the directories if they don't exist\n",
    "            os.makedirs(range_dir, exist_ok=True)\n",
    "    \n",
    "            # Copy the template PDB into the range directory\n",
    "            shutil.copy(template_path, range_dir)\n",
    "            template_dst_path = os.path.join(range_dir, os.path.basename(template_path))\n",
    "    \n",
    "            # Initialize a list to store the paths of other structures\n",
    "            other_structures = []\n",
    "    \n",
    "            # Copy other PDBs associated with the current range_oligostate into its directory\n",
    "            for pdb_path, tm, rmsd in pdb_details:\n",
    "                shutil.copy(pdb_path, range_dir)\n",
    "                pdb_dst_path = os.path.join(range_dir, os.path.basename(pdb_path))\n",
    "                other_structures.append(pdb_dst_path)\n",
    "    \n",
    "            # Store the template and list of other structures in the dictionary\n",
    "            dir_info_dict[range_dir] = (template_dst_path, other_structures)\n",
    "    \n",
    "        # At this point, dir_info_dict contains the required information\n",
    "        # and can be used as needed\n",
    "        self.oligo_split_dict = dict(dir_info_dict)  # Convert defaultdict to dict if necessary\n",
    "\n",
    "\n",
    "    # At this point, all required directories are created and PDB files are organized accordingly\n",
    "    def _separate_oligostates(self, dict_to_process):\n",
    "\n",
    "        #lets separate our found results and make new directories.   \n",
    "        oligodirdict = defaultdict()\n",
    "        \n",
    "        oligos = defaultdict(dict)\n",
    "\n",
    "        print(f\"{dict_to_process=}\")\n",
    "        #for each range we go through all potential oligos.\n",
    "        for ranges, dicts in dict_to_process.items():\n",
    "            for key, value in dicts.items():  #dicts = dict with : key = oligostate, value = paths \n",
    "                oligos[ranges][key] = value\n",
    "                    \n",
    "        self.oligomers = oligos\n",
    "        #now lets make the dirs.\n",
    "\n",
    "        self._shuffle(oligos)\n",
    "\n",
    "\n",
    "    def _shuffle(self, dict_to_shuffle):\n",
    "        # Initialize the new dictionary structure for oligostates, ranges, and PDB paths\n",
    "        new_dict = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        for range, oligo_data in dict_to_shuffle.items():\n",
    "            for oligo, path_tm_rmsd in oligo_data.items():\n",
    "                # Create the main directory for each \"oligostate\" if it doesn't exist\n",
    "                oligo_path = os.path.join(work_dir, oligo)\n",
    "                if not os.path.exists(oligo_path):\n",
    "                    os.mkdir(oligo_path)\n",
    "        \n",
    "                # Create a subdirectory for each \"range\" within the \"oligostate\" directory\n",
    "                range_path = os.path.join(oligo_path, range)\n",
    "                if not os.path.exists(range_path):\n",
    "                    os.mkdir(range_path)\n",
    "            \n",
    "                # Copy files associated with the current range into its directory\n",
    "                for (path, tm, rsmd) in path_tm_rmsd:\n",
    "                    shutil.copy(path, range_path)  # Copy file to the new location\n",
    "                    \n",
    "                    # Append only the PDB paths to the list associated with each range within each oligostate\n",
    "                    new_dict[oligo][range].append((path, tm, rsmd))\n",
    "        \n",
    "        # Convert the defaultdict to a regular dict for the final output, if preferred\n",
    "        final_dict = {oligo: dict(ranges) for oligo, ranges in new_dict.items()}\n",
    "        \n",
    "        # Now 'final_dict' contains the structure you want, and can be used as needed\n",
    "        self.result_dict = final_dict\n",
    "        #return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86e94a98-d25d-4f69-9677-92d71816c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_to_check = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/dimer/1-151\"\n",
    "\n",
    "#USAligner = USAlign(work_dir=work_dir, script_dir=os.path.join(work_dir, \"scripts\"), structure_dict=assemblied_structures, \n",
    "#                   cluster_min_identity=0.4,\n",
    "#                   num_strucs_per_cluster=10)\n",
    "\n",
    "\n",
    "#USAligner._prep_for_mustang(dir_to_check=dir_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e088c2-e53e-441e-853e-4a7e271a07e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7dca8664-fb74-4634-ac35-ab8017695ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModellerRepairEnsemble:\n",
    "    '''\n",
    "    Modeller Class that can be used to repair structures if needed.\n",
    "    Returned structures should be compatible with format for downstream analysis.\n",
    "    '''\n",
    "    def __init__(self, work_dir, ensemble_dict):\n",
    "\n",
    "        self.repair_ensemble = ensemble_dict\n",
    "        self.work_dir = work_dir\n",
    "        self.monomers = None\n",
    "        self.oligomers = None\n",
    "        self.monomeric_templates= None\n",
    "        self.oligomeric_templates = None\n",
    "        self.usalign_exe = os.path.join(work_dir, \"USalign\")\n",
    "        self.repairable_structures = None\n",
    "        self.repaired_monomers = None\n",
    "        self.repaired_oligomers = None\n",
    "\n",
    "    \n",
    "    def _separate_oligostates(self):\n",
    "\n",
    "        lst = [(\"1\",'monomer'),(\"2\", 'dimer'),(\"3\", 'trimer'),(\"4\", 'tetramer'),(\"5\", 'pentamer'),\n",
    "           (\"6\", 'hexamer'),(\"7\", 'heptamer'),(\"8\", 'oktamer'),(\"9\", 'nonamer'),(\"10\", 'decamer'),\n",
    "           (\"11\", 'undecamer'),(\"12\", 'dodecamer'),(\"13\", 'tridecamer'),\n",
    "           (\"14\", 'tetradecamer'),(\"15\", 'pentadecamer'),(\"16\", 'hexadecamer'),\n",
    "           (\"17\", 'heptadecamer'),(\"18\", 'oktadecamer'),(\"19\", 'nonadecamer'),(\"20\", 'eicosamer'),\n",
    "           (\"21\", 'eicosameundamer'),(\"22\", 'eicosadodamer'),(\"23\", 'eicosatrimer'),(\"24\", 'eicosatetramer')]\n",
    "\n",
    "        oligodirdict = defaultdict(lambda: \"X-mer\", lst)\n",
    "\n",
    "        \n",
    "        oligos = defaultdict(dict)\n",
    "        monos = defaultdict(dict)\n",
    "        for ranges, dicts in self.repair_ensemble.items():\n",
    "            for key, value in dicts.items():\n",
    "                if key == 1:\n",
    "                    monos[ranges][oligodirdict[str(key)]] = value\n",
    "                else:\n",
    "                    oligos[ranges][oligodirdict[str(key)]] = value\n",
    "\n",
    "        self.monomers, self.oligomers = monos, oligos\n",
    "\n",
    "    def _shuffle(self, dict_to_shuffle):\n",
    "\n",
    "\n",
    "        lst = [(\"1\",'monomer'),(\"2\", 'dimer'),(\"3\", 'trimer'),(\"4\", 'tetramer'),(\"5\", 'pentamer'),\n",
    "           (\"6\", 'hexamer'),(\"7\", 'heptamer'),(\"8\", 'oktamer'),(\"9\", 'nonamer'),(\"10\", 'decamer'),\n",
    "           (\"11\", 'undecamer'),(\"12\", 'dodecamer'),(\"13\", 'tridecamer'),\n",
    "           (\"14\", 'tetradecamer'),(\"15\", 'pentadecamer'),(\"16\", 'hexadecamer'),\n",
    "           (\"17\", 'heptadecamer'),(\"18\", 'oktadecamer'),(\"19\", 'nonadecamer'),(\"20\", 'eicosamer'),\n",
    "           (\"21\", 'eicosameundamer'),(\"22\", 'eicosadodamer'),(\"23\", 'eicosatrimer'),(\"24\", 'eicosatetramer')]\n",
    "\n",
    "        oligodirdict = defaultdict(lambda: \"X-mer\", lst)\n",
    "\n",
    "\n",
    "        \n",
    "        dirs_to_check = []\n",
    "        \n",
    "        for keys, vals in dict_to_shuffle.items():\n",
    "            print(keys)\n",
    "            keys = keys.split(\",\") #CONVERT tuple to a string.\n",
    "            start, stop = keys[0][1:], keys[1][:-1]\n",
    "            print(start)\n",
    "            print(stop)\n",
    "            new_range_obj = start+\"-\"+stop\n",
    "            dir_path = os.path.join(work_dir, new_range_obj)\n",
    "            if not os.path.exists(dir_path):\n",
    "                #get rid of whitespace which makes issues later downstream\n",
    "                os.mkdir(dir_path)\n",
    "                \n",
    "            for oligo, path_tm_rmsd in vals.items():\n",
    "                print(oligo)\n",
    "                oligo_path = os.path.join(work_dir, new_range_obj, oligodirdict[str(oligo)])\n",
    "                if not os.path.exists(oligo_path):\n",
    "                    os.mkdir(oligo_path)\n",
    "                dirs_to_check.append(oligo_path) #this is a list that contains the locations of all directories that will be visited during repair.\n",
    "                for path, _, _ in path_tm_rmsd:\n",
    "                    shutil.copy(path, oligo_path) #new location\n",
    "        \n",
    "        return dirs_to_check\n",
    "\n",
    "    \n",
    "    def repair_structures(self):\n",
    "        # first the monomeric case.\n",
    "        self._separate_oligostates()\n",
    "        \n",
    "        # run now repairs on both.\n",
    "        if self.monomers:\n",
    "            #we need a separate environment for modeller etc.\n",
    "            self.monomer_dirs = self._shuffle(self.monomers)\n",
    "            oligo_dir_dict = defaultdict()\n",
    "            if self.monomer_dirs:\n",
    "                for directory in self.monomer_dirs:\n",
    "                    #check which pdbs need repair.\n",
    "                    pdb_to_check = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".pdb\")] \n",
    "                    #first get the gap ranges\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        gap_list = list(executor.map(self._gap_localization_1, pdb_to_check))\n",
    "                    #each gaplist member is a tuple of (path, list of gaps)\n",
    "                    # keys: templates:   inner keys: the targets inner vals: their seqid and their TM score. which we will now filter and take the top X target to repair our structure.\n",
    "                    \n",
    "                    #print(f\"{gap_list=}\")\n",
    "                    \n",
    "                    suitable_templates = self._get_templates_for_monomeric_multitemplate_modelling(gap_list)\n",
    "\n",
    "                    #print(f\"{suitable_templates=}\")\n",
    "                    # now lets go through each and every of them an check if there are gaps.\n",
    "                    \n",
    "                    potential_template_dict = self._check_repairability(suitable_templates, gap_list)\n",
    "\n",
    "                    #print(f\"{potential_template_dict=}\")\n",
    "                    \n",
    "                    #next step is repair. if empty we skip. else we go through all repairable structures.\n",
    "                    if potential_template_dict:\n",
    "                        with ProcessPoolExecutor() as executor:\n",
    "                            key, value = zip(*potential_template_dict.items()) #unpack first \n",
    "                            try:\n",
    "                                print(\"we try intelligent_monomeric_repair. \")\n",
    "                                results = list(executor.map(self._intelligent_monomeric_repair, key, value))\n",
    "                            except Exception as e:\n",
    "                                results = []\n",
    "                                print(e)\n",
    "                        oligo_dir_dict[directory] = results\n",
    "                        \n",
    "                    else:\n",
    "                        print(f\"We have no repaired structures for directory: {directory}\")\n",
    "                        oligo_dir_dict[directory] = []\n",
    "                        \n",
    "            #this is a dict of paths for each oligostate and range as a instance of the class saved.\n",
    "            self.repaired_monomers =  oligo_dir_dict\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.oligomers:\n",
    "            #we need a separate environment for modeller etc.\n",
    "            self.oligomer_dirs = self._shuffle(self.oligomers)\n",
    "\n",
    "            oligo_dir_dict = defaultdict()\n",
    "            if self.oligomer_dirs:\n",
    "                for directory in self.oligomer_dirs:\n",
    "                    #check which pdbs need repair.\n",
    "                    pdb_to_check = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".pdb\")] \n",
    "\n",
    "\n",
    "                    \"\"\"Lets implement this later. First we just take all structures as they are and continue.\"\"\"\n",
    "                    oligo_dir_dict[directory] = pdb_to_check\n",
    "\n",
    "                    \n",
    "                    #first get the gap ranges\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        gap_list = list(executor.map(self._gap_localization_1_oligomeric, pdb_to_check))\n",
    "        \n",
    "\n",
    "                    suitable_templates = self._get_templates_for_oligo_multitemplate_modelling(template_path=template_path,\n",
    "                                                                    potential_templates=template_backup_list,\n",
    "                                                                    gap_dict=gap_dict)\n",
    "\n",
    "\n",
    "            self.repaired_oligos = oligo_dir_dict\n",
    "\n",
    "\n",
    "    def _check_repairability(self, template_dict: dict, gap_list: list, min_seq_id: float = 0.6) -> dict:\n",
    "        \"\"\"\n",
    "        Checks each structure's eligibility for repair based on gap alignment and minimum sequence identity.\n",
    "    \n",
    "        Args:\n",
    "        - template_dict (dict): A dictionary where keys are template paths and values are dicts containing template details.\n",
    "        - gap_list (list): A list of tuples representing the paths and gaps in the structure to be repaired (path, [(start, stop), ...]).\n",
    "        - min_seq_id (float): The minimum sequence identity required for a template to be considered suitable.\n",
    "    \n",
    "        Returns:\n",
    "        - dict: A dictionary of potential templates that can be used for repair.\n",
    "        \"\"\"\n",
    "        \n",
    "        potential_template_dict = defaultdict(list)\n",
    "        for (temp_path, temp_gaps) in gap_list:\n",
    "            for template_path, details in template_dict.items():\n",
    "                # Assume details structure is a dict with keys as some identifiers and values as tuples of details\n",
    "                for key, val in details.items():\n",
    "                    try:\n",
    "                        # Attempt to unpack expecting 3 elements; adjust according to your data structure\n",
    "                        tm_score, seq_id, template_gaps = val[0][0], val[0][1], val[1]\n",
    "                    except ValueError:\n",
    "                        # Handle cases where unpacking fails due to unexpected structure\n",
    "                        print(f\"Skipping {key} due to unpacking error: expected 3 values, got {len(val)}\")\n",
    "                        continue\n",
    "                        \n",
    "                    #print(seq_id, min_seq_id)\n",
    "                    if float(seq_id) > min_seq_id and not self._has_large_gap_overlap(temp_gaps, template_gaps):\n",
    "                        #print(seq_id, min_seq_id)\n",
    "                        #print(temp_path, template_path)\n",
    "                        potential_template_dict[(temp_path, temp_gaps)].append((template_path, (tm_score, seq_id, template_gaps)))\n",
    "        \n",
    "        return potential_template_dict\n",
    "\n",
    "    def _has_large_gap_overlap(self, gaps1: list, gaps2: list) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if there is a large overlap in gaps between two structures.\n",
    "    \n",
    "        Args:\n",
    "        - gaps1 (list): The gaps in the first structure as a list of tuples (start, stop).\n",
    "        - gaps2 (list): The gaps in the second structure as a list of tuples (start, stop).\n",
    "    \n",
    "        Returns:\n",
    "        - bool: True if there is a large overlap, False otherwise.\n",
    "        \"\"\"\n",
    "        for start1, end1 in gaps1:\n",
    "            for start2, end2 in gaps2:\n",
    "                if start1 <= end2 and start2 <= end1:  # Check if gaps overlap\n",
    "                    return True  # Overlapping gaps found\n",
    "        return False  # No overlapping gaps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _gap_localization_1(self, pdb_path: str):\n",
    "        \"\"\"Helper function to compute the start and stops of gaps \n",
    "        for later potential reconstruction.\"\"\"\n",
    "    \n",
    "        gap_ranges = []\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "        \n",
    "        test_res = test_structure.get_residues()\n",
    "        start = end = None\n",
    "    \n",
    "        for res in test_res:\n",
    "            res_id = int(res.get_id()[1])\n",
    "            if end is None:\n",
    "                start = end = res_id\n",
    "            elif res_id == end + 1:\n",
    "                end = res_id\n",
    "            else:\n",
    "                if start != end:\n",
    "                    gap_ranges.append((start, end))\n",
    "                start = end = res_id\n",
    "    \n",
    "        if start is not None and start != end:\n",
    "            gap_ranges.append((start, end))\n",
    "\n",
    "        if not gap_ranges:\n",
    "            return (pdb_path, [])  # Return an empty list if there are no gaps\n",
    "\n",
    "        # Convert the list of gap ranges to a list of gap tuples\n",
    "        gap_tuples = [(start, end) for start, end in gap_ranges]\n",
    "        merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "        return (pdb_path, merged_gaps)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_templates_for_monomeric_multitemplate_modelling(self, pdb_gap_list):\n",
    "        parser = PDBParser()\n",
    "        suitable_templates = {}\n",
    "\n",
    "        for template_index, (template_path, tmp_gap) in enumerate(pdb_gap_list):\n",
    "            template_results = {}\n",
    "\n",
    "            for target_index, (target_path, gaps) in enumerate(pdb_gap_list):\n",
    "                if target_index != template_index:  # Avoid self-comparison\n",
    "                    score = self._align(target_path, template_path)\n",
    "                    template_results[target_path] = (score, gaps)\n",
    "\n",
    "            suitable_templates[template_path] = template_results\n",
    "\n",
    "        #print(suitable_templates)\n",
    "        # outer keys: templates, inner keys: target_structures. inner values: (seq_id, tm., gaps) \n",
    "        return suitable_templates\n",
    "    \n",
    "    \n",
    "    def _gap_localization_1_oligomeric(self,pdb_path: str):\n",
    "        \"\"\"Helper function to compute the start and stops of gaps for later potential reconstruction.\"\"\"\n",
    "        \n",
    "        full_gaps_dict = defaultdict()\n",
    "        gap_dict = defaultdict(list)\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "        \n",
    "        for model in test_structure:\n",
    "            for chain in model:\n",
    "                for residues in chain:\n",
    "                    resnum = residues.get_id()\n",
    "                    gap_dict[chain.id].append(resnum[1])\n",
    "    \n",
    "        #now we have all chains and all sequence nums from start to finish\n",
    "        chain_to_query = []\n",
    "    \n",
    "        for chains, residues in gap_dict.items():\n",
    "            chain_to_query.append(chains)\n",
    "            \n",
    "            start = end = None\n",
    "            gap_ranges = []\n",
    "            for res in residues:\n",
    "                if end is None:\n",
    "                    start = end = res\n",
    "                elif res == end + 1:\n",
    "                    end = res\n",
    "                else:\n",
    "                    if start != end:\n",
    "                        gap_ranges.append((start, end))\n",
    "                    start = end = res\n",
    "    \n",
    "            if start is not None and start != end:\n",
    "                gap_ranges.append((start, end))\n",
    "    \n",
    "            gap_tuples = [(start, end) for start, end in gap_ranges]\n",
    "            merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "            full_gaps_dict[chains] = merged_gaps\n",
    "        \n",
    "        return full_gaps_dict, chain_to_query\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _align(self, target_path, template_path):\n",
    "        \n",
    "        # Implement the logic to align the target_path with the template_path\n",
    "        # focusing on the region defined by 'gap', and return a scoring metric\n",
    "        bash_tm_and_rmsd_calc = f\"{self.usalign_exe} {target_path} {template_path} -TMscore 0 -outfmt 1\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        \n",
    "        result_scores_for_templates = defaultdict()\n",
    "        \n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "            #print(result.stdout)\n",
    "            result_list = self._get_aligned_fastas(result.stdout)  # gap list not used here... can be removed in future version.\n",
    "\n",
    "            #print(result_list)\n",
    "            return result_list\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e)\n",
    "            return {}\n",
    "            \n",
    "\n",
    "\n",
    "    def _get_aligned_fastas(self, result):\n",
    "\n",
    "        lines = result.strip().split('\\n')\n",
    "        #this is gap_list : [(1, 3), (263, 276), (396, 407)]\n",
    "        #print(\"this is gap_list :\", gap_list)\n",
    "        result_list = []\n",
    "        #print(lines)\n",
    "        # Initialize an empty dictionary\n",
    "        template_header = lines[2].split(\"\\t\")\n",
    "        #template_full_seq = lines[3]  #replace - to get the full seq.\n",
    "        template_seq_id = float(template_header[3].split(\"=\")[-1])\n",
    "        template_tm_score = float(template_header[4].split(\"=\")[-1])\n",
    "        template_pdb_path = template_header[0].replace(\">\",\"\")\n",
    "        template_pdb_path = template_pdb_path.split(\":\")[0]\n",
    "    \n",
    "        potential_template_header = lines[0].split(\"\\t\")\n",
    "        #potential_template_full_seq = lines[1]\n",
    "        potential_template_seq_id = float(potential_template_header[3].split(\"=\")[-1])\n",
    "        potential_template_tm_score = float(potential_template_header[4].split(\"=\")[-1])\n",
    "        potential_template_pdb_path = potential_template_header[0].replace(\">\",\"\")\n",
    "        potential_template_pdb_path = potential_template_pdb_path.split(\":\")[0]\n",
    "        #print(template_seq_id, template_tm_score)\n",
    "        #print(potential_template_seq_id, potential_template_tm_score)\n",
    "        #result_dict[potential_template_pdb_path] = (potential_template_seq_id, potential_template_tm_score, potential_template_full_seq)\n",
    "        #result_dict[template_pdb_path] = (template_seq_id, template_tm_score, template_full_seq)\n",
    "        return template_seq_id, template_tm_score\n",
    "    \n",
    "    def _score_meets_criteria(self, score):\n",
    "        # Implement the logic to check if the score meets your criteria\n",
    "        # for a suitable template\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _intelligent_oligomeric_repair(self, path_to_pdb:str, gap_list:list, selected_templates:list,chains_query:list):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function repairs structures with gaps less than 8 residues per gap.\n",
    "    \n",
    "        Args:\n",
    "        - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "        - stop_pos (int): Stop position.\n",
    "        - main_prot_seq (str): Main protein sequence.\n",
    "        - use_main (bool): Whether to use the main protein.\n",
    "    \n",
    "        Output:\n",
    "        Repaired structures.\n",
    "        \"\"\"\n",
    "    \n",
    "        #new_template_path='/home/micnag/test_modeller_oligomer/6hyr/6hyr.pdb'\n",
    "        #,gap_list_all_chains=[[(1, 5), (96, 116)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)]], \n",
    "        #repair_templates_non_redundant={'/home/micnag/test_modeller_oligomer/6hz3_A.pdb'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"inside oligomeric_repair\")\n",
    "        log.none()  # no stdout spam\n",
    "        env = Environ()  # setup env for modelling\n",
    "        aln = Alignment(env)  # setup the alignment\n",
    "        mdl = Model(env)  # setup the model\n",
    "    \n",
    "        #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "        # current working directory\n",
    "        pdb_id_target = os.path.basename(path_to_pdb) #results in 2duk_A.pdb\n",
    "    \n",
    "        #pdb_id_target='6hyr.pdb'\n",
    "        #print(f\"{pdb_id_target=}\")\n",
    "        \n",
    "        pdb_id_chain = chains_query #this corresponds to all chains in oligomeric struc. We shall also pass this\n",
    "        #to our automodell derived child class.\n",
    "    \n",
    "        #we need first and last chain to set boundaries for our model \n",
    "        first_chain, last_chain = pdb_id_chain[0], pdb_id_chain[-1]\n",
    "        \n",
    "        #pdb_code_name is passed to modeller later.\n",
    "        pdb_code_name = pdb_id_target[:-4] #2duk_ABCDEF\n",
    "        \n",
    "        pdb_4_digit_id = pdb_id_target[0:4] #2duk\n",
    "    \n",
    "        #print(f\"{pdb_4_digit_id=}\")\n",
    "        \n",
    "        #get uniprot_id second. HERE CHECK HOW WE GET X UNIPROT IDS FOR X PROTEINS IN A HETERO_OLIGOMER\n",
    "        uniprot_id = return_uniprot_id_from_rcsb(pdb_4_digit_id)\n",
    "    \n",
    "        #print(f\"{uniprot_id=}\")\n",
    "        #get associated fasta from uniprot id.\n",
    "        fasta_seq = get_gene_fasta(uniprot_id)\n",
    "    \n",
    "        #print(f\"{fasta_seq=}\")\n",
    "    \n",
    "        #first_chain='A', last_chain='E', pdb_4_digit_id='6hyr', pdb_code_name='6hyr',uniprot_id='Q7NDN8', pdb_id_chain=['A', 'B', 'C', 'D', 'E']\n",
    "        #print(f\"{first_chain=}, {last_chain=}, {pdb_4_digit_id=}, {pdb_code_name=},{uniprot_id=}, {pdb_id_chain=}, {fasta_seq=}\")\n",
    "        \"\"\"HERE WE NEED TO CHECK HOW TO MERGE OUR FASTAS.. ESPECIALLY CRUCIAL IF WE HAVE A MIXED OLIGOMER.\n",
    "        SUBSEQUENT CHAINS NEEDS TO BE SEPARATED BY / \"\"\"\n",
    "        \n",
    "        #if mixed oligomer this needs to be taken into account. \n",
    "        \"\"\"WE NEED TO CHECK THIS IN THE FUTURE... ADAPT GET_GENE_FASTA for multiple seqs in hetero X mers.\"\"\"\n",
    "        \n",
    "        merged_fasta = '/'.join([fasta_seq] * len(pdb_id_chain))\n",
    "        \n",
    "        #first we check what is the first ID in our struc. because otherwise we can miss structures that simply miss \n",
    "        #N terminus e.g gap 1-21 but they in fact start with residue 21 and are otherwise good or might just have small\n",
    "        # gaps to fix otherwhere in the structure.\n",
    "    \n",
    "        start_stop_dict = defaultdict()\n",
    "    \n",
    "        temp_codes = []\n",
    "        temp_abs_paths = []\n",
    "        temp_chains = []\n",
    "    \n",
    "    \n",
    "    \n",
    "        #print(f\"{merged_fasta=}\")\n",
    "    \n",
    "        for structure in set(selected_templates):  #no duplicates here.\n",
    "            #print(f\"Structure: {structure} selected as template\")\n",
    "            \n",
    "            temp_codes.append(structure.split(\"/\")[-1][:-4]) #extract codes for all strucs. this works for oligomers.\n",
    "            \n",
    "            temp_abs_paths.append(structure) #grab abspath\n",
    "            \n",
    "            temp_chains.append(structure.split(\"/\")[-1][5:-4]) #this are the chains we need.\n",
    "    \n",
    "    \n",
    "        #print(f\"{temp_codes=}, {temp_abs_paths=}, {temp_chains}\")\n",
    "    \n",
    "        for temp_code, temp_paths, chains in zip(temp_codes, temp_abs_paths, temp_chains):\n",
    "            if len(temp_code) <= 6:\n",
    "                #append all start stop and shifts here.\n",
    "                start_struc_temp, stop_struc_temp = get_struc_stop_oligomer(temp_paths)  #this works also for oligomers in theory. But it is questionable.. if chain A is 100 res and chain B is 40.\n",
    "                #how should we give this info to modeller... 1:A until B:40 problably. include \n",
    "                \n",
    "                start_stop_dict[temp_code] = ((start_struc_temp, stop_struc_temp, chains))\n",
    "    \n",
    "        start_struc_query, stop_struc_query = get_struc_stop_oligomer(path_to_pdb) \n",
    "        \n",
    "        start_stop_dict[pdb_code_name] = ((start_struc_query, stop_struc_query, pdb_id_chain))\n",
    "    \n",
    "    \n",
    "        #print(f\"{start_stop_dict=}\")\n",
    "        \n",
    "        #now the dict contains all paths and start stops.\n",
    "        \n",
    "        #this is the only shift we care about!\n",
    "    \n",
    "        #careful here.. shifts need to be taken into account FOR EACH CHAIN!\n",
    "        \n",
    "        shift = abs(1-start_struc_query)  #e.g 1- 8 abs means 7 shift.\n",
    "        \n",
    "        #if we find that the first end of gap corresponds to the start resi number of the pdb... we skip this gap.\n",
    "    \n",
    "        #would be better to check if one of the templates might cover this gap. But currently not implemented.\n",
    "    \n",
    "        #print(f\"{shift=}\")\n",
    "    \n",
    "    \n",
    "        #gap_list=[((96, 116), 'A')]\n",
    "        #print(f\"{gap_list=}\")\n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        gap_list_all_chains=[[(1, 5), (96, 116)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)]]\n",
    "        we enter oligomeric repair with: new_template_path='/home/micnag/test_modeller_oligomer/6hyr/6hyr.pdb',repairable_gaps=[((96, 116), 'A')], repair_templates_non_redundant={'/home/micnag/test_modeller_oligomer/6hz3_A.pdb'}\n",
    "        inside oligomeric_repair\n",
    "        Structure: /home/micnag/test_modeller_oligomer/6hz3_A.pdb selected as template\n",
    "        temp_codes=['6hz3_A'], temp_abs_paths=['/home/micnag/test_modeller_oligomer/6hz3_A.pdb'], ['A']\n",
    "        start_stop_dict=defaultdict(None, {'6hz3_A': (5, 315, 'A'), '6hyr': (5, 315, ['A', 'B', 'C', 'D', 'E'])})\n",
    "        shift=4\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        \n",
    "        \"\"\"CONTINUE HERE AFTER COURSE TO PROCEED WITH HETEROMERIC OLIGOMER REPAIRS.  NOV 17 2023  18:33 \"\"\"\n",
    "        \n",
    "    \n",
    "        for gaps, chains in gap_list:\n",
    "            #print(gaps, chains)\n",
    "            \n",
    "            found_N_terminus = False\n",
    "            found_C_terminus = False\n",
    "    \n",
    "            #can only concatenate str (not \"int\") to str#\n",
    "    \n",
    "            #    n_terminus = [x for x in range(gap_list[0][0], gap_list[0][1]+1)] #including the last residue.\n",
    "            #    c_terminus = [x for x in range(gap_list[-1][0], gap_list[-1][1]+1)] #including the last residue.\n",
    "        \n",
    "            #(96, 116) A\n",
    "    \n",
    "            if len(gaps) > 2: #means we have more than 1 gap. each gap = len 2\n",
    "                n_terminus = [x for x in range(gaps[0][0], gaps[0][1]+1)] #including the last residue.\n",
    "                c_terminus = [x for x in range(gaps[-1][0], gaps[-1][1]+1)] #including the last residue.\n",
    "            else:\n",
    "                n_terminus = None\n",
    "                c_terminus = None\n",
    "                #we dont deal with n or c terminus here if we only have 1 gap.\n",
    "    \n",
    "            #print(f\"this is {n_terminus=}\")\n",
    "            #print(f\"this is {c_terminus=}\")\n",
    "    \n",
    "            max_n_terminus_found = None\n",
    "            max_c_terminus_found = None\n",
    "    \n",
    "            #print(f\"works until here inside intelligent oligomeric repair!!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "            for paths, (start, stop, chain) in start_stop_dict.items():\n",
    "                # paths='6hz3_A',start=5, stop=315, chain='A'\n",
    "                # paths='6hyr',start=5, stop=315, chain=['A', 'B', 'C', 'D', 'E']\n",
    "                \n",
    "                #print(f\"{paths=},{start=}, {stop=}, {chain=}\")\n",
    "                #lets check for the n-terminus if we find something that can be repaired.\n",
    "                \n",
    "                if n_terminus:\n",
    "                    if start < n_terminus[-1]:\n",
    "                    \n",
    "                        #then we set n terminus found because there is something to repair\n",
    "                        found_N_terminus = True\n",
    "                    \n",
    "                        #but if we have no template... we cant repair beyond what we have and we stay with start.\n",
    "                        if max_n_terminus_found == None or start < max_n_terminus_found:\n",
    "                            max_n_terminus_found = start\n",
    "                            \n",
    "                #lets check for the c terminus in the same fashion\n",
    "                if c_terminus: \n",
    "                    if stop > c_terminus[0]:\n",
    "                    \n",
    "                        #that means we have something that goes beyond the max available struc len.\n",
    "                        found_C_terminus = True\n",
    "                    \n",
    "                        #but if we have no template, we still cant repair beyond.\n",
    "                        if max_c_terminus_found == None or stop > max_c_terminus_found:\n",
    "                            max_c_terminus_found = stop\n",
    "                    \n",
    "    \n",
    "    \n",
    "            #now lets check outside the loop\n",
    "    \n",
    "            if found_N_terminus == False and len(gaps) != 2: #means its not our only hit.\n",
    "    \n",
    "                if len(gaps) > 2:\n",
    "    \n",
    "                    gap_list = gaps[1:] #skip first tuple because we cant repair N terminus\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"We could not repair the only gap because we have no template. this is {gap_list=}\")         \n",
    "                    return\n",
    "            \n",
    "            if found_C_terminus == False and len(gaps) != 2:\n",
    "                #this means we have no suitable template and we skip it.\n",
    "                \n",
    "                if len(gap_list) > 2:\n",
    "                    \n",
    "                    gap_list = gap_list[:-1]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"We could not repair the only gap because we have no template. this is {gap_list=}\")\n",
    "                    return\n",
    "    \n",
    "            \n",
    "            #lets grab the max range of gaps in our structure.\n",
    "            if len(gaps) != 2:\n",
    "                max_gap_range = sorted([y - x for x, y in gaps], reverse=True)\n",
    "            else:\n",
    "                max_gap_range = [gaps[1] - gaps[0]] #simple 1 gap, list for downstream compatibility\n",
    "    \n",
    "    \n",
    "            if len(selected_templates) == 0 and max_gap_range[0] > 8:  \n",
    "                #this last part checks for the case we have\n",
    "                #no templates found but only small gaps of lenght < 8 and still want to repair.\n",
    "                print(\"no suitable templates found.\")\n",
    "                return\n",
    "    \n",
    "        #print(\"we went through this chaos.\")\n",
    "    \n",
    "        \"\"\"Modeller aln.salign treats the FIRST structure provided as QUERY. so we need to load this one FIRST.\"\"\"\n",
    "        #code to be passed to mdl.read\n",
    "    \n",
    "        #start struc query is already above computed at the beginning so we re use it again here.\n",
    "        #CONTINUE HERE\n",
    "    \n",
    "        \"\"\"model_segment specifies the range we look into. Ideally we look for start - stop based on majority vote. Chain is always the same in monomeric\"\"\"\n",
    "        mdl.read(file=pdb_code_name, model_segment=(f\"{start_struc_query}:{pdb_id_chain[0]}\", f\"{stop_struc_query}:{pdb_id_chain[-1]}\"))\n",
    "    \n",
    "        #pdb_code_name='6hyr',start_struc_query=5,pdb_id_chain[0]='A',stop_struc_query=315,pdb_id_chain[-1]='E'\n",
    "        #print(f\"{pdb_code_name=},{start_struc_query=},{pdb_id_chain[0]=},{stop_struc_query=},{pdb_id_chain[-1]=}\")\n",
    "        #append model object to alignment object.\n",
    "    \n",
    "        aln.append_model(mdl, align_codes=pdb_code_name, atom_files=pdb_code_name)\n",
    "    \n",
    "         \n",
    "        for codes, (start_temp, stop_temp, chains) in start_stop_dict.items():\n",
    "            #dont add our query again to the stack.\n",
    "            if codes == pdb_code_name:\n",
    "                continue\n",
    "    \n",
    "            #rest of templates...add to the stack.\n",
    "            mdl.read(file=codes, model_segment=(f\"{start_temp}:{chains[0]}\", f\"{stop_temp}:{chains[-1]}\"))\n",
    "            \n",
    "            aln.append_model(mdl, align_codes=codes, atom_files=codes)\n",
    "    \n",
    "    \n",
    "        #now add fasta sequence as last entry to the align model.\n",
    "        with open(f\"./{pdb_code_name}x.fasta\", \"w\") as fastaout:\n",
    "            fastaout.write(f\">{pdb_code_name}x\\n\")\n",
    "            fastaout.write(merged_fasta)\n",
    "    \n",
    "        aln_code = f\"{pdb_code_name}x\"\n",
    "    \n",
    "        #align fasta file to our alignment object which contains now a fasta sequence and a structure object.\n",
    "        aln.append(file=f\"./{pdb_code_name}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        #Overwrite MyModel and Inherit from AutoModel.\n",
    "        class MyModel(AutoModel):\n",
    "            def __init__(self, env, alnfile, knowns, sequence, gaps, chains, shift, **kwargs):\n",
    "                super(MyModel, self).__init__(env, alnfile, knowns, sequence, **kwargs)\n",
    "                self.gaps = gaps\n",
    "                self.chain = chains\n",
    "                self.shift = shift #shift because modeller always renumbers all stuff to be 1 based.\n",
    "                \n",
    "            def select_atoms(self):\n",
    "                selections = []\n",
    "                chain = self.chain\n",
    "                #this needs to be adjusted for multi chain modells.\n",
    "                #this is self.gaps: [((96, 116), 'A')]\n",
    "                #print(f\"this is self.gaps: {self.gaps}\")\n",
    "                for (start, end), chain in self.gaps:\n",
    "                    if start > 1:\n",
    "                        \n",
    "                        #print(f\"Selecting atoms for range {start-self.shift}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start-self.shift}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                        \n",
    "                    else:\n",
    "                        #print(f\"Selecting atoms for range {start}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "    \n",
    "                selected_atoms = Selection(*selections)\n",
    "                \n",
    "                # Combine all selections into a single Selection object\n",
    "                print(f\"Selected {len(selected_atoms)} atoms for optimization\")\n",
    "                \n",
    "                return selected_atoms\n",
    "    \n",
    "    \n",
    "        #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "        env.io.atom_files_directory = ['.','../.']\n",
    "        \n",
    "        #aln.malign3d(fit=True)\n",
    "    \n",
    "    \n",
    "        # Additional debugging: Print alignment content\n",
    "        #print(\"Alignment content before salign:\")\n",
    "        #for record in aln:\n",
    "            #print(record.code)\n",
    "    \n",
    "        #align sequence to structure.\n",
    "    \n",
    "        #overhang = 0 because we are confident the structures as templates are suitable candidates (seq id > 0.8)\n",
    "        # gap penalties are default.\n",
    "        # alignment_type = progressive : each template pairwise against query comparison.\n",
    "        # \n",
    "    \n",
    "        \n",
    "        aln.salign(overhang=0, gap_penalties_1d=(-450, -50), alignment_type=\"progressive\", output=\"ALIGNMENT\")\n",
    "    \n",
    "        #write out alignmentfile for automodell usage later\n",
    "        aln.write(file=f\"{pdb_code_name}.ali\")\n",
    "        \n",
    "        #add template codes to the code list.\n",
    "        alternate_templates = [x for x in temp_codes]\n",
    "    \n",
    "        # merge with our codes.\n",
    "        full_knowns = (pdb_code_name, *alternate_templates) #empty list from alternate_templates\n",
    "        \n",
    "        #print(f\"{full_knowns=}\")\n",
    "        #custom class\n",
    "    \n",
    "        # selected atoms do not feel the neighborhood\n",
    "        #env.edat.nonbonded_sel_atoms = 2\n",
    "    \n",
    "        #here we need to rechain again and make sure that this is what modeller sees.. e.g BCD Will need to be ABC\n",
    "    \n",
    "        pdb_original = None #this is considered false by default == 1 == False\n",
    "    \n",
    "        letters = [chr(ord('A') + i) for i in range(26)]\n",
    "        \n",
    "        \n",
    "        if pdb_id_chain != letters[0:len(pdb_id_chain)]:  # ABC for len 3 e.g\n",
    "            #print(f\"we are inside pdb_id_chain not fitting with modeller. : {pdb_id_chain=}\")\n",
    "            pdb_original = list(pdb_id_chain)\n",
    "            #we set it to A for repair.. but afterwards we swap it back!\n",
    "            pdb_id_chain = letters[0:len(pdb_id_chain)]\n",
    "    \n",
    "        #print(f\"{pdb_id_chain=}\")\n",
    "    \n",
    "        #print(\"works until here.\")\n",
    "    \n",
    "        a = MyModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code,\n",
    "                    gaps=gap_list, shift=shift, chains=pdb_id_chain)\n",
    "    \n",
    "    \n",
    "        #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "        #a = AutoModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code)\n",
    "    \n",
    "        a.starting_model = 1\n",
    "        a.ending_model = 1\n",
    "    \n",
    "        # Thorough MD optimization:\n",
    "        #a.md_level = refine.slow\n",
    "    \n",
    "        # Repeat the whole cycle 2 times and do not stop unless obj.func. > 1E6\n",
    "        #a.repeat_optimization = 2\n",
    "    \n",
    "        #env.libs.topology.make(aln)\n",
    "        #env.libs.parameters.read(file='$(LIB)/par.lib')\n",
    "        #mdl.generate_topology(aln[pdb_code_name])\n",
    "        #a.generate_topology(aln[pdb_code_name])\n",
    "        # Assign the average of the equivalent template coordinates to MODEL:\n",
    "        #a.transfer_xyz(aln) #lets try this.\n",
    "        \n",
    "        # Get the remaining undefined coordinates from internal coordinates:\n",
    "        #a.build(initialize_xyz=True, build_method='INTERNAL_COORDINATES')\n",
    "    \n",
    "        try:\n",
    "            # Build the model(s)\n",
    "            a.make();\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during modeling: {e}\")\n",
    "    \n",
    "        #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            #remove artifacts.\n",
    "            #remove_repair_artefacts(pdb_basep=path_to_pdb, pdb_code_name=pdb_code_name)\n",
    "    \n",
    "            #ok now we need to check what the updated start stop range is!\n",
    "    \n",
    "            if max_n_terminus_found and max_n_terminus_found < start_struc_query:\n",
    "                \n",
    "                keep_start = max_n_terminus_found\n",
    "                if not max_c_terminus_found:\n",
    "                    keep_stop = stop_struc_query\n",
    "                \n",
    "                    \n",
    "            if max_c_terminus_found and max_c_terminus_found > stop_struc_query:\n",
    "                keep_stop = max_n_terminus_found\n",
    "    \n",
    "                if not max_n_terminus_found:\n",
    "                    keep_start = start_struc_query\n",
    "                    \n",
    "            if not max_n_terminus_found and not max_c_terminus_found:\n",
    "                keep_start, keep_stop = start_struc_query, stop_struc_query\n",
    "            \n",
    "    \n",
    "            #here we can switch chain back to original if required:\n",
    "    \n",
    "            \n",
    "            #print(f\"{keep_start=}, {keep_stop=}\")\n",
    "            \n",
    "            #this should select only from the start to end and excludes potentially repaired N and C termini that would only introduce more noise.\n",
    "            select_c_alpha_and_correct_range(path_to_pdb, start=keep_start, stop=keep_stop)\n",
    "    \n",
    "            \n",
    "            \n",
    "            #not required!\n",
    "            #renumber_structure_monomeric(path_to_pdb, start=start_struc, chain=pdb_id_chain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _intelligent_monomeric_repair(self, repair_struc_gap_lst, temp_paths_tm_score_seq_id_template_gaps):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function repairs structures with gaps less than 7 residues per gap.\n",
    "    \n",
    "        Args:\n",
    "        - template_dict: key = (pdb to be repaired, temp_gaps), vals = (path_for_template_structure,  (tm_score, seq_id, template_gaps)) \n",
    "\n",
    "        We need to make sure that the selection of residues used for repair does not fall into the region of template_gaps.\n",
    "\n",
    "        \n",
    "        Attention:\n",
    "    \n",
    "        Modeller INTERNALLY RECHAINS AND RENUMBERS EVERY QUERY.\n",
    "        ALL SINGLE CHAINS == A chain. irrespective of natural chain.. e.g if you supply chain B single chain it will go back to rechaining it to chain A.\n",
    "        Also dangerous... it will renumber structures from 1... so you need to correct for this shift based on your desired selection target. \n",
    "        If you structure starts with residue 30... and you want to remodell a gap between 40-50 you need to take this into account. shift = abs(1-start_struc_query)\n",
    "        \n",
    "        Output:\n",
    "        Repaired structures.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(repair_struc_gap_lst)\n",
    "        print(temp_paths_tm_score_seq_id_template_gaps)\n",
    "\n",
    "        #lets unpack everything properly.\n",
    "\n",
    "        pdb_id_target_path = repair_struc_gap_lst[0] #the path\n",
    "        pdb_id_target_gaps = repair_struc_gap_lst[1] #the list of gaps to be checked\n",
    "\n",
    "        template_paths = temp_paths_tm_score_seq_id_template_gaps[0] #the path to the templates\n",
    "        template_gaps = temp_paths_tm_score_seq_id_template_gaps[1][2] # the list of gaps in the templates\n",
    "\n",
    "\n",
    "        if len(pdb_id_target_gaps) == 0:\n",
    "            #means we have nothing to add.\n",
    "            return pdb_id_target_path \n",
    "        \n",
    "        log.none()  # no stdout spam\n",
    "        env = Environ()  # setup env for modelling\n",
    "        aln = Alignment(env)  # setup the alignment\n",
    "        mdl = Model(env)  # setup the model\n",
    "        #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "        # current working directory\n",
    "        #print(\"works until here step 1:\")\n",
    "        pdb_id_target = os.path.basename(pdb_id_target_path) #results in 2duk_A.pdb\n",
    "        pdb_id_chain = pdb_id_target[-5] #this corresponds to chain in single monomeric struc. We shall also pass this\n",
    "        #to our automodell derived child class.\n",
    "        #pdb_code_name is passed to modeller later.\n",
    "        pdb_code_name = pdb_id_target[:6] #2duk_A\n",
    "        pdb_4_digit_id = pdb_id_target[0:4]\n",
    "        #print(f\"this is {pdb_4_digit_id=}, {pdb_id_chain=}, {pdb_code_name=}\")\n",
    "        #print(\"works until here step 2:\")\n",
    "        #get uniprot_id second.\n",
    "        uniprot_id = self._return_uniprot_id_from_rcsb(pdb_4_digit_id)\n",
    "        #get associated fasta from uniprot id.\n",
    "        fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "    \n",
    "        #first we check what is the first ID in our struc. because otherwise we can miss structures that simply miss \n",
    "        #N terminus e.g gap 1-21 but they in fact start with residue 21 and are otherwise good or might just have small\n",
    "        # gaps to fix otherwhere in the structure.\n",
    "    \n",
    "\n",
    "        start_stop_dict = self._setup_and_prep_templates(template_paths)\n",
    "        start_struc_query, stop_struc_query = self._get_struc_stop(pdb_id_target_path) \n",
    "\n",
    "        #leave this as this adds the QUERY to the start_stop_dic\n",
    "        start_stop_dict[pdb_code_name] = ((start_struc_query, stop_struc_query, pdb_id_chain))\n",
    "\n",
    "        \n",
    "        #now the dict contains all paths and start stops.\n",
    "        #this is the only shift we care about!\n",
    "        shift = abs(1-start_struc_query)  #e.g 1- 8 abs means 7 shift.\n",
    "        \n",
    "        #if we find that the first end of gap corresponds to the start resi number of the pdb... we skip this gap.\n",
    "        #would be better to check if one of the templates might cover this gap. But currently not implemented.\n",
    "        #[(275, 284), (882, 889)]\n",
    "        #print(f\"this is {gap_list=}\")\n",
    "        \n",
    "        found_N_terminus, found_C_terminus, pdb_id_target_gaps = self._check_terminus_coverage(pdb_id_target_gaps, start_stop_dict)\n",
    "            \n",
    "        #print(f\"gaplist after cutting: {gap_list=}\")\n",
    "        \n",
    "        #lets grab the max range of gaps in our structure.\n",
    "        #max_gap_range = sorted([y - x for x, y in pdb_id_target_gaps], reverse=True)\n",
    "    \n",
    "        #check if we found suitable templates... IF NO and there are still only small gaps < 8 then we use the structure itself as template and still repair.\n",
    "        #if len(selected_templates) == 0 and max_gap_range[0] > 8:  \n",
    "            #this last part checks for the case we have\n",
    "            #no templates found but only small gaps of lenght < 8 and still want to repair.\n",
    "            #print(\"no suitable templates found.\")\n",
    "        #    return\n",
    "            \n",
    "        #elif len(selected_templates) == 0 and max_gap_range[0] < 8:\n",
    "            #nothing to do since we always append our own structure to repair ensemble. But in this case we can continue.\n",
    "            #print(\"we found gaps but they are small so we shall continue\")\n",
    "        #    pass\n",
    "        \n",
    "    \n",
    "        \"\"\"Modeller aln.salign treats the FIRST structure provided as QUERY. so we need to load this one FIRST.\"\"\"\n",
    "        #code to be passed to mdl.read\n",
    "    \n",
    "        #start struc query is already above computed at the beginning so we re use it again here.\n",
    "        #CONTINUE HERE\n",
    "    \n",
    "        \"\"\"model_segment specifies the range we look into. Ideally we look for start - stop based on majority vote. Chain is always the same in monomeric\"\"\"\n",
    "        mdl.read(file=pdb_code_name, model_segment=(f\"{start_struc_query}:{pdb_id_chain}\", f\"{stop_struc_query}:{pdb_id_chain}\"))\n",
    "        #append model object to alignment object.\n",
    "        aln.append_model(mdl, align_codes=pdb_code_name, atom_files=pdb_code_name)\n",
    "    \n",
    "        #now lets add all the templates.\n",
    "        #now we need to append all template strucs.\n",
    "        \n",
    "        for codes, (start_temp, stop_temp, chains) in start_stop_dict.items():\n",
    "            #dont add our query again to the stack.\n",
    "            if codes == pdb_code_name:\n",
    "                continue\n",
    "    \n",
    "            #rest of templates...add to the stack.\n",
    "            #print(f\"{start_temp}:{chains}\", f\"{stop_temp}:{chains}\")\n",
    "            mdl.read(file=codes, model_segment=(f\"{start_temp}:{chains}\", f\"{stop_temp}:{chains}\"))  \n",
    "            aln.append_model(mdl, align_codes=codes, atom_files=codes)\n",
    "    \n",
    "        #now add fasta sequence as last entry to the align model.\n",
    "        with open(f\"./{pdb_code_name}x.fasta\", \"w\") as fastaout:\n",
    "            fastaout.write(f\">{pdb_code_name}x\\n\")\n",
    "            fastaout.write(fasta_seq)\n",
    "    \n",
    "        aln_code = f\"{pdb_code_name}x\"\n",
    "        #align fasta file to our alignment object which contains now a fasta sequence and a structure object.\n",
    "        aln.append(file=f\"./{pdb_code_name}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "        \n",
    "        class MyModel(AutoModel):\n",
    "            def __init__(self, env, alnfile, knowns, sequence, gaps, chain, shift, **kwargs):\n",
    "                super(MyModel, self).__init__(env, alnfile, knowns, sequence, **kwargs)\n",
    "                self.gaps = gaps\n",
    "                self.chain = chain\n",
    "                self.shift = shift #shift because modeller always renumbers all stuff to be 1 based.\n",
    "                \n",
    "            def select_atoms(self):\n",
    "                selections = []\n",
    "                chain = self.chain\n",
    "    \n",
    "                #print(f\"this is self.gaps: {self.gaps}\")\n",
    "                for start, end in self.gaps:\n",
    "                    if start > 1:  #negative indx make problems so we start from 1 in this cases.\n",
    "                        #print(f\"Selecting atoms for range {start-self.shift}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start-self.shift}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                    else:\n",
    "                        #print(f\"Selecting atoms for range {start}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                \n",
    "                selected_atoms = Selection(*selections)\n",
    "                # Combine all selections into a single Selection object\n",
    "                #print(f\"Selected {len(selected_atoms)} atoms for optimization\")\n",
    "                return selected_atoms\n",
    "    \n",
    "    \n",
    "        #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "        env.io.atom_files_directory = [f'{self.work_dir}', f'../.']\n",
    "\n",
    "        #aln.malign3d(fit=True)\n",
    "        # Additional debugging: Print alignment content\n",
    "        #print(\"Alignment content before salign:\")\n",
    "        #for record in aln:\n",
    "            #print(record.code)\n",
    "        #align sequence to structure.\n",
    "        #overhang = 0 because we are confident the structures as templates are suitable candidates (seq id > 0.8)\n",
    "        # gap penalties are default.\n",
    "        # alignment_type = progressive : each template pairwise against query comparison.\n",
    "        #    \n",
    "        aln.salign(overhang=0, gap_penalties_1d=(-450, -50), alignment_type=\"progressive\", output=\"ALIGNMENT\")\n",
    "        #write out alignmentfile for automodell usage later\n",
    "        aln.write(file=f\"{pdb_code_name}.ali\")\n",
    "        #add template codes to the code list.\n",
    "        alternate_templates = [x for x in temp_codes]\n",
    "        # merge with our codes.\n",
    "        full_knowns = (pdb_code_name, *alternate_templates) #empty list from alternate_templates\n",
    "        #print(full_knowns)\n",
    "        #custom class\n",
    "        # selected atoms do not feel the neighborhood\n",
    "        #env.edat.nonbonded_sel_atoms = 2\n",
    "        #check chain if not A... take A and afterwards change it back to original chain. \n",
    "        pdb_original = None #this is considered false by default == 1 == False\n",
    "        if pdb_id_chain != \"A\":\n",
    "            #print(f\"we are inside pdb_id_chain != A: {pdb_id_chain=}\")\n",
    "            pdb_original = list(pdb_id_chain)\n",
    "            #we set it to A for repair.. but afterwards we swap it back!\n",
    "            pdb_id_chain = \"A\"\n",
    "    \n",
    "        a = MyModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code,\n",
    "                    gaps=pdb_id_target_gaps, shift=shift, chain=pdb_id_chain) #pdb_id_chain\n",
    "     \n",
    "\n",
    "        #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "        #a = AutoModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code)\n",
    "    \n",
    "        a.starting_model = 1\n",
    "        a.ending_model = 1\n",
    "    \n",
    "        # Thorough MD optimization:\n",
    "        #a.md_level = refine.slow\n",
    "    \n",
    "        # Repeat the whole cycle 2 times and do not stop unless obj.func. > 1E6\n",
    "        #a.repeat_optimization = 2\n",
    "    \n",
    "        #env.libs.topology.make(aln)\n",
    "        #env.libs.parameters.read(file='$(LIB)/par.lib')\n",
    "        #mdl.generate_topology(aln[pdb_code_name])\n",
    "        #a.generate_topology(aln[pdb_code_name])\n",
    "        # Assign the average of the equivalent template coordinates to MODEL:\n",
    "        #a.transfer_xyz(aln) #lets try this.\n",
    "        \n",
    "        # Get the remaining undefined coordinates from internal coordinates:\n",
    "        #a.build(initialize_xyz=True, build_method='INTERNAL_COORDINATES')\n",
    "        \n",
    "        try:\n",
    "            # Build the model(s)\n",
    "            a.make();\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during modeling: {e}\")\n",
    "    \n",
    "        #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            #remove artifacts.\n",
    "            self._remove_repair_artefacts(pdb_basep=pdb_id_target_path, pdb_code_name=pdb_code_name)\n",
    "    \n",
    "            #ok now we need to check what the updated start stop range is!\n",
    "    \n",
    "            #print(f\"{max_n_terminus_found=}, {max_n_terminus_found < start_struc_query=}\")\n",
    "            \n",
    "            #print(f\"{max_c_terminus_found=}, {max_c_terminus_found > start_struc_query=}\")\n",
    "    \n",
    "            #print(f\"{start_struc_query=}\")\n",
    "            #print(f\"{stop_struc_query=}\")\n",
    "            \n",
    "            \"\"\"\n",
    "            max_n_terminus_found=-2, max_n_terminus_found < start_struc_query=True\n",
    "            max_c_terminus_found=994, max_c_terminus_found > start_struc_query=True\n",
    "            start_struc_query=1\n",
    "            stop_struc_query=992\n",
    "            keep_start=-2, keep_stop=-2\n",
    "            \"\"\"\n",
    "    \n",
    "            if pdb_original:\n",
    "                #print(f\"this is inside pdb_original {pdb_id_chain=}\")\n",
    "                #rechain back and set pdb_id_chain to original pdb chain.\n",
    "                pdb_id_chain = self._rechain_back(path_to_pdb=pdb_id_target_path, pdb_original=pdb_original)\n",
    "                \n",
    "            #should take both chains and pdb path.. then rechain back and return back the new chain.\n",
    "    \n",
    "    \n",
    "            keep_start = start_struc_query\n",
    "            keep_stop = stop_struc_query\n",
    "            \n",
    "            if max_n_terminus_found and max_n_terminus_found < start_struc_query:\n",
    "                keep_start = max_n_terminus_found\n",
    "                    \n",
    "            if max_c_terminus_found and max_c_terminus_found > stop_struc_query:\n",
    "                keep_stop = max_c_terminus_found\n",
    "            \n",
    "            #print(f\"{keep_start=}, {keep_stop=}\")\n",
    "            #this should select only from the start to end and excludes potentially repaired N and C termini that would only introduce more noise.\n",
    "            self._select_correct_range(pdb_id_target_path, start=keep_start, stop=keep_stop)\n",
    "    \n",
    "    \n",
    "            monomeric_chain = \"\".join(pdb_id_chain)\n",
    "            #print(f\"{monomeric_chain=}\")\n",
    "            #not required! ?? lets see later if it is .\n",
    "            self._renumber_structure_monomeric(pdb_id_target_path, start=keep_start, chain=pdb_id_chain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        #if everyhing worked out we return the path!\n",
    "        return pdb_id_target_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _return_uniprot_id_from_rcsb(self, uniprot_id:str):\n",
    "    \n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        \n",
    "        searchp = f\"{link_path}/{uniprot_id}\"\n",
    "        #print(searchp)\n",
    "        resp = get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        \n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                return uniprot_id\n",
    "\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "    \n",
    "        #print(\"we are in get gene fasta\")\n",
    "        \"this is already overworked. should work.\"\n",
    "        #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "        \n",
    "        fields = \"sequence\"\n",
    "        \n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        \n",
    "        seq = \"\"\n",
    "        \n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "                #print(lines)\n",
    "            i += 1\n",
    "        \n",
    "        #print(seq)\n",
    "        return seq\n",
    "\n",
    "\n",
    "\n",
    "    def _setup_and_prep_templates(self, template_paths)->dict:\n",
    "\n",
    "        #print(\"works until here step 3:\")\n",
    "        start_stop_dict = defaultdict()\n",
    "    \n",
    "        temp_codes = []\n",
    "        temp_abs_paths = []\n",
    "        temp_chains = []\n",
    "        \n",
    "        for structure in set(template_paths):  #no duplicates here.\n",
    "            #print(f\"Structure: {structure} selected as template\")\n",
    "            temp_codes.append(structure.split(\"/\")[-1][:-4])\n",
    "            temp_abs_paths.append(structure)\n",
    "            temp_chains.append(structure.split(\"/\")[-1][-5]) #this is the chain we need.\n",
    "    \n",
    "        for temp_code, temp_paths, chains in zip(temp_codes, temp_abs_paths, temp_chains):\n",
    "            if len(temp_code) <= 6:\n",
    "                #append all start stop and shifts here.\n",
    "                start_struc_temp, stop_struc_temp = get_struc_stop(temp_paths)\n",
    "                start_stop_dict[temp_code] = ((start_struc_temp, stop_struc_temp, chains))\n",
    "    \n",
    "        return start_stop_dict\n",
    "\n",
    "    \n",
    "    def _get_struc_stop(self, path_to_pdb):\n",
    "    \n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"none\", path_to_pdb)\n",
    "        seq_ids = [x.get_id()[1] for x in structure.get_residues()]\n",
    "        seq_ids = sorted(seq_ids)\n",
    "        return seq_ids[0], seq_ids[-1] #this corresponds to the last residue.\n",
    "\n",
    "\n",
    "    def _remove_repair_artefacts(self, pdb_basep, pdb_code_name):\n",
    "    \n",
    "        pattern = f\"{pdb_code_name}\"\n",
    "        #print(pattern)\n",
    "        # Remove original_****_*.ali file\n",
    "        ali_files = glob.glob(f\"{pattern}.ali\")\n",
    "        for file in ali_files:\n",
    "            os.remove(file)\n",
    "        # Remove original_****_A.pdb\n",
    "        old_pdb_file = f\"{pattern}.pdb\"\n",
    "        if os.path.exists(old_pdb_file):\n",
    "            os.remove(old_pdb_file)\n",
    "            \n",
    "        # Rename original_****_Ax.B99990001.pdb to original_****_A.pdb\n",
    "        repaired_pdb_file = f\"{pattern}x.B99990001.pdb\"\n",
    "        if os.path.exists(repaired_pdb_file):\n",
    "            new_pdb_file = repaired_pdb_file.replace('x.B99990001', '')\n",
    "            shutil.move(repaired_pdb_file, new_pdb_file)\n",
    "    \n",
    "        # Remove original_****_*x.D00000001\n",
    "        d_files = glob.glob(f\"{pattern}x.D00000001\")\n",
    "        for file in d_files:\n",
    "            os.remove(file)\n",
    "    \n",
    "        # Remove other files\n",
    "        extensions_to_remove = ['.fasta', '.ini', '.rsr', '.sch', '.V99990001']\n",
    "        for ext in extensions_to_remove:\n",
    "            files = glob.glob(f\"{pattern}x{ext}\")\n",
    "            for file in files:\n",
    "                os.remove(file)\n",
    "                \n",
    "\n",
    "    def _check_terminus_coverage(self, pdb_id_target_gaps, start_stop_dict):\n",
    "        \"\"\"\n",
    "        Checks if the N-terminus and C-terminus gaps in the target structure can be covered by any of the templates.\n",
    "    \n",
    "        Args:\n",
    "        - pdb_id_target_gaps: A list of tuples representing the gaps in the target structure.\n",
    "        - start_stop_dict: A dictionary with template start and stop residue numbers and chains.\n",
    "    \n",
    "        Returns:\n",
    "        - A tuple of booleans indicating whether suitable templates for the N-terminus and C-terminus were found.\n",
    "        - Updated pdb_id_target_gaps after excluding uncovered terminus gaps.\n",
    "        \"\"\"\n",
    "        found_N_terminus = False\n",
    "        found_C_terminus = False\n",
    "    \n",
    "        n_terminus_range = range(pdb_id_target_gaps[0][0], pdb_id_target_gaps[0][1] + 1)\n",
    "        c_terminus_range = range(pdb_id_target_gaps[-1][0], pdb_id_target_gaps[-1][1] + 1)\n",
    "    \n",
    "        max_n_terminus_found = None\n",
    "        max_c_terminus_found = None\n",
    "    \n",
    "        for _, (start, stop, _) in start_stop_dict.items():\n",
    "            if int(start) < n_terminus_range[-1]:\n",
    "                found_N_terminus = True\n",
    "                max_n_terminus_found = max(max_n_terminus_found, start) if max_n_terminus_found is not None else start\n",
    "    \n",
    "            if int(stop) > c_terminus_range[0]:\n",
    "                found_C_terminus = True\n",
    "                max_c_terminus_found = max(max_c_terminus_found, stop) if max_c_terminus_found is not None else stop\n",
    "    \n",
    "        # Adjust the gaps list based on the terminus coverage\n",
    "        if not found_N_terminus and len(pdb_id_target_gaps) > 1:\n",
    "            pdb_id_target_gaps = pdb_id_target_gaps[1:]\n",
    "        elif not found_N_terminus:\n",
    "            return False, False, []\n",
    "    \n",
    "        if not found_C_terminus and len(pdb_id_target_gaps) > 1:\n",
    "            pdb_id_target_gaps = pdb_id_target_gaps[:-1]\n",
    "        elif not found_C_terminus:\n",
    "            return False, False, []\n",
    "    \n",
    "        return found_N_terminus, found_C_terminus, pdb_id_target_gaps\n",
    "\n",
    "\n",
    "    def _rechain_back(self, path_to_pdb:str, pdb_original:list)-> None:\n",
    "\n",
    "        #first check if its multichain or single chain.\n",
    "        new_chains = \"\".join(pdb_original)\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"default\", path_to_pdb)\n",
    "\n",
    "        for model in structure:\n",
    "            for idx, chain in enumerate(model):\n",
    "                if chain.id != new_chains[idx]:\n",
    "                    chain.id = new_chains[idx]\n",
    "    \n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(path_to_pdb)\n",
    "        return pdb_original\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _select_correct_range(self, path:str, start:int, stop:int):\n",
    "    \n",
    "        #sel only c_alpha\n",
    "        class CAlphaOnlyInCorrectRange(Select):\n",
    "            def __init__(self, start, stop, *args):\n",
    "                super().__init__(*args)\n",
    "                self.start = start\n",
    "                self.stop = stop\n",
    "\n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] >= self.start and residue.id[1] <= self.stop else 0    \n",
    "            \n",
    "        #filelst    path\n",
    "        #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        \n",
    "        #print(\"we are here\")\n",
    "        #print(fullpath)\n",
    "        structure = parser.get_structure(prot_name, path)\n",
    "        \n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(path, CAlphaOnlyInCorrectRange(start=start, stop=stop))\n",
    "\n",
    "\n",
    "\n",
    "    def _renumber_structure_monomeric(self, path_to_pdb:str, start:int, chain:str):\n",
    "\n",
    "        shiftres_location = f\"{self.work_dir}/pdb_shiftres_by_chain.py\"\n",
    "        bash_cmd = f\"python {shiftres_location} {path_to_pdb} {start-1} {chain}\"\n",
    "        bash_cmd_rdy = bash_cmd.split()\n",
    "        \n",
    "        with open(f\"{path_to_pdb}_tmp\", \"w\") as fh_tmp:\n",
    "            result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                 universal_newlines=True)\n",
    "        \n",
    "        #now replace the original one with the temp file.\n",
    "        os.replace(f\"{path_to_pdb}_tmp\", f\"{path_to_pdb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89962721-bbcd-4489-9b4a-72a2c0c15293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrincipalComponentAnalysis:\n",
    "\n",
    "    def __init__(self, script_dir, pca_dir, work_dir,multiseq_dict=None,store_original=True):\n",
    "        self.work_dir = work_dir #The work dir where our pdbs are located\n",
    "        self.script_dir = script_dir #directory containing all scripts required for PCA. \n",
    "        self.store_full_atomistic = store_original #if we want to store full atomistic untouched structures\n",
    "        self.atomistic_models_directory = None\n",
    "        self.seqalignment_dict = multiseq_dict\n",
    "        self.seqalignment_fasta = os.path.join(self.work_dir, \"multiseq_fasta.fa\")\n",
    "        self.prepared_dir = None\n",
    "        self.multi_pdb_codes = None\n",
    "        self.reference_pdb = None\n",
    "        self.pca_dir = pca_dir\n",
    "\n",
    "    \n",
    "    def _copy_original_pdbs(self):\n",
    "        '''Helper function in order to store full atomistic PDBs untouched\n",
    "        and uncutted for downstream analysis like mutational mapping\n",
    "        We simply copy all files from the working directory to another directory called:\n",
    "        --------------------------------------------------------------------------------\n",
    "        work_dir/full_pdb_models/ \n",
    "        --------------------------------------------------------------------------------\n",
    "        There we will find all pdbs that are present in the work dir before PCA run.\n",
    "        '''\n",
    "        if self.store_full_atomistic:\n",
    "            # List all PDB files in the working directory\n",
    "            pdbs = [f for f in os.listdir(self.work_dir) if os.path.isfile(os.path.join(self.work_dir, f)) and f.endswith(\".pdb\")]\n",
    "            # Define the directory where PDB files will be stored\n",
    "            store_dir = os.path.join(self.work_dir, \"full_pdb_models\")\n",
    "            # Create the directory if it doesn't exist\n",
    "            if not os.path.exists(store_dir):\n",
    "                os.mkdir(store_dir)\n",
    "            # Copy each PDB file to the storage directory\n",
    "            for pdb in pdbs:\n",
    "                src = os.path.join(self.work_dir, pdb)  # Source file path\n",
    "                dst = os.path.join(store_dir, pdb)  # Destination file path\n",
    "                shutil.copy(src, dst)  # Copy file to new dir\n",
    "            self.atomistic_models_directory = store_dir\n",
    "\n",
    "\n",
    "    \n",
    "    def prepare_ensemble(self, strict_cutting=\"strict\"):\n",
    "        '''\n",
    "        This function prepares the ensemble of PDBs and does the following steps:\n",
    "        - Removal of hetero atoms like H2O and Ligands.\n",
    "        - Skeletonization of the models to strip down all atoms except CAs which will be used for PCA (full atomistic models contain too much\n",
    "        noise to be used in PCA due to side chain fluctations while CAs should not fluctuate much besides for conformers which is the aim\n",
    "        of finding in this PCA)\n",
    "        \n",
    "        - Cutting based on a structure based alignment.\n",
    "        Here we offer two options: strict cutting or intelligent cutting.\n",
    "        \n",
    "            - strict cutting: We directly take the ensemble, check for gaps in aligned positions \n",
    "            and REMOVE the atoms of a column IF there is a gap in 1 position.\n",
    "\n",
    "            - intelligent cutting: We first try to find structures in the ensemble that introduce many gaps.\n",
    "            We proceed by removal of those structures, rerunning the alignment and then continue with normal strict cutting.\n",
    "            This can significantly improve the number of retained positions in the final PCA, but comes at the cost of dropping potentially\n",
    "            relevant structures.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #1 we select CA only. / we obtain info about potential non canonical aas.\n",
    "        pdbs = [os.path.join(self.work_dir,f) for f in os.listdir(self.work_dir) if os.path.isfile(os.path.join(self.work_dir, f)) and f.endswith(\".pdb\")]\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            remove_het_results = list(executor.map(self._select_c_alpha_and_check_non_canonical, pdbs))\n",
    "        \n",
    "        #2 remove non canonical aas.\n",
    "        for non_canonical, pdb_path in remove_het_results:\n",
    "            if non_canonical:\n",
    "                for keys, vals in non_canonical.items():\n",
    "                    self._mutate_non_standard_aa(pdb_path, non_standard_residue=vals[0], residue=keys, chain=vals[1])\n",
    "\n",
    "        #now we leverage the seq alignment from USAlign.\n",
    "        if cutting == \"strict\":\n",
    "            #print(\"we move into strict cutting\")\n",
    "            residues_to_keep = self._hard_trimmer(self.seqalignment_fasta)\n",
    "            save_dir = os.path.join(self.work_dir, \"trimmed_strucs\")\n",
    "            print(f\"{save_dir=}\")\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.mkdir(save_dir)\n",
    "            #this will be used to run PCA.\n",
    "            self.prepared_dir = save_dir\n",
    "            self._select_proper_residues_after_trim(residues_to_keep, save_dir)\n",
    "\n",
    "        elif cutting == \"intelligent\":\n",
    "            #these strucs need to be removed.\n",
    "            strucs_to_remove = self._soft_trimmer(self.seqalignment_fasta)\n",
    "            #and then we rerun usalign.\n",
    "            #we remove the strucs that are found to introduce many gaps.\n",
    "            self._hard_trimmer(self.seqalignment_fasta)\n",
    "            \n",
    "    \n",
    "    def _soft_trimmer(self, result_fasta:str):\n",
    "        #maybe better idea is instead of removing so harshly to set a treshold. if its a long prot seq \n",
    "        #we dont want to kick out stuff that introduces 5 gaps e.g serca because mean is already 2... so very low..\n",
    "        #in this case we are perfectly fine in introducing a gap of up to 10% of total len\n",
    "        gap_counter = defaultdict()\n",
    "        ali = pytrimal.Alignment.load(result_fasta)\n",
    "        \n",
    "        trimmer = pytrimal.AutomaticTrimmer(\"gappyout\")\n",
    "        trimmed = trimmer.trim(ali)\n",
    "        \n",
    "        for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "            structure_name = name.decode().rjust(6)\n",
    "            gap_count = seq.count(\"-\")\n",
    "            #print(seq)\n",
    "            gap_counter[structure_name] = gap_count\n",
    "\n",
    "        gap_values = list(gap_counter.values())\n",
    "        mean_gap_count = statistics.mean(gap_values)\n",
    "        median_gap_count = statistics.median(gap_values)\n",
    "    \n",
    "        print(mean_gap_count)\n",
    "        print(median_gap_count)\n",
    "        high_gap_count_sequence_ids = [seq_id for seq_id, gap_count in gap_counter.items() if gap_count > mean_gap_count]\n",
    "    \n",
    "        print(high_gap_count_sequence_ids)\n",
    "        return high_gap_count_sequence_ids\n",
    "    \n",
    "\n",
    "    def _hard_trimmer(self, result_fasta:str):\n",
    "\n",
    "        keep_res_dict = defaultdict()\n",
    "        ali = pytrimal.Alignment.load(result_fasta)\n",
    "        #trimm all gaps unless this leaves 10% of the original alignment.\n",
    "        trimmer = pytrimal.ManualTrimmer(gap_threshold=1) #changed to 80% now as test.\n",
    "        trimmed = trimmer.trim(ali)\n",
    "        for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "            keep_res_dict[name.decode().rjust(6)] = seq\n",
    "\n",
    "        return keep_res_dict\n",
    "\n",
    "    def _select_proper_residues_after_trim(self, keep_residue_dict:dict, save_dir:dir):\n",
    "        \n",
    "        for pdb, seq in keep_residue_dict.items():\n",
    "            #seq in keep_residue_dict is extracted from hard trimmer.\n",
    "        \n",
    "            full_p = os.path.join(self.work_dir, pdb)\n",
    "            result_seq = self._grab_sequence_from_struc(full_p)\n",
    "            positions_to_keep = self._find_positions(result_seq[0], seq)\n",
    "            #print(f\"this is seq to keep: {seq}, this is full_seq from structure: {result_seq}\")\n",
    "            #print(f\"this is our savepath for cutted struc: {full_p}\")\n",
    "            self._select_trimmed_positions(path_to_pdb=full_p, list_to_keep=positions_to_keep, save_dir=save_dir)\n",
    "            \n",
    "\n",
    "    def _select_trimmed_positions(self, path_to_pdb:str, list_to_keep:str, save_dir):\n",
    "        #sel only c_alpha\n",
    "        class CAlphaTrimmedPositions(Select):\n",
    "            def __init__(self, list_to_keep_shifts, *args):\n",
    "                super().__init__(*args)\n",
    "                self.list_to_keep_shifts = list_to_keep_shifts\n",
    "                \n",
    "            #overload accept_residue inherited from Select with this conditional return\n",
    "            def accept_atom(self, atom):\n",
    "                return 1 if atom.id == \"CA\" else 0\n",
    "            \n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] in self.list_to_keep_shifts else 0    \n",
    "\n",
    "        #print(f\"{path_to_pdb=}, {list_to_keep=}\")\n",
    "        #lets make a new dir where we store the trimmed strucs.\n",
    "        pdb_name = os.path.basename(path_to_pdb)\n",
    "        new_location = os.path.join(save_dir, pdb_name) #here we save\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, path_to_pdb)\n",
    "        shift = [x.get_id()[1] for x in structure.get_residues()][0] #first residue number = shift\n",
    "        list_to_keep_shifted = [x+shift-1 for x in list_to_keep] #correct for shift. lets try with shift\n",
    "        #print(f\"this is structure: {path_to_pdb} for list to keep shifted: {list_to_keep_shifted}\")\n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(new_location, CAlphaTrimmedPositions(list_to_keep_shifts=list_to_keep_shifted))\n",
    "    \n",
    "\n",
    "    def _find_positions(self, full_sequence, partial_sequence):\n",
    "        i = 0  # Index for full_sequence\n",
    "        j = 0  # Index for partial_sequence\n",
    "        found_positions = []\n",
    "    \n",
    "        while i < len(full_sequence) and j < len(partial_sequence):\n",
    "            if partial_sequence[j] == '-':  # Skip gap positions in partial_sequence\n",
    "                j += 1\n",
    "            elif partial_sequence[j] == full_sequence[i]:  # Match found\n",
    "                found_positions.append(i)\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1  # Advance in full_sequence if no match\n",
    "    \n",
    "        found_pos = [x + 1 for x in found_positions]  # Adjust for 1-based indexing\n",
    "        return found_pos\n",
    "\n",
    "    def _grab_sequence_from_struc(self, pdb):\n",
    "\n",
    "        lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                        ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                        ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                        ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "        \n",
    "        canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        pdb_name = os.path.basename(pdb)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, pdb)\n",
    "        struc_seq = [canonical_aas[x.get_resname()] for x in structure.get_residues()]\n",
    "        struc_seq = \"\".join(struc_seq)\n",
    "    \n",
    "        return (struc_seq, pdb_name)\n",
    "    \n",
    "    \n",
    "    def run_PCA(self):\n",
    "        if not self.prepared_dir:\n",
    "            return\n",
    "        if self.store_full_atomistic:\n",
    "            self._copy_original_pdbs()\n",
    "            \n",
    "        self.multi_pdb_codes, self.reference_pdb = self._create_multi_pdb()\n",
    "        \n",
    "        print(f\"{self.multi_pdb_codes=}, {self.reference_pdb=}\")\n",
    "        if self.multi_pdb_codes and self.reference_pdb:\n",
    "            #now lets run it.\n",
    "            print(\"we shuffle and go\")\n",
    "            self._shuffle_exe_and_run_pca()\n",
    "        \n",
    "    \n",
    "    def _create_multi_pdb(self, reference=None):\n",
    "        \n",
    "        #this function acts as an helper function in pca_laura_pipeline_1\n",
    "        pdb_files = [f for f in os.listdir(self.prepared_dir) if f.endswith(\".pdb\")]\n",
    "        #lets do first a check of length so we only keep strucs that are pca-able.\n",
    "        count_dict = Counter()\n",
    "        for file in pdb_files:\n",
    "            location = os.path.join(self.prepared_dir, file)\n",
    "            prot_name = file[:-4]\n",
    "            try:\n",
    "                structure = PDBParser(QUIET=True).get_structure(prot_name, location)\n",
    "                struc_len = len([x for x in structure.get_residues()])\n",
    "                count_dict[struc_len] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "        most_common_lengths = count_dict.most_common()\n",
    "        most_common_length = most_common_lengths[0][0]\n",
    "        print(most_common_length)\n",
    "        ms = Structure.Structure(\"master\")\n",
    "        i = 0\n",
    "        \n",
    "        list_of_pdb_codes = []\n",
    "        for file in pdb_files:\n",
    "            location = os.path.join(self.prepared_dir, file)\n",
    "            prot_name = file[:-4]\n",
    "    \n",
    "            try:\n",
    "                structure = PDBParser(QUIET=True).get_structure(prot_name, location)\n",
    "                struc_len = len([x for x in structure.get_residues()])\n",
    "    \n",
    "                if struc_len != most_common_length:\n",
    "                    print(f\"This structure has wrong amount of residues: {location=} {struc_len=} instead of {most_common_length=}\")\n",
    "                    #dont proceed in this case.\n",
    "                    continue    \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            list_of_pdb_codes.append(file)\n",
    "            for model in structure:\n",
    "                new_model = model.copy()\n",
    "                new_model.id = i\n",
    "                new_model.serial_num = i + 1\n",
    "    \n",
    "                # Iterate through atoms and set B-factor to 0 because this causes issues with pcatoolS\n",
    "                for chain in new_model:\n",
    "                    for residue in chain:\n",
    "                        for atom in residue:\n",
    "                            atom.set_bfactor(0)\n",
    "    \n",
    "                i += 1\n",
    "                ms.add(new_model)\n",
    "    \n",
    "        pdb_io = PDBIO()\n",
    "        pdb_io.set_structure(ms)\n",
    "        pdb_io.save(f\"{self.prepared_dir}/multi_ensemble.pdb\")\n",
    "\n",
    "        if reference:\n",
    "            path_of_ref = None\n",
    "        else:\n",
    "            path_of_ref = os.path.join(self.prepared_dir, list_of_pdb_codes[0])\n",
    "\n",
    "        \n",
    "        return list_of_pdb_codes, path_of_ref \n",
    "\n",
    "\n",
    "    def _move_executables(self, basepath, work_dir):\n",
    "        #this helper function is required for domenico and laura stuff to work.\n",
    "        os.chdir(work_dir) #i dislike this solution but its fine\n",
    "        try:\n",
    "            moved_files_new_location = []\n",
    "            files_to_move = [f for f in os.listdir(basepath) if os.path.isfile(os.path.join(basepath, f))]\n",
    "            for file in files_to_move:\n",
    "                shutil.copy(os.path.join(basepath, file), work_dir)\n",
    "                #we want to delete them afterwards.\n",
    "                moved_files_new_location.append(os.path.join(basepath, file), work_dir)\n",
    "                return moved_files_new_location\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "    def _shuffle_exe_and_run_pca(self, protein=\"test\"):\n",
    "        file_lst_to_be_removed = self._move_executables(self.pca_dir, self.prepared_dir)\n",
    "        template = os.path.basename(self.reference_pdb) # this is the template pdb file name\n",
    "        \n",
    "        #_move_executables(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/PCA_pipeline\", work_dir)\n",
    "        # Part 1: PCA\n",
    "        bash_cmd1 = f\"./getpca.sh multi_ensemble.pdb {template} refpdb_ref\"\n",
    "        print(f\"this is bashcmd 1: {bash_cmd1}\")\n",
    "        self._run_command(bash_cmd1, \"Part 1\")\n",
    "        # Part 2: Convert evec files\n",
    "        bash_cmd2 = f\"./convert.sh refpdb_ref_pca.evec\"\n",
    "        print(f\"this is bashcmd 2: {bash_cmd2}\")\n",
    "        self._run_command(bash_cmd2, \"Part 2\")\n",
    "        # Part 3: Projections\n",
    "        bash_cmd3 = f\"./getproj.sh {template} refpdb_ref_pca.evec {protein} multi_ensemble.pdb\"\n",
    "        print(f\"this is bashcmd 3: {bash_cmd3}\")\n",
    "        self._run_command(bash_cmd3, \"Part 3\")\n",
    "        # NOW lets remove the exe etc again. \n",
    "        for filep in file_lst_to_be_removed:\n",
    "            try:\n",
    "                os.remove(filep)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    def _run_command(self, bash_cmd, part_name):\n",
    "        try:\n",
    "            result = run(bash_cmd.split(), stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            print(f\"{part_name} output:\\n{result.stdout}\")\n",
    "            print(f\"{part_name} errors:\\n{result.stderr}\")\n",
    "        except Exception:\n",
    "            print(f\"Script did not work. Parameters: {bash_cmd.split()}\")\n",
    "\n",
    "    def plot_PCA(self, name:str, pdb_codes:list, \n",
    "                  save_df=True, save_img=True, **kwargs):\n",
    "\n",
    "        \"\"\"kwargs can be used to modify the plot: e.g pass\n",
    "        layout_config={'width': 1000, 'height': 700}, trace_config={'marker_size': 12}\n",
    "        to the function call beforehand.\"\"\"\n",
    "        \n",
    "        proj_dict = defaultdict()\n",
    "    \n",
    "        save_pdb_file_codes = os.path.join(self.work_dir, \"pdb_codes_PCA.txt\")\n",
    "        with open(save_pdb_file_codes, \"w\") as pdb_fh:\n",
    "            for pdb_code in pdb_codes:\n",
    "                pdb_fh.write(pdb_code)\n",
    "                pdb_fh.write(\"\\n\")\n",
    "    \n",
    "        var_list = []\n",
    "    \n",
    "        #they are already sorted.\n",
    "        with open(f\"{input_dir}/{protein}_evec3.dat\", \"r\") as var_readin:\n",
    "            for lines in var_readin:\n",
    "                line = lines.split()\n",
    "                if len(line) == 2:\n",
    "                    var_list.append(float(line[1]))\n",
    "\n",
    "            expl_var = var_list / np.sum(var_list)\n",
    "\n",
    "        mode_proj_path = os.path.join(self.work_dir, name)\n",
    "        with open(f\"{mode_proj_path}.mode_12.proj\", \"r\") as res_proj:\n",
    "            for i, lines in enumerate(res_proj):\n",
    "                # Split the data into lines\n",
    "                lines = lines.strip().split('\\n')\n",
    "                # Extract the last two columns from each line\n",
    "                res_lines = [(line.split()[-2], line.split()[-1]) for line in lines]\n",
    "                PC1 = float(res_lines[0][0])\n",
    "                PC2 = float(res_lines[0][1])\n",
    "                proj_dict[str(i)] = (PC1, PC2)\n",
    "            \n",
    "        PC_1 = []\n",
    "        PC_2 = []\n",
    "        for keys, vals in proj_dict.items():\n",
    "            #print(vals)\n",
    "            PC_1.append(vals[0])\n",
    "            PC_2.append(vals[1])\n",
    "    \n",
    "        plot_df = pd.DataFrame(columns=[\"PC1\", \"PC2\", \"labels\"])\n",
    "        plot_df[\"PC1\"] = PC_1\n",
    "        plot_df[\"PC2\"] = PC_2\n",
    "        plot_df[\"labels\"] = pdb_codes\n",
    "\n",
    "        if save_df:\n",
    "            save_location_df = os.path.join(self.work_dir, f\"{protein}_pca_df.csv\")\n",
    "            plot_df.to_csv(save_location_df)\n",
    "\n",
    "        # Default configurations for plot\n",
    "        default_layout_config = {\n",
    "        'plot_bgcolor': 'lavender',\n",
    "        'paper_bgcolor': 'white',\n",
    "        'width': 800,\n",
    "        'height': 600,\n",
    "        'xaxis_title': f'PC 1 {expl_var[0]*100:.2f}%',\n",
    "        'yaxis_title': f'PC 2 {expl_var[1]*100:.2f}%',\n",
    "        'legend_title_text': 'Clusters',\n",
    "        'showlegend': True,\n",
    "        'title': {\n",
    "            'text': f'PCA of {name}: {len(pdb_codes)} structures',\n",
    "            'x': 0.5,\n",
    "            'font': {'size': 30}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Update default configurations with any kwargs provided\n",
    "        default_layout_config.update(kwargs.get('layout_config', {}))\n",
    "\n",
    "        fig = px.scatter(plot_df, x= \"PC1\",y= \"PC2\",\n",
    "                hover_data=[\"labels\"],labels={\"labels\": \"labels\"},\n",
    "                custom_data = [\"labels\"])\n",
    "\n",
    "        fig.update_traces(marker_size=10,\n",
    "                      hovertemplate=\"<b>RCSB: %{customdata[0]}</b>\",\n",
    "                      hoverlabel=dict(font_size=40),  # Set the hover label font size\n",
    "                      mode=\"markers+text\", selector=dict(type='scatter'),\n",
    "                      **kwargs.get('trace_config', {}))  # Apply trace configurations from kwargs\n",
    "    \n",
    "        fig.update_layout(**default_layout_config)\n",
    "    \n",
    "        if save_image:\n",
    "            img_save_path = os.path.join(self.work_dir, f\"{name}_PC_plot.png\")\n",
    "            pio.write_image(fig, img_save_path, format=\"png\")\n",
    "    \n",
    "        return fig.show()\n",
    "    \n",
    "    def _select_c_alpha_and_check_non_canonical(self, pdb_path):\n",
    "        #sel only c_alpha\n",
    "        class CAlphaOnlyInCorrectRange(Select):\n",
    "            #overload accept_residue inherited from Select with this conditional return to save only CA\n",
    "            def accept_atom(self, atom):\n",
    "                return 1 if atom.id == \"CA\" else 0\n",
    "            def accept_residue(self, residue):\n",
    "                return 1 if residue.id[0] == \" \" else 0\n",
    "\n",
    "\n",
    "        non_canonical_aas = defaultdict()\n",
    "\n",
    "        canonical_aas = {'VAL', 'ILE', 'LEU', 'GLU', 'GLN',\n",
    "                     'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR',\n",
    "                     'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA',\n",
    "                     'GLY', 'PRO', 'CYS'}\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure('default', pdb_path)\n",
    "\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                for residue in chain:\n",
    "                    if residue.id[0] == \" \": \n",
    "                        curr_res = residue.get_resname()\n",
    "                        curr_pos = residue.get_id()[1]\n",
    "\n",
    "                        if curr_res not in canonical_aas:\n",
    "                            non_canonical_aas[curr_pos] = (curr_res, chain.get_id())\n",
    "\n",
    "            \n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        #print(f\"we save {pdb_path=}\")\n",
    "        io.save(pdb_path, CAlphaOnlyInCorrectRange())\n",
    "        return non_canonical_aas, pdb_path\n",
    "\n",
    "\n",
    "    def _count_unique_values_in_dict(self, input_dict):\n",
    "        start_counts = Counter(val[0] for val in input_dict.values())\n",
    "        stop_counts = Counter(val[1] for val in input_dict.values())\n",
    "        return start_counts, stop_counts\n",
    "\n",
    "    \n",
    "    def _mutate_non_standard_aa(self, path_to_pdb):\n",
    "        \"\"\"Mutates a non-standard amino acid in a PDB file.\"\"\"\n",
    "        \n",
    "        path_to_script = f\"{self.script_dir}/pdb_mutate.py\"\n",
    "        path_to_error_script = f\"{self.script_dir}/pdb_delresname.py\"\n",
    "        path_to_tidy = f\"{self.script_dir}/pdb_tidy.py\"\n",
    "        input_file = f\"{path_to_pdb}_new\" #temp name to not overwrite files we currently read from\n",
    "        try:\n",
    "            with open(input_file, \"w\") as pdb_out:\n",
    "                bash_code = f\"python {path_to_script} {path_to_pdb} {chain} {residue} {non_standard_residue} ALA\"\n",
    "                run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "            resulting_success = True\n",
    "    \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            print(\"Attempting to delete non-standard residue...\")\n",
    "            \n",
    "            with open(input_file, \"w\") as pdb_out:\n",
    "                bash_code = f\"python {path_to_error_script} -{non_standard_residue} {path_to_pdb}\"\n",
    "                run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "        #now we tidy the file to adhere to the most common pdb standard\n",
    "        try:\n",
    "            with open(path_to_pdb, \"w\") as pdb_out:\n",
    "                bash_code = f\"python {path_to_tidy} {input_file}\"\n",
    "                run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "            #print(\"Cleaned the file. Outfile is at\", path_to_pdb)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "        #we dont need the intermediate file that was only created to prevent read/write from same file.\n",
    "        try:\n",
    "            os.remove(input_file)  # Remove intermediate file\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "        return resulting_success\n",
    "        \n",
    "    def _get_struc_stop(self, path_to_pdb):\n",
    "\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"none\", path_to_pdb)\n",
    "        seq_ids = [x.get_id()[1] for x in structure.get_residues()]\n",
    "        seq_ids = sorted(seq_ids)\n",
    "        return seq_ids[0], seq_ids[-1] #this corresponds to the last residue.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c52894-a149-4aca-8a54-502ba3784c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c90d2-edd3-4be3-8f57-4dbdd3942fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930e257-3936-493e-8403-7f588f530e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932ef9e-bd31-4b78-b032-8f0fd251f270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b2b3f-ce20-419f-975c-7a557cc3c8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9d0f5-5a1c-4885-a855-ae2646a7c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bda26-3394-40a8-b6e4-00360fce9c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c7472-aaed-41e6-8a9b-a27fb4492c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dbf07-2989-42fd-a806-0e226a51f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5b551-f8b4-4fae-acf6-631e935ea35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a8d1e-f1bb-4163-ac82-44fb0b82a786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8356b1-99e5-4718-8677-91b96ab98faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a8f85b3-1994-433e-9ebe-ff92658da97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We already have a pdb_download file at /home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/pdb_list.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa88d07a-a796-4ebb-9fd0-ff73a334d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PdbEnsemble_meta = Downloader.meta_dict\n",
    "#PdbEnsemble_chains_seq_id = Downloader.chain_seqid_dict\n",
    "\n",
    "#Downloader.conservation(uniprot_id=\"Q9NZJ9\")\n",
    "#shifts = PdbEnsemble.parallel_shift_calculation()\n",
    "#Downloader.conservation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43495d81-2216-4a19-82ba-e0140cfceaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc3e2a62-c0e1-4f3c-a414-72b8b922d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have no meta dict to implement a cutoff\n"
     ]
    }
   ],
   "source": [
    "PDB_Cleaner = PDBCleaning(work_dir=work_dir, meta_dict=PdbEnsemble_meta, chain_seq_dict=PdbEnsemble_chains_seq_id)\n",
    "PDB_Cleaner.setup_cutoff(cutoff=3, apply_filter=True)  #apply filter to only include structures that are of good quality\n",
    "#PDB_Cleaner.parallel_shift_calculation()  # compute shift for each structure\n",
    "#PDB_Cleaner.parallel_renumbering()  # renumber based on shifts.\n",
    "#PDB_cleaned_ensemble.chain_dict\n",
    "\n",
    "#PDB_Cleaner.filtered_structures\n",
    "struct = PDB_Cleaner.filtered_structures\n",
    "#struct = PDB_Cleaner.get_unfiltered_strucs()\n",
    "#chain_seqid = PDB_Cleaner.chain_seq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a74e32ee-4977-414c-b264-ac0d0490ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3i7v', '6woh', '6wog', '4hfq', '6wob', '3h95', '5ltu', '7aui', '6woe', '6pcl', '2q9p', '7aul', '6wod', '6wo8', '6woa', '6wo9', '2duk', '6wof', '3i7u', '7auj', '7auo', '6pck', '7auq', '7aup', '7auk', '7aus', '7aum', '7aut', '2fvv', '6wo7', '7auu', '7nnj', '7tn4', '6woi', '7aun', '6woc', '3mcf', '7aur']\n"
     ]
    }
   ],
   "source": [
    "print(struct) #get_unfiltered_strucs() misses out on all files with multi chains\n",
    "#['3i7v_A', '6woh_A', '6wog_A', '4hfq_AB', '6wob_A', '3h95_AB', '7aui_A', '6woe_A', '6pcl_A', '2q9p_A', '7aul_A', '6wod_A', '6wo8_A', '6woa_A', '6wo9_A', '6wof_A', '3i7u_ABCD', '6pck_A', '7aup_A', '7auk_A', '7aus_A', '7aut_A', '2fvv_A', '6wo7_A', '7nnj_A', '7tn4_A', '6woi_A', '7aun_A', '6woc_A', '3mcf_A', '7aur_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c1b7c97-8a19-4cba-a3f6-e29c1e2ff3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chain_seqid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f206e949-8850-4e5a-85a5-fd69b6f90125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDB_Builder = PDBBuilder(work_dir=work_dir, structures=struct, remove_intermediates=True) #structures that are filtered\n",
    "#PDB_Builder.build_assembly()\n",
    "#oligo_dict = PDB_Builder.oligodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8c60a70-471f-4a6e-8425-f9c3a3c7cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(oligo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c96d2633-96a6-4a40-bc36-b451a7d5b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preper = PDBEnsemblePrep(work_dir=work_dir, oligo_dict=oligo_dict, chain_seq_dict=chain_seqid, main_prot_seq=None)\n",
    "#Preper.create_domain_boundaries()\n",
    "#Preper.get_oligostates(num_most_common_oligostates=3)\n",
    "#Preper.process_templates()\n",
    "#top_templates = Preper.top_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85adc600-e911-41b6-a8b0-0639335e0f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1429f1f-3a0a-454f-aa21-259e6722280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USAligner = USAlign(work_dir=work_dir, structure_seqid_dict=Preper.templates_for_oligos, \n",
    "#                    template_min_identity=0.1,\n",
    "#                   num_top_clusters_per_range=4)\n",
    "\n",
    "#USAligner.USAlign_run()\n",
    "#USAligner.filter_results(tm_cutoff=0.8,rmsd_min_cutoff=0.2, log_file=True)\n",
    "#USAligner.setup_oligo_directories()\n",
    "#result_dict = USAligner.result_dict\n",
    "#USAligner.run_multiseq_alignment(directory=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddf31e48-db6e-4ed8-9803-2c3027b362ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repairstation = ModellerRepairEnsemble(ensemble_dict=result_dict, work_dir=work_dir)\n",
    "#print(Repairstation.repair_ensemble)\n",
    "#print(Repairstation.monomers)\n",
    "#Repairstation.repair_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c49e0a1a-b6fa-4a78-a902-e3a42426176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\"\n",
    "\n",
    "#work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203\"\n",
    "\n",
    "#fasta = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203/multiseq_fasta.fa\"\n",
    "\n",
    "#pca_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/pca_part\"\n",
    "\n",
    "#PCA_object = PrincipalComponentAnalysis(work_dir=work_dir, script_dir=script_dir, \n",
    "#                                        pca_dir=pca_dir,store_original=True)\n",
    "#PCA_object.run_PCA(work_dir)\n",
    "#PCA_object.prepare_ensemble()\n",
    "#PCA_object.run_PCA()  # works but is crap because of forced path locations for executables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614c4b0-b1c7-4069-b746-5bca97620321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA_object.multi_pdb_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81521527-400c-45d0-b49b-bec452fac237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bb0bb-7cfe-4ce1-b2d0-cde888f3029e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcdf9f-caaa-4d5e-9cdd-1ba317478f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad68230-5c43-4ca3-92f6-e9fcdb6c3a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2735516-ab1a-4c87-ae06-4a623907efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac38b3f-b54f-4051-b5b3-a8a2902af69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb825eb4-2ab6-4584-9c1d-8913a20c21fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910e871-c075-446b-8186-e051e0ea8d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ff7b5-72cf-44b5-b088-825ab691322e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8894f40-9733-44d6-bf9c-bb85a6b16544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e09dd-c2aa-45c1-abf3-6a22a6a95087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b374a-70f6-4562-b425-1342f3399170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b68735-2013-4f25-ad03-857ab72e9466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec6037-a0f3-4823-bc9d-fe3346d369b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4121fe-cc4e-437b-9ae6-76e07ad8e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
