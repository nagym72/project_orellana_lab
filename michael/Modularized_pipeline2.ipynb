{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d66ae7-ee8f-4252-b32f-8d1e45c6b1de",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be60346-a603-46d9-a77a-d05e88d76a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdb\n",
    "import requests\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict, OrderedDict\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil\n",
    "import statistics\n",
    "from datetime import date\n",
    "import logging\n",
    "#import hail as hl\n",
    "import glob\n",
    "import time\n",
    "import pytrimal\n",
    "# Import from installed package\n",
    "#from pypdb.clients.pdb.pdb_client import *\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import pandas as pd\n",
    "#import plotly as px\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO, Align, PDB, Seq, AlignIO\n",
    "from Bio.PDB import PDBParser, PDBIO, Select, MMCIFParser, Structure, Chain, Atom, Superimposer\n",
    "from Bio.PDB import Model as Bio_Model\n",
    "from Bio.PDB import Chain as Bio_chain\n",
    "from Bio.SeqIO import PirIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align import substitution_matrices\n",
    "#from Bio import pairwise2\n",
    "from io import StringIO\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "from modeller.parallel import job, local_slave\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "import subprocess\n",
    "import shlex\n",
    "from subprocess import PIPE, run\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup  #required later to download SIFT files.\n",
    "import atomium\n",
    "from itertools import compress\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.gridspec as gridspe\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from bravado.client import SwaggerClient\n",
    "from pycanal import Canal\n",
    "#import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from threading import Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from packman import molecule\n",
    "from packman.apps import predict_hinge\n",
    "\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "#logging.getLogger(\"requests\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b5520-26a5-4a6a-bb19-d75f2ee28d79",
   "metadata": {},
   "source": [
    "### Input Conversion Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63a6a72b-a444-45c0-a611-bbab2ffbc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInputPreprocessor:\n",
    "    \"\"\"\n",
    "    This class acts as an entry point for searches through GeneNames or UniprotIDs. \n",
    "    You can try both GeneName or UniprotID to fetch all homologs of a given species but the UniprotID is more reliable!\n",
    "    First you can fetch the UniprotID or the GeneName and then you call SetupNewEnv. This first step can be skipped if you already know your Uniprot target.\n",
    "    In that case you can directly call SetupNewEnv(uniprot_id=\"YourID\"). After setting up an environment, you can use different DBs to query against. \n",
    "    Default is currently Cosmic DB which only contains Human queries but this can be easily extended in the future.\n",
    "    For retrieving queries you simply call QueryAgainstDB(query=\"YourUniprotID\") which you can fetch through the helper function GeneToUniprot if you are uncertain about it.\n",
    "    The return of QueryAgainstDB is a pandas.DataFrame that contains all your pdbs and their associated sequence identities for your query.\n",
    "    This output is the Input for the next class: DownloadPipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, db_dir, cutoff=None, db=None, work_dir=None):\n",
    "        self.work_dir = work_dir\n",
    "        #self.script_dir = script_dir\n",
    "        self.db_dir = db_dir\n",
    "        self.cutoff = cutoff\n",
    "        self.uniprot_ids = None\n",
    "        self.gene_name = None\n",
    "        self.db = db\n",
    "        self.hits = None\n",
    "\n",
    "\n",
    "    def UniprotToGene(self, uniprot_id):\n",
    "        # we fetch the id which is the gene name.\n",
    "        fields = \"id\"\n",
    "    \n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.text\n",
    "        resp = resp.split(\"\\n\")\n",
    "        self.gene_name = resp[1] # list because we need compatibility with cases where users search for genetouniprot which can get multiple uniprots.\n",
    "        return self.gene_name\n",
    "\n",
    "    def GeneToUniprot(self, gene_name):\n",
    "        # we fetch the gene which is the accession\n",
    "        fields = \"accession\"\n",
    "\n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query=gene:{gene_name}\"\n",
    "    \n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.text\n",
    "        resp_lines = resp.split(\"\\n\")\n",
    "        # If there are multiple UniProt IDs for the gene, they will be on separate lines\n",
    "        uniprot_ids = resp_lines[1:]  # Skip the header\n",
    "        # There can be multiple Uniprot IDs per gene so we return all of them and the user selects which one he wants to query against.\n",
    "        self.uniprot_ids = [id for id in uniprot_ids if id]\n",
    "        return self.uniprot_ids[0] # for convenience the first hit.\n",
    "\n",
    "\n",
    "    def QueryAgainstDb(self, query, prefered_db=\"Cosmic_Query_DB\", show_options=False):\n",
    "\n",
    "        extensions = [\".csv\", \".tsv\", \".m8\", \".db\"] #these will be accepted as hits. \n",
    "        pattern = os.path.join(self.db_dir, \"*DB.csv\")\n",
    "\n",
    "        dict_with_dbs = {\n",
    "        os.path.splitext(os.path.basename(f))[0]: os.path.join(self.db_dir, f)\n",
    "        for f in os.listdir(self.db_dir)\n",
    "        if os.path.isfile(os.path.join(self.db_dir, f)) and any(f.endswith(ext) for ext in extensions)\n",
    "        }\n",
    "        \n",
    "        if show_options:\n",
    "            print(dict_with_dbs)\n",
    "        \n",
    "        df_all = pd.read_csv(dict_with_dbs[prefered_db], sep=\",\",index_col=0)\n",
    "        \n",
    "        #print(df_all.head())\n",
    "        mask = df_all[\"Query_id\"] == query # mask to filter true \n",
    "        df_queries_hits = df_all[mask]\n",
    "        self.hits = df_queries_hits\n",
    "        return df_queries_hits\n",
    "\n",
    "\n",
    "    def SetupNewEnv(self, work_dir=None, uniprot_id=None):\n",
    "\n",
    "        #if user wants another work dir, he can specify. otherwise we use default work dir \n",
    "        if not work_dir:\n",
    "            if self.work_dir:\n",
    "                work_dir = self.work_dir\n",
    "            else:\n",
    "                print(f\"We need a directory environment to set up a new directory for the new Query.\\nPlease either initialize the Class with work_dir='/your/work/dir' or call this function with work_dir='your/work/dir'\")\n",
    "                return\n",
    "                \n",
    "        dir_name = None  # Initialize dir_name\n",
    "        # If a gene name is set, use it\n",
    "        if uniprot_id:\n",
    "            dir_name = self.UniprotToGene(uniprot_id)\n",
    "        # If a UniProt ID is provided, use it to get the gene name\n",
    "        elif self.gene_name:\n",
    "            dir_name = self.gene_name\n",
    "        # If no gene name or UniProt ID is provided, try using the first UniProt ID from self.uniprot_ids\n",
    "        elif self.uniprot_ids:\n",
    "            try:\n",
    "                #this is the first uniprot found. \n",
    "                first_hit = self.uniprot_ids[0]\n",
    "                dir_name = self.UniprotToGene(first_hit)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching gene name from UniProt ID: {e}\")\n",
    "    \n",
    "        # Proceed if dir_name was successfully determined\n",
    "        if dir_name:\n",
    "            dir_to_make = os.path.join(work_dir, dir_name)  # Use the passed work_dir parameter\n",
    "            if not os.path.exists(dir_to_make):\n",
    "                os.makedirs(dir_to_make)\n",
    "                print(f\"Directory '{dir_to_make}' was created.\")\n",
    "            else:\n",
    "                print(f\"Directory '{dir_to_make}' already exists.\")\n",
    "                return dir_to_make\n",
    "        else:\n",
    "            dir_to_make = None\n",
    "            print(\"No directory name could be determined. Try first calling UniprotToGene() or GeneToUniprot()!\")\n",
    "\n",
    "        #we want the new dir as return\n",
    "        return dir_to_make\n",
    "\n",
    "    #helper function to download\n",
    "    def _get_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url) \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit()\n",
    "\n",
    "        return response\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d814735-84d8-4faa-ad20-194410a1fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline\"\n",
    "#db_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/DB\"\n",
    "\n",
    "#UIP = UserInputPreprocessor(db_dir=db_dir, work_dir=work_dir)\n",
    "# O14983 Serca\n",
    "#both work, chose whatever you prefer but UniprotToGene is more stable due to 1 uniprot = 1 gene but 1 gene = N uniprots\n",
    "\n",
    "#UIP.GeneToUniprot(\"PARP1\")\n",
    "#UIP.UniprotToGene(\"P06213\")\n",
    "#new_work_dir = UIP.SetupNewEnv(work_dir=work_dir)\n",
    "#print(new_work_dir)\n",
    "#UIP.UniprotToGene(\"P06213\")\n",
    "#first= UIP.GeneToUniprot(\"INS\")[0]\n",
    "#UIP.UniprotToGene(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04771f49-893f-4754-9574-e950b6e3d442",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Download Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd73c953-08ee-4ab2-ae2f-b9e660a0e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadPipe:\n",
    "    '''Class object containing the download function that will download all pdbs \n",
    "    which we need for downstream analysis of a particular uniprot ID'''\n",
    "\n",
    "    def __init__(self, input_df, work_dir, script_dir, seq_id_cutoff=None, download_type=\"pdb\", logging=True):\n",
    "        self.work_dir = work_dir # Storage of seq identity useful later for temp selection.\n",
    "        self.script_dir = script_dir #here we store all scripts\n",
    "        self.seq_id_cutoff = seq_id_cutoff\n",
    "        self.download_type = download_type # Download PDB or also mmCIF (currently only PDB)\n",
    "        self.input_df = input_df\n",
    "        self.pdbs_to_download = input_df.loc[:, \"Target_id\"]\n",
    "        self.seq_id = input_df.loc[:, \"Seq_identity\"]\n",
    "        # The bash script location which will download the pdbs. \n",
    "        self.download_script = os.path.join(script_dir, \"batch_download_modified.sh\") #modify for script location\n",
    "        self.download_tmp = os.path.join(work_dir, \"pdb_list.csv\") # The location for the temporary file that is required for the download_script as input.\n",
    "        self.log_dir = os.path.join(work_dir, \"log_files\")\n",
    "        # The list of chains that will be used later to fetch correct structures.\n",
    "        self.chain_seqid_dict = self._setup_download_list()\n",
    "        self.temp_seqid_dict = {template: seq_id for template, seq_id in zip(self.pdbs_to_download, self.seq_id)}\n",
    "        # We store also meta info as a json dict\n",
    "        self.meta_dict = None\n",
    "        #we store high resolution structures as a list if the user wants to separate based on resolution.\n",
    "        self.high_resolution = None\n",
    "        # set a flag that stops redownloading.\n",
    "        self.already_downloaded = None\n",
    "        # collect conservation\n",
    "        self.conservation_df = None\n",
    "        #filtered structures based on meta resolution\n",
    "        self.filtered_structures = None\n",
    "        #store shifts.\n",
    "        self.shift_dict = None\n",
    "        self.logging = logging # for report purpose.\n",
    "            \n",
    "    def paralellized_download(self):\n",
    "        '''\n",
    "        This function is going to call _download_files n times to parallelize download. \n",
    "        It is going to pass the function call itself **_download_file**,\n",
    "        self.download_tmp (the location of the tmp file which is pdb_id comma separated), \n",
    "        p (an additional parameter specifying that \n",
    "        we want to download pdbs, and self.work_dir(the current work dir)\n",
    "        '''\n",
    "        \n",
    "        self.already_downloaded = self._check_for_pdbs_present()\n",
    "        # ThreadPoolExecutor\n",
    "        if self.already_downloaded == False:\n",
    "            print(\"we start downloading now:\")\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                # Submit your tasks to the executor.\n",
    "                futures_pdb = [executor.submit(self._download_files, self.download_tmp, 'p', self.work_dir)]\n",
    "                # Optionally, you can use as_completed to wait for and retrieve completed results.\n",
    "                for future in as_completed(futures_pdb):\n",
    "                    result = future.result()\n",
    "            self.already_downloaded = True\n",
    "        else:\n",
    "            print(\"we already have pdbs from the templates downloaded\")\n",
    "    \n",
    "    def _setup_download_list(self):\n",
    "        '''Helper function to setup the list of comma-separated pdb\n",
    "        ids for the download_files function'''\n",
    "        \n",
    "        \n",
    "        if not self.input_df.empty:\n",
    "            pdbs = self.input_df.loc[:, \"Target_id\"]\n",
    "            seq_ids = self.input_df.loc[:, \"Seq_identity\"]\n",
    "        else:\n",
    "            # we cant proceed\n",
    "            return\n",
    "\n",
    "        #initialize dict\n",
    "        chain_seqid_dict = defaultdict(list)\n",
    "\n",
    "        self.pdbs_to_download = [] # overwrite to set it blank for seq_id filtering.\n",
    "\n",
    "        original_pdbs = len(set([x[0:4] for x in pdbs])) #for logging purpose. tells us how many pdbs originally were there before cutoff\n",
    "        for pdb, seq_id in zip(pdbs, seq_ids):\n",
    "            if float(seq_id) > float(self.seq_id_cutoff):\n",
    "                pdb_4_digit_id = pdb[:4] # e.g 4CFR\n",
    "                chain = pdb[-1] # e.g A\n",
    "                chain_seqid_dict[pdb_4_digit_id].append((chain, seq_id)) #map chains and seq id to pdb id\n",
    "                # We only want to download pdb files once. \n",
    "                # No reason to download a PDB-file 4 times just because we need chain [A, B, C, D]\n",
    "                self.pdbs_to_download.append(pdb_4_digit_id) # we store it for a later check \n",
    "                \n",
    "        unique_pdbs = chain_seqid_dict.keys() # Keys : PDB-IDs, Vals: Chains, seq_id\n",
    "        # Create download_files input list\n",
    "        \n",
    "        if unique_pdbs:\n",
    "            with open(self.download_tmp, \"w\") as pdb_tar:\n",
    "                pdb_tar.write(\",\".join(unique_pdbs))\n",
    "            \n",
    "            #return dict {key: pdb_id = [(chain, seq_id)]}\n",
    "            print(f\"Before applying cutoff: {original_pdbs} Structures\\nAfter applying cutoff: {len(unique_pdbs)} Structures\")\n",
    "            return chain_seqid_dict\n",
    "        else:\n",
    "            print(f\"No structures available for cutoff {self.seq_id_cutoff}. Try lowering cutoff.\")\n",
    "\n",
    "    \n",
    "    def _download_files(self, download_tmp, download_type, path)->list:\n",
    "        \"\"\"This helper function runs inside paralellized_download \n",
    "        and will be used to get the PDB files that we require for downstream analysis.\"\"\"\n",
    "        results = []\n",
    "        # Input for subprocess\n",
    "        bash_curl_cmd = f\"{self.download_script} -f {download_tmp} -o {path} -{download_type}\"\n",
    "        # split into list \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "        \n",
    "        try:\n",
    "            # Run subprocess\n",
    "            result = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            # Append result output to results\n",
    "            results.append(result.stdout.split(\"\\n\")[:-1])  # Skip the last empty element\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error downloading: {e}\")\n",
    "\n",
    "        return results    \n",
    "\n",
    "    def _check_for_pdbs_present(self):\n",
    "        '''\n",
    "        Could be good to improve so that if we miss SOME structures we fetch them as well and download ONLY those.\n",
    "        For those structures we also need seqid per chain and then also update the seqid_chain dict for the whole directory after\n",
    "        successful download.\n",
    "        Currently we only check if pdbs are present and if yes we dont download anything further.\n",
    "        '''\n",
    "        pdbs_to_retrieve = {f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")}  # Use a set for efficient lookups\n",
    "        template_codes = {f[:4] for f in self.pdbs_to_download}  # Convert list to set for efficient intersection operation\n",
    "\n",
    "        # Check for any overlap between the two sets\n",
    "        overlap = pdbs_to_retrieve.intersection(template_codes)\n",
    "\n",
    "        print(f\"This is overlap in the directory: {overlap}\")\n",
    "        # Return 1 if there is an overlap, else 0\n",
    "        return True if overlap else False\n",
    "\n",
    "    \n",
    "    def retrieve_meta(self, dict_location=None, human_readable=True)->dict:\n",
    "        '''\n",
    "        We also want to store meta information about resolution etc.\n",
    "        This function takes each pdb file and retrieves the following information:\n",
    "        - Title\n",
    "        - Keywords\n",
    "        - PDBcode\n",
    "        - Authors\n",
    "        - Deposition date\n",
    "        - Technique\n",
    "        - Resolution\n",
    "        - R_value : If crystallography else None\n",
    "        - R_free : If crystallographe else None\n",
    "        - Classification\n",
    "        - Organism\n",
    "        - Expression System\n",
    "        - Number of amino acids in the asymmetric unit\n",
    "        - Mass of amino acids in the asymmetric unit (Da)\n",
    "        - Number of amino acids in the biological unit\n",
    "        - Mass of amino acids in the biological unit (Da)\n",
    "        '''\n",
    "        \n",
    "        json_file_path = os.path.join(self.log_dir, 'meta_dictionary.json')\n",
    "\n",
    "        for path in [json_file_path, dict_location]: #check first the supposed location alternatively the user supplied location.\n",
    "            if path and os.path.exists(path):\n",
    "                try:\n",
    "                    with open(path, 'r') as json_fh:\n",
    "                        self.meta_dict = json.load(json_fh)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {path}: {e}\")\n",
    "                \n",
    "        #little helper function to deal with date data\n",
    "        def _date_encoder(obj):\n",
    "            if isinstance(obj, date):\n",
    "                return obj.isoformat()  # Convert date to ISO format\n",
    "\n",
    "        #grab all PDB files which contain the meta information.\n",
    "        pdbs_to_retrieve = [f for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "        #here we store info about ALL pdbs.\n",
    "        meta_dictionary = dict()\n",
    "        \n",
    "        for pdbs in pdbs_to_retrieve:\n",
    "            if len(pdbs) == 8: #lets exclude preprocessed pdbs that are longer or shorter.\n",
    "                sub_dict = dict()\n",
    "                pdb_code = pdbs[:4]\n",
    "                try:\n",
    "                    fullp = os.path.join(self.work_dir, pdbs)\n",
    "                    pdb = atomium.open(fullp)\n",
    "                    sub_dict[\"title\"] = pdb.title\n",
    "                    sub_dict[\"key_words\"] = pdb.keywords\n",
    "                    sub_dict[\"code\"] = pdb.code\n",
    "                    sub_dict[\"authors\"] = pdb.authors\n",
    "                    #sub_dict[\"deposition_date\"] = pdb.deposition_date.isoformat()  #isoformat because it is a time object\n",
    "                    sub_dict[\"technique\"] = pdb.technique\n",
    "                    sub_dict[\"resolution\"] = pdb.resolution\n",
    "                    sub_dict[\"r_val\"] = pdb.rvalue\n",
    "                    sub_dict[\"r_free\"] = pdb.rfree\n",
    "                    sub_dict[\"classification\"] = pdb.classification\n",
    "                    sub_dict[\"organism\"] = pdb.source_organism\n",
    "                    sub_dict[\"expression_system\"] = pdb.expression_system\n",
    "                    sub_dict['number_of_residues_asymmetric_unit'] = len(pdb.model.residues())\n",
    "                    sub_dict['mass_dalton_asymetric_unit'] = f\"{pdb.model.mass:.2f}\" \n",
    "                    try:\n",
    "                        assembly = pdb.generate_assembly(1) #build the biological assembly \n",
    "                        sub_dict['number_of_residues_biological_unit'] = len(assembly.residues())\n",
    "                        sub_dict['mass_dalton_biological_unit'] = f\"{assembly.mass:.2f}\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"We could not build the assembly for: {pdb_code}\")\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"We had an error with file: {pdb_code}\")\n",
    "                # store meta info and return\n",
    "                meta_dictionary[pdb_code] = sub_dict\n",
    "\n",
    "\n",
    "        #lets store meta info as json dict\n",
    "        self.meta_dict = meta_dictionary\n",
    "        \n",
    "        # Code block to store meta info as a txt file.\n",
    "        self._save_meta_dict(self.meta_dict, human_readable=human_readable)\n",
    "\n",
    "\n",
    "    def _save_meta_dict(self, meta_dictionary, human_readable=True):\n",
    "        '''Helper function to store meta info as a txt file.'''\n",
    "        #check if log file dir exists, else make it.\n",
    "        \n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        #lets store the dict in json to read it in for later useage.\n",
    "        json_file_path = os.path.join(self.log_dir, 'meta_dictionary.json')\n",
    "        #convert defaultdict to normal dict.\n",
    "        \n",
    "        with open(json_file_path, 'w') as json_fh:\n",
    "            json.dump(meta_dictionary, json_fh, indent=4, default=str)  # Use default=str to handle non-serializable objects\n",
    "\n",
    "    \n",
    "    def conservation(self, uniprot_id):\n",
    "        '''Gets 3 different types of Conservation:\n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "\n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        \n",
    "        mmseq_fasta_result = self._mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=self.work_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = self._get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "        self.conservation_df = conserv_df\n",
    "\n",
    "        conserv_df.to_csv(f\"{self.log_dir}/conservation_df.csv\")\n",
    "        \n",
    "    def _mmseq_multi_fasta(self, uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "        \"\"\"\n",
    "        uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "        outdir: location where result files will be stored.\n",
    "        sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "        filter_msa = 0 default. if 1 hits are stricter.\n",
    "        query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "        \"\"\"\n",
    "\n",
    "        #we blast with this fasta as query.\n",
    "        trgt_fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "        #Make outdir for all required files.\n",
    "        #we need to write it out to file.\n",
    "        with open(f\"{self.work_dir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "            fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "            fasta_out.write(trgt_fasta_seq)\n",
    "\n",
    "        #fetch pre downloaded database from a parent folder.\n",
    "        msa_file = None\n",
    "        new_location = None\n",
    "        try:\n",
    "            DB_storage_location = f\"{work_dir}\"\n",
    "            #shutil.copy(previous_path, savepath)\n",
    "            bash_curl_cmd = f\"mmseqs createdb {self.work_dir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\" \n",
    "            bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "            #run first cmd which setups query database based on our input fasta file\n",
    "            result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"    \n",
    "            bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "            #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "            result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "            bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "            bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "            result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #delete last line.. required.\n",
    "            sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'        \n",
    "            bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "            #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "                result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            # Specify the path to your MSA file\n",
    "            msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            #transfer the meta file to another location and delete useless files.\n",
    "            # we need to delete : all uniprot* files. \n",
    "            # all query*. All result* \n",
    "            new_location = f\"{self.work_dir}/{uniprot_id}.fasta\"\n",
    "            shutil.copy(msa_file, new_location)\n",
    "            #remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        #we want the path to msa_file for downstream analysis.\n",
    "        return new_location\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "        '''\n",
    "        Helper function to grab the sequence \n",
    "        based on the Uniprot ID\n",
    "        '''\n",
    "        fields = \"sequence\"\n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        seq = \"\"\n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "            i += 1\n",
    "        return seq\n",
    "\n",
    "    def _get_conservation(self, path_to_msa:str):    \n",
    "        '''\n",
    "        Helper function to compute 3 different types of conservation.\n",
    "        \n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "        canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "        result_cons = canal.analysis(method=\"all\")\n",
    "        return result_cons\n",
    "\n",
    "    def _get_url(self, url):\n",
    "        '''Helper function that uses requests for Downloads.'''\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit() \n",
    "        return response\n",
    "    \n",
    "    def setup_cutoff(self, cutoff=10, apply_filter=False):\n",
    "        '''If we want to setup a resolution cutoff filter for further downstream analysis, \n",
    "        this function helps with it.'''\n",
    "        # If there is no meta dict we cant proceed and filter based on resolution.\n",
    "        if self.meta_dict:\n",
    "            #here we store the pdb codes that we keep\n",
    "            pdbs_to_keep = []\n",
    "            #Now lets parse through the whole meta dict and fetch the cutoffs for structures.\n",
    "            for _, single_pdbs in self.meta_dict.items():\n",
    "                if single_pdbs['resolution'] <= cutoff:\n",
    "                    pdbs_to_keep.append(single_pdbs['code'].lower()) #normalize to lower in order to have uniform list members.   \n",
    "                    \n",
    "            self.filtered_structures = pdbs_to_keep\n",
    "            #now if we directly want to apply the filter to remove files that dont match our criteria.\n",
    "            if apply_filter:\n",
    "                #check for union between files and kept structures.\n",
    "                pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "                #lets fetch the intersect between the 2 sets which corresponds to the pdbs we want to keep.\n",
    "                common_pdb = set(pdbs_to_retrieve) & set(pdbs_to_keep) #intersection\n",
    "                intersect_lst = list(common_pdb)\n",
    "                self.filtered_structures = intersect_lst\n",
    "                if self.chain_seqid_dict:\n",
    "                    #now we need to update the chain_dict as well:\n",
    "                    filtered_dict = {pdb: v for pdb, v in self.chain_seqid_dict.items() if pdb[:4] in self.filtered_structures}\n",
    "                    self.filtered_structures = filtered_dict\n",
    "                    \n",
    "        else:\n",
    "            print(\"We have no meta dict to implement a cutoff\")\n",
    "            #In this case we take all.\n",
    "            pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\") and len(f) == 8] #exclude non original files. Only store pdb + _ + chains.\n",
    "            self.filtered_structures = pdbs_to_retrieve\n",
    "\n",
    "    def parallel_shift_calculation(self):\n",
    "        '''Here we compute the shift according to uniprot or authors\n",
    "        in order to be in line with UNIPROT numbering which is crucial for later renumbering.'''\n",
    "        \n",
    "        pdbs_to_retrieve = [f[0:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]  \n",
    "        pdbs_to_retrieve = set(pdbs_to_retrieve) & set(x[:4] for x in self.oligodict.keys()) #here we check the first 4 which is pdb code\n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        shift_dict = defaultdict()\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            calculate_shift_bound = partial(self._calculate_shift)\n",
    "            tasks = ((link_path, pdb) for pdb in pdbs_to_retrieve)\n",
    "            # Map the bound function to the arguments in parallel\n",
    "            results = executor.map(calculate_shift_bound, tasks)\n",
    "            for result in results:\n",
    "                for keys, vals in result.items():\n",
    "                    shift_dict[keys] = vals\n",
    "                    \n",
    "        self.shifts = shift_dict\n",
    "\n",
    "    def _calculate_shift(self, args):\n",
    "        '''\n",
    "        Helper function to compute the shift.\n",
    "        Args: link_path to UNIPROT page and the pdb path.\n",
    "        '''\n",
    "        link_path, pdb = args\n",
    "        shift_dict = defaultdict()\n",
    "        searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "        resp = self._get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                for mapping in uniprot_info['mappings']:\n",
    "                    chain_id = mapping['chain_id']\n",
    "                    unp_start = mapping['unp_start']\n",
    "                    unp_end = mapping['unp_end']\n",
    "                    author_start = mapping['start']['author_residue_number']\n",
    "                    author_end = mapping['end']['author_residue_number']\n",
    "    \n",
    "                    if author_start is None:\n",
    "                        author_start = unp_start\n",
    "                    if author_end is None:\n",
    "                        author_end = unp_end\n",
    "                    shift_start = unp_start - author_start\n",
    "                    shift_end = unp_end - author_end\n",
    "                    shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "                    \n",
    "        self.shift_dict = shift_dict\n",
    "        return shift_dict\n",
    "\n",
    "    \n",
    "    def parallel_renumbering(self):\n",
    "        '''\n",
    "        Helper function to do parallelized renumbering.\n",
    "        If already renumbered, don't do it again.\n",
    "        '''\n",
    "        if self.renumbered:\n",
    "            print(\"You already renumbered your structures based on shift.\")\n",
    "            return  # Exit the function early\n",
    "\n",
    "        if not self.shifts:\n",
    "            print(\"You first need to obtain shifts which will be used as reference in order to start renumbering.\\nCall .parallel_shift_calculation() first.\")\n",
    "            return  # Exit the function if no shifts are available\n",
    "\n",
    "        # At this point, we know renumbering has not been done and shifts are available\n",
    "        relevant_files = self.chain_seq_dict.keys()\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Using partial to create a function with fixed parameters (shift_dict, path)\n",
    "            renumber_structure_partial = partial(self._renumber_structure, shift_dict=self.shifts, path=self.work_dir)\n",
    "            # Map the renumbering function to each relevant file in parallel\n",
    "            executor.map(renumber_structure_partial, relevant_files)\n",
    "\n",
    "        self.renumbered = True\n",
    "\n",
    "    \n",
    "    def _renumber_structure(self, files, shift_dict, path):\n",
    "        '''Function that is going to apply pdb_shiftres_by_chain.py to each pdb file that is shifted.\n",
    "        Will apply renumbering to ALL structures if you did not set a cutoff previously and applied filter. \n",
    "        If filter applied for resolution will only renumber those structures that are left after filtering.'''\n",
    "        for keys, vals in shift_dict.items():\n",
    "            #dont renumber if there is not shift\n",
    "            if files == keys[0:4] and vals != str(0):\n",
    "                chain = keys[-1]\n",
    "                shift = int(vals)\n",
    "                filepath = f\"{self.work_dir}/{files}.pdb\"\n",
    "                # Should we really shift by shift + 1??? or just shift?\n",
    "                bash_cmd = f\"python {self.script_dir}/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "            \n",
    "                with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                    result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, universal_newlines=True)\n",
    "                    # Now replace the original one with the temp file.\n",
    "                    os.replace(f\"{filepath}_tmp\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da75ff-9a47-4198-adad-972405facde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets merge now the uniprot search to df input output as input for the download class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b083f18-f017-472a-9f5f-9c4942a0dc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15f16d62-26cf-4716-befe-07285e9d81d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PDBBUILDER class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf74f3d-61e6-42f7-a1d5-5f5f20eaa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBBuilder:\n",
    "    '''Class to build biological units from asymmetric units'''\n",
    "    def __init__(self, work_dir, script_dir, structures, remove_intermediates=True, main_protein_sequence=None, logging=True):\n",
    "        self.work_dir = work_dir\n",
    "        #Here we store the structures that need to be built.\n",
    "        self.structures = structures\n",
    "        #script dir\n",
    "        self.script_dir = script_dir\n",
    "        #we store intermediate files per default.\n",
    "        self.remove_intermediates = remove_intermediates\n",
    "        #we set oligo to None but store later the oligodict\n",
    "        self.oligodict = None\n",
    "        # dict to store the potential ranges.\n",
    "        self.range_dict = None\n",
    "        # logging purpose\n",
    "        self.log_dir = os.path.join(work_dir, \"log_files\")\n",
    "        # enable logging.\n",
    "        self.logging = logging\n",
    "        #main_prot_seq\n",
    "        self.main_protein_sequence = main_protein_sequence\n",
    "        # result dict for established domains\n",
    "        self.established_domain_dict = defaultdict(dict)\n",
    "        # top templates found for each range\n",
    "        self.top_templates = None\n",
    "        self.assembled_structures = None\n",
    "\n",
    "    def build_assembly(self):\n",
    "        '''\n",
    "        This function takes all pdbs in self.structures that were passed to class initialization:\n",
    "\n",
    "        structure:\n",
    "        self.structures -> dict\n",
    "        keys: pdb_id_4_digit\n",
    "        values: tuple (chain_id, seq_identity)\n",
    "\n",
    "        example: {'5ltu': [('A', 0.96), ('B', 0.96)]}\n",
    "\n",
    "        We rechain all pdbs in order to have uniformized chain labels for downstream processing.\n",
    "        Then, we build biological assemblies based on the asymmetric unit deposition and caputure the oligomeric status.\n",
    "        We return then an updated self.oligodict that has the following structure:\n",
    "\n",
    "        key: PDB_ID_CHAIN(s)\n",
    "        values: tuple(chain, seq_id)\n",
    "        example: {'5ltu_AB.pdb': [('A', 0.96), ('B', 0.96)]}\n",
    "        This corresponds to the fully assembled structure.\n",
    "        \n",
    "        '''\n",
    "        # These files need to be opened, rechained and assemblies built.\n",
    "        full_pdb_paths = [os.path.join(self.work_dir, f\"{file}.pdb\") for file in self.structures]\n",
    "        oligostates = defaultdict(str)\n",
    "        #this letterdict is used for rechaining.\n",
    "        letterdict = {i: chr(65 + i) for i in range(26)}\n",
    "        #changed this here from threadpool to process pool\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Define your processing function, partially applied with gene_name and main_protein_seq\n",
    "            process_func = partial(self._process_pdb, letterdict=letterdict)\n",
    "            results = executor.map(process_func, full_pdb_paths)\n",
    "            for result in results:\n",
    "                #print(f\"this is result: {result}\")\n",
    "                oligostates.update(result)\n",
    "\n",
    "        #beautifully chaotic.... basically we update... 4 digit ID + chains i.e 4 means ABCD 2 means AB 1 means A, and concatenate it with .pdb to get 4rt4_ABCD.pdb\n",
    "        new_oligostates = {f\"{k[0:4]}_{''.join(letterdict[i] for i in range(v))}.pdb\": v for k, v in oligostates.items()}\n",
    "\n",
    "        #lets update the dict now with info about seq id.\n",
    "        self._update_oligostates_with_sequence_identities(new_oligostates)\n",
    "\n",
    "    def _update_oligostates_with_sequence_identities(self, dict_to_update):\n",
    "        '''\n",
    "        This function helps to bring back the sequence identity information to the new oligodict\n",
    "        For those chains that wont end up in the biological unit, we drop the information.\n",
    "        '''\n",
    "        updated_oligostates = {}\n",
    "        for pdb_chain_file, oligostate in dict_to_update.items():\n",
    "            pdb_id = pdb_chain_file.split('_')[0]  # Extract the PDB ID from the filename\n",
    "            chain_ids = pdb_chain_file.split('_')[1].rstrip('.pdb')  # Extract chain IDs\n",
    "            # Initialize a list to hold sequence identities for each chain\n",
    "            seq_identities = []\n",
    "            \n",
    "            if pdb_id in self.structures:\n",
    "                # Iterate through each chain and its corresponding sequence identity in the structures\n",
    "                for chain_id, seq_identity in self.structures[pdb_id]:\n",
    "                    # If the current chain is part of the oligostate (i.e., it's in the chain_ids string), add its sequence identity\n",
    "                    if chain_id in chain_ids:\n",
    "                        seq_identities.append((chain_id, seq_identity))\n",
    "            \n",
    "            # Update the dictionary with the new value format\n",
    "            updated_oligostates[pdb_chain_file] = (seq_identities)\n",
    "        \n",
    "        # Update the class attribute\n",
    "        self.oligodict = updated_oligostates\n",
    "    \n",
    "    def _process_pdb(self, path:str,letterdict:dict)->dict:\n",
    "        #helper function to split between nmr and xray / cryoem\n",
    "        try:\n",
    "            pdb_file_name = os.path.basename(path)\n",
    "            if len(pdb_file_name) != 8:\n",
    "                #this means its already processed previously because its not 5DUK_A.pdb instead of 5DUK.pdb\n",
    "                oligostate = len(pdb_file_name[5:-4]) # this is the oligostate e.g ABC = 3 \n",
    "                return {pdb_file_name: oligostate}\n",
    "                \n",
    "            pdb_file = atomium.open(path)\n",
    "            model_len = len(pdb_file.models)\n",
    "            \n",
    "            if model_len > 5:  # if multiple models => NMR OR CRYO-EM?\n",
    "                return {pdb_file_name: self._NMR_ensemble(path=path, letterdict=letterdict)}\n",
    "            else:\n",
    "                return {pdb_file_name: self._non_NMR_structures(path=path, letterdict=letterdict)}\n",
    "\n",
    "            #now we remove the original file.\n",
    "            os.remove(path)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(\"process pdb did not work\")\n",
    "            print(error)\n",
    "            return {}\n",
    "    \n",
    "    #helper function for XRAY and CRYO-EM ensembles.\n",
    "    def _non_NMR_structures(self, path:str, letterdict:dict):\n",
    "    \n",
    "        \"\"\"This function takes in the the pdb file that is xray or cryoem and rechains each chain. \n",
    "        Additionally, we merge the new labelled chains into a merged_pdb file for further use.\"\"\"\n",
    "    \n",
    "        #store base dir.\n",
    "        base_dir = os.path.dirname(path)\n",
    "        pdb_name = os.path.basename(path)[:4]\n",
    "        #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95.pdb\n",
    "        pdb_file = atomium.open(path)\n",
    "        assemblies = [pdb_file.generate_assembly(n + 1) for n in range(len(pdb_file.assemblies))]\n",
    "        #we take the first one(this is the biological unit built from the asymmetric unit)\n",
    "        assembly = assemblies[0]\n",
    "        #for logger purpose\n",
    "        #assembly_info = f\"{pdb_name=}, {assemblies=}\"\n",
    "        #self._logger(assembly_info)        \n",
    "        #tuple containing chain ID and LEN of each chain.\n",
    "        seq_chains = [(chain.id, len(chain.sequence)) for chain in assembly.chains()]\n",
    "        sorted_lens = sorted(seq_chains, key= lambda x: x[1], reverse=True) #reverse = true :largest first.\n",
    "        accepted_chains = []  #this will be used to store and evalute oligomeric state.\n",
    "        min_accepted_length = float('inf')   # Minimum length of accepted chains init as pos inf.\n",
    "\n",
    "        for chain, length in seq_chains:\n",
    "            #for each chain and length.\n",
    "            if not accepted_chains or length > 0.8 * min_accepted_length:\n",
    "                #we accept it if either we have no other chain so far OR length > 80% of the chain we have so far.\n",
    "                accepted_chains.append(chain)\n",
    "                #first chain will be accepted and then will be the standard for the next chains to follow.\n",
    "                min_accepted_length = min(min_accepted_length, length)\n",
    "\n",
    "        oligostate = len(accepted_chains)  #this excludes small peptides ect from being mistaken as oligomers.\n",
    "        \"\"\"Part 2. We investigate oligomeric state.\"\"\"\n",
    "        accepted_chains_set = set(accepted_chains)\n",
    "        oligomeric_status = None\n",
    "        if len(accepted_chains_set) != len(accepted_chains):\n",
    "            #this means we have a homo-oligomer!\n",
    "            #e.g A vs A A A .. len(1) != len(0)\n",
    "            #hetero-mers are not caught here.. A B C == A B C == len(3)\n",
    "            oligomeric_status = \"homo_oligomer\"\n",
    "        \n",
    "        elif len(accepted_chains) == 1:\n",
    "            #this means we deal with a monomer.\n",
    "            oligomeric_status = \"monomer\"\n",
    "        \n",
    "        elif len(accepted_chains) > 1 and len(accepted_chains) == len(accepted_chains_set):\n",
    "            #this means its a mixed heteromer. becaue len(1) > AND set == list aka no redundancy ergo heteromer.\n",
    "            oligomeric_status = \"hetero_oligomer\"\n",
    "\n",
    "        \"\"\"Part 3: We follow through and now save individual chains + send them to proper rechaining. \"\"\"    \n",
    "        path_list = []\n",
    "\n",
    "        for idx, chain in enumerate(assembly.chains()):\n",
    "            chain_label = chain.id\n",
    "            if chain_label in accepted_chains_set:\n",
    "                path_to_pdb = f\"{self.work_dir}/{pdb_name}_{idx}.pdb\"\n",
    "                #save it here.\n",
    "                path_list.append(path_to_pdb)\n",
    "                #and also save the structure in its wrong chain state first.\n",
    "                chain.save(path_to_pdb)\n",
    "        \n",
    "            path_list = sorted(path_list, key=lambda x: int(x[-5]))\n",
    "    \n",
    "        \"\"\"Part 4: We now deal with all kind of oligomers, and also save all single chains in the procedure.\n",
    "        Normal monomers are also simply saved and rechained. Everything according to a general schema for efficient\n",
    "        downstream processing.\"\"\"\n",
    "\n",
    "\n",
    "        self._merge_pdb_chains(path_list, pdb_name=pdb_name, oligomeric_status=oligomeric_status, \n",
    "                      letterdict=letterdict, accepted_chains = accepted_chains, accepted_chains_set=accepted_chains_set)\n",
    "        \n",
    "        #we return the oligostate of this file and merge it into dict as return value.\n",
    "        return oligostate\n",
    "\n",
    "    #helper function for NMR ensembles.\n",
    "    def _NMR_ensemble(self, path:str, letterdict:dict):\n",
    "\n",
    "        \"\"\"This function takes in the NMR ensemble and \n",
    "        splits each state into a respective PDB file.\"\"\"\n",
    "    \n",
    "        #open the pdb file\n",
    "        print(f\"we currently open with atomium: {path}\")\n",
    "        pdb_name = os.path.basename(path)[:4] #4 digit ID\n",
    "        base_dir = os.path.dirname(path) #base dir name\n",
    "        pdb_file = atomium.open(path)\n",
    "        oligostate = 1 #default initialize\n",
    "        path_list = []\n",
    "    \n",
    "        for i, model in enumerate(pdb1.models):\n",
    "            #now iterate through each model and its respective chains.\n",
    "            chain_len = len([x.id for x in model.chains()]) > 1 # True if multiple chains.\n",
    "            #if larger than 1 : we need to merge.\n",
    "            chain_paths = []\n",
    "            new_chains = []\n",
    "            \n",
    "            for j, chain in enumerate(model.chains()): #enumerate because there are NMR models with MULTIPLE CHAINS\n",
    "                #here we save the structure. as number.. we need to check the chain id.\n",
    "                new_chain = chain.copy(id=letterdict[j]) #new chain ID.\n",
    "                #this effectively rechained the chain.\n",
    "                save_location = f\"{base_dir}/{pdb_name}_{i}_{letterdict[j]}.pdb\"\n",
    "                new_chain.save(save_location) # e.g 4ND5_0_A.pdb 4ND5_1_A etc..\n",
    "                  \n",
    "                if chain_len:\n",
    "                    chain_paths.append(save_location)\n",
    "                    new_chains.append(new_chain.id)\n",
    "        \n",
    "            #if done: check if there are multiple chains. if yes. merge.\n",
    "            if chain_len:\n",
    "                oligostate = len(new_chains)\n",
    "                #we merge the chains.\n",
    "                save_nmr_oligomer = f\"{base_dir}/{pdb_name}_{i}_{''.join(new_chains)}.pdb\"\n",
    "                merge_command = f\"python {self.script_dir}/pdb_merge.py {' '.join(chain_paths)}\"\n",
    "                merge_command_rdy = merge_command.split()\n",
    "                merge_output_file = f\"{save_nmr_oligomer}_tmp.pdb\"  #tmp\n",
    "    \n",
    "                with open(merge_output_file, \"w\") as fh_out:\n",
    "                    result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "                # Run tidy on the merged PDB\n",
    "                tidy_command = f\"python {self.script_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "                tidy_command_rdy = tidy_command.split()\n",
    "                tidy_output_file = f\"{save_nmr_oligomer}.pdb\"\n",
    "    \n",
    "                with open(tidy_output_file, \"w\") as fh_out2:\n",
    "                    results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "                #we remove tmp intermediate files.\n",
    "                if self.remove_intermediates:\n",
    "                    #print(\"we remove\", merge_output_file)\n",
    "                    os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "\n",
    "        return oligostate\n",
    "    \n",
    "    def _merge_pdb_chains(self, path_list:list, pdb_name:str, oligomeric_status:str, letterdict:dict,\n",
    "                     accepted_chains:list, accepted_chains_set:set):\n",
    "\n",
    "        if oligomeric_status == \"homo_oligomer\":\n",
    "            self._pure_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "            \n",
    "        elif oligomeric_status == \"hetero_oligomer\":\n",
    "            self._mixed_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name,\n",
    "                                  accepted_chains=accepted_chains, accepted_chains_set=accepted_chains_set)\n",
    "            \n",
    "        elif oligomeric_status == \"monomer\":\n",
    "            self._monomeric_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "            \n",
    "        else:\n",
    "            print(f\"There was an issue with: {oligomeric_status=}\")\n",
    "            \n",
    "        return\n",
    "\n",
    "    def _pure_oligomer_rechaining(self, path_list:list, letterdict:dict, pdb_name:str): \n",
    "        #path_list = ['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_0.pdb',\n",
    "        # '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_1.pdb']\n",
    "        directory = os.path.dirname(path_list[0]) #this does not change. so no reason to constantly evaluate it in the loop\n",
    "        #store path to chains here.\n",
    "        lst_to_merge_paths = []\n",
    "        #store lists here for later merge.\n",
    "        chain_lst = []\n",
    "\n",
    "        #remove later downstream the intermediates\n",
    "        paths_to_remove = []\n",
    "        \n",
    "        for path_to_pdb in path_list:\n",
    "            \n",
    "            filename = os.path.basename(path_to_pdb) #same as above.\n",
    "            new_chain_digit = filename[5]  # Get the single chain id (e.g., '0')\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            individual_structure = parser.get_structure(\"default\", path_to_pdb)\n",
    "            new_chain = letterdict[int(new_chain_digit)]\n",
    "\n",
    "            for models in individual_structure:\n",
    "                for chain in models:\n",
    "                    if chain.id == new_chain:\n",
    "                        #then we simply save it under its original chain.\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        chain_lst.append(chain.id)\n",
    "                        io.save(save_location)\n",
    "                        paths_to_remove.append(save_location)\n",
    "                    \n",
    "                    else:\n",
    "                        chain.id = new_chain\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        chain_lst.append(chain.id)\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        io.save(save_location)\n",
    "                        paths_to_remove.append(save_location)\n",
    "\n",
    "\n",
    "        #prepare subprocess for pdb_merge.py\n",
    "        merge_command = f\"python {self.script_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "        merge_command_rdy = merge_command.split()\n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}_tmp.pdb\"  #tmp\n",
    "\n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        # prepare subprocess for pdb_tidy.py\n",
    "        tidy_command = f\"python {self.script_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}.pdb\"\n",
    "    \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "        #lets remove artifacts if the user wishes\n",
    "        if self.remove_intermediates:\n",
    "            for path_to_pdb in paths_to_remove:\n",
    "                try:\n",
    "                    os.remove(path_to_pdb)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing file {path_to_pdb}: {e}\")\n",
    "            for path_to_pdb_num in path_list:\n",
    "                try:\n",
    "                    os.remove(path_to_pdb_num)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing file {path_to_pdb_num}: {e}\")\n",
    "\n",
    "    def _mixed_oligomer_rechaining(self, accepted_chains:list,\n",
    "                               accepted_chains_set:set,\n",
    "                               path_list:list,\n",
    "                               letterdict:dict, pdb_name:str):\n",
    "\n",
    "        seen_chains = sorted(accepted_chains, reverse=False)\n",
    "        chain_seq_len = len(seen_chains) #e.g 6\n",
    "        shift = len(accepted_chains_set) # e.g 3\n",
    "        blocksize = chain_seq_len // shift # e.g 2\n",
    "        block_count = int(chain_seq_len/blocksize) \n",
    "        '''\n",
    "        # A A B B C C becomes A D B E C F\n",
    "        # B B C C becomes A C B D \n",
    "        # i = 1\n",
    "        # A D \n",
    "        # block 1 2 3 for A A B B C C \n",
    "        # 0 2 1 3\n",
    "        '''\n",
    "        j = 0\n",
    "        new_chain_seq = []\n",
    "        lst_to_merge_paths = []\n",
    "        paths_to_remove = []\n",
    "        \n",
    "        for blocks in range(0, block_count):\n",
    "            for i in range(0, blocksize):\n",
    "                '''\n",
    "                # first iteration A D\n",
    "                # second iteration B E\n",
    "                # third iteration C F\n",
    "                '''\n",
    "                new_chain = letterdict[blocks+i*shift]\n",
    "                new_chain_seq.append(new_chain)\n",
    "                path_to_pdb = path_list[j]\n",
    "                directory = os.path.dirname(path_to_pdb)\n",
    "                j += 1\n",
    "                parser = PDBParser(QUIET=True)\n",
    "                prot_name = f\"default\"\n",
    "                #open the correct pdb and rechain it.\n",
    "                structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "                \n",
    "                for models in structure_template:\n",
    "                    for chain in models:\n",
    "                        if chain.id != new_chain:\n",
    "                            chain.id = new_chain\n",
    "                \n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        #print(f\"This is single chain save inside oligomer rechain: {directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        io.save(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        paths_to_remove.append(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "\n",
    "        # Prepare subprocess for pdb_merge.py\n",
    "        merge_command = f\"python {self.script_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "        merge_command_rdy = merge_command.split()\n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}_tmp.pdb\"  #tmp\n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        # Prepare subprocess for pdb_tidy.py\n",
    "        tidy_command = f\"python {self.script_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}.pdb\"\n",
    "        \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.  \n",
    "        #lets remove artifacts if the user wishes\n",
    "        if self.remove_intermediates:\n",
    "            for path_to_pdb in paths_to_remove:\n",
    "                try:\n",
    "                    os.remove(path_to_pdb)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing file {path_to_pdb}: {e}\")\n",
    "            for path_to_pdb_num in path_list:\n",
    "                try:\n",
    "                    os.remove(path_to_pdb_num)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing file {path_to_pdb_num}: {e}\")\n",
    "\n",
    "\n",
    "    def _monomeric_rechaining(self, path_list:list,\n",
    "                          letterdict:dict,\n",
    "                          pdb_name:str):\n",
    "\n",
    "        #path_list=['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2q9p_0.pdb'], \n",
    "        #letterdict={0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K',\n",
    "        #11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21:\n",
    "        #'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}, \n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = \"default\"\n",
    "        #We only have 1 path in this list.\n",
    "        pdb = path_list[0]\n",
    "        # Open the correct PDB and rechain it.\n",
    "        dir_name = os.path.dirname(pdb)\n",
    "        structure_template = parser.get_structure(prot_name, pdb) \n",
    "        # Get the new chain ID\n",
    "        new_chain = \"A\"  #always... A\n",
    "        for model in structure_template:\n",
    "            for original_chain in model:\n",
    "                if original_chain.id != new_chain:\n",
    "                    original_chain.id = new_chain\n",
    "        save_path = os.path.join(dir_name, f\"{pdb_name}_{new_chain}.pdb\")\n",
    "        #print(f\"This is save path: {save_path=}\")\n",
    "        # Save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure_template)\n",
    "        #print(\"we save now:\")\n",
    "        io.save(save_path)\n",
    "        if self.remove_intermediates:\n",
    "            #print(\"we remove\", pdb)\n",
    "            os.remove(pdb)\n",
    "    \n",
    "    def create_domain_boundaries(self, save=True):\n",
    "        '''\n",
    "        Process PDB data by getting real ranges and then splitting domains.\n",
    "        This is the single entry point for users to execute the workflow.\n",
    "        '''\n",
    "        \n",
    "        # Step 1: Get Real Ranges\n",
    "        self.range_dict = self._get_real_ranges()\n",
    "        \n",
    "        #print(f\"{self.range_dict=}\")  #keys : oligostate , val: tuple (path, minresnum, maxresnum)\n",
    "        # Step 2: Split Domains\n",
    "        # Ensure that range_dict is not None or handle the case if it is\n",
    "        if self.range_dict is not None:\n",
    "\n",
    "            for oligostate, path_min_max_lst in self.range_dict.items():\n",
    "                if oligostate == \"1\":\n",
    "                    #path min max is a list of tuples (path, min , max)\n",
    "                    self.established_domain_dict[oligostate] = self._split_domains_pdb(path_min_max_lst)\n",
    "                else:\n",
    "                    self.established_domain_dict[oligostate] = self._split_domains_pdb(path_min_max_lst)\n",
    "            \n",
    "            #logging\n",
    "            #if logging:\n",
    "            #    outp = os.path.join(self.log_dir, \"established_domains.json\")\n",
    "            #    converted_dict = self._convert_keys_to_string(self.established_domain_dict)\n",
    "            #    self._logging(outp, converted_dict)\n",
    "            \n",
    "        else:\n",
    "            print(\"Range dictionary is empty. Cannot proceed with splitting domains.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _get_real_ranges(self):\n",
    "        '''\n",
    "        Here we try to find out if the protein of interest is a multi-domain or single domain protein.\n",
    "        If there are many structures that consist of multiple different ranges that might or might not overlap,\n",
    "        we will end up with N domains corresponding to K ranges where K can be larger than N (if the domain itself has many different ranges)\n",
    "        '''\n",
    "        real_range_dict = defaultdict(list)\n",
    "        pdbs = [os.path.join(self.work_dir, k) for k in self.oligodict]\n",
    "\n",
    "        \"\"\"\n",
    "        self.oligodict={'5ltu_AB.pdb': [('A', 0.96), ('B', 0.96)], '7nnj_A.pdb': [('A', 0.956)], '2duk_A.pdb': [('A', 0.967)], \n",
    "        '3mcf_A.pdb': [('A', 0.952)], '7tn4_A.pdb': [('A', 0.787)], '2q9p_A.pdb': [('A', 0.787)], '2fvv_A.pdb': [('A', 0.787)], \n",
    "        '6pck_A.pdb': [('A', 0.81)], '6pcl_A.pdb': [('A', 0.81)], '6wo7_A.pdb': [('A', 0.81)], '6wo8_A.pdb': [('A', 0.81)], \n",
    "        '6wo9_A.pdb': [('A', 0.81)], '6woa_A.pdb': [('A', 0.81)], '6wob_A.pdb': [('A', 0.81)], '6woc_A.pdb': [('A', 0.81)], \n",
    "        '6wod_A.pdb': [('A', 0.81)], '6woe_A.pdb': [('A', 0.81)], '6wof_A.pdb': [('A', 0.81)], '6wog_A.pdb': [('A', 0.81)], \n",
    "        '6woh_A.pdb': [('A', 0.81)], '6woi_A.pdb': [('A', 0.81)], '7aut_A.pdb': [('A', 0.4)], '7aui_A.pdb': [('A', 0.395)],\n",
    "        '7auk_A.pdb': [('A', 0.395)], '7aul_A.pdb': [('A', 0.395)], '7aum_A.pdb': [('A', 0.395)], '7aun_A.pdb': [('A', 0.395)], \n",
    "        '7auo_A.pdb': [('A', 0.395)], '7aup_A.pdb': [('A', 0.395)], '7auq_A.pdb': [('A', 0.395)], '7aur_A.pdb': [('A', 0.395)], \n",
    "        '7aus_A.pdb': [('A', 0.395)], '7auu_A.pdb': [('A', 0.419)], '7auj_A.pdb': [('A', 0.388)], '3h95_AB.pdb': [('A', 0.295)], \n",
    "        '3i7u_ABCD.pdb': [('A', 0.438), ('B', 0.438), ('C', 0.438), ('D', 0.438)], '3i7v_A.pdb': [('A', 0.438)], '4hfq_AB.pdb': [('A', 0.355), ('B', 0.355)]}\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = list(executor.map(self._get_range_parallelized, pdbs))\n",
    "        # Process the results\n",
    "        for hit in results:\n",
    "            #hit=[('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/5ltu_AB.pdb', 'AB', 1, 257)]\n",
    "            #hit=[('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/7nnj_A.pdb', 'A', 9, 146)]\n",
    "            #hit=[('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/2duk_A.pdb', 'A', 1, 138)]\n",
    "            for pdb_path, chains, min_num, max_num in hit:\n",
    "                real_range_dict[str(len(chains))].append((pdb_path, min_num, max_num))\n",
    "\n",
    "        #print(f\"{real_range_dict=}\")\n",
    "        return real_range_dict\n",
    "    \n",
    "    def _get_range_parallelized(self, pdb_path:str):\n",
    "        \"\"\"\n",
    "        This function gets called by get_real_ranges and leverages parallelization \n",
    "        to speed up domain boundary computations\n",
    "        \"\"\"\n",
    "        hits = []\n",
    "        try:\n",
    "            parser = PDBParser()\n",
    "            structure = parser.get_structure(\"none\", pdb_path)\n",
    "            chains_residues = []\n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    chain_id = chain.id\n",
    "                    residues_in_chain = [res for res in chain if res.get_id()[0] == \" \"]\n",
    "                    result = [res.get_id()[1] for res in residues_in_chain]\n",
    "                    chains_residues.append((chain_id, result))\n",
    "    \n",
    "            # Now we have all chains and their ids\n",
    "            if len(chains_residues) == 1:  # Monomer\n",
    "                for chain_id, resids in chains_residues:\n",
    "                    min_num, max_num = min(resids), max(resids)\n",
    "                    hits.append((pdb_path, chain_id, min_num, max_num))\n",
    "            else:  # Oligomer or other\n",
    "                chain_names = ''\n",
    "                total_length = 0\n",
    "                for chain_id, resids in chains_residues:\n",
    "                    chain_names += chain_id\n",
    "                    total_length += len(resids)\n",
    "                    \n",
    "                hits.append((pdb_path, chain_names, 1, total_length))\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdb_path}: {e}\")\n",
    "\n",
    "        return hits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # SPLIT DOMAINS NEED OVERHAUL NOW.\n",
    "    # WE WANT TO END UP WITH SEPARATE LENGTHS FOR EACH OLIGOMER.\n",
    "    def _split_domains_pdb(self, range_lst):\n",
    "        '''\n",
    "        Function to split domains based on the ranges obtained in the previous step.\n",
    "        '''\n",
    "        # lets first do a quick merge\n",
    "        range_dict = {tup[0]: (tup[1],tup[2]) for tup in range_lst} #establish dict.\n",
    "        merged_dict = self._merge_paths_within_interval(range_dict)\n",
    "        #print(f\"This is merged_dict_length: {len(merged_dict.values())}\")\n",
    "        #now we start iterative merging of overlaps.\n",
    "        domain_dict = self._make_groups_iter_pdb(merged_dict, num_of_iterations=0 , break_point=10)\n",
    "        #print(f\"This is domain_dict_length after make_groups_iter: {len(domain_dict.values())}\")\n",
    "        cleaned_dict = defaultdict()\n",
    "    \n",
    "        for ranges, twisted_path_lists in domain_dict.items():\n",
    "            #now we need to flatten domain_dict\n",
    "            flattened_data = self._flatten_nested_lists(twisted_path_lists)\n",
    "            #print(flattened_data)\n",
    "            # Use a set to remove duplicates\n",
    "            unique_flattened_data = set(flattened_data)\n",
    "            unique_flattened_list = list(unique_flattened_data)\n",
    "            #this works.\n",
    "            cleaned_dict[ranges] = unique_flattened_list\n",
    "\n",
    "        prot_length = 10 #len(main_prot_seq) test purpose\n",
    "\n",
    "        #lets try this first.\n",
    "        if self.main_protein_sequence: # this case we can handle by fetching the uniprot id and the associated structure.\n",
    "            if len(self.main_protein_sequence) > 300:\n",
    "                tolerance = 0.3 * prot_length\n",
    "            else:\n",
    "                tolerance = 80\n",
    "        else:\n",
    "            tolerance = 80\n",
    "            \n",
    "        #print(f\"This is cleaned_dict_length before merge_overlapping_int: {len(cleaned_dict.values())}\")\n",
    "        domain_dict = self._merge_overlapping_intervals(cleaned_dict, tolerance=tolerance)\n",
    "    \n",
    "        #print(f\"This is cleaned_dict_length after merge_overlapping_int: {len(domain_dict.values())}\")\n",
    "        return domain_dict\n",
    "\n",
    "    def _make_groups_iter_pdb(self, merged_dict:dict, num_of_iterations, break_point):\n",
    "        #print(f\"This is iteration : {num_of_iterations}\")\n",
    "        while True:\n",
    "            if num_of_iterations > break_point:\n",
    "                 #that means we cant progress.\n",
    "                 break\n",
    "            #print(f\"iteration number: {num_of_iterations}\")\n",
    "            if len(merged_dict) == 1:\n",
    "                return merged_dict\n",
    "    \n",
    "            num_of_iterations += 1\n",
    "            ranges_1 = []\n",
    "            ranges_2 = []\n",
    "    \n",
    "            #attach key ranges to both lists\n",
    "            for keys, vals in merged_dict.items():\n",
    "                ranges_1.append(keys)\n",
    "                ranges_2.append(keys)\n",
    "        \n",
    "            union_dict = defaultdict(list)        \n",
    "            \n",
    "            for range_to_check in ranges_1:  #lets parse through the ranges.\n",
    "                union_new = False  #default false for start.\n",
    "                for range_to_compare in ranges_2: #we compare our hit against all potential ranges.\n",
    "                    if range_to_check != range_to_compare:  #means they are different.\n",
    "                        \n",
    "                        range_to_check_paths = merged_dict[range_to_check]\n",
    "                        range_to_comp_paths = merged_dict[range_to_compare]\n",
    "                        \n",
    "                        range_abs_check = np.abs(int(range_to_check[1])-int(range_to_check[0]))\n",
    "                        range_abs_compare = np.abs(int(range_to_compare[1])-int(range_to_compare[0]))\n",
    "    \n",
    "                        intersect_between_both = self._get_intersect(range_to_check, range_to_compare)# gets abs length of intersect.\n",
    "                        #range abs check and range abs comp are integers. \n",
    "                        #intersect is int and corresponds to length between intersection of both.\n",
    "                        #cond 1: if the intersection between the 2 ranges is LESS than the absolute length of range to check and \n",
    "                        #the intersection is also GREATER than 80% of the first interval.\n",
    "                        condition_1 = intersect_between_both <= range_abs_check and intersect_between_both >= 0.8* range_abs_check  \n",
    "                        #cond 2: if the intersection between the 2 ranges is LESS than the absolute length of range to compare and \n",
    "                        #the intersection is also GREATER than 80% of the 2nd interval.\n",
    "                        condition_2 = intersect_between_both <= range_abs_compare and intersect_between_both >= 0.8* range_abs_compare\n",
    "                        #this means the intersect is less than the original range and the intersect is also larger than 80% of the orignal range.\n",
    "                        #as well as the same for the second range to compare.\n",
    "                        if condition_1 and condition_2:\n",
    "                            union_new = self._merge_into_union(range_to_check, range_to_compare)\n",
    "    \n",
    "                if union_new:\n",
    "                    if union_new in union_dict:\n",
    "                        union_dict[union_new].append(range_to_check_paths + range_to_comp_paths)\n",
    "                    else:\n",
    "                        union_dict[union_new] = range_to_check_paths + range_to_comp_paths\n",
    "    \n",
    "                else:\n",
    "                    union_dict[range_to_check].extend(range_to_check_paths)\n",
    "                \n",
    "            merged_dict = union_dict\n",
    "            merged_dict = self._make_groups_iter_pdb(merged_dict, num_of_iterations, break_point)\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "    def _get_intersect(self, range_to_check, range_to_compare):\n",
    "\n",
    "        # range_to_check = (start, stop)\n",
    "        # range_to_compare = (start, stop)\n",
    "        start_check = int(range_to_check[0])\n",
    "        stop_check = int(range_to_check[1])\n",
    "    \n",
    "        start_comp = int(range_to_compare[0])\n",
    "        stop_comp = int(range_to_compare[1])\n",
    "        \n",
    "        full_range_to_check = set([x for x in range(start_check, stop_check+1)])\n",
    "        full_range_to_compare = set([x for x in range(start_comp, stop_comp+1)])\n",
    "    \n",
    "        #now lets grab set for both\n",
    "        intersection = full_range_to_check.intersection(full_range_to_compare)\n",
    "        #we are interested in the length of the intersect\n",
    "        return len(intersection)\n",
    "\n",
    "    def _merge_dicts_range_seqid(self, range_dict, seqid_dict):\n",
    "        \n",
    "        merged_dict = {}\n",
    "\n",
    "        for range_key, pdb_entries in range_dict.items():\n",
    "            for pdb_path, chain_group in pdb_entries:\n",
    "                pdb_file = os.path.basename(pdb_path)  # Extracts filename from the full path\n",
    "    \n",
    "                # Initialize a dict to hold (chain_id, seq_id) pairs, ensuring uniqueness\n",
    "                chain_seqid_dict = {}\n",
    "    \n",
    "                if pdb_file in seqid_dict:\n",
    "                    # Extract expected chains from the filename (e.g., 'AB' from '5ltu_AB.pdb')\n",
    "                    expected_chains = pdb_file.rstrip('.pdb').split('_')[-1]\n",
    "                    \n",
    "                    # First, fill in the sequence identities for chains that have data\n",
    "                    for chain_id, seq_id in seqid_dict[pdb_file]:\n",
    "                        chain_seqid_dict[chain_id] = seq_id\n",
    "                    \n",
    "                    # If any expected chains are missing data, duplicate from the first available chain\n",
    "                    if len(chain_seqid_dict) < len(expected_chains):\n",
    "                        # Assume the first chain's sequence identity for missing chains\n",
    "                        default_seq_id = next(iter(chain_seqid_dict.values()))\n",
    "                        for chain_id in expected_chains:\n",
    "                            if chain_id not in chain_seqid_dict:\n",
    "                                chain_seqid_dict[chain_id] = default_seq_id\n",
    "                    \n",
    "                    # Convert the dict back to a list of tuples and add to merged_dict\n",
    "                    chain_seqid_list = list(chain_seqid_dict.items())\n",
    "                    merged_dict[pdb_path] = chain_seqid_list\n",
    "                else:\n",
    "                    print(f\"Warning: No seqid found for {pdb_file}\")\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _merge_into_union(self, range_to_check, range_to_compare):\n",
    "\n",
    "        start_check = int(range_to_check[0])\n",
    "        stop_check = int(range_to_check[1])\n",
    "    \n",
    "        start_comp = int(range_to_compare[0])\n",
    "        stop_comp = int(range_to_compare[1])\n",
    "    \n",
    "        full_range_to_check = set([x for x in range(start_check, stop_check+1)])\n",
    "        full_range_to_compare = set([x for x in range(start_comp, stop_comp+1)])\n",
    "    \n",
    "        union_merge = full_range_to_check.union(full_range_to_compare)\n",
    "        \n",
    "        sorted_union = sorted(union_merge)\n",
    "        result = (sorted_union[0], sorted_union[-1])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def _merge_overlapping_intervals(self, path_interval_dict, tolerance):\n",
    "\n",
    "        #input: key: ranges values = list of paths.\n",
    "        intervals = list(path_interval_dict.keys())\n",
    "    \n",
    "        #[(5, 500), (1, 507), (3, 507), (1, 535), (8, 500), (4, 507)]\n",
    "        #print(f\"these are the intervals we deal with in merging: {intervals}\")\n",
    "        merged_intervals = self._merge_intervals(intervals, tolerance)\n",
    "    \n",
    "        merged_dict = {}\n",
    "        #print(f\"this is path_interval_dict : {path_interval_dict}\")\n",
    "        for merged_interval in merged_intervals:\n",
    "            merged_paths = []\n",
    "                \n",
    "            for interval, path_chain_list in path_interval_dict.items():\n",
    "                if self._is_within_tolerance(merged_interval, interval, tolerance):\n",
    "                    #print(f\"{path_chain_list=}\")\n",
    "                    for path, chain in path_chain_list: # list of lists consisting of tuples a (path / chain)\n",
    "                        merged_paths.append((path, chain))\n",
    "            merged_dict[merged_interval] = merged_paths\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _merge_intervals(self, intervals, tolerance):\n",
    "\n",
    "        merged = [intervals[0]]  # Initialize with the first interval\n",
    "        for start, end in intervals[1:]:\n",
    "            merged_interval = None\n",
    "    \n",
    "            for i, (merged_start, merged_end) in enumerate(merged):\n",
    "                if abs(start - merged_start) <= tolerance and abs(end - merged_end) <= tolerance:\n",
    "                    # Merge the interval into the existing one\n",
    "                    merged_interval = (min(start, merged_start), max(end, merged_end))\n",
    "                    merged[i] = merged_interval\n",
    "                    break\n",
    "    \n",
    "            if merged_interval is None:\n",
    "                # No suitable merged interval found, create a new one\n",
    "                merged.append((start, end))\n",
    "    \n",
    "        return merged\n",
    "\n",
    "\n",
    "    def _merge_paths_within_interval(self, path_start_stop_dict):\n",
    "        \n",
    "        merged_dict = {}  # Create a new dictionary to store merged paths\n",
    "        #print(f\"{path_start_stop_dict=}\")\n",
    "        for pdb_path, (start, stop) in path_start_stop_dict.items():\n",
    "            pdb_location, chain = pdb_path, os.path.basename(pdb_path)[5:-4]  #location + chain\n",
    "            #print(f\"{pdb_location=}, {chain=}\")\n",
    "            interval = (start, stop)\n",
    "            \n",
    "            if interval in merged_dict:\n",
    "                merged_dict[interval].append((pdb_path, chain))\n",
    "            else:\n",
    "                merged_dict[interval] = [(pdb_path, chain)]\n",
    "            \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _flatten_nested_lists(self, lst):\n",
    "        flattened = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(self._flatten_nested_lists(item))  #recursive call\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "\n",
    "    def _is_within_tolerance(self, interval1, interval2, tolerance):\n",
    "    \n",
    "        # Calculate the differences in starts and stops for both intervals\n",
    "        start_diff = abs(interval1[0] - interval2[0])\n",
    "        stop_diff = abs(interval1[1] - interval2[1])\n",
    "        # Check if both differences are within the tolerance\n",
    "        return start_diff <= tolerance and stop_diff <= tolerance\n",
    "\n",
    "\n",
    "    def _save_human_readable(self, save=False):\n",
    "        #helper function to save the result in human readable format.\n",
    "        with open(os.path.join(self.log_dir, \"domain_boundaries.txt\"), \"w\") as db_out:\n",
    "            for keys, vals in self.established_domain_dict.items():\n",
    "                db_out.write(str(keys))\n",
    "                db_out.write(\":\")\n",
    "                db_out.write(\"\\n\")\n",
    "                for pdb in vals:\n",
    "                    pdb_name = os.path.basename(pdb[0])[:4]\n",
    "                    pdb_chain = pdb[1]\n",
    "                    db_out.write(pdb_name)\n",
    "                    db_out.write(\"_\")\n",
    "                    db_out.write(pdb_chain)\n",
    "                    db_out.write(\"\\n\")\n",
    "\n",
    "\n",
    "    #{'5ltu_AB.pdb': [('A', 0.96), ('B', 0.96)], is oligodict\n",
    "\n",
    "    def get_oligostates(self, num_most_common_oligostates=2):\n",
    "        \n",
    "        grouped_dict = self.established_domain_dict  # Assuming this is already populated\n",
    "\n",
    "        oligostate_paths_with_seq_id = defaultdict(lambda: defaultdict(list))\n",
    "        oligostate_counter = Counter()\n",
    "\n",
    "        # Count occurrences for each oligostate\n",
    "        for oligo, range_path_chain in grouped_dict.items():\n",
    "            for range, struc_paths in range_path_chain.items():\n",
    "                oligostate_counter[oligo] += len(struc_paths)\n",
    "            \n",
    "        # Get the top oligostates\n",
    "        print(oligostate_counter)\n",
    "        top_oligostates = oligostate_counter.most_common(num_most_common_oligostates)\n",
    "        top_oligostate_set = set(oligo for oligo, _ in top_oligostates)\n",
    "\n",
    "        # Map sequence identities and calculate averages\n",
    "        for oligo, range_path_chain in grouped_dict.items():\n",
    "            print(f\"{oligo=}, {range_path_chain=}\")\n",
    "            if oligo in top_oligostate_set:\n",
    "                for range_key, paths in range_path_chain.items():\n",
    "                    for path, chain_group in paths:\n",
    "                        pdb_file = path.split('/')[-1]  # Extract filename\n",
    "                        chain_ids = chain_group  # Extract chain IDs (e.g., 'AB')\n",
    "\n",
    "                        # Initialize a list to store sequence identities for the current path\n",
    "                        seq_ids = []\n",
    "\n",
    "                        # Iterate over each chain ID and fetch the corresponding sequence identity\n",
    "                        for chain_id in chain_ids:\n",
    "                            if pdb_file in self.oligodict:\n",
    "                                for chain_seq_id in self.oligodict[pdb_file]:\n",
    "                                    if chain_seq_id[0] == chain_id:\n",
    "                                        seq_ids.append(chain_seq_id[1])\n",
    "\n",
    "                        # Calculate the average sequence identity if seq_ids is not empty\n",
    "                        avg_seq_id = np.mean(seq_ids) if seq_ids else None\n",
    "\n",
    "                        # Append the path along with its average sequence identity\n",
    "                        oligostate_paths_with_seq_id[oligo][range_key].append((path, avg_seq_id))\n",
    "\n",
    "        self.most_common_filtered_oligostates = top_oligostates\n",
    "        self.oligostates_filtered_paths_with_seq_id = oligostate_paths_with_seq_id\n",
    "\n",
    "        # Print or return the results as needed\n",
    "        #print(f\"{self.most_common_filtered_oligostates=}, {self.oligostates_filtered_paths_with_seq_id=}\")\n",
    "\n",
    "        #print(f\"{self.most_common_filtered_oligostates=}, {self.oligostates_filtered_paths=}\")\n",
    "        # Display the major oligostates and their entries\n",
    "        if self.logging:\n",
    "            outp = os.path.join(self.log_dir, \"filtered_oligostates.json\")\n",
    "            self._logging(outp, dict_to_write=self.oligostates_filtered_paths_with_seq_id)\n",
    "\n",
    "        # Optionally, return the major oligostates and their entries if needed for further processing\n",
    "        #return most_common_oligostates, oligostate_paths\n",
    "\n",
    "        #lets fetch now for suitable templates for each of these states.\n",
    "        self._process_templates()\n",
    "\n",
    "    \n",
    "    def _process_templates(self):\n",
    "        #first we split into groups based on boundaries\n",
    "        if not self.oligostates_filtered_paths_with_seq_id:\n",
    "            print(\"we have no self.oligostates_filtered_paths\")\n",
    "            return\n",
    "\n",
    "        oligostate_dict = {}\n",
    "\n",
    "        for oligo, range_path_seqid in self.oligostates_filtered_paths_with_seq_id.items():\n",
    "            if oligo not in oligostate_dict:\n",
    "                oligostate_dict[oligo] = {}  # Initialize a new dictionary for this oligo\n",
    "    \n",
    "            for range_key, path_seqid_list in range_path_seqid.items():\n",
    "                # Sort the list of tuples for each oligostate by seq_id in descending order\n",
    "                sorted_strucs = sorted(path_seqid_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "                # Create a new key based on the range tuple\n",
    "                new_key = f\"{range_key[0]}_{range_key[1]}\"\n",
    "                oligostate_dict[oligo][new_key] = sorted_strucs\n",
    "\n",
    "        self.assembled_structures = oligostate_dict\n",
    "    \n",
    "        self.top_templates = {}\n",
    "    \n",
    "        # Store the first hit of each oligostate for each range\n",
    "        for oligostate, inner_dict in oligostate_dict.items():\n",
    "            if oligostate not in self.top_templates:\n",
    "                self.top_templates[oligostate] = {}  # Initialize if not already present\n",
    "    \n",
    "            for range_key, paths_seq in inner_dict.items():\n",
    "                self.top_templates[oligostate][range_key] = paths_seq[0]  # Take the first template\n",
    "    \n",
    "        # Perform logging if enabled\n",
    "        if self.logging:\n",
    "            outp = os.path.join(self.log_dir, \"templates_oligos.json\")\n",
    "            converted_dict = self._convert_keys_to_string(self.top_templates)\n",
    "            self._logging(outp, converted_dict)\n",
    "\n",
    "\n",
    "    def _split_and_group_by_chain(self, input_dict):\n",
    "        grouped_dict = {}\n",
    "        for interval, pdb_list in input_dict.items():\n",
    "            # Initialize a sub-dictionary for each interval\n",
    "            grouped_dict[interval] = {}\n",
    "\n",
    "            for pdb_path, seq_id in pdb_list:\n",
    "                chain_length = len(os.path.basename(pdb_path)[5:-4])\n",
    "                \n",
    "                if chain_length not in grouped_dict[interval]:\n",
    "                    grouped_dict[interval][chain_length] = []\n",
    "\n",
    "                grouped_dict[interval][chain_length].append((pdb_path, seq_id))\n",
    "        return grouped_dict\n",
    "\n",
    "    \n",
    "    def _logging(self, outp, dict_to_write):\n",
    "        '''Logger to keep track of changes and get infos about potential debugging'''\n",
    "        dict_to_write = self._convert_keys_to_string(dict_to_write)\n",
    "        with open(outp, \"w\") as json_fh:\n",
    "            json.dump(dict_to_write, json_fh, indent=4, default=str)\n",
    "  \n",
    "    def _convert_keys_to_string(self, dictionary):\n",
    "        \"\"\"Converts all the keys of the given dictionary to strings, including nested dictionaries.\"\"\"\n",
    "        converted_dict = {}\n",
    "        for key, value in dictionary.items():\n",
    "            # Convert the key to a string\n",
    "            str_key = '_'.join(map(str, key)) if isinstance(key, tuple) else str(key)\n",
    "    \n",
    "            # If the value is a dictionary, recursively convert its keys\n",
    "            if isinstance(value, dict):\n",
    "                value = self._convert_keys_to_string(value)\n",
    "            elif isinstance(value, list):\n",
    "                # If the value is a list, check if it contains dictionaries and convert their keys\n",
    "                value = [self._convert_keys_to_string(item) if isinstance(item, dict) else item for item in value]\n",
    "    \n",
    "            converted_dict[str_key] = value\n",
    "        return converted_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d423c46-b89b-4f60-bf11-3f41ffafda74",
   "metadata": {},
   "source": [
    "## Execute PDB_download and PDB_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79696b1a-1ab5-4fe1-8262-e42a74c10c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN' already exists.\n",
      "Before applying cutoff: 218 Structures\n",
      "After applying cutoff: 193 Structures\n",
      "This is overlap in the directory: {'7e1z', '5aw6', '4bew', '5avs', '7w7v', '7w47', '2yfy', '2zbd', '5aw8', '3ba6', '5aw9', '3n23', '4h1w', '7ddi', '4ux1', '3w5b', '7wyx', '5aw7', '5a3s', '6ln7', '2o9j', '6yaa', '4uu0', '6jxk', '2xzb', '6zhf', '2zbe', '3tlm', '6jxj', '1iwo', '7e20', '7ny1', '7qtv', '3ar2', '7efl', '2agv', '7wyv', '7e21', '3w5a', '4ycm', '5aw0', '4hqj', '7bt2', '1t5t', '6ln6', '6a69', '6lle', '4hyt', '5avv', '2eat', '4ux2', '5avu', '5ksd', '6yso', '2zxe', '7ddk', '7x21', '3fpb', '6zhg', '5xa7', '7vh6', '5avq', '7vh5', '3ar7', '2eau', '6ln8', '4res', '7w4a', '7wz0', '3b9r', '5zmv', '8d3x', '3n5k', '3n8g', '7wyz', '8d3y', '2by4', '2zbf', '2c8k', '4j2t', '3fps', '6jxi', '3b8e', '7yzr', '1vfp', '5a3q', '3ar5', '5avr', '7x20', '2c8l', '8d3u', '7wyw', '3w5d', '5zmw', '4ret', '7nxf', '4xe5', '5a3r', '7wyy', '5ylv', '3ar9', '7w7t', '1xp5', '8d3w', '7x23', '7ddl', '5ztf', '5avy', '7x22', '8d3v', '7ddf', '3b9b', '3wgu', '3nan', '2yn9', '7d92', '5avw', '5avz', '5mpm', '2c88', '6hef', '2ear', '4ycl', '6ln9', '1kju', '6zhh', '5aw5', '5aw3', '3ar4', '5avx', '7e7s', '4y3u', '3nal', '1wpg', '7ddh', '5aw1', '3a3y', '7d91', '7w7w', '7et1', '6jju', '7efm', '5avt', '6rb2', '1su4', '3ar6', '7wys', '7wyt', '5xa9', '5aw2', '3nam', '4kyt', '5xaa', '5ylu', '7w48', '6ln5', '7w7u', '4nab', '7d94', '3ar3', '3j7t', '4xou', '7efn', '3wgv', '7x24', '7y46', '5xab', '7y45', '6lly', '2dqs', '5ncq', '2c9m', '4ycn', '2voy', '5xa8', '5aw4', '2oa0', '3fgo', '7w49', '2zbg', '6hxb', '4uu1', '7z04', '3ixz', '7wyu', '1mhs', '3kdp', '3w5c', '1t5s', '7d93', '3ar8', '6jxh', '5y0b'}\n",
      "we already have pdbs from the templates downloaded\n",
      "Counter({'1': 195, '2': 8, '6': 3, '4': 1})\n",
      "oligo='1', range_path_chain={(-5, 1063): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3n5k_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar3_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4j2t_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2oa0_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3b9r_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6a69_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7bt2_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4h1w_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6lly_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2o9j_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4bew_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2by4_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4xou_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/1t5s_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ncq_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6ln8_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3nan_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2c9m_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2c8k_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3nam_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7e7s_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4ycn_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6ln6_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6rb2_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5a3r_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6hef_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6yso_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3fpb_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6lle_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3fps_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4ycm_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w7w_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3nal_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/1su4_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2c88_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5mpm_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar8_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar9_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3tlm_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w7v_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3n8g_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4kyt_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w7u_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w7t_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3b9b_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar6_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ba6_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar4_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/1xp5_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6jju_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6ln7_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2c8l_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4y3u_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2yfy_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/1t5t_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6yaa_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6ln9_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3fgo_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ztf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6ln5_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5xa9_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar5_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar2_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3w5c_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5xaa_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5xab_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2zbg_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2zbf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2dqs_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2eat_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3ar7_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5xa8_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3w5d_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2eau_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5xa7_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4uu1_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2zbd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2zbe_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5a3q_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5a3s_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2ear_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4uu0_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4ycl_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3w5b_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5zmw_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4nab_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5zmv_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6zhf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6hxb_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6zhh_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7x22_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7x21_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7x23_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6zhg_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7x24_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7x20_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w48_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7efn_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6jxi_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6jxj_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7efl_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6jxk_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w4a_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ylv_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/6jxh_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w47_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7efm_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7et1_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7w49_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ylu_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7ddk_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4hqj_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7d91_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4res_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7ddf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wyt_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3n23_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3wgu_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4hyt_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7ddl_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4ret_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7d92_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7d93_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7ddi_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wys_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3b8e_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7d94_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3wgv_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7ddh_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3kdp_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/8d3u_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/8d3y_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/8d3v_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/8d3w_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4xe5_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw6_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw5_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avv_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3a3y_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wyy_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw9_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw1_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7e21_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avw_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wz0_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw2_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avr_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avq_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw3_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wyw_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wyz_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avz_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avs_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wyu_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avu_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wyv_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avx_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7e20_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw4_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7wyx_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw0_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw7_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5aw8_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2zxe_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avy_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7e1z_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5avt_A.pdb', 'A')], (12, 920): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/4nab_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7nxf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/5ksd_A.pdb', 'A')]}\n",
      "oligo='2', range_path_chain={(1, 1992): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/2agv_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/1iwo_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3j7t_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/1vfp_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/3w5a_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7qtv_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7y45_AB.pdb', 'AB')], (1, 1274): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/8d3x_AB.pdb', 'AB')]}\n",
      "oligo='4', range_path_chain={(1, 3976): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/1wpg_ABCD.pdb', 'ABCD')]}\n",
      "oligo='6', range_path_chain={(1, 4608): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7vh6_ABCDEF.pdb', 'ABCDEF')], (1, 4824): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7vh5_ABCDEF.pdb', 'ABCDEF')], (1, 4974): [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/AT2A1_HUMAN/7ny1_ABCDEF.pdb', 'ABCDEF')]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'USAlign' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 57\u001b[0m\n\u001b[1;32m     39\u001b[0m assemblied_structures \u001b[38;5;241m=\u001b[39m PDB_Builder\u001b[38;5;241m.\u001b[39massembled_structures\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# USAlign Module\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Filter homologs based on sequence / structure identity. \u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Multiple filter options possible:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# This means you will keep as many strucs as possible while keeping as many positions in each structure as possible.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Will get finetuning parameters soon in order to control focus on : keep more strucs, but less positions. Keep more positions while not keeping so many strucs etc.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m USAligner \u001b[38;5;241m=\u001b[39m \u001b[43mUSAlign\u001b[49m(work_dir\u001b[38;5;241m=\u001b[39mnew_work_dir, script_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\u001b[39m\u001b[38;5;124m\"\u001b[39m, structure_dict\u001b[38;5;241m=\u001b[39massemblied_structures, \n\u001b[1;32m     58\u001b[0m                    cluster_min_identity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     59\u001b[0m                    num_strucs_per_cluster\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     61\u001b[0m USAligner\u001b[38;5;241m.\u001b[39mUSAlign_run(tm_cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_rmsd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_rmsd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, min_seqID\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, min_aln_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m USAligner\u001b[38;5;241m.\u001b[39moptimize_usalign_filtering(log_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'USAlign' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline\"\n",
    "script_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\"\n",
    "db_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/DB\"\n",
    "#print(hit_list[0:1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Setup Environment and query against DB\n",
    "UIP = UserInputPreprocessor(db_dir=db_dir)\n",
    "uniprot_id = UIP.GeneToUniprot(\"ATP2A1\") # O14983 uniprot\n",
    "new_work_dir = UIP.SetupNewEnv(work_dir=work_dir)\n",
    "query_df = UIP.QueryAgainstDb(query=uniprot_id)\n",
    "\n",
    "\n",
    "#Download and filter pdbs. Retrieve Meta information. \n",
    "# Additional filtering criteria \n",
    "# -Type of Structure Determination (X-Ray only, NMR excluded etc)\n",
    "# -Meta information retrieval\n",
    "# -Sequence Identity based cutoff possible.\n",
    "Downloader = DownloadPipe(input_df=query_df, work_dir=new_work_dir, seq_id_cutoff=0.2, script_dir=script_dir)\n",
    "Downloader.paralellized_download()\n",
    "Downloader.retrieve_meta() \n",
    "Downloader.setup_cutoff(cutoff=5, apply_filter=True)\n",
    "#\n",
    "struct = Downloader.filtered_structures\n",
    "#print(struct)\n",
    "\n",
    "#PDB_builder block\n",
    "\n",
    "# Assemble PDBs into correct biological units.\n",
    "# Establish domain boundaries and group according to oligomeric states\n",
    "# Filter based on most common oligostates for downstream analysis possible.\n",
    "PDB_Builder = PDBBuilder(work_dir=new_work_dir, script_dir=script_dir, structures=struct, remove_intermediates=True) #structures that are filtered\n",
    "PDB_Builder.build_assembly()\n",
    "PDB_Builder.create_domain_boundaries()\n",
    "#print(PDB_Builder.established_domain_dict)\n",
    "PDB_Builder.get_oligostates(num_most_common_oligostates=3)\n",
    "assemblied_structures = PDB_Builder.assembled_structures\n",
    "\n",
    "# USAlign Module\n",
    "# Filter homologs based on sequence / structure identity. \n",
    "# Multiple filter options possible:\n",
    "# - Seq identity\n",
    "# - Template score identity\n",
    "# - Minimum RMSD (avoid redundancy)\n",
    "# - Max RMSD (Control of False Positives that are not really biologically relevant)\n",
    "# - Min Alignment length: In cases were Seq ID is low, you might still retain structurally similar homologs, but for those you might want to impose quality criteria as well\n",
    "# - Min aln length corresponds to the STRUCTURE based superposition alignment. Therefore high min aln length will correspond to only those structures that will NOT introduce many gaps downstream in a PCA analysis.\n",
    "# - Strict mode: if True: Conservative. No homologs included. if False: Also include homologs based on above selected criteria.\n",
    "\n",
    "# Optimized Filtering :\n",
    "# Smart Implementation of Clustal O to maximize retained entropy in alignment\n",
    "# This means you will keep as many strucs as possible while keeping as many positions in each structure as possible.\n",
    "# Will get finetuning parameters soon in order to control focus on : keep more strucs, but less positions. Keep more positions while not keeping so many strucs etc.\n",
    "\n",
    "USAligner = USAlign(work_dir=new_work_dir, script_dir=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\", structure_dict=assemblied_structures, \n",
    "                   cluster_min_identity=0.2,\n",
    "                   num_strucs_per_cluster=200)\n",
    "\n",
    "USAligner.USAlign_run(tm_cutoff=0.5, min_rmsd=0, max_rmsd=100, min_seqID=0.9, min_aln_len=0.90, strict=False)\n",
    "USAligner.optimize_usalign_filtering(log_file=True)\n",
    "\n",
    "# Utililze PCA module in order to identify different Conformational States.\n",
    "# Possible to specifically follow certain pdbs in the PDB.\n",
    "# Handles non Canonical Residues and works on the CA-only level. No sidechain fluctuations captured in PCA.\n",
    "# TBD: Utilize automated clustering based on \n",
    "\n",
    "PCA_tool = PrincipalComponentAnalysis2(script_dir=script_dir, work_dir=new_work_dir,\n",
    "                                      multiseq_dict=multiseq_dict, store_original=True)\n",
    "\n",
    "PCA_tool.prepare_ensemble(USAligner.clustal_seqs_dict) # seems to work. but careful with non canonical residues. might be handled differently later.This version introduces an ALA instead.\n",
    "PCA_tool.run_PCA()\n",
    "PCA_tool.plot_pca_projections(show_specific_strucs=[\"2c9m\", \"1t5s\", \"4hyt\", \"1t5t\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6557c97-815c-437a-a053-d6e0fd1fb171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfdb8c-8dd6-4641-97e7-92ffbb2b42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblied_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79806070-db56-4620-97be-9006d5686910",
   "metadata": {},
   "source": [
    "## USAlign class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c54a8682-7d4c-4d3d-b766-a332247fb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USAlign:\n",
    "    '''Explain class USAlign'''\n",
    "    def __init__(self, work_dir,\n",
    "                 structure_dict,\n",
    "                 script_dir=None,\n",
    "                 top_templates=None,\n",
    "                 cluster_min_identity=None, \n",
    "                 num_strucs_per_cluster=None,\n",
    "                 logging=True):\n",
    "\n",
    "        #current work dir\n",
    "        self.work_dir = work_dir\n",
    "        #location of all scripts required for USAlign\n",
    "        self.script_dir = script_dir\n",
    "        #template dict: key: range   value: dict : key: oligostate, value: tuple (list of pdbs, tuple(Chain, Seq_ID))\n",
    "        self.structure_dict = structure_dict\n",
    "        #this parameter sets a cutoff for structures to be included in the final ensemble.\n",
    "        self.cluster_min_identity = cluster_min_identity\n",
    "        #additional control to select how oligoclusters to be included for each range.\n",
    "        self.num_strucs_per_cluster = num_strucs_per_cluster\n",
    "        #\n",
    "        self.logging = logging\n",
    "        self.log_dir = os.path.join(work_dir, \"log_files\")\n",
    "        #input normalized \n",
    "        self.oligo_split_dict = None\n",
    "        self.normalized_dict = None\n",
    "        self.filtered_cleaned_dict = None\n",
    "        self.filtered_dict_after_usalign = None\n",
    "        self.msa_seqs = None\n",
    "        self.list_of_sizes_per_range_and_cluster_cleaned = None\n",
    "        self.filtered_identity_clusters = None\n",
    "        self.filtered_injector_dict = None\n",
    "        self.list_of_sizes_per_cluster = None\n",
    "        self.results_usalign = None\n",
    "        self.result_dict = None\n",
    "        self.multiseq_alignment = None\n",
    "        self.pca_ready_ensemble = None\n",
    "        self.template_dict_for_usalign_optimize = defaultdict()\n",
    "        self.pdb_strucs_seqs_for_CA  = None\n",
    "        self.consensus_positions = None\n",
    "        self.result_dict_trimmed = None\n",
    "        self.clustal_seqs_dict = None\n",
    "    \n",
    "    def USAlign_run(self, tm_cutoff=0.5, min_seqID=0.8, min_aln_len=0.95, min_rmsd=0.5, max_rmsd=50, strict=False):\n",
    "        '''Function call that runs USAlign.\n",
    "        - First we convert our input into the correct input format.\n",
    "        - If specified, we put constraints on the retrieved ensembles. \n",
    "        (minimum_template_identity:0.9, top_oligo_clusters=3)\n",
    "        Means we only accept clusters that have at least a 0.9 or higher Seq identity in their cluster and top oligo_clusters=3 means\n",
    "        the top 3 oligostates only.'''\n",
    "        \n",
    "        #this function converts the input to a dict with a value list.\n",
    "        #self._convert_input_to_value_list()\n",
    "\n",
    "        #If specified we further filter the subclusters that we find for each oligomeric state based on sequence identity.\n",
    "\n",
    "        \n",
    "        self.normalized_dict = self._restructure_input()\n",
    "\n",
    "        if self.cluster_min_identity:\n",
    "            #function to filter subclusters based on the passed threshold.\n",
    "            self.filtered_identity_clusters = self._filter_subclusters()\n",
    "        else:\n",
    "            self.filtered_identity_clusters = self.normalized_dict\n",
    "\n",
    "        #lets run USAlign\n",
    "        self.filtered_cleaned_dict = self._prep_clusters_USAlign()\n",
    "\n",
    "        #print(f\"{self.filtered_cleaned_dict=}\")\n",
    "        #print(f\"{self.filtered_cleaned_dict=}\")\n",
    "        #first dict is containing a dict: outer_key: range, inner key: oligostate val: path, tm_score, rmsd after alignment.\n",
    "        #second dict templates is a dict: key: outer_key: range, inner key: oligostate, val: path\n",
    "        self.results_usalign = self._parallelized_execution_USAlign(tm_cutoff, min_seqID, min_aln_len, min_rmsd, max_rmsd, strict)\n",
    "\n",
    "    \n",
    "    def _restructure_input(self):\n",
    "\n",
    "        \"\"\"Input needs to be normalized\"\"\"\n",
    "\n",
    "        oligomer_mapping = {\n",
    "            \"1\": \"monomer\", \"2\": \"dimer\", \"3\": \"trimer\", \"4\": \"tetramer\",\n",
    "            \"5\": \"pentamer\", \"6\": \"hexamer\", \"7\": \"heptamer\", \"8\": \"oktamer\",\n",
    "            \"9\": \"nonamer\", \"10\": \"decamer\", \"11\": \"undecamer\", \"12\": \"dodecamer\",\n",
    "            \"13\": \"tridecamer\", \"14\": \"tetradecamer\", \"15\": \"pentadecamer\",\n",
    "            \"16\": \"hexadecamer\", \"17\": \"heptadecamer\", \"18\": \"oktadecamer\",\n",
    "            \"19\": \"nonadecamer\", \"20\": \"eicosamer\", \"21\": \"eicosameundamer\",\n",
    "            \"22\": \"eicosadodamer\", \"23\": \"eicosatrimer\", \"24\": \"eicosatetramer\"\n",
    "        }\n",
    "\n",
    "        filtered_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        #print(f\"{self.structure_dict=}\")\n",
    "        #self.structure_dict=defaultdict(<function PDBBuilder._process_templates.<locals>.<lambda> at 0x7f634f871280>, {'2': defaultdict(<class 'list'>, {'1_322': [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/5ltu_AB.pdb', 0.96),\n",
    "        for oligo, range_dict in self.structure_dict.items():\n",
    "            # Convert oligostate_key to string and fetch oligomer name, defaulting to \"X-mer\" if not found\n",
    "            oligomer_name = oligomer_mapping.get(str(oligo), \"X-mer\")    \n",
    "            for range, path_seqid in range_dict.items():\n",
    "                filtered_dict[oligomer_name][range] = path_seqid\n",
    "\n",
    "        #print(f\"{filtered_dict=}\")\n",
    "        filtered_dict = self._flatten_dict(filtered_dict)\n",
    "        #print(f\"{filtered_dict=}\")\n",
    "        # this filtered_dict flattened structure has keys: start-stop-oligomer values: list of tuples (path, chain, seqid)\n",
    "        return filtered_dict\n",
    "\n",
    "\n",
    "    def _flatten_dict(self, nested_dict):\n",
    "        flat_dict = {}\n",
    "        for outer_key, inner_dict in nested_dict.items():\n",
    "            for inner_key, value_list in inner_dict.items():\n",
    "                combined_key = f\"{outer_key}_{inner_key}\"\n",
    "                flat_dict[combined_key] = value_list\n",
    "\n",
    "        return flat_dict\n",
    "\n",
    "    def _defaultdict_to_dict(self, d):\n",
    "        if isinstance(d, defaultdict):\n",
    "            # Convert the defaultdict to a dict\n",
    "            d = dict(d)\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, defaultdict):\n",
    "                # Recursively convert nested defaultdicts\n",
    "                d[key] = defaultdict_to_dict(value)\n",
    "            elif isinstance(value, list):\n",
    "                # Check if list items are defaultdicts and convert them\n",
    "                d[key] = [defaultdict_to_dict(item) if isinstance(item, defaultdict) else item for item in value]\n",
    "        return d\n",
    "\n",
    "\n",
    "    def _parse_tmalign_output(self, output_text):\n",
    "        \n",
    "        pdb_info_pattern = re.compile(r\"^>(.*\\.pdb):([A-Z])\\s*L=(\\d+)\\s*d0=([\\d.]+)\\s*seqID=([\\d.]+)\\s*TM-score=([\\d.]+)\")\n",
    "        sequence_pattern = re.compile(r\"^[A-Z-]+$\")\n",
    "        rmsd_pattern = re.compile(r\"^# Lali=(\\d+)\\s*RMSD=([\\d.]+)\\s*seqID_ali=([\\d.]+)\")\n",
    "        user_specified_alignment_pattern = re.compile(r\"^# User-specified initial alignment: TM=([\\d.]+)\\s*Lali=(\\d+)\\s*rmsd=([\\d.]+)\")\n",
    "    \n",
    "        alignments = {}\n",
    "        current_key_parts = []\n",
    "        current_sequences = []\n",
    "        current_tms = []\n",
    "        rmsd = None\n",
    "        seqID_ali = None\n",
    "        structure_lengths = []\n",
    "    \n",
    "        for line in output_text.split(\"\\n\"):\n",
    "            pdb_match = pdb_info_pattern.match(line)\n",
    "            sequence_match = sequence_pattern.match(line)\n",
    "            rmsd_match = rmsd_pattern.match(line)\n",
    "            user_specified_alignment_match = user_specified_alignment_pattern.match(line)\n",
    "    \n",
    "            if pdb_match:\n",
    "                pdb_path, chain, length, d0, seqID, tm_score = pdb_match.groups()\n",
    "                pdb_basename = pdb_path #os.path.basename(pdb_path)\n",
    "    \n",
    "                # Update the current key\n",
    "                current_key_parts.append(pdb_basename)\n",
    "                current_tms.append(float(tm_score))\n",
    "                # Capture the length of the structure\n",
    "                structure_lengths.append(int(length))\n",
    "    \n",
    "            elif sequence_match:\n",
    "                # Append sequence to the current sequences list\n",
    "                current_sequences.append(sequence_match.group())\n",
    "    \n",
    "            elif rmsd_match:\n",
    "                # Extract RMSD value, seqID_ali, and Lali\n",
    "                lali, rmsd_value, seqID_ali_value = rmsd_match.groups()\n",
    "                rmsd = float(rmsd_value)\n",
    "                seqID_ali = float(seqID_ali_value)\n",
    "                len_aligned = int(lali)\n",
    "    \n",
    "            elif user_specified_alignment_match:\n",
    "                # You can extract additional values here if needed\n",
    "                pass\n",
    "    \n",
    "        # Calculate the mean of the lengths of the structures\n",
    "        mean_structure_length = sum(structure_lengths) / len(structure_lengths) if structure_lengths else None\n",
    "    \n",
    "        # Construct the final key and calculate the average TM-score\n",
    "        if current_key_parts and current_sequences:\n",
    "            key = \"*\".join(current_key_parts)\n",
    "            avg_tm = sum(current_tms) / len(current_tms) if current_tms else None\n",
    "            # Calculate the ratio of the length of aligned region to the mean length of the structures\n",
    "            alignment_ratio = len_aligned / mean_structure_length if mean_structure_length else None\n",
    "    \n",
    "            # Include seqID_ali and the alignment ratio in the output\n",
    "            alignments[key] = (current_sequences, avg_tm, rmsd, seqID_ali, alignment_ratio)\n",
    "    \n",
    "        return alignments  \n",
    "\n",
    "\n",
    "    def _parallel_sequence_extraction_1(self, input_dict):\n",
    "        # This dictionary will hold the future to sequence mapping\n",
    "        future_to_data = {}\n",
    "    \n",
    "        # This will be your new structure to store the results\n",
    "        results_structure = {}\n",
    "    \n",
    "        # Initialize ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:  # We have 12 cpu cores here on my HP machine\n",
    "            for oligo_range, value_list in input_dict.items():\n",
    "                for pdb_info in value_list:\n",
    "                    pdb_path = pdb_info[0]  # Extract the PDB file path\n",
    "                    # Submit the _grab_sequence_from_struc task for each PDB file\n",
    "                    future = executor.submit(self._grab_sequence_from_struc, pdb_path)\n",
    "                    future_to_data[future] = (oligo_range, pdb_path)\n",
    "    \n",
    "            # Collect the results as they are completed\n",
    "            for future in as_completed(future_to_data):\n",
    "                oligo_range, pdb_path = future_to_data[future]\n",
    "                try:\n",
    "                    seq, _ = future.result()  # Get the result from future\n",
    "                    seq_length = len(seq)\n",
    "                    print(f\"Processed {pdb_path}: Sequence Length = {seq_length}\")\n",
    "                    \n",
    "                    # Store the results in the new structure\n",
    "                    if oligo_range not in results_structure:\n",
    "                        results_structure[oligo_range] = []\n",
    "                    results_structure[oligo_range].append((pdb_path, seq_length))\n",
    "                    \n",
    "                except Exception as exc:\n",
    "                    print(f\"File {pdb_path} generated an exception: {exc}\")\n",
    "    \n",
    "        return results_structure\n",
    "\n",
    "    def _parallel_sequence_extraction_2(self, input_dict):\n",
    "        # This dictionary will hold the future to sequence mapping\n",
    "        future_to_data = {}\n",
    "    \n",
    "        # This will be your new structure to store the results\n",
    "        results_structure = {}\n",
    "    \n",
    "        # Initialize ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:  # We have 12 cpu cores here on my HP machine\n",
    "            for oligo_range, pdb_paths in input_dict.items():\n",
    "                for pdb_path in pdb_paths:  # pdb_path is already the full path to the PDB file\n",
    "                    # Submit the _grab_sequence_from_struc task for each PDB file\n",
    "                    future = executor.submit(self._grab_sequence_from_struc, pdb_path)\n",
    "                    future_to_data[future] = (oligo_range, pdb_path)\n",
    "    \n",
    "            # Collect the results as they are completed\n",
    "            for future in as_completed(future_to_data):\n",
    "                oligo_range, pdb_path = future_to_data[future]\n",
    "                try:\n",
    "                    seq, _ = future.result()  # Get the result from future\n",
    "                    print(f\"Processed {pdb_path}: Sequence Length = {len(seq)=}\")\n",
    "                    \n",
    "                    # Store the results in the new structure\n",
    "                    if oligo_range not in results_structure:\n",
    "                        results_structure[oligo_range] = []\n",
    "                    results_structure[oligo_range].append((pdb_path, seq))\n",
    "                    \n",
    "                except Exception as exc:\n",
    "                    print(f\"File {pdb_path} generated an exception: {exc}\")\n",
    "    \n",
    "        return results_structure\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _majority_vote(self, results_structure):\n",
    "        # This dictionary will hold the majority vote result for each oligo_range\n",
    "        majority_vote_results = {}\n",
    "    \n",
    "        for oligo_range, data_list in results_structure.items():\n",
    "            # Extract sequence lengths for the current oligo_range\n",
    "            lengths = [data[1] for data in data_list]\n",
    "    \n",
    "            # Count the occurrence of each sequence length\n",
    "            length_counts = Counter(lengths)\n",
    "    \n",
    "            # Find the highest occurrence count(s)\n",
    "            max_count = max(length_counts.values())\n",
    "            most_common_lengths = [length for length, count in length_counts.items() if count == max_count]\n",
    "    \n",
    "            # Store the result in the majority_vote_results dictionary\n",
    "            majority_vote_results[oligo_range] = most_common_lengths\n",
    "    \n",
    "        return majority_vote_results\n",
    "\n",
    "    \n",
    "    def _filter_structures_based_on_majority_vote(self, results_structure, majority_results, tolerance=8):\n",
    "        # This will store the filtered results\n",
    "        filtered_results = {}\n",
    "    \n",
    "        for oligo_range, data_list in results_structure.items():\n",
    "            # Get the majority vote lengths for the current oligo_range\n",
    "            common_lengths = majority_results[oligo_range]\n",
    "    \n",
    "            # Calculate the acceptable length range based on the majority vote and tolerance\n",
    "            # Note: If multiple majority lengths, this takes the broadest acceptable range\n",
    "            min_length = min(common_lengths) - tolerance\n",
    "            max_length = max(common_lengths) + tolerance\n",
    "    \n",
    "            # Filter out structures outside the acceptable range or beyond strict limits\n",
    "            filtered_data = [data for data in data_list if min_length <= data[1] <= max_length]\n",
    "    \n",
    "            # Store the filtered data in the filtered_results dictionary\n",
    "            if filtered_data:\n",
    "                filtered_results[oligo_range] = filtered_data\n",
    "    \n",
    "        return filtered_results\n",
    "    \n",
    "\n",
    "    \n",
    "    def _parallelized_execution_USAlign(self, tm_cutoff, min_seqID, min_aln_len, min_rmsd, max_rmsd, strict):\n",
    "        usalign_results = {}\n",
    "        input_dict = self.filtered_cleaned_dict\n",
    "\n",
    "        #print(f\"{input_dict=}\")\n",
    "        #maybe before running usalign we can already trash structures that are of bad quality from the get go \n",
    "        #e.g too little residues, too large gaps etc.\n",
    "        template_dict = {range_oligo: items[0][0] for range_oligo, items in input_dict.items()}\n",
    "        \n",
    "        total_input_items = sum(len(value_list) for value_list in input_dict.values())\n",
    "        print(f\"{total_input_items=}\")\n",
    "        \n",
    "        length_dict = self._parallel_sequence_extraction_1(input_dict)\n",
    "        majorities_lengths = self._majority_vote(length_dict)\n",
    "        #print(f\"{majorities_lengths=}\")\n",
    "        new_filtered_dict = self._filter_structures_based_on_majority_vote(length_dict, majorities_lengths)\n",
    "        total_inner_items = sum(len(value_list) for value_list in new_filtered_dict.values())\n",
    "        print(f\"{total_inner_items=}\")\n",
    "        \n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            futures_dict = {}\n",
    "            futures = []\n",
    "            \n",
    "            for range_oligo, path_chain_seqid_lst in new_filtered_dict.items():\n",
    "                if len(path_chain_seqid_lst) > 1:\n",
    "                    template = template_dict[range_oligo] #this is the best hit we had prior to filtering.\n",
    "                    queries = [item[0] for item in path_chain_seqid_lst[1:]]\n",
    "                    \n",
    "                    self.template_dict_for_usalign_optimize[range_oligo] = template # full path stored\n",
    "                    for query in queries:\n",
    "                        if range_oligo.startswith(\"monomer\"):\n",
    "                            future = executor.submit(self._run_usalign_monomer, template, query, tm_cutoff, min_seqID, min_aln_len, min_rmsd, max_rmsd, strict)\n",
    "                            futures.append(future)\n",
    "                            futures_dict[future] = range_oligo\n",
    "\n",
    "                    #implement oligo logic as well.\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                range_oligo = futures_dict[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    #logging.debug(f\"Result for {range_oligo}: {result}\")\n",
    "                    \n",
    "                    if result:\n",
    "                        usalign_results.setdefault(range_oligo, {}).update(result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing future for {range_oligo}: {e}\")\n",
    "    \n",
    "        #logging.info(f\"Final usalign_results: {usalign_results}\")\n",
    "        return usalign_results\n",
    "\n",
    "    \n",
    "    def _filter_subclusters(self):\n",
    "         \n",
    "        #print(self.cleaned_input)\n",
    "        filtered_dict = defaultdict(list)\n",
    "        # {'1-151-monomer': [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/2duk_A.pdb', 'A', 0.967)\n",
    "        for range_key_oligomer, path_chain_seqid_lst in self.normalized_dict.items():\n",
    "            if path_chain_seqid_lst:\n",
    "                #print(f\"{path_chain_seqid_lst=}\")\n",
    "                for (path, seq_id) in path_chain_seqid_lst: \n",
    "                    if seq_id > self.cluster_min_identity:\n",
    "                        filtered_dict[range_key_oligomer].append((path, seq_id))\n",
    "                    \n",
    "        # Removing any range keys that no longer have any oligostates after filtering\n",
    "        filtered_dict = {k: v for k, v in filtered_dict.items() if v}\n",
    "        #print(f\"{filtered_dict=}\")\n",
    "        return filtered_dict\n",
    "\n",
    "    def _extract_template_and_queries(self):\n",
    "\n",
    "        dir_path_pdb_codes = defaultdict(list)\n",
    "\n",
    "        for dir_path, pdb_template_pdb_query_tuple in self.oligo_split_dict.items():\n",
    "            if pdb_template_pdb_query_tuple:\n",
    "                # Extracts all query PDB codes using list comprehension\n",
    "                queries = [os.path.basename(f)[:-4] for f in pdb_template_pdb_query_tuple[1]]\n",
    "\n",
    "                # Extract the PDB code from the template\n",
    "                template_code = os.path.basename(pdb_template_pdb_query_tuple[0])[:-4]\n",
    "\n",
    "                # Combine the template code with the query codes\n",
    "                pdb_codes = queries + [template_code]  # Append the template code to the list of query codes\n",
    "\n",
    "                # Store the combined list in the dictionary keyed by directory path\n",
    "                dir_path_pdb_codes[dir_path] = pdb_codes\n",
    "\n",
    "        # Print the resulting dictionary after the loop\n",
    "        return dir_path_pdb_codes\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def multisequence_alignment(self, directory=None, run_all_potential_oligos=False, log_file=True):\n",
    "\n",
    "        dir_path_pdb_codes = self._run_multiseq_alignment(directory=directory, run_all_potential_oligos=run_all_potential_oligos)\n",
    "\n",
    "        #print(f\"{dir_path_pdb_codes}\")\n",
    "        #lets check if not None.\n",
    "        if dir_path_pdb_codes and run_all_potential_oligos:\n",
    "            fasta_paths, fasta_cleared_paths = self._execute_msa_all_dirs(dir_path_pdb_codes)\n",
    "        else:\n",
    "            #implement for later use if someone wants only 1 directory.\n",
    "            #self._execute_msa_single_dir(dir_path_pdb_codes)\n",
    "            pass\n",
    "\n",
    "        sequences = self._read_msa_output_converter(fasta_paths)\n",
    "\n",
    "        #here we store all MSA for all oligos\n",
    "        merged_dict_all_oligos_msa = defaultdict(dict)\n",
    "        #this deals with monomeric cases\n",
    "        self._merge_both_oligo_dicts(merged_dict_all_oligos_msa, sequences)\n",
    "        #this deals with the oligomeric cases from mustang\n",
    "        self._merge_both_oligo_dicts(merged_dict_all_oligos_msa, fasta_cleared_paths)\n",
    "\n",
    "        #lets store it in msa_seqs\n",
    "        normal_dict = dict(merged_dict_all_oligos_msa)\n",
    "        self.msa_seqs = normal_dict\n",
    "\n",
    "        \n",
    "        #now lets also logg the results.\n",
    "        if log_file:\n",
    "            self._log_results(normal_dict)\n",
    "        \n",
    "\n",
    "\n",
    "    def _merge_both_oligo_dicts(self, dict1, dict2):\n",
    "        for path, pdb_dict in dict2.items():\n",
    "            if path in dict1:\n",
    "                # Update the existing dictionary with new PDB ID and sequence pairs\n",
    "                dict1[path].update(pdb_dict)\n",
    "            else:\n",
    "                # Add new directory path as a key with its PDB ID and sequence pairs\n",
    "                dict1[path] = pdb_dict\n",
    "\n",
    "    def _log_results(self, d, log_file_name=\"msa_seq_dict.json\"):\n",
    "        \n",
    "        \"\"\"Store the converted dictionary in a JSON log file.\"\"\"\n",
    "        \n",
    "        # Ensure the log directory exists\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        \n",
    "        # Define the full path for the log file\n",
    "        filename = os.path.join(self.log_dir, log_file_name)\n",
    "        \n",
    "        # Write the converted dictionary to the JSON file\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(d, f, indent=4)\n",
    "        print(f\"Log file saved as '{filename}'.\")\n",
    "\n",
    "        \n",
    "    def prepare_for_PCA(self, directory, msa_alignment,cutting=\"strict\"):\n",
    "        #now we leverage the seq alignment from USAlign.\n",
    "        '''\n",
    "        Cutting based on a structure based alignment.\n",
    "        Here we offer two options: strict cutting or intelligent cutting.\n",
    "        \n",
    "        - strict cutting:\n",
    "        We directly take the ensemble, check for gaps in aligned positions \n",
    "        and REMOVE the atoms of a column IF there is a gap in 1 position.\n",
    "\n",
    "        - intelligent cutting: \n",
    "        We first try to find structures in the ensemble that introduce many gaps.    \n",
    "        We proceed by removal of those structures, rerunning the alignment and then continue with normal strict cutting.\n",
    "        This can significantly improve the number of retained positions in the final PCA, but comes at the cost of dropping potentially\n",
    "        relevant structures.\n",
    "        '''\n",
    "\n",
    "        # Directory to store results\n",
    "        save_dir = os.path.join(directory, \"trimmed_strucs\")\n",
    "        \n",
    "        if cutting == \"strict\":\n",
    "            #print(\"we move into strict cutting\")\n",
    "            residues_to_keep = self._hard_trimmer(msa_alignment)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.mkdir(save_dir)\n",
    "            #this will be used to run PCA.\n",
    "            self.pca_ready_ensemble = save_dir\n",
    "            self._select_proper_residues_after_trim(residues_to_keep, save_dir)\n",
    "\n",
    "        elif cutting == \"intelligent\":\n",
    "            #these strucs need to be removed.\n",
    "            strucs_to_remove = self._soft_trimmer(msa_alignment)\n",
    "            #and then we rerun usalign.\n",
    "            #we remove the strucs that are found to introduce many gaps.\n",
    "            self._hard_trimmer(msa_alignment)\n",
    "\n",
    "    def _soft_trimmer(self, result_fasta:str):\n",
    "        '''Explain soft trimmer'''\n",
    "        gap_counter = defaultdict()\n",
    "        ali = pytrimal.Alignment.load(result_fasta)\n",
    "        trimmer = pytrimal.AutomaticTrimmer(\"gappyout\")\n",
    "        trimmed = trimmer.trim(ali)\n",
    "        for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "            structure_name = name.decode().rjust(6)\n",
    "            gap_count = seq.count(\"-\")\n",
    "            gap_counter[structure_name] = gap_count\n",
    "\n",
    "        gap_values = list(gap_counter.values())\n",
    "        mean_gap_count = statistics.mean(gap_values)\n",
    "        median_gap_count = statistics.median(gap_values)\n",
    "\n",
    "        high_gap_count_sequence_ids = [seq_id for seq_id, gap_count in gap_counter.items() if gap_count > mean_gap_count]\n",
    "        print(high_gap_count_sequence_ids)\n",
    "        return high_gap_count_sequence_ids\n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "    def _hard_trimmer(self, input_dict):\n",
    "        trimmed_results = defaultdict(dict)\n",
    "    \n",
    "        for oligo_range, pdb_data_list in input_dict.items():\n",
    "            # Initialize lists to hold pdb codes and sequences for current oligo_range\n",
    "            pdb_codes = []\n",
    "            full_sequences = []\n",
    "    \n",
    "            for pdb_data in pdb_data_list:\n",
    "                pdb_path, sequence_data = pdb_data  # Directly unpack the tuple\n",
    "                pdb_codes.append(pdb_path.encode('utf-8'))  # Encode pdb_path for pytrimal\n",
    "                \n",
    "                # Concatenate the amino acid letters from the tuples to form the full sequence\n",
    "                full_sequence = ''.join([aa for aa, pos in sequence_data])\n",
    "                full_sequences.append(full_sequence)\n",
    "    \n",
    "            # Create an Alignment object for pytrimal\n",
    "            alignment = pytrimal.Alignment(pdb_codes, full_sequences)\n",
    "            # Initialize the ManualTrimmer\n",
    "            trimmer = pytrimal.ManualTrimmer(gap_threshold=1)  # Adjust the gap_threshold as needed\n",
    "            # Perform the trimming\n",
    "            trimmed_alignment = trimmer.trim(alignment)\n",
    "    \n",
    "            # Store the trimmed sequences back into the dictionary\n",
    "            for name, trimmed_seq in zip(trimmed_alignment.names, trimmed_alignment.sequences):\n",
    "                # Decode the name from bytes to string, remove any left whitespace\n",
    "                pdb_name = name.decode().strip()\n",
    "                # Store the trimmed sequence in the dictionary under the corresponding oligo_range and PDB path\n",
    "                trimmed_results[oligo_range][pdb_name] = trimmed_seq\n",
    "    \n",
    "        return dict(trimmed_results)\n",
    "\n",
    "    def _select_proper_residues_after_trim(self, keep_residue_dict:dict, save_dir:dir):\n",
    "        '''Helper function that takes the proper residues after trimming.'''\n",
    "        for pdb, seq in keep_residue_dict.items():\n",
    "            #seq in keep_residue_dict is extracted from hard trimmer.\n",
    "            full_p = os.path.join(self.work_dir, pdb)\n",
    "            result_seq = self._grab_sequence_from_struc(full_p)\n",
    "            positions_to_keep = self._find_positions(result_seq[0], seq)\n",
    "            #print(f\"this is seq to keep: {seq}, this is full_seq from structure: {result_seq}\")\n",
    "            #print(f\"this is our savepath for cutted struc: {full_p}\")\n",
    "            self._select_trimmed_positions(path_to_pdb=full_p, list_to_keep=positions_to_keep, save_dir=save_dir)\n",
    "\n",
    "    def _select_trimmed_positions(self, path_to_pdb:str, list_to_keep:str, save_dir):\n",
    "        '''Helper function that select only specific residues based on list_to_keep.'''\n",
    "        class CAlphaTrimmedPositions(Select):\n",
    "            def __init__(self, list_to_keep_shifts, *args):\n",
    "                super().__init__(*args)\n",
    "                self.list_to_keep_shifts = list_to_keep_shifts\n",
    "            #overload accept_residue inherited from Select with this conditional return\n",
    "            def accept_atom(self, atom):\n",
    "                return 1 if atom.id == \"CA\" else 0\n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] in self.list_to_keep_shifts else 0    \n",
    "\n",
    "        #lets make a new dir where we store the trimmed strucs.\n",
    "        pdb_name = os.path.basename(path_to_pdb)\n",
    "        new_location = os.path.join(save_dir, pdb_name) #here we save\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, path_to_pdb)\n",
    "        shift = [x.get_id()[1] for x in structure.get_residues()][0] #first residue number = shift\n",
    "        list_to_keep_shifted = [x+shift-1 for x in list_to_keep] #correct for shift. lets try with shift\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(new_location, CAlphaTrimmedPositions(list_to_keep_shifts=list_to_keep_shifted))\n",
    "\n",
    "\n",
    "    \n",
    "    def prepare_CA_for_PCA(self):\n",
    "        #lets use pytrimmal to find the optimal amount of structures to keep and residues to cut.\n",
    "        \n",
    "        if self.consensus_positions and self.pdb_strucs_seqs_for_CA:\n",
    "\n",
    "            self._aligner_hard()\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def _find_positions(self, full_sequence, partial_sequence):\n",
    "        # Convert the tuple list to two separate lists for easier comparison\n",
    "        full_seq_types, full_seq_nums = zip(*full_sequence)\n",
    "    \n",
    "        j = 0  # Index for partial_sequence\n",
    "        found_residue_nums = []\n",
    "    \n",
    "        for i in range(len(full_seq_types)):\n",
    "            if j >= len(partial_sequence):\n",
    "                break  # Exit if we've processed the entire partial_sequence\n",
    "    \n",
    "            if partial_sequence[j] == full_seq_types[i]:\n",
    "                found_residue_nums.append(full_seq_nums[i])\n",
    "                j += 1  # Advance in partial_sequence only if a match is found\n",
    "\n",
    "        print(f\"{full_sequence=}, {len(full_sequence)=}\")\n",
    "        return found_residue_nums\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _grab_sequence_from_struc(self, pdb):\n",
    "        '''Helper function that fetches sequence based on the structure'''\n",
    "        lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                        ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                        ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                        ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "        \n",
    "        canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        pdb_name = os.path.basename(pdb)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, pdb)\n",
    "        struc_seq = [canonical_aas[x.get_resname()] for x in structure.get_residues()]\n",
    "        struc_seq = \"\".join(struc_seq)\n",
    "        return (struc_seq, pdb_name)\n",
    "\n",
    "\n",
    "    def run_clustal_omega(self, result_dict_with_seqs):\n",
    "        \n",
    "        dict_with_all_seqs_per_range = defaultdict(dict)\n",
    "        \n",
    "        for oligo_range, pdb_data in result_dict_with_seqs.items():\n",
    "            fasta_path = os.path.join(self.work_dir, f\"{oligo_range}.fa\")\n",
    "            with open(fasta_path, \"w\") as fasta_file:\n",
    "                for pdb_path, sequence_data in pdb_data:\n",
    "                    fasta_file.write(f\">{pdb_path}\\n\")\n",
    "                    fasta_file.write(''.join([aa for aa, pos in sequence_data]) + \"\\n\")\n",
    "    \n",
    "            try:\n",
    "                clustalo_command = f\"clustalo -i {fasta_path} --outfmt=fasta --force -o {fasta_path.replace('.fa', '.clu')}\"\n",
    "                result = run(clustalo_command.split(), stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "                if result.stderr:\n",
    "                    print(f\"Error running Clustal Omega for {oligo_range}: {result.stderr}\")\n",
    "                else:\n",
    "                    print(f\"Clustal Omega alignment completed for {oligo_range}\")\n",
    "    \n",
    "                ali = pytrimal.Alignment.load(fasta_path.replace('.fa', '.clu'))\n",
    "                trimmer = pytrimal.ManualTrimmer(gap_threshold=1)\n",
    "                trimmed_alignment = trimmer.trim(ali)\n",
    "    \n",
    "                for name, trimmed_seq in zip(trimmed_alignment.names, trimmed_alignment.sequences):\n",
    "                    dict_with_all_seqs_per_range[oligo_range][name.decode().strip() if isinstance(name, bytes) else name] = trimmed_seq\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Exception running Clustal Omega for {oligo_range}: {e}\")\n",
    "    \n",
    "        return dict_with_all_seqs_per_range\n",
    "    \n",
    "    def optimize_usalign_filtering(self, log_file=True):\n",
    "        \n",
    "        if self.results_usalign:\n",
    "\n",
    "            optimized_alignments = self._optimize_ensemble_sequences(self.results_usalign)\n",
    "            print(f\"{optimized_alignments=}\")\n",
    "            # now for all these optimized alignments we need to fetch the seq from the struc.\n",
    "            result_dict_with_seqs = self._parallel_sequence_extraction_2(optimized_alignments)\n",
    "\n",
    "\n",
    "\n",
    "            #before pytrimmal we need another alignment through clustal o\n",
    "\n",
    "            clustal_sequences = self.run_clustal_omega(result_dict_with_seqs)\n",
    "\n",
    "            self.clustal_seqs_dict = clustal_sequences\n",
    "            #lets run pytrymmal on those.\n",
    "            #self.result_dict_trimmed = self._hard_trimmer(result_dict_with_seqs)\n",
    "            \n",
    "            return\n",
    "\n",
    "            \n",
    "            filtered_pairwise_alignments = self._filter_pairwise_alignments(optimized_alignments, self.results_usalign)\n",
    "            print(f\"{filtered_pairwise_alignments=}\")\n",
    "            \n",
    "\n",
    "            \n",
    "            #self._identify_consensus_positions_and_sequences(filtered_pairwise_alignments)\n",
    "            #print(f\"{self.consensus_positions=}\")\n",
    "            ##this can be parallelized... we pass now consensus positions and the dict filtered_pairwise_alignments to the function.\n",
    "            ##function takes: 1 entry from the dict and consensus positions.\n",
    "            #retain_sequences = self._parallel_extraction(filtered_pairwise_alignments, self.consensus_positions)\n",
    "            #print(f\"{retain_sequences=}\")\n",
    "            #\n",
    "            ##Now lets grab the real seq resinums.\n",
    "            #self.pdb_strucs_for_CA = self._process_pdbs_parallel(retain_sequences)\n",
    "            #print(f\"{self.pdb_strucs_for_CA }=\")\n",
    "            \n",
    "\n",
    "    def _extract_partial_sequence(self, key_val_pair, consensus_pos):\n",
    "        key, val = key_val_pair  # Now we're using both the key and the value\n",
    "        query_pdb = key.split(\"*\")[1]  # Correctly extract the query PDB ID from the key\n",
    "        query_seq_with_gaps = val[0][1]  # Access the query sequence, ensuring we're accessing the correct part of the value structure\n",
    "        retain_seq = \"\".join([query_seq_with_gaps[i] for i in consensus_pos if query_seq_with_gaps[i] != '-'])  # Exclude gaps\n",
    "        print(f\"{query_pdb=},{retain_seq=}\")\n",
    "        return query_pdb, retain_seq\n",
    "\n",
    "\n",
    "\n",
    "    def _parallel_extraction(self, filtered_pairwise_alignment, consensus_positions_by_range):\n",
    "        retain_sequences = defaultdict(dict)  # Use a dict of dicts to retain the oligo range structure\n",
    "    \n",
    "        for oligo_range, alignments in filtered_pairwise_alignment.items():\n",
    "            consensus_pos = consensus_positions_by_range[oligo_range]  # Fetch consensus positions for the current oligo range\n",
    "    \n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                # Submit tasks for the current oligo range, ensuring we're passing the correct structure\n",
    "                print(f\"{consensus_pos=}\")\n",
    "                future_to_pdb = {executor.submit(self._extract_partial_sequence, (inner_key, val), consensus_pos): inner_key for inner_key, val in alignments.items()}\n",
    "    \n",
    "                for future in as_completed(future_to_pdb):\n",
    "                    pdb_id, partial_seq = future.result()\n",
    "                    retain_sequences[oligo_range][pdb_id] = partial_seq  # Store in the dict under the corresponding oligo range\n",
    "    \n",
    "        return dict(retain_sequences)  # Convert the defaultdict to a regular dict for the output\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def _identify_consensus_positions_and_sequences(self, filtered_pairwise_alignments):\n",
    "        \"\"\"\n",
    "        Identifies consensus positions and sequences that are present across all pairwise alignments for each oligo range.\n",
    "        Args:\n",
    "            filtered_pairwise_alignments (dict): A dictionary containing filtered pairwise alignments with oligo ranges as keys.\n",
    "        Sets:\n",
    "            self.consensus_positions (dict): A dictionary where each key is an oligo range and the value is a list of consensus positions.\n",
    "            self.consensus_sequences (dict): A dictionary where each key is an oligo range and the value is the consensus sequence.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.consensus_positions = {}\n",
    "        self.consensus_sequences = {}\n",
    "    \n",
    "        for oligo_range, alignments in filtered_pairwise_alignments.items():\n",
    "            consensus_positions = []\n",
    "            consensus_sequence = []\n",
    "            # Assuming the template sequence is consistent across all alignments and is the first in each pair\n",
    "            template_sequences = [alignment[0][0] for alignment in alignments.values()]\n",
    "    \n",
    "            # Identify positions that are not gaps in the template sequence across all alignments and construct the consensus sequence\n",
    "            for i in range(len(template_sequences[0])):\n",
    "                if all(seq[i] != '-' for seq in template_sequences):\n",
    "                    consensus_positions.append(i)\n",
    "                    consensus_sequence.append(template_sequences[0][i])  # Use the first template sequence to construct the consensus sequence\n",
    "    \n",
    "            self.consensus_positions[oligo_range] = consensus_positions\n",
    "            self.consensus_sequences[oligo_range] = ''.join(consensus_sequence)  # Join the sequence list into a string\n",
    "\n",
    "\n",
    "    def _filter_pairwise_alignments(self, retained_pdbs, results_usalign):\n",
    "        # Initialize a new dictionary to store the filtered alignments\n",
    "        filtered_alignments = {}\n",
    "    \n",
    "        # Iterate over the outer keys (e.g., 'monomer_-5_1063')\n",
    "        for outer_key, alignments_dict in results_usalign.items():\n",
    "            # Initialize a dictionary to store the filtered alignments for the current outer key\n",
    "            filtered_alignments_for_outer_key = {}\n",
    "    \n",
    "            # Check if the current outer_key is in the retained_pdbs\n",
    "            if outer_key in retained_pdbs:\n",
    "                # Get the list of PDBs to retain for this outer_key\n",
    "                retained_list = retained_pdbs[outer_key]\n",
    "    \n",
    "                # Iterate over the inner dictionary, where the keys are pairs of PDBs\n",
    "                for pdb_pair_key, alignment_data in alignments_dict.items():\n",
    "                    # Split the pdb_pair_key on '*' to get the individual PDB IDs\n",
    "                    pdb1, pdb2 = pdb_pair_key.split('*')\n",
    "    \n",
    "                    # Check if the second PDB is in the list of retained PDBs for this outer_key\n",
    "                    if pdb2 in retained_list:\n",
    "                        # Add the alignment to the filtered dictionary under the current outer key\n",
    "                        filtered_alignments_for_outer_key[pdb_pair_key] = alignment_data\n",
    "    \n",
    "            # If there are any filtered alignments for the current outer key, add them to the filtered_alignments dictionary\n",
    "            if filtered_alignments_for_outer_key:\n",
    "                filtered_alignments[outer_key] = filtered_alignments_for_outer_key\n",
    "    \n",
    "        return filtered_alignments\n",
    "    \n",
    "    def _aggregate_alignments(self, inner_dict):\n",
    "        \n",
    "        aggregated_alignments = []\n",
    "        \n",
    "        for pairings, seq_lst_tm_rmsd in inner_dict.items():\n",
    "            aligned_seq1, aligned_seq2 = seq_lst_tm_rmsd[0]  # Extract the aligned sequences\n",
    "            aggregated_alignments.append((pairings, list(zip(*seq_lst_tm_rmsd[0]))))\n",
    "        return aggregated_alignments\n",
    "\n",
    "    def _analyze_and_optimize_alignments(self, aggregated_alignments, retention_threshold=0.9):\n",
    "        \n",
    "        retained_structures = set()  # Set to store the pdb_ids of the structures to retain\n",
    "    \n",
    "        if not aggregated_alignments or not aggregated_alignments[0][1]:\n",
    "            print(\"No alignments or empty alignments provided.\")\n",
    "            return retained_structures\n",
    "    \n",
    "        num_structures = len(aggregated_alignments)\n",
    "        num_positions = len(aggregated_alignments[0][1])\n",
    "        \n",
    "        # Analyze alignments to determine which structures to retain\n",
    "        for pair_alignment in aggregated_alignments:\n",
    "            pair, alignment = pair_alignment\n",
    "            # Calculate the ratio of non-gap positions for this structure in all positions\n",
    "            non_gap_ratio = sum(1 for res1, res2 in alignment if res1 != '-' and res2 != '-') / num_positions\n",
    "    \n",
    "            if non_gap_ratio >= retention_threshold:\n",
    "                retained_structures.update(pair.split('*'))  # Assuming pair is in the format 'pdbid1*pdbid2'\n",
    "\n",
    "        #print(f\"{retained_structures=}\")\n",
    "        return retained_structures\n",
    "\n",
    "    \n",
    "    def _optimize_ensemble_sequences(self, alignments):\n",
    "        \n",
    "        optimized_results = {}\n",
    "    \n",
    "        for range_oligos, inner_dict in alignments.items():\n",
    "            # Aggregate alignment information for this ensemble\n",
    "            aggregated_alignments = self._aggregate_alignments(inner_dict)\n",
    "    \n",
    "            # Analyze alignments to identify structures to retain\n",
    "            retained_structures = self._analyze_and_optimize_alignments(aggregated_alignments)\n",
    "    \n",
    "            # Store the retained pdb_ids for this range_oligos key\n",
    "            optimized_results[range_oligos] = list(retained_structures)  # Convert set to list\n",
    "    \n",
    "        return optimized_results\n",
    "\n",
    "\n",
    "    \n",
    "    def _process_pdbs_parallel(self, dict_with_retain_seqs):\n",
    "        pdb_sequences_by_range = defaultdict(dict)  # Use a dict of dicts to retain the oligo range structure\n",
    "    \n",
    "        for oligo_range, retain_seqs in dict_with_retain_seqs.items():\n",
    "            template_pdb_path = self.template_dict_for_usalign_optimize[oligo_range]\n",
    "            pdb_paths = [pdb for pdb in retain_seqs.keys()]  # These are already full paths\n",
    "            pdb_paths.append(template_pdb_path)\n",
    "    \n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                future_to_pdb = {}  # Initialize the dictionary to store futures to pdb mappings\n",
    "                for pdb_path in pdb_paths:\n",
    "                    future_to_pdb[executor.submit(self._grab_sequence_from_struc, pdb_path)] = pdb_path\n",
    "    \n",
    "                for future in as_completed(future_to_pdb):\n",
    "                    pdb_path = future_to_pdb[future]\n",
    "                    try:\n",
    "                        struc_seq_and_res, _ = future.result()\n",
    "                        if pdb_path != template_pdb_path:\n",
    "                            print(f\"{pdb_path=}, {retain_seqs.get(pdb_path, '')=}\")\n",
    "                            pdb_sequences_by_range[oligo_range][pdb_path] = (struc_seq_and_res, retain_seqs.get(pdb_path, \"\"))\n",
    "                        else:\n",
    "                            pdb_sequences_by_range[oligo_range][pdb_path] = (struc_seq_and_res, \"\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {pdb_path}: {e}\")\n",
    "    \n",
    "        return dict(pdb_sequences_by_range)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _grab_sequence_from_struc(self, pdb):\n",
    "\n",
    "        lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                        ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                        ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                        ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "        \n",
    "        canonical_aas = defaultdict(lambda: \"A\", lst) #alanine if its non canonical for debug purpose\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        pdb_name = os.path.basename(pdb)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, pdb)\n",
    "        struc_seq_and_res = [(canonical_aas[res.get_resname()], res.get_id()[1])\n",
    "                         for res in structure.get_residues()\n",
    "                         if res.get_id()[0] == ' ']\n",
    "\n",
    "        return struc_seq_and_res, pdb\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _analyze_gap_distribution(self, aggregated_alignments):\n",
    "        num_structures = len(aggregated_alignments)\n",
    "        num_positions = max(len(alignments) for _, alignments in aggregated_alignments)\n",
    "        gap_counts_per_position = [0] * num_positions\n",
    "        gap_counts_per_structure = [0] * num_structures\n",
    "    \n",
    "        # Iterate over each structure's alignments\n",
    "        for i, (_, alignments) in enumerate(aggregated_alignments):  # Use _ to ignore the first item (pair identifier)\n",
    "            # Iterate over each position in the alignments\n",
    "            for j, (res1, res2) in enumerate(alignments):  # Now correctly accessing the list of tuples\n",
    "                if res1 == '-' or res2 == '-':\n",
    "                    gap_counts_per_position[j] += 1\n",
    "                    gap_counts_per_structure[i] += 1\n",
    "    \n",
    "        # Determine positions to retain (e.g., positions with gaps in less than 50% of structures)\n",
    "        positions_to_retain = [j for j, count in enumerate(gap_counts_per_position) if count / (j+1) < 1]\n",
    "    \n",
    "        # Determine structures to retain (e.g., structures with gaps in less than 20% of positions)\n",
    "        structures_to_retain = [i for i, count in enumerate(gap_counts_per_structure) if count / (i+1) < 0.05]\n",
    "\n",
    "        print(f\"{positions_to_retain=}\")\n",
    "        print(f\"{len(positions_to_retain)=}\")\n",
    "        print(f\"{structures_to_retain=}\")\n",
    "        print(f\"{len(structures_to_retain)=}\")\n",
    "        \n",
    "    def _apply_optimization(self, aggregated_alignments, positions_to_retain, structures_to_retain):\n",
    "        \n",
    "        optimized_alignments = {}\n",
    "    \n",
    "        for i, alignment in enumerate(aggregated_alignments):\n",
    "            if i in structures_to_retain:\n",
    "                # Retain only the selected positions for this structure\n",
    "                optimized_alignment = [alignment[j] for j in positions_to_retain]\n",
    "                # Convert the optimized alignment back to a pair of sequences\n",
    "                seq1, seq2 = zip(*optimized_alignment)\n",
    "                optimized_alignments[i] = (''.join(seq1).replace('-', ''), ''.join(seq2).replace('-', ''))\n",
    "    \n",
    "        return optimized_alignments\n",
    "\n",
    "    \n",
    "    #helper function that either passes filtered or unfiltered results downstream\n",
    "    def setup_oligo_directories(self):\n",
    "        #setup directories from here.\n",
    "        if self.filtered_dict_after_usalign:\n",
    "            self._organize_and_shuffle(self.filtered_dict_after_usalign)\n",
    "            #self._separate_oligostates(self.filtered_dict_after_usalign)\n",
    "        else:\n",
    "            self._organize_and_shuffle(self.results_usalign)\n",
    "            #self._separate_oligostates(self.results_usalign)\n",
    "            \n",
    "    \n",
    "    def _logger_us_results(self, result_dict:dict):\n",
    "\n",
    "        regular_dict = dict(result_dict) # convert to normal dict\n",
    "        with open(os.path.join(self.log_dir, \"filtered_usalign_log.json\"), \"w\") as json_file:\n",
    "            json.dump(regular_dict, json_file, indent=4)\n",
    "        \n",
    "    def _prep_clusters_USAlign(self):\n",
    "\n",
    "        filtered_cleaned_dict = {}\n",
    "\n",
    "        for range_oligostate, path_chain_seqid_lst in self.filtered_identity_clusters.items():\n",
    "            # Remove duplicates and sort by seqid in descending order\n",
    "            unique_tuples = set(path_chain_seqid_lst)\n",
    "            sorted_tuples = sorted(unique_tuples, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "            # Retain only the top 'num_strucs_per_cluster' entries\n",
    "            top_sorted_tuples = sorted_tuples[:self.num_strucs_per_cluster]\n",
    "    \n",
    "            # Store the size (number of structures) for the 'range_oligostate'\n",
    "            # No need to check for key existence explicitly here since we're assigning a new list each time\n",
    "            filtered_cleaned_dict[range_oligostate] = top_sorted_tuples\n",
    "\n",
    "        # The dictionary is already a regular dict, so we can return it directly\n",
    "        return filtered_cleaned_dict\n",
    "\n",
    "\n",
    "    \n",
    "    def _run_usalign_monomer(self, template, path_struc_1, tm_cutoff, min_seqID, min_aln_len, min_rmsd, max_rmsd, strict):\n",
    "\n",
    "        bash_tm_and_rmsd_calc = f\"{self.script_dir}/USalign {template} {path_struc_1} -outfmt 1 -TMscore 5\" #first sequence based initial alignment. if this works... super.\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            alignment = self._parse_tmalign_output(result.stdout)\n",
    "            #alignment: key: 2duk_A*3dun_A val: ([seq1, seq2], avg_tm, rmsd)\n",
    "\n",
    "            for pdb_couple_name, seqs_tm_rmsd_seqid in alignment.items():\n",
    "                tm_1 = float(seqs_tm_rmsd_seqid[1]) #this is avg tm\n",
    "                rmsd_1 = float(seqs_tm_rmsd_seqid[2]) #this is rmsd\n",
    "                seqid_ali_1 = float(seqs_tm_rmsd_seqid[3]) \n",
    "                aligned_length_1 = float(seqs_tm_rmsd_seqid[4])\n",
    "                # TM-score=0.5, corresponds to a P-value of 5.5  107  taken from \n",
    "                #figure 3 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2913670/ \n",
    "        \n",
    "                if tm_1 > tm_cutoff and rmsd_1 > min_rmsd and rmsd_1 < max_rmsd and seqid_ali_1 > min_seqID:  # based on assumption that seq based initial alignment established 1 to 1 pairing between residues\n",
    "                    #works fine then and we keep it.\n",
    "                    print(f\"tm_1={tm_1:.2f}, rmsd_1={rmsd_1:.2f}, seqid_ali_1={seqid_ali_1:.2f}\")\n",
    "                    return alignment\n",
    "                else:\n",
    "\n",
    "                    # now if strict = True, we dont fetch homologs.\n",
    "                    if not strict:\n",
    "                        # now we try 1 last thing.\n",
    "                        print(f\"Initial seq_based alignment did not fulfill criteria of match: {template=}, {path_struc_1=} tm_1={tm_1:.2f}, rmsd_1={rmsd_1:.2f}, seqid_ali_1={seqid_ali_1:.2f}\")\n",
    "                        \n",
    "                        bash_tm_and_rmsd_calc_round_2 = f\"{self.script_dir}/USalign {template} {path_struc_1} -outfmt 1 -TMscore 0\" #first sequence based initial alignment. if this works... super.\n",
    "                        bash_command_2 = bash_tm_and_rmsd_calc_round_2.split()\n",
    "    \n",
    "                        result = run(bash_command_2, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "                        alignment_2 = self._parse_tmalign_output(result.stdout)\n",
    "                        \n",
    "                        for pdb_couple_name, seqs_tm_rmsd_seqid in alignment_2.items():\n",
    "                            tm_2 = float(seqs_tm_rmsd_seqid[1]) #this is avg tm\n",
    "                            rmsd_2 = float(seqs_tm_rmsd_seqid[2]) #this is rmsd\n",
    "                            aligned_length_2 = float(seqs_tm_rmsd_seqid[4])\n",
    "                            \n",
    "                            if tm_2 > tm_cutoff and rmsd_2 > min_rmsd and rmsd_2 < max_rmsd and aligned_length_2 > min_aln_len:\n",
    "                                print(f\"Final structure based only alignment {template=}, {path_struc_1=}: tm_2={tm_2:.2f}, rmsd={rmsd_2:.2f}, aln_length_ratio={aligned_length_2:.2f}\")\n",
    "                                \n",
    "                                # now lets judge if aln1 or 2 is better.\n",
    "                                if aligned_length_2 > aligned_length_1:\n",
    "                                    #then we conclude structure based is better.\n",
    "                                    print(f\"{template=}, {path_struc_1=}, {aligned_length_1=}, {aligned_length_2=}\")\n",
    "                                    return alignment_2\n",
    "                                else:\n",
    "                                    #else we conclude seq based initial is better.\n",
    "                                    print(f\"{template=}, {path_struc_1=}, {aligned_length_1=}, {aligned_length_2=}\")\n",
    "                                    return alignment\n",
    "                                    \n",
    "                            else:\n",
    "                                print(f\"we remove {path_struc_1} because of tm: {tm_2:.2f} and rmsd: {rmsd_2:.2f} and aln_length_ratio={aligned_length_2:.2f}in round 2\")\n",
    "                                return {}\n",
    "                                #now we give up.\n",
    "                    else:\n",
    "                        print(f\"Initial seq_based alignment did not fulfill criteria of match: {template=}, {path_struc_1=} tm_1={tm_1:.2f}, rmsd_1={rmsd_1:.2f}, seqid_ali_1={seqid_ali_1:.2f}\")\n",
    "                        print(f\"We are using {strict=} so we dont include Homologs.\")\n",
    "                        return \n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return {}\n",
    "\n",
    "    \n",
    "\n",
    "    def _run_usalign_oligomer(self, template, path_struc_1):\n",
    "\n",
    "        # -outfmt 2 : tsv outfile, \n",
    "        # -mol prot : only consider protein\n",
    "        # -ter 0:align all chains from all models (recommended for aligning biological assemblies, i.e. biounits)\n",
    "        # -mm 1: alignment of two multi-chain oligomeric structures\n",
    "    \n",
    "        bash_tm_and_rmsd_calc = f\"{self.script_dir}/USalign {path_struc_1} {template} -outfmt 2 -mol prot -ter 0 -mm 1\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        #print(bash_command)\n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            tm_2, rmsd = self._get_tm_scores_and_rmsd(result.stdout)\n",
    "            if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "            else:\n",
    "                print(f\"we remove {path_struc_1} because of tm: {tm_2} and rmsd: {rmsd}\")\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _extract_files_per_cluster(self):\n",
    "        \n",
    "        files_per_cluster = {}\n",
    "        for range_key, clusters in self.list_of_sizes_per_cluster.items():\n",
    "            # Check if the range key exists in the filtered_injector_dict\n",
    "            if range_key in self.filtered_injector_dict:\n",
    "                files_per_cluster[range_key] = {}\n",
    "                for cluster_id in clusters.keys():\n",
    "                    # Fetch the file paths for each cluster id\n",
    "                    files_per_cluster[range_key][cluster_id] = self.filtered_injector_dict[range_key][cluster_id]\n",
    "\n",
    "        self.filtered_injector_dict = files_per_cluster\n",
    "\n",
    "    def _get_top_clusters_per_range(self):\n",
    "    \n",
    "        top_clusters = {}\n",
    "        for range_key, clusters in self.list_of_sizes_per_cluster.items():\n",
    "            # Sort clusters based on size, and in case of a tie, use larger key\n",
    "            sorted_clusters = sorted(clusters.items(), key=lambda x: (-x[1], -x[0]))\n",
    "            # Take the top n clusters, where n is specified by num_top_clusters_per_range\n",
    "            top_n_clusters = dict(sorted_clusters[:self.num_top_clusters_per_range])\n",
    "            top_clusters[range_key] = top_n_clusters\n",
    "\n",
    "        self.list_of_sizes_per_cluster = top_clusters\n",
    "        \n",
    "\n",
    "    def _organize_and_shuffle(self, dict_to_process):\n",
    "    \n",
    "        dir_info_dict = defaultdict(tuple)\n",
    "\n",
    "        #print(f\"{dict_to_process=}\")\n",
    "\n",
    "        \"\"\"\n",
    "        {'monomer_1_187': ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/2duk_A.pdb',\n",
    "        [('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/6woe_A.pdb', 0.9519, 0.64), \n",
    "        ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/6woi_A.pdb', 0.9507, 0.66), \n",
    "        ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/6wo8_A.pdb', 0.9503, 0.67)])})\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        for range_oligostate, template_path_pdb_details_lst in dict_to_process.items():\n",
    "            # Extract range and oligostate from the key\n",
    "            # template_path_pdb_details_lst first entry is template path second entry is list with usalign results\n",
    "            parts = range_oligostate.rsplit('_', 2)\n",
    "            #print(f\"{range_key=}, {oligostate=}\")\n",
    "\n",
    "            range_key = str(parts[1]) + \"_\" + str(parts[2])\n",
    "            oligostate = parts[0]\n",
    "            \n",
    "            # Define paths for oligostate and range directories\n",
    "            oligo_dir = os.path.join(self.work_dir, oligostate)\n",
    "            range_dir = os.path.join(oligo_dir, range_key)\n",
    "    \n",
    "            # Create the directories if they don't exist\n",
    "            os.makedirs(range_dir, exist_ok=True)\n",
    "    \n",
    "            # Copy the template PDB into the range directory\n",
    "            shutil.copy(template_path_pdb_details_lst[0], range_dir)\n",
    "            template_dst_path = os.path.join(range_dir, os.path.basename(template_path_pdb_details_lst[0])) # this is the first template\n",
    "    \n",
    "            # Initialize a list to store the paths of other structures\n",
    "            other_structures = []\n",
    "    \n",
    "            # Copy other PDBs associated with the current range_oligostate into its directory\n",
    "            for pdb_path_tm_rmsd in template_path_pdb_details_lst[1]:\n",
    "                pdb_path = pdb_path_tm_rmsd[0]\n",
    "                tm = pdb_path_tm_rmsd[1]\n",
    "                rmsd = pdb_path_tm_rmsd[2]\n",
    "                \n",
    "                shutil.copy(pdb_path, range_dir)\n",
    "                pdb_dst_path = os.path.join(range_dir, os.path.basename(pdb_path))\n",
    "                other_structures.append(pdb_dst_path)\n",
    "    \n",
    "            # Store the template and list of other structures in the dictionary\n",
    "            dir_info_dict[range_dir] = (template_dst_path, other_structures)\n",
    "    \n",
    "        # At this point, dir_info_dict contains the required information\n",
    "        # and can be used as needed\n",
    "        self.oligo_split_dict = dict(dir_info_dict)  # Convert defaultdict to dict if necessary\n",
    "\n",
    "\n",
    "    # At this point, all required directories are created and PDB files are organized accordingly\n",
    "    def _separate_oligostates(self, dict_to_process):\n",
    "\n",
    "        #lets separate our found results and make new directories.   \n",
    "        oligodirdict = defaultdict()\n",
    "        \n",
    "        oligos = defaultdict(dict)\n",
    "\n",
    "        #print(f\"{dict_to_process=}\")\n",
    "        #for each range we go through all potential oligos.\n",
    "        for ranges, dicts in dict_to_process.items():\n",
    "            for key, value in dicts.items():  #dicts = dict with : key = oligostate, value = paths \n",
    "                oligos[ranges][key] = value\n",
    "                    \n",
    "        self.oligomers = oligos\n",
    "        #now lets make the dirs.\n",
    "\n",
    "        self._shuffle(oligos)\n",
    "\n",
    "\n",
    "    def _shuffle(self, dict_to_shuffle):\n",
    "        # Initialize the new dictionary structure for oligostates, ranges, and PDB paths\n",
    "        new_dict = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        for range, oligo_data in dict_to_shuffle.items():\n",
    "            for oligo, path_tm_rmsd in oligo_data.items():\n",
    "                # Create the main directory for each \"oligostate\" if it doesn't exist\n",
    "                oligo_path = os.path.join(work_dir, oligo)\n",
    "                if not os.path.exists(oligo_path):\n",
    "                    os.mkdir(oligo_path)\n",
    "        \n",
    "                # Create a subdirectory for each \"range\" within the \"oligostate\" directory\n",
    "                range_path = os.path.join(oligo_path, range)\n",
    "                if not os.path.exists(range_path):\n",
    "                    os.mkdir(range_path)\n",
    "            \n",
    "                # Copy files associated with the current range into its directory\n",
    "                for (path, tm, rsmd) in path_tm_rmsd:\n",
    "                    shutil.copy(path, range_path)  # Copy file to the new location\n",
    "                    \n",
    "                    # Append full new location\n",
    "                    new_dict[oligo][range].append((range_path, tm, rsmd))\n",
    "\n",
    "        \n",
    "        # Convert the defaultdict to a regular dict for the final output, if preferred\n",
    "        final_dict = {oligo: dict(ranges) for oligo, ranges in new_dict.items()}\n",
    "        \n",
    "        # Now 'final_dict' contains the structure you want, and can be used as needed\n",
    "        self.result_dict = final_dict\n",
    "        #return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7011c02-6797-4891-afb6-2f5c33c5eb43",
   "metadata": {},
   "source": [
    "### USALIGN execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5c257-e541-42e0-82c3-6a507d748fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblied_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab30756-8286-4eff-b69f-68411c6b169f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/SERCA\"\n",
    "\n",
    "USAligner = USAlign(work_dir=work_dir, script_dir=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\", structure_dict=assemblied_structures, \n",
    "                   cluster_min_identity=0.2,\n",
    "                   num_strucs_per_cluster=200)\n",
    "\n",
    "USAligner.USAlign_run(tm_cutoff=0.5, min_rmsd=0, max_rmsd=100, min_seqID=0.9, min_aln_len=0.90, strict=False)\n",
    "USAligner.optimize_usalign_filtering(log_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743d0e4-36b4-4bb2-b29d-8bc9ad52fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in USAligner.clustal_seqs_dict.items():\n",
    "    for code, seq in y.items():\n",
    "        #print(len(seq))\n",
    "        print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85550b2a-9eeb-4d4f-b812-316db1c89f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ae2d3-f50d-4bdc-a4b2-38f75f004821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2af57-4683-4ea9-b7c6-f55ec4ef4aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678ce359-0008-4f76-ab62-5f3b06f6c0bf",
   "metadata": {},
   "source": [
    "### CURRENT WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "812d4780-9fd7-4a23-b8ef-645581dad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrincipalComponentAnalysis2:\n",
    "\n",
    "    \"\"\"Implement LOGGING.\"\"\"\n",
    "    def __init__(self, script_dir, work_dir, multiseq_dict=None, store_original=True, logging=True):\n",
    "        self.work_dir = work_dir\n",
    "        self.logging_dir = os.path.join(work_dir, \"PCA_logs\")\n",
    "        self.script_dir = script_dir\n",
    "        self.store_full_atomistic = store_original\n",
    "        self.seqalignment_dict = multiseq_dict if multiseq_dict is not None else defaultdict(dict)\n",
    "        self.atomistic_model_directories = defaultdict(list)\n",
    "        self.prepared_ensemble_msa = defaultdict(dict)\n",
    "        self.ca_only_dict = {}\n",
    "        self.multi_pdb = None\n",
    "        self.reference_pdb_idx = 0\n",
    "        self.models = []\n",
    "        self.coord_matrix = None\n",
    "        self.mean_structure = None\n",
    "        self.eigenvalues = None\n",
    "        self.eigenvectors = None\n",
    "        self.explained_var = None\n",
    "        self.common_length = None\n",
    "        self.pdb_pc_dict = None\n",
    "        self.logging = logging #store results.\n",
    "        self.pdb_paths_for_multi_pdb = {} #initialize to save for all prepared ensembles.\n",
    "        self.pdb_codes_in_multi_pdb = {} \n",
    "        self.multi_pdb_locations = {}\n",
    "        self.reference_of_oligo_range = {}\n",
    "        self.projections_dict = {} # store pca results\n",
    "        self.ranges_kept = {}\n",
    "    \n",
    "    def _load_structure(self, pdb_path):\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(\"Protein\", pdb_path)\n",
    "        self.models = list(structure.get_models())\n",
    "\n",
    "    def _align_structure(self, reference_model_index=0):\n",
    "        \n",
    "        if reference_model_index >= len(self.models) or reference_model_index < 0:\n",
    "            raise ValueError(f\"Reference model index {reference_model_index} is out of bounds.\")\n",
    "\n",
    "        si = Superimposer()\n",
    "        ref_atoms = [atom for atom in self.models[reference_model_index].get_atoms() if atom.get_name() == \"CA\"]\n",
    "        for model in self.models:\n",
    "            alt_atoms = [atom for atom in model.get_atoms() if atom.get_name() == \"CA\"]\n",
    "            if len(ref_atoms) == len(alt_atoms):\n",
    "                si.set_atoms(ref_atoms, alt_atoms)\n",
    "                si.apply(model.get_atoms())\n",
    "\n",
    "\n",
    "    def _extract_coordinates(self):\n",
    "        \n",
    "        n_atoms = len([atom for atom in self.models[0].get_atoms() if atom.get_name() == \"CA\"])\n",
    "        n_models = len(self.models)\n",
    "        self.coord_matrix = np.zeros((n_models, n_atoms * 3))  # 3 coordinates (x, y, z) per atom\n",
    "\n",
    "        for i, model in enumerate(self.models):\n",
    "            atoms = [atom for atom in model.get_atoms() if atom.get_name() == \"CA\"]\n",
    "            for j, atom in enumerate(atoms):\n",
    "                self.coord_matrix[i, j*3:(j+1)*3] = atom.get_coord()\n",
    "\n",
    "\n",
    "    def _perform_pca(self):\n",
    "        \n",
    "        self.mean_structure = np.mean(self.coord_matrix, axis=0)\n",
    "        centered_matrix = self.coord_matrix - self.mean_structure\n",
    "        cov_matrix = np.cov(centered_matrix.T)\n",
    "        self.eigenvalues, self.eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        idx = self.eigenvalues.argsort()[::-1]\n",
    "        self.eigenvalues = self.eigenvalues[idx]\n",
    "        self.eigenvalues = np.real(self.eigenvalues) * 100\n",
    "        self.eigenvalues = np.round(self.eigenvalues, 2) # only 2 digits, and only real part.\n",
    "        self.eigenvectors = self.eigenvectors[:, idx]\n",
    "\n",
    "        explained_variance_first_two_pcs = sum(self.eigenvalues[:2]) / sum(self.eigenvalues)\n",
    "        self.explained_variance_ratio_all = self.eigenvalues / sum(self.eigenvalues) # this we will fetch for plotting. EXPL var for PC1 and 2\n",
    "            \n",
    "    \n",
    "    def _project_structures(self, n_components=2):\n",
    "        \n",
    "        centered_matrix = self.coord_matrix - self.mean_structure\n",
    "        projection = np.dot(centered_matrix, self.eigenvectors[:, :n_components])\n",
    "        return projection\n",
    "\n",
    "    def run_PCA(self, reference_model_index=0, n_components=2):\n",
    "        #print(\"inside run pca\")\n",
    "        #print(f\"{self.multi_pdb_locations=}, {self.pdb_codes_in_multi_pdb=}\")\n",
    "        \n",
    "        if not self.multi_pdb_locations or not self.pdb_codes_in_multi_pdb:\n",
    "            print(\"No multi PDB locations or PDB codes available to run PCA.\")\n",
    "            return\n",
    "\n",
    "            #self.multi_pdb_locations={'monomer_-5_1063': '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/SERCA/monomer_-5_1063_multi_ensemble.pdb'}, \n",
    "            #self.pdb_codes_in_multi_pdb={'monomer_-5_1063': ['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/SERCA/CA_only_models/monomer_-5_1063/6yaa_A.pdb',\n",
    "            #'/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/SERCA/CA_only_models/monomer_-5_1063/5avr_A.pdb', \n",
    "            #'/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/SERCA/CA_only_models/monomer_-5_1063/7w47_A.pdb',...\n",
    "        \n",
    "        for range_key in self.multi_pdb_locations: # both dicts need to have the same key, which they have so lets go with that.\n",
    "            \n",
    "            multi_pdb_path = self.multi_pdb_locations[range_key]\n",
    "            pdb_code_list = self.pdb_codes_in_multi_pdb[range_key]\n",
    "            #print(f\"{multi_pdb_path=}, {pdb_code_list=}\")\n",
    "            #if we get valid hits for both then we continue\n",
    "            if multi_pdb_path and pdb_code_list:\n",
    "                print(f\"Processing PCA for range key: {range_key}\")\n",
    "                self._load_structure(multi_pdb_path)\n",
    "                self._align_structure(reference_model_index=reference_model_index)\n",
    "                self._extract_coordinates()\n",
    "                self._perform_pca()\n",
    "                self.projections_dict[range_key] = self._project_structures(n_components=n_components), pdb_code_list\n",
    "        \n",
    "\n",
    "    def prepare_ensemble(self, msa_dict):\n",
    "        if self.store_full_atomistic:\n",
    "            self._copy_original_pdbs(msa_dict)\n",
    "        \n",
    "        for oligo_range, inner_dict in msa_dict.items():\n",
    "            store_dir = os.path.join(self.work_dir, \"CA_only_models\", oligo_range)\n",
    "            os.makedirs(store_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "            # Initialize an empty list for this oligo_range\n",
    "            self.pdb_paths_for_multi_pdb[oligo_range] = []\n",
    "            \n",
    "            tasks = [(pdb_path, seqs_to_extract, store_dir) for pdb_path, seqs_to_extract in inner_dict.items()]\n",
    "            \n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = {executor.submit(self._fetch_correct_res_parallelized, *task): task[0] for task in tasks}\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    pdb_path = futures[future]  # Original PDB path\n",
    "                    try:\n",
    "                        save_path = future.result()  # Path of the processed PDB\n",
    "                        self.pdb_paths_for_multi_pdb[oligo_range].append(save_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {pdb_path}: {e}\")\n",
    "\n",
    "        # After processing all PDBs, create the multi-model PDB\n",
    "        for oligo_range in self.pdb_paths_for_multi_pdb:\n",
    "            if self.pdb_paths_for_multi_pdb[oligo_range]:\n",
    "                self._create_multi_pdb(oligo_range)\n",
    "                \n",
    "\n",
    "    def _fetch_correct_res_parallelized(self, pdb_path, partial_seq_to_extract, new_save_dir):\n",
    "\n",
    "\n",
    "\n",
    "        class CASelect(Select):\n",
    "            def __init__(self, residue_nums):\n",
    "                self.residue_nums = residue_nums\n",
    "        \n",
    "            def accept_residue(self, residue):\n",
    "                return residue.id[1] in self.residue_nums\n",
    "        \n",
    "            def accept_atom(self, atom):\n",
    "                return atom.name == 'CA'\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)    \n",
    "        structure = parser.get_structure('default', pdb_path)\n",
    "\n",
    "\n",
    "        # lets implement another step previously here. for security reason. We just take the struc. check for \n",
    "        #non_canoncial = self.process_structure(pdb_path)\n",
    "\n",
    "        #if non_canoncial:\n",
    "        #    print(f\"we had {non_canoncial=}, for {pdb_path=}\")\n",
    "\n",
    "        \n",
    "        lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                        ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                        ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                        ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "\n",
    "\n",
    "        canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "        \n",
    "        struc_seq_and_res = [(canonical_aas[res.get_resname()], res.get_id()[1]) for res in structure.get_residues() if res.get_id()[0] == ' ']\n",
    "        #list of tuples (resname \"A\", \"14\") e.g\n",
    "        #print(f\"{struc_seq_and_res=}\")\n",
    "        \n",
    "        full_seq_types, full_seq_nums = zip(*struc_seq_and_res)\n",
    "\n",
    "        found_residue_nums = []\n",
    "\n",
    "        j = 0  # Index for partial_sequence\n",
    "\n",
    "        #print(f\"{full_seq_types=}, {partial_seq_to_extract=},{len(partial_seq_to_extract)=}\")\n",
    "        print(f\"{len(partial_seq_to_extract)=}\")\n",
    "        \n",
    "        for i in range(len(full_seq_types)):  # the full sequence\n",
    "            if j >= len(partial_seq_to_extract):\n",
    "                break  # Exit if we've processed the entire partial_sequence\n",
    "    \n",
    "            if partial_seq_to_extract[j] == full_seq_types[i] or partial_seq_to_extract[j] == \"X\": # we continue with X because its a posttranslat. modified residue.\n",
    "                found_residue_nums.append(full_seq_nums[i])\n",
    "                j += 1  # Advance in partial_sequence only if a match is found\n",
    "\n",
    "        #print(f\"{found_residue_nums=}, {len(found_residue_nums)=}\")\n",
    "        pdb_io = PDBIO()\n",
    "        pdb_io.set_structure(structure)\n",
    "        save_path = os.path.join(new_save_dir, os.path.basename(pdb_path))\n",
    "        pdb_io.save(save_path, CASelect(found_residue_nums))\n",
    "        \n",
    "        return save_path\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def _copy_original_pdbs(self,msa_dict):\n",
    "        \n",
    "        for oligo_range, inner_dict in msa_dict.items():\n",
    "            store_dir = os.path.join(self.work_dir, \"full_pdb_models\",oligo_range)\n",
    "            if not os.path.exists(store_dir):\n",
    "                os.makedirs(store_dir, exist_ok=True)\n",
    "            \n",
    "            for pdb_path ,_ in inner_dict.items():\n",
    "                pdb_base_id = os.path.basename(pdb_path)\n",
    "                dst = os.path.join(store_dir, pdb_base_id)\n",
    "                shutil.copy(pdb_path, dst)\n",
    "\n",
    "    \n",
    "    def _process_structures(self):\n",
    "\n",
    "        for pdb_id, algnd_seq in self.seqalignment_dict.items():\n",
    "            pdb_path = os.path.join(self.work_dir, pdb_id)\n",
    "            if pdb_path.endswith(\"ca_only.pdb\"):\n",
    "                return\n",
    "                    \n",
    "            processed_path, _ = self.process_structure(pdb_path)\n",
    "            # Update seqalignment_dict with the new path\n",
    "            print(f\"{processed_path=}\")\n",
    "            #self.seqalignment_dict[dir_path][pdb_id] = os.path.basename(processed_path)\n",
    "\n",
    "\n",
    "    def process_structure(self, pdb_path):\n",
    "        \"\"\"Selects C-alpha atoms, checks for non-canonical amino acids, and mutates them.\"\"\"\n",
    "        canonical_aas = {'VAL', 'ILE', 'LEU', 'GLU', 'GLN',\n",
    "                         'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR',\n",
    "                         'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA',\n",
    "                         'GLY', 'PRO', 'CYS'}\n",
    "    \n",
    "        # Initialize parser and parse the structure\n",
    "        parser = PDBParser(QUIET=True)\n",
    "            \n",
    "        structure = parser.get_structure('default', pdb_path)\n",
    "    \n",
    "        # Initialize defaultdict for non-canonical amino acids\n",
    "        non_canonical_aas = defaultdict(list)\n",
    "    \n",
    "        class StructureProcessor(Select):\n",
    "            def accept_atom(self, atom):\n",
    "                # Keep only C-alpha atoms\n",
    "                return atom.get_id() == \"CA\"\n",
    "    \n",
    "            def accept_residue(self, residue):\n",
    "                # Keep residues if they are canonical and standard (i.e., not heteroatoms or water)\n",
    "                is_canonical = residue.get_resname().strip() in canonical_aas\n",
    "                is_standard = residue.id[0] == ' '\n",
    "                if not is_canonical and is_standard:\n",
    "                    # Mutate non-standard amino acid to alanine\n",
    "                    residue.resname = \"ALA\"\n",
    "                    non_canonical_aas[(residue.get_resname(), residue.id[1])].append(residue.get_parent().id)\n",
    "                return is_standard\n",
    "    \n",
    "        # Process structure: select C-alpha atoms and mutate non-canonical residues\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        \n",
    "        #processed_path = f\"{os.path.splitext(pdb_path)[0]}_ca_only.pdb\"\n",
    "        io.save(pdb_path, select=StructureProcessor())\n",
    "    \n",
    "        # Return path of the processed structure and non-canonical amino acids info\n",
    "        return dict(non_canonical_aas)\n",
    "\n",
    "    \n",
    "\n",
    "    def _gather_sequences(self):\n",
    "        # Gather sequences and paths from seqalignment_dict\n",
    "        sequences = []\n",
    "        for pdb_id, seq in self.ca_only_dict.items():\n",
    "            #print(f\"{dir_path=}\")\n",
    "            #print(f\"{pdb_dict=}\")\n",
    "                \n",
    "            sequences.append((os.path.join(self.work_dir, pdb_id), seq))\n",
    "        return sequences\n",
    "\n",
    "    def _trim_structure(self, path_seq_lst, cutting_method=\"strict\"):\n",
    "        \n",
    "        # Implement the trimming logic based on the cutting method\n",
    "        # This should return the path to the trimmed structure and the list of retained residues\n",
    "        # For example:\n",
    "\n",
    "        if cutting_method == \"strict\":\n",
    "            trimmed_residues = self._hard_trimmer(path_seq_lst)\n",
    "            #print(f\"{pdb_path=}\")\n",
    "\n",
    "            \n",
    "            return trimmed_residues\n",
    "\n",
    "    def _hard_trimmer(self, input_dict):\n",
    "        # This will store the trimmed sequences for each oligo_range\n",
    "        trimmed_results = defaultdict(dict)\n",
    "    \n",
    "        for oligo_range, pdb_data in input_dict.items():\n",
    "            # Extract pdb paths and sequences for the current oligo range\n",
    "            pdb_paths, sequences_data = zip(*pdb_data.items())\n",
    "            # Concatenate the amino acid letters from the tuples to form the full sequence for each PDB\n",
    "            full_sequences = [''.join([aa for aa, pos in seq_data]) for seq_data in sequences_data]\n",
    "    \n",
    "            # Convert pdb_paths to a format suitable for pytrimal\n",
    "            pdb_codes = [path.encode('utf-8') for path in pdb_paths]\n",
    "    \n",
    "            # Create an Alignment object for pytrimal\n",
    "            alignment = pytrimal.Alignment(pdb_codes, full_sequences)\n",
    "            # Initialize the ManualTrimmer\n",
    "            trimmer = pytrimal.ManualTrimmer(gap_threshold=1)  # Adjust the gap_threshold as needed\n",
    "            # Perform the trimming\n",
    "            trimmed_alignment = trimmer.trim(alignment)\n",
    "    \n",
    "            # Store the trimmed sequences back into the dictionary\n",
    "            for name, trimmed_seq in zip(trimmed_alignment.names, trimmed_alignment.sequences):\n",
    "                # Decode the name from bytes to string, remove any left whitespace\n",
    "                pdb_name = name.decode().strip()\n",
    "                # Store the trimmed sequence in the dictionary under the corresponding oligo_range and PDB path\n",
    "                trimmed_results[oligo_range][pdb_name] = trimmed_seq\n",
    "    \n",
    "        return dict(trimmed_results)\n",
    "        \n",
    "\n",
    "    def _select_proper_residues_after_trim(self, keep_residue_dict:dict, save_dir:dir):\n",
    "        \n",
    "        for pdb, seq in keep_residue_dict.items():\n",
    "        \n",
    "            full_p = os.path.join(self.work_dir, pdb)\n",
    "            result_seq = self._grab_sequence_from_struc(full_p)\n",
    "            positions_to_keep = self._find_positions(result_seq[0], seq)\n",
    "            #print(f\"{len(positions_to_keep)=}\")\n",
    "            #print(f\"this is seq to keep: {seq}, this is full_seq from structure: {result_seq}\")\n",
    "            #print(f\"this is our savepath for cutted struc: {full_p}\")\n",
    "            self._select_trimmed_positions(path_to_pdb=full_p, list_to_keep=positions_to_keep, save_dir=save_dir)\n",
    "            \n",
    "\n",
    "    def _grab_sequence_from_struc(self, pdb):\n",
    "\n",
    "        lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                        ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                        ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                        ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "        \n",
    "        canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        pdb_name = os.path.basename(pdb)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, pdb)\n",
    "        \n",
    "        struc_seq_and_res = [(canonical_aas[res.get_resname()], res.get_id()[1]) for res in structure.get_residues() if res.get_id()[0] == ' ']\n",
    "        print(f\"{struc_seq_and_res=}\")\n",
    "    \n",
    "        return (struc_seq_and_res, pdb)\n",
    "\n",
    "    def _find_positions(self, full_sequence, partial_sequence):\n",
    "        \n",
    "        i = 0  # Index for full_sequence\n",
    "        j = 0  # Index for partial_sequence\n",
    "        found_positions = []\n",
    "\n",
    "        #print(f\"{full_sequence=}, {partial_sequence=}\")\n",
    "        while i < len(full_sequence) and j < len(partial_sequence):\n",
    "            if partial_sequence[j] == '-':  # Skip gap positions in partial_sequence\n",
    "                j += 1\n",
    "            elif full_sequence[i][0] == partial_sequence[j]:  # Match found\n",
    "                found_positions.append(full_sequence[i][1])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1  # Advance in full_sequence if no match\n",
    "    \n",
    "        #found_pos = [x + 1 for x in found_positions]  # Adjust for 1-based indexing\n",
    "        return found_positions\n",
    "\n",
    "\n",
    "    def _select_trimmed_positions(self, path_to_pdb:str, list_to_keep:str, save_dir):\n",
    "        #sel only c_alpha\n",
    "        class CAlphaTrimmedPositions(Select):\n",
    "            def __init__(self, list_to_keep, *args):\n",
    "                super().__init__(*args)\n",
    "                self.list_to_keep = list_to_keep\n",
    "                \n",
    "            #overload accept_residue inherited from Select with this conditional return\n",
    "            def accept_atom(self, atom):\n",
    "                return 1 if atom.id == \"CA\" else 0\n",
    "            \n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] in self.list_to_keep else 0    \n",
    "\n",
    "        #print(f\"{path_to_pdb=}, {list_to_keep=}\")\n",
    "        #lets make a new dir where we store the trimmed strucs.\n",
    "        pdb_name = os.path.basename(path_to_pdb)\n",
    "        new_location = os.path.join(save_dir, pdb_name) #here we save\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, path_to_pdb)\n",
    "        #print(f\"this is structure: {path_to_pdb} for list to keep shifted: {list_to_keep_shifted}\")\n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(new_location, CAlphaTrimmedPositions(list_to_keep=list_to_keep))\n",
    "\n",
    "\n",
    "    def _create_multi_pdb(self, oligo_range, reference=None):\n",
    "\n",
    "        pdb_files = self.pdb_paths_for_multi_pdb[oligo_range] #the paths as values in a list.\n",
    "\n",
    "        count_dict = Counter()\n",
    "        \n",
    "        for file in pdb_files:\n",
    "            try:\n",
    "                structure = PDBParser(QUIET=True).get_structure(\"default\", file)\n",
    "                struc_len = len(list(structure.get_residues()))\n",
    "                count_dict[struc_len] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "        \n",
    "        most_common_length = count_dict.most_common(1)[0][0]\n",
    "\n",
    "        master_structure = Structure.Structure(\"master\")\n",
    "        i = 0\n",
    "        \n",
    "        list_of_pdb_codes = []\n",
    "        for file in pdb_files:\n",
    "            try:\n",
    "                structure = PDBParser(QUIET=True).get_structure(\"default\", file)\n",
    "                struc_len = len(list(structure.get_residues()))\n",
    "    \n",
    "                if struc_len != most_common_length:\n",
    "                    print(f\"Skipping {file} due to incorrect residue count: {struc_len} (expected: {most_common_length})\")\n",
    "                    continue\n",
    "    \n",
    "                list_of_pdb_codes.append(file)\n",
    "                for model in structure:\n",
    "                    new_model = model.copy()\n",
    "                    new_model.id = len(master_structure)\n",
    "                    for chain in new_model:\n",
    "                        for residue in chain:\n",
    "                            for atom in residue:\n",
    "                                atom.set_bfactor(0)\n",
    "                    master_structure.add(new_model)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {file}: {e}\")\n",
    "    \n",
    "        pdb_io = PDBIO()\n",
    "        pdb_io.set_structure(master_structure)\n",
    "        multi_pdb_path = os.path.join(self.work_dir, f\"{oligo_range}_multi_ensemble.pdb\")\n",
    "        pdb_io.save(multi_pdb_path)\n",
    "        self.ranges_kept[oligo_range] = most_common_length\n",
    "\n",
    "        \n",
    "        self.multi_pdb_locations[oligo_range] = multi_pdb_path\n",
    "        self.pdb_codes_in_multi_pdb[oligo_range] = list_of_pdb_codes\n",
    "        \n",
    "\n",
    "\n",
    "    def plot_pca_projections(self, show_specific_strucs=False):\n",
    "\n",
    "        #check first if there is data available to show!\n",
    "        if self.projections_dict:\n",
    "\n",
    "            self.pdb_all_ranges = {}\n",
    "            \n",
    "            for range_key, pdb_proj_arr_pdb_codes in self.projections_dict.items():\n",
    "                \n",
    "                pdb_proj_arr = pdb_proj_arr_pdb_codes[0]\n",
    "                pdb_codes_labels = pdb_proj_arr_pdb_codes[1]\n",
    "                \n",
    "                pca_real = pdb_proj_arr.real\n",
    "        \n",
    "                pc1 = pca_real[:, 0]* -1 # lets flip it \n",
    "                pc2 = pca_real[:, 1]\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.scatter(pc1, pc2, c='blue', marker='o', edgecolor='k', s=60)\n",
    "        \n",
    "        \n",
    "                self.pdb_pc_dict = {}\n",
    "\n",
    "                print(f\"{len(pdb_codes_labels)} structures in the ensemble\")\n",
    "                for i, prot_label in enumerate(pdb_codes_labels):\n",
    "                    self.pdb_pc_dict[prot_label] = (pc1[i], pc2[i])\n",
    "    \n",
    "            \n",
    "                if show_specific_strucs:\n",
    "                    # lets add jitter\n",
    "                    jitter = np.random.normal(0, 0.05, size=pc1.shape)\n",
    "            \n",
    "                    # Annotating each point with its index for identification\n",
    "                    for i, (x, y) in enumerate(zip(pc1 + jitter, pc2 + jitter)):\n",
    "                        \n",
    "                        pdb_label = os.path.basename(pdb_codes_labels[i])[0:4]\n",
    "                        print(pdb_label)\n",
    "                        if pdb_label in show_specific_strucs:\n",
    "                            plt.text(x, y, f'{pdb_label}', fontsize=9)\n",
    "\n",
    "                \n",
    "                plt.title(f'PCA Results: num_residues: {self.ranges_kept[range_key]}, structures: {len(pdb_codes_labels)}')\n",
    "                plt.xlabel(f'Principal Component 1 (PC1) {np.round(self.explained_variance_ratio_all[0],4)*100:.2f}%')\n",
    "                plt.ylabel(f'Principal Component 2 (PC2) {np.round(self.explained_variance_ratio_all[1],4)*100:.2f}%')\n",
    "                #plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "                self.pdb_all_ranges[range_key] = self.pdb_pc_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d78e85-3a72-44c4-9f65-fd3c34d9e8a5",
   "metadata": {},
   "source": [
    "# run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e088c2-e53e-441e-853e-4a7e271a07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/SERCA\"\n",
    "script_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\"\n",
    "multiseq_dict = USAligner.clustal_seqs_dict\n",
    "\n",
    "PCA_tool = PrincipalComponentAnalysis2(script_dir=script_dir, work_dir=work_dir,\n",
    "                                      multiseq_dict=multiseq_dict, store_original=True)\n",
    "\n",
    "PCA_tool.prepare_ensemble(USAligner.clustal_seqs_dict) # seems to work. but careful with non canonical residues. might be handled differently later.This version introduces an ALA instead.\n",
    "PCA_tool.run_PCA()\n",
    "PCA_tool.plot_pca_projections(show_specific_strucs=[\"2c9m\", \"1t5s\", \"4hyt\", \"1t5t\"])\n",
    "#for x, y in zip(projected_pcs, protein_labels):\n",
    "#    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794630e-3fea-4c67-b7f1-a1a487516101",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0448a8-92a7-420b-be8e-8eab6bcae1a4",
   "metadata": {},
   "source": [
    "### OLD PCA CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca8664-fb74-4634-ac35-ab8017695ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModellerRepairEnsemble:\n",
    "    '''\n",
    "    Modeller Class that can be used to repair structures if needed.\n",
    "    Returned structures should be compatible with format for downstream analysis.\n",
    "    '''\n",
    "    def __init__(self, work_dir, ensemble_dict):\n",
    "\n",
    "        self.repair_ensemble = ensemble_dict\n",
    "        self.work_dir = work_dir\n",
    "        self.monomers = None\n",
    "        self.oligomers = None\n",
    "        self.monomeric_templates= None\n",
    "        self.oligomeric_templates = None\n",
    "        self.usalign_exe = os.path.join(work_dir, \"USalign\")\n",
    "        self.repairable_structures = None\n",
    "        self.repaired_monomers = None\n",
    "        self.repaired_oligomers = None\n",
    "\n",
    "    \n",
    "    def _separate_oligostates(self):\n",
    "\n",
    "        lst = [(\"1\",'monomer'),(\"2\", 'dimer'),(\"3\", 'trimer'),(\"4\", 'tetramer'),(\"5\", 'pentamer'),\n",
    "           (\"6\", 'hexamer'),(\"7\", 'heptamer'),(\"8\", 'oktamer'),(\"9\", 'nonamer'),(\"10\", 'decamer'),\n",
    "           (\"11\", 'undecamer'),(\"12\", 'dodecamer'),(\"13\", 'tridecamer'),\n",
    "           (\"14\", 'tetradecamer'),(\"15\", 'pentadecamer'),(\"16\", 'hexadecamer'),\n",
    "           (\"17\", 'heptadecamer'),(\"18\", 'oktadecamer'),(\"19\", 'nonadecamer'),(\"20\", 'eicosamer'),\n",
    "           (\"21\", 'eicosameundamer'),(\"22\", 'eicosadodamer'),(\"23\", 'eicosatrimer'),(\"24\", 'eicosatetramer')]\n",
    "\n",
    "        oligodirdict = defaultdict(lambda: \"X-mer\", lst)\n",
    "\n",
    "        \n",
    "        oligos = defaultdict(dict)\n",
    "        monos = defaultdict(dict)\n",
    "        for ranges, dicts in self.repair_ensemble.items():\n",
    "            for key, value in dicts.items():\n",
    "                if key == 1:\n",
    "                    monos[ranges][oligodirdict[str(key)]] = value\n",
    "                else:\n",
    "                    oligos[ranges][oligodirdict[str(key)]] = value\n",
    "\n",
    "        self.monomers, self.oligomers = monos, oligos\n",
    "\n",
    "    def _shuffle(self, dict_to_shuffle):\n",
    "\n",
    "\n",
    "        lst = [(\"1\",'monomer'),(\"2\", 'dimer'),(\"3\", 'trimer'),(\"4\", 'tetramer'),(\"5\", 'pentamer'),\n",
    "           (\"6\", 'hexamer'),(\"7\", 'heptamer'),(\"8\", 'oktamer'),(\"9\", 'nonamer'),(\"10\", 'decamer'),\n",
    "           (\"11\", 'undecamer'),(\"12\", 'dodecamer'),(\"13\", 'tridecamer'),\n",
    "           (\"14\", 'tetradecamer'),(\"15\", 'pentadecamer'),(\"16\", 'hexadecamer'),\n",
    "           (\"17\", 'heptadecamer'),(\"18\", 'oktadecamer'),(\"19\", 'nonadecamer'),(\"20\", 'eicosamer'),\n",
    "           (\"21\", 'eicosameundamer'),(\"22\", 'eicosadodamer'),(\"23\", 'eicosatrimer'),(\"24\", 'eicosatetramer')]\n",
    "\n",
    "        oligodirdict = defaultdict(lambda: \"X-mer\", lst)\n",
    "\n",
    "\n",
    "        \n",
    "        dirs_to_check = []\n",
    "        \n",
    "        for keys, vals in dict_to_shuffle.items():\n",
    "            print(keys)\n",
    "            keys = keys.split(\",\") #CONVERT tuple to a string.\n",
    "            start, stop = keys[0][1:], keys[1][:-1]\n",
    "            print(start)\n",
    "            print(stop)\n",
    "            new_range_obj = start+\"-\"+stop\n",
    "            dir_path = os.path.join(work_dir, new_range_obj)\n",
    "            if not os.path.exists(dir_path):\n",
    "                #get rid of whitespace which makes issues later downstream\n",
    "                os.mkdir(dir_path)\n",
    "                \n",
    "            for oligo, path_tm_rmsd in vals.items():\n",
    "                print(oligo)\n",
    "                oligo_path = os.path.join(work_dir, new_range_obj, oligodirdict[str(oligo)])\n",
    "                if not os.path.exists(oligo_path):\n",
    "                    os.mkdir(oligo_path)\n",
    "                dirs_to_check.append(oligo_path) #this is a list that contains the locations of all directories that will be visited during repair.\n",
    "                for path, _, _ in path_tm_rmsd:\n",
    "                    shutil.copy(path, oligo_path) #new location\n",
    "        \n",
    "        return dirs_to_check\n",
    "\n",
    "    \n",
    "    def repair_structures(self):\n",
    "        # first the monomeric case.\n",
    "        self._separate_oligostates()\n",
    "        \n",
    "        # run now repairs on both.\n",
    "        if self.monomers:\n",
    "            #we need a separate environment for modeller etc.\n",
    "            self.monomer_dirs = self._shuffle(self.monomers)\n",
    "            oligo_dir_dict = defaultdict()\n",
    "            if self.monomer_dirs:\n",
    "                for directory in self.monomer_dirs:\n",
    "                    #check which pdbs need repair.\n",
    "                    pdb_to_check = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".pdb\")] \n",
    "                    #first get the gap ranges\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        gap_list = list(executor.map(self._gap_localization_1, pdb_to_check))\n",
    "                    #each gaplist member is a tuple of (path, list of gaps)\n",
    "                    # keys: templates:   inner keys: the targets inner vals: their seqid and their TM score. which we will now filter and take the top X target to repair our structure.\n",
    "                    \n",
    "                    #print(f\"{gap_list=}\")\n",
    "                    \n",
    "                    suitable_templates = self._get_templates_for_monomeric_multitemplate_modelling(gap_list)\n",
    "\n",
    "                    #print(f\"{suitable_templates=}\")\n",
    "                    # now lets go through each and every of them an check if there are gaps.\n",
    "                    \n",
    "                    potential_template_dict = self._check_repairability(suitable_templates, gap_list)\n",
    "\n",
    "                    #print(f\"{potential_template_dict=}\")\n",
    "                    \n",
    "                    #next step is repair. if empty we skip. else we go through all repairable structures.\n",
    "                    if potential_template_dict:\n",
    "                        with ProcessPoolExecutor() as executor:\n",
    "                            key, value = zip(*potential_template_dict.items()) #unpack first \n",
    "                            try:\n",
    "                                print(\"we try intelligent_monomeric_repair. \")\n",
    "                                results = list(executor.map(self._intelligent_monomeric_repair, key, value))\n",
    "                            except Exception as e:\n",
    "                                results = []\n",
    "                                print(e)\n",
    "                        oligo_dir_dict[directory] = results\n",
    "                        \n",
    "                    else:\n",
    "                        print(f\"We have no repaired structures for directory: {directory}\")\n",
    "                        oligo_dir_dict[directory] = []\n",
    "                        \n",
    "            #this is a dict of paths for each oligostate and range as a instance of the class saved.\n",
    "            self.repaired_monomers =  oligo_dir_dict\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.oligomers:\n",
    "            #we need a separate environment for modeller etc.\n",
    "            self.oligomer_dirs = self._shuffle(self.oligomers)\n",
    "\n",
    "            oligo_dir_dict = defaultdict()\n",
    "            if self.oligomer_dirs:\n",
    "                for directory in self.oligomer_dirs:\n",
    "                    #check which pdbs need repair.\n",
    "                    pdb_to_check = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".pdb\")] \n",
    "\n",
    "\n",
    "                    \"\"\"Lets implement this later. First we just take all structures as they are and continue.\"\"\"\n",
    "                    oligo_dir_dict[directory] = pdb_to_check\n",
    "\n",
    "                    \n",
    "                    #first get the gap ranges\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        gap_list = list(executor.map(self._gap_localization_1_oligomeric, pdb_to_check))\n",
    "        \n",
    "\n",
    "                    suitable_templates = self._get_templates_for_oligo_multitemplate_modelling(template_path=template_path,\n",
    "                                                                    potential_templates=template_backup_list,\n",
    "                                                                    gap_dict=gap_dict)\n",
    "\n",
    "\n",
    "            self.repaired_oligos = oligo_dir_dict\n",
    "\n",
    "\n",
    "    def _check_repairability(self, template_dict: dict, gap_list: list, min_seq_id: float = 0.6) -> dict:\n",
    "        \"\"\"\n",
    "        Checks each structure's eligibility for repair based on gap alignment and minimum sequence identity.\n",
    "    \n",
    "        Args:\n",
    "        - template_dict (dict): A dictionary where keys are template paths and values are dicts containing template details.\n",
    "        - gap_list (list): A list of tuples representing the paths and gaps in the structure to be repaired (path, [(start, stop), ...]).\n",
    "        - min_seq_id (float): The minimum sequence identity required for a template to be considered suitable.\n",
    "    \n",
    "        Returns:\n",
    "        - dict: A dictionary of potential templates that can be used for repair.\n",
    "        \"\"\"\n",
    "        \n",
    "        potential_template_dict = defaultdict(list)\n",
    "        for (temp_path, temp_gaps) in gap_list:\n",
    "            for template_path, details in template_dict.items():\n",
    "                # Assume details structure is a dict with keys as some identifiers and values as tuples of details\n",
    "                for key, val in details.items():\n",
    "                    try:\n",
    "                        # Attempt to unpack expecting 3 elements; adjust according to your data structure\n",
    "                        tm_score, seq_id, template_gaps = val[0][0], val[0][1], val[1]\n",
    "                    except ValueError:\n",
    "                        # Handle cases where unpacking fails due to unexpected structure\n",
    "                        print(f\"Skipping {key} due to unpacking error: expected 3 values, got {len(val)}\")\n",
    "                        continue\n",
    "                        \n",
    "                    #print(seq_id, min_seq_id)\n",
    "                    if float(seq_id) > min_seq_id and not self._has_large_gap_overlap(temp_gaps, template_gaps):\n",
    "                        #print(seq_id, min_seq_id)\n",
    "                        #print(temp_path, template_path)\n",
    "                        potential_template_dict[(temp_path, temp_gaps)].append((template_path, (tm_score, seq_id, template_gaps)))\n",
    "        \n",
    "        return potential_template_dict\n",
    "\n",
    "    def _has_large_gap_overlap(self, gaps1: list, gaps2: list) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if there is a large overlap in gaps between two structures.\n",
    "    \n",
    "        Args:\n",
    "        - gaps1 (list): The gaps in the first structure as a list of tuples (start, stop).\n",
    "        - gaps2 (list): The gaps in the second structure as a list of tuples (start, stop).\n",
    "    \n",
    "        Returns:\n",
    "        - bool: True if there is a large overlap, False otherwise.\n",
    "        \"\"\"\n",
    "        for start1, end1 in gaps1:\n",
    "            for start2, end2 in gaps2:\n",
    "                if start1 <= end2 and start2 <= end1:  # Check if gaps overlap\n",
    "                    return True  # Overlapping gaps found\n",
    "        return False  # No overlapping gaps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _gap_localization_1(self, pdb_path: str):\n",
    "        \"\"\"Helper function to compute the start and stops of gaps \n",
    "        for later potential reconstruction.\"\"\"\n",
    "    \n",
    "        gap_ranges = []\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "        \n",
    "        test_res = test_structure.get_residues()\n",
    "        start = end = None\n",
    "    \n",
    "        for res in test_res:\n",
    "            res_id = int(res.get_id()[1])\n",
    "            if end is None:\n",
    "                start = end = res_id\n",
    "            elif res_id == end + 1:\n",
    "                end = res_id\n",
    "            else:\n",
    "                if start != end:\n",
    "                    gap_ranges.append((start, end))\n",
    "                start = end = res_id\n",
    "    \n",
    "        if start is not None and start != end:\n",
    "            gap_ranges.append((start, end))\n",
    "\n",
    "        if not gap_ranges:\n",
    "            return (pdb_path, [])  # Return an empty list if there are no gaps\n",
    "\n",
    "        # Convert the list of gap ranges to a list of gap tuples\n",
    "        gap_tuples = [(start, end) for start, end in gap_ranges]\n",
    "        merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "        return (pdb_path, merged_gaps)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_templates_for_monomeric_multitemplate_modelling(self, pdb_gap_list):\n",
    "        parser = PDBParser()\n",
    "        suitable_templates = {}\n",
    "\n",
    "        for template_index, (template_path, tmp_gap) in enumerate(pdb_gap_list):\n",
    "            template_results = {}\n",
    "\n",
    "            for target_index, (target_path, gaps) in enumerate(pdb_gap_list):\n",
    "                if target_index != template_index:  # Avoid self-comparison\n",
    "                    score = self._align(target_path, template_path)\n",
    "                    template_results[target_path] = (score, gaps)\n",
    "\n",
    "            suitable_templates[template_path] = template_results\n",
    "\n",
    "        #print(suitable_templates)\n",
    "        # outer keys: templates, inner keys: target_structures. inner values: (seq_id, tm., gaps) \n",
    "        return suitable_templates\n",
    "    \n",
    "    \n",
    "    def _gap_localization_1_oligomeric(self,pdb_path: str):\n",
    "        \"\"\"Helper function to compute the start and stops of gaps for later potential reconstruction.\"\"\"\n",
    "        \n",
    "        full_gaps_dict = defaultdict()\n",
    "        gap_dict = defaultdict(list)\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "        \n",
    "        for model in test_structure:\n",
    "            for chain in model:\n",
    "                for residues in chain:\n",
    "                    resnum = residues.get_id()\n",
    "                    gap_dict[chain.id].append(resnum[1])\n",
    "    \n",
    "        #now we have all chains and all sequence nums from start to finish\n",
    "        chain_to_query = []\n",
    "    \n",
    "        for chains, residues in gap_dict.items():\n",
    "            chain_to_query.append(chains)\n",
    "            \n",
    "            start = end = None\n",
    "            gap_ranges = []\n",
    "            for res in residues:\n",
    "                if end is None:\n",
    "                    start = end = res\n",
    "                elif res == end + 1:\n",
    "                    end = res\n",
    "                else:\n",
    "                    if start != end:\n",
    "                        gap_ranges.append((start, end))\n",
    "                    start = end = res\n",
    "    \n",
    "            if start is not None and start != end:\n",
    "                gap_ranges.append((start, end))\n",
    "    \n",
    "            gap_tuples = [(start, end) for start, end in gap_ranges]\n",
    "            merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "            full_gaps_dict[chains] = merged_gaps\n",
    "        \n",
    "        return full_gaps_dict, chain_to_query\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _align(self, target_path, template_path):\n",
    "        \n",
    "        # Implement the logic to align the target_path with the template_path\n",
    "        # focusing on the region defined by 'gap', and return a scoring metric\n",
    "        bash_tm_and_rmsd_calc = f\"{self.usalign_exe} {target_path} {template_path} -TMscore 0 -outfmt 1\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        \n",
    "        result_scores_for_templates = defaultdict()\n",
    "        \n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "            #print(result.stdout)\n",
    "            result_list = self._get_aligned_fastas(result.stdout)  # gap list not used here... can be removed in future version.\n",
    "\n",
    "            #print(result_list)\n",
    "            return result_list\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e)\n",
    "            return {}\n",
    "            \n",
    "\n",
    "\n",
    "    def _get_aligned_fastas(self, result):\n",
    "\n",
    "        lines = result.strip().split('\\n')\n",
    "        #this is gap_list : [(1, 3), (263, 276), (396, 407)]\n",
    "        #print(\"this is gap_list :\", gap_list)\n",
    "        result_list = []\n",
    "        #print(lines)\n",
    "        # Initialize an empty dictionary\n",
    "        template_header = lines[2].split(\"\\t\")\n",
    "        #template_full_seq = lines[3]  #replace - to get the full seq.\n",
    "        template_seq_id = float(template_header[3].split(\"=\")[-1])\n",
    "        template_tm_score = float(template_header[4].split(\"=\")[-1])\n",
    "        template_pdb_path = template_header[0].replace(\">\",\"\")\n",
    "        template_pdb_path = template_pdb_path.split(\":\")[0]\n",
    "    \n",
    "        potential_template_header = lines[0].split(\"\\t\")\n",
    "        #potential_template_full_seq = lines[1]\n",
    "        potential_template_seq_id = float(potential_template_header[3].split(\"=\")[-1])\n",
    "        potential_template_tm_score = float(potential_template_header[4].split(\"=\")[-1])\n",
    "        potential_template_pdb_path = potential_template_header[0].replace(\">\",\"\")\n",
    "        potential_template_pdb_path = potential_template_pdb_path.split(\":\")[0]\n",
    "        #print(template_seq_id, template_tm_score)\n",
    "        #print(potential_template_seq_id, potential_template_tm_score)\n",
    "        #result_dict[potential_template_pdb_path] = (potential_template_seq_id, potential_template_tm_score, potential_template_full_seq)\n",
    "        #result_dict[template_pdb_path] = (template_seq_id, template_tm_score, template_full_seq)\n",
    "        return template_seq_id, template_tm_score\n",
    "    \n",
    "    def _score_meets_criteria(self, score):\n",
    "        # Implement the logic to check if the score meets your criteria\n",
    "        # for a suitable template\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _intelligent_oligomeric_repair(self, path_to_pdb:str, gap_list:list, selected_templates:list,chains_query:list):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function repairs structures with gaps less than 8 residues per gap.\n",
    "    \n",
    "        Args:\n",
    "        - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "        - stop_pos (int): Stop position.\n",
    "        - main_prot_seq (str): Main protein sequence.\n",
    "        - use_main (bool): Whether to use the main protein.\n",
    "    \n",
    "        Output:\n",
    "        Repaired structures.\n",
    "        \"\"\"\n",
    "    \n",
    "        #new_template_path='/home/micnag/test_modeller_oligomer/6hyr/6hyr.pdb'\n",
    "        #,gap_list_all_chains=[[(1, 5), (96, 116)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)]], \n",
    "        #repair_templates_non_redundant={'/home/micnag/test_modeller_oligomer/6hz3_A.pdb'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"inside oligomeric_repair\")\n",
    "        log.none()  # no stdout spam\n",
    "        env = Environ()  # setup env for modelling\n",
    "        aln = Alignment(env)  # setup the alignment\n",
    "        mdl = Model(env)  # setup the model\n",
    "    \n",
    "        #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "        # current working directory\n",
    "        pdb_id_target = os.path.basename(path_to_pdb) #results in 2duk_A.pdb\n",
    "    \n",
    "        #pdb_id_target='6hyr.pdb'\n",
    "        #print(f\"{pdb_id_target=}\")\n",
    "        \n",
    "        pdb_id_chain = chains_query #this corresponds to all chains in oligomeric struc. We shall also pass this\n",
    "        #to our automodell derived child class.\n",
    "    \n",
    "        #we need first and last chain to set boundaries for our model \n",
    "        first_chain, last_chain = pdb_id_chain[0], pdb_id_chain[-1]\n",
    "        \n",
    "        #pdb_code_name is passed to modeller later.\n",
    "        pdb_code_name = pdb_id_target[:-4] #2duk_ABCDEF\n",
    "        \n",
    "        pdb_4_digit_id = pdb_id_target[0:4] #2duk\n",
    "    \n",
    "        #print(f\"{pdb_4_digit_id=}\")\n",
    "        \n",
    "        #get uniprot_id second. HERE CHECK HOW WE GET X UNIPROT IDS FOR X PROTEINS IN A HETERO_OLIGOMER\n",
    "        uniprot_id = return_uniprot_id_from_rcsb(pdb_4_digit_id)\n",
    "    \n",
    "        #print(f\"{uniprot_id=}\")\n",
    "        #get associated fasta from uniprot id.\n",
    "        fasta_seq = get_gene_fasta(uniprot_id)\n",
    "    \n",
    "        #print(f\"{fasta_seq=}\")\n",
    "    \n",
    "        #first_chain='A', last_chain='E', pdb_4_digit_id='6hyr', pdb_code_name='6hyr',uniprot_id='Q7NDN8', pdb_id_chain=['A', 'B', 'C', 'D', 'E']\n",
    "        #print(f\"{first_chain=}, {last_chain=}, {pdb_4_digit_id=}, {pdb_code_name=},{uniprot_id=}, {pdb_id_chain=}, {fasta_seq=}\")\n",
    "        \"\"\"HERE WE NEED TO CHECK HOW TO MERGE OUR FASTAS.. ESPECIALLY CRUCIAL IF WE HAVE A MIXED OLIGOMER.\n",
    "        SUBSEQUENT CHAINS NEEDS TO BE SEPARATED BY / \"\"\"\n",
    "        \n",
    "        #if mixed oligomer this needs to be taken into account. \n",
    "        \"\"\"WE NEED TO CHECK THIS IN THE FUTURE... ADAPT GET_GENE_FASTA for multiple seqs in hetero X mers.\"\"\"\n",
    "        \n",
    "        merged_fasta = '/'.join([fasta_seq] * len(pdb_id_chain))\n",
    "        \n",
    "        #first we check what is the first ID in our struc. because otherwise we can miss structures that simply miss \n",
    "        #N terminus e.g gap 1-21 but they in fact start with residue 21 and are otherwise good or might just have small\n",
    "        # gaps to fix otherwhere in the structure.\n",
    "    \n",
    "        start_stop_dict = defaultdict()\n",
    "    \n",
    "        temp_codes = []\n",
    "        temp_abs_paths = []\n",
    "        temp_chains = []\n",
    "    \n",
    "    \n",
    "    \n",
    "        #print(f\"{merged_fasta=}\")\n",
    "    \n",
    "        for structure in set(selected_templates):  #no duplicates here.\n",
    "            #print(f\"Structure: {structure} selected as template\")\n",
    "            \n",
    "            temp_codes.append(structure.split(\"/\")[-1][:-4]) #extract codes for all strucs. this works for oligomers.\n",
    "            \n",
    "            temp_abs_paths.append(structure) #grab abspath\n",
    "            \n",
    "            temp_chains.append(structure.split(\"/\")[-1][5:-4]) #this are the chains we need.\n",
    "    \n",
    "    \n",
    "        #print(f\"{temp_codes=}, {temp_abs_paths=}, {temp_chains}\")\n",
    "    \n",
    "        for temp_code, temp_paths, chains in zip(temp_codes, temp_abs_paths, temp_chains):\n",
    "            if len(temp_code) <= 6:\n",
    "                #append all start stop and shifts here.\n",
    "                start_struc_temp, stop_struc_temp = get_struc_stop_oligomer(temp_paths)  #this works also for oligomers in theory. But it is questionable.. if chain A is 100 res and chain B is 40.\n",
    "                #how should we give this info to modeller... 1:A until B:40 problably. include \n",
    "                \n",
    "                start_stop_dict[temp_code] = ((start_struc_temp, stop_struc_temp, chains))\n",
    "    \n",
    "        start_struc_query, stop_struc_query = get_struc_stop_oligomer(path_to_pdb) \n",
    "        \n",
    "        start_stop_dict[pdb_code_name] = ((start_struc_query, stop_struc_query, pdb_id_chain))\n",
    "    \n",
    "    \n",
    "        #print(f\"{start_stop_dict=}\")\n",
    "        \n",
    "        #now the dict contains all paths and start stops.\n",
    "        \n",
    "        #this is the only shift we care about!\n",
    "    \n",
    "        #careful here.. shifts need to be taken into account FOR EACH CHAIN!\n",
    "        \n",
    "        shift = abs(1-start_struc_query)  #e.g 1- 8 abs means 7 shift.\n",
    "        \n",
    "        #if we find that the first end of gap corresponds to the start resi number of the pdb... we skip this gap.\n",
    "    \n",
    "        #would be better to check if one of the templates might cover this gap. But currently not implemented.\n",
    "    \n",
    "        #print(f\"{shift=}\")\n",
    "    \n",
    "    \n",
    "        #gap_list=[((96, 116), 'A')]\n",
    "        #print(f\"{gap_list=}\")\n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        gap_list_all_chains=[[(1, 5), (96, 116)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)]]\n",
    "        we enter oligomeric repair with: new_template_path='/home/micnag/test_modeller_oligomer/6hyr/6hyr.pdb',repairable_gaps=[((96, 116), 'A')], repair_templates_non_redundant={'/home/micnag/test_modeller_oligomer/6hz3_A.pdb'}\n",
    "        inside oligomeric_repair\n",
    "        Structure: /home/micnag/test_modeller_oligomer/6hz3_A.pdb selected as template\n",
    "        temp_codes=['6hz3_A'], temp_abs_paths=['/home/micnag/test_modeller_oligomer/6hz3_A.pdb'], ['A']\n",
    "        start_stop_dict=defaultdict(None, {'6hz3_A': (5, 315, 'A'), '6hyr': (5, 315, ['A', 'B', 'C', 'D', 'E'])})\n",
    "        shift=4\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        \n",
    "        \"\"\"CONTINUE HERE AFTER COURSE TO PROCEED WITH HETEROMERIC OLIGOMER REPAIRS.  NOV 17 2023  18:33 \"\"\"\n",
    "        \n",
    "    \n",
    "        for gaps, chains in gap_list:\n",
    "            #print(gaps, chains)\n",
    "            \n",
    "            found_N_terminus = False\n",
    "            found_C_terminus = False\n",
    "    \n",
    "            #can only concatenate str (not \"int\") to str#\n",
    "    \n",
    "            #    n_terminus = [x for x in range(gap_list[0][0], gap_list[0][1]+1)] #including the last residue.\n",
    "            #    c_terminus = [x for x in range(gap_list[-1][0], gap_list[-1][1]+1)] #including the last residue.\n",
    "        \n",
    "            #(96, 116) A\n",
    "    \n",
    "            if len(gaps) > 2: #means we have more than 1 gap. each gap = len 2\n",
    "                n_terminus = [x for x in range(gaps[0][0], gaps[0][1]+1)] #including the last residue.\n",
    "                c_terminus = [x for x in range(gaps[-1][0], gaps[-1][1]+1)] #including the last residue.\n",
    "            else:\n",
    "                n_terminus = None\n",
    "                c_terminus = None\n",
    "                #we dont deal with n or c terminus here if we only have 1 gap.\n",
    "    \n",
    "            #print(f\"this is {n_terminus=}\")\n",
    "            #print(f\"this is {c_terminus=}\")\n",
    "    \n",
    "            max_n_terminus_found = None\n",
    "            max_c_terminus_found = None\n",
    "    \n",
    "            #print(f\"works until here inside intelligent oligomeric repair!!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "            for paths, (start, stop, chain) in start_stop_dict.items():\n",
    "                # paths='6hz3_A',start=5, stop=315, chain='A'\n",
    "                # paths='6hyr',start=5, stop=315, chain=['A', 'B', 'C', 'D', 'E']\n",
    "                \n",
    "                #print(f\"{paths=},{start=}, {stop=}, {chain=}\")\n",
    "                #lets check for the n-terminus if we find something that can be repaired.\n",
    "                \n",
    "                if n_terminus:\n",
    "                    if start < n_terminus[-1]:\n",
    "                    \n",
    "                        #then we set n terminus found because there is something to repair\n",
    "                        found_N_terminus = True\n",
    "                    \n",
    "                        #but if we have no template... we cant repair beyond what we have and we stay with start.\n",
    "                        if max_n_terminus_found == None or start < max_n_terminus_found:\n",
    "                            max_n_terminus_found = start\n",
    "                            \n",
    "                #lets check for the c terminus in the same fashion\n",
    "                if c_terminus: \n",
    "                    if stop > c_terminus[0]:\n",
    "                    \n",
    "                        #that means we have something that goes beyond the max available struc len.\n",
    "                        found_C_terminus = True\n",
    "                    \n",
    "                        #but if we have no template, we still cant repair beyond.\n",
    "                        if max_c_terminus_found == None or stop > max_c_terminus_found:\n",
    "                            max_c_terminus_found = stop\n",
    "                    \n",
    "    \n",
    "    \n",
    "            #now lets check outside the loop\n",
    "    \n",
    "            if found_N_terminus == False and len(gaps) != 2: #means its not our only hit.\n",
    "    \n",
    "                if len(gaps) > 2:\n",
    "    \n",
    "                    gap_list = gaps[1:] #skip first tuple because we cant repair N terminus\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"We could not repair the only gap because we have no template. this is {gap_list=}\")         \n",
    "                    return\n",
    "            \n",
    "            if found_C_terminus == False and len(gaps) != 2:\n",
    "                #this means we have no suitable template and we skip it.\n",
    "                \n",
    "                if len(gap_list) > 2:\n",
    "                    \n",
    "                    gap_list = gap_list[:-1]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"We could not repair the only gap because we have no template. this is {gap_list=}\")\n",
    "                    return\n",
    "    \n",
    "            \n",
    "            #lets grab the max range of gaps in our structure.\n",
    "            if len(gaps) != 2:\n",
    "                max_gap_range = sorted([y - x for x, y in gaps], reverse=True)\n",
    "            else:\n",
    "                max_gap_range = [gaps[1] - gaps[0]] #simple 1 gap, list for downstream compatibility\n",
    "    \n",
    "    \n",
    "            if len(selected_templates) == 0 and max_gap_range[0] > 8:  \n",
    "                #this last part checks for the case we have\n",
    "                #no templates found but only small gaps of lenght < 8 and still want to repair.\n",
    "                print(\"no suitable templates found.\")\n",
    "                return\n",
    "    \n",
    "        #print(\"we went through this chaos.\")\n",
    "    \n",
    "        \"\"\"Modeller aln.salign treats the FIRST structure provided as QUERY. so we need to load this one FIRST.\"\"\"\n",
    "        #code to be passed to mdl.read\n",
    "    \n",
    "        #start struc query is already above computed at the beginning so we re use it again here.\n",
    "        #CONTINUE HERE\n",
    "    \n",
    "        \"\"\"model_segment specifies the range we look into. Ideally we look for start - stop based on majority vote. Chain is always the same in monomeric\"\"\"\n",
    "        mdl.read(file=pdb_code_name, model_segment=(f\"{start_struc_query}:{pdb_id_chain[0]}\", f\"{stop_struc_query}:{pdb_id_chain[-1]}\"))\n",
    "    \n",
    "        #pdb_code_name='6hyr',start_struc_query=5,pdb_id_chain[0]='A',stop_struc_query=315,pdb_id_chain[-1]='E'\n",
    "        #print(f\"{pdb_code_name=},{start_struc_query=},{pdb_id_chain[0]=},{stop_struc_query=},{pdb_id_chain[-1]=}\")\n",
    "        #append model object to alignment object.\n",
    "    \n",
    "        aln.append_model(mdl, align_codes=pdb_code_name, atom_files=pdb_code_name)\n",
    "    \n",
    "         \n",
    "        for codes, (start_temp, stop_temp, chains) in start_stop_dict.items():\n",
    "            #dont add our query again to the stack.\n",
    "            if codes == pdb_code_name:\n",
    "                continue\n",
    "    \n",
    "            #rest of templates...add to the stack.\n",
    "            mdl.read(file=codes, model_segment=(f\"{start_temp}:{chains[0]}\", f\"{stop_temp}:{chains[-1]}\"))\n",
    "            \n",
    "            aln.append_model(mdl, align_codes=codes, atom_files=codes)\n",
    "    \n",
    "    \n",
    "        #now add fasta sequence as last entry to the align model.\n",
    "        with open(f\"./{pdb_code_name}x.fasta\", \"w\") as fastaout:\n",
    "            fastaout.write(f\">{pdb_code_name}x\\n\")\n",
    "            fastaout.write(merged_fasta)\n",
    "    \n",
    "        aln_code = f\"{pdb_code_name}x\"\n",
    "    \n",
    "        #align fasta file to our alignment object which contains now a fasta sequence and a structure object.\n",
    "        aln.append(file=f\"./{pdb_code_name}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        #Overwrite MyModel and Inherit from AutoModel.\n",
    "        class MyModel(AutoModel):\n",
    "            def __init__(self, env, alnfile, knowns, sequence, gaps, chains, shift, **kwargs):\n",
    "                super(MyModel, self).__init__(env, alnfile, knowns, sequence, **kwargs)\n",
    "                self.gaps = gaps\n",
    "                self.chain = chains\n",
    "                self.shift = shift #shift because modeller always renumbers all stuff to be 1 based.\n",
    "                \n",
    "            def select_atoms(self):\n",
    "                selections = []\n",
    "                chain = self.chain\n",
    "                #this needs to be adjusted for multi chain modells.\n",
    "                #this is self.gaps: [((96, 116), 'A')]\n",
    "                #print(f\"this is self.gaps: {self.gaps}\")\n",
    "                for (start, end), chain in self.gaps:\n",
    "                    if start > 1:\n",
    "                        \n",
    "                        #print(f\"Selecting atoms for range {start-self.shift}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start-self.shift}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                        \n",
    "                    else:\n",
    "                        #print(f\"Selecting atoms for range {start}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "    \n",
    "                selected_atoms = Selection(*selections)\n",
    "                \n",
    "                # Combine all selections into a single Selection object\n",
    "                print(f\"Selected {len(selected_atoms)} atoms for optimization\")\n",
    "                \n",
    "                return selected_atoms\n",
    "    \n",
    "    \n",
    "        #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "        env.io.atom_files_directory = ['.','../.']\n",
    "        \n",
    "        #aln.malign3d(fit=True)\n",
    "    \n",
    "    \n",
    "        # Additional debugging: Print alignment content\n",
    "        #print(\"Alignment content before salign:\")\n",
    "        #for record in aln:\n",
    "            #print(record.code)\n",
    "    \n",
    "        #align sequence to structure.\n",
    "    \n",
    "        #overhang = 0 because we are confident the structures as templates are suitable candidates (seq id > 0.8)\n",
    "        # gap penalties are default.\n",
    "        # alignment_type = progressive : each template pairwise against query comparison.\n",
    "        # \n",
    "    \n",
    "        \n",
    "        aln.salign(overhang=0, gap_penalties_1d=(-450, -50), alignment_type=\"progressive\", output=\"ALIGNMENT\")\n",
    "    \n",
    "        #write out alignmentfile for automodell usage later\n",
    "        aln.write(file=f\"{pdb_code_name}.ali\")\n",
    "        \n",
    "        #add template codes to the code list.\n",
    "        alternate_templates = [x for x in temp_codes]\n",
    "    \n",
    "        # merge with our codes.\n",
    "        full_knowns = (pdb_code_name, *alternate_templates) #empty list from alternate_templates\n",
    "        \n",
    "        #print(f\"{full_knowns=}\")\n",
    "        #custom class\n",
    "    \n",
    "        # selected atoms do not feel the neighborhood\n",
    "        #env.edat.nonbonded_sel_atoms = 2\n",
    "    \n",
    "        #here we need to rechain again and make sure that this is what modeller sees.. e.g BCD Will need to be ABC\n",
    "    \n",
    "        pdb_original = None #this is considered false by default == 1 == False\n",
    "    \n",
    "        letters = [chr(ord('A') + i) for i in range(26)]\n",
    "        \n",
    "        \n",
    "        if pdb_id_chain != letters[0:len(pdb_id_chain)]:  # ABC for len 3 e.g\n",
    "            #print(f\"we are inside pdb_id_chain not fitting with modeller. : {pdb_id_chain=}\")\n",
    "            pdb_original = list(pdb_id_chain)\n",
    "            #we set it to A for repair.. but afterwards we swap it back!\n",
    "            pdb_id_chain = letters[0:len(pdb_id_chain)]\n",
    "    \n",
    "        #print(f\"{pdb_id_chain=}\")\n",
    "    \n",
    "        #print(\"works until here.\")\n",
    "    \n",
    "        a = MyModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code,\n",
    "                    gaps=gap_list, shift=shift, chains=pdb_id_chain)\n",
    "    \n",
    "    \n",
    "        #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "        #a = AutoModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code)\n",
    "    \n",
    "        a.starting_model = 1\n",
    "        a.ending_model = 1\n",
    "    \n",
    "        # Thorough MD optimization:\n",
    "        #a.md_level = refine.slow\n",
    "    \n",
    "        # Repeat the whole cycle 2 times and do not stop unless obj.func. > 1E6\n",
    "        #a.repeat_optimization = 2\n",
    "    \n",
    "        #env.libs.topology.make(aln)\n",
    "        #env.libs.parameters.read(file='$(LIB)/par.lib')\n",
    "        #mdl.generate_topology(aln[pdb_code_name])\n",
    "        #a.generate_topology(aln[pdb_code_name])\n",
    "        # Assign the average of the equivalent template coordinates to MODEL:\n",
    "        #a.transfer_xyz(aln) #lets try this.\n",
    "        \n",
    "        # Get the remaining undefined coordinates from internal coordinates:\n",
    "        #a.build(initialize_xyz=True, build_method='INTERNAL_COORDINATES')\n",
    "    \n",
    "        try:\n",
    "            # Build the model(s)\n",
    "            a.make();\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during modeling: {e}\")\n",
    "    \n",
    "        #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            #remove artifacts.\n",
    "            #remove_repair_artefacts(pdb_basep=path_to_pdb, pdb_code_name=pdb_code_name)\n",
    "    \n",
    "            #ok now we need to check what the updated start stop range is!\n",
    "    \n",
    "            if max_n_terminus_found and max_n_terminus_found < start_struc_query:\n",
    "                \n",
    "                keep_start = max_n_terminus_found\n",
    "                if not max_c_terminus_found:\n",
    "                    keep_stop = stop_struc_query\n",
    "                \n",
    "                    \n",
    "            if max_c_terminus_found and max_c_terminus_found > stop_struc_query:\n",
    "                keep_stop = max_n_terminus_found\n",
    "    \n",
    "                if not max_n_terminus_found:\n",
    "                    keep_start = start_struc_query\n",
    "                    \n",
    "            if not max_n_terminus_found and not max_c_terminus_found:\n",
    "                keep_start, keep_stop = start_struc_query, stop_struc_query\n",
    "            \n",
    "    \n",
    "            #here we can switch chain back to original if required:\n",
    "    \n",
    "            \n",
    "            #print(f\"{keep_start=}, {keep_stop=}\")\n",
    "            \n",
    "            #this should select only from the start to end and excludes potentially repaired N and C termini that would only introduce more noise.\n",
    "            select_c_alpha_and_correct_range(path_to_pdb, start=keep_start, stop=keep_stop)\n",
    "    \n",
    "            \n",
    "            \n",
    "            #not required!\n",
    "            #renumber_structure_monomeric(path_to_pdb, start=start_struc, chain=pdb_id_chain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _intelligent_monomeric_repair(self, repair_struc_gap_lst, temp_paths_tm_score_seq_id_template_gaps):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function repairs structures with gaps less than 7 residues per gap.\n",
    "    \n",
    "        Args:\n",
    "        - template_dict: key = (pdb to be repaired, temp_gaps), vals = (path_for_template_structure,  (tm_score, seq_id, template_gaps)) \n",
    "\n",
    "        We need to make sure that the selection of residues used for repair does not fall into the region of template_gaps.\n",
    "\n",
    "        \n",
    "        Attention:\n",
    "    \n",
    "        Modeller INTERNALLY RECHAINS AND RENUMBERS EVERY QUERY.\n",
    "        ALL SINGLE CHAINS == A chain. irrespective of natural chain.. e.g if you supply chain B single chain it will go back to rechaining it to chain A.\n",
    "        Also dangerous... it will renumber structures from 1... so you need to correct for this shift based on your desired selection target. \n",
    "        If you structure starts with residue 30... and you want to remodell a gap between 40-50 you need to take this into account. shift = abs(1-start_struc_query)\n",
    "        \n",
    "        Output:\n",
    "        Repaired structures.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(repair_struc_gap_lst)\n",
    "        print(temp_paths_tm_score_seq_id_template_gaps)\n",
    "\n",
    "        #lets unpack everything properly.\n",
    "\n",
    "        pdb_id_target_path = repair_struc_gap_lst[0] #the path\n",
    "        pdb_id_target_gaps = repair_struc_gap_lst[1] #the list of gaps to be checked\n",
    "\n",
    "        template_paths = temp_paths_tm_score_seq_id_template_gaps[0] #the path to the templates\n",
    "        template_gaps = temp_paths_tm_score_seq_id_template_gaps[1][2] # the list of gaps in the templates\n",
    "\n",
    "\n",
    "        if len(pdb_id_target_gaps) == 0:\n",
    "            #means we have nothing to add.\n",
    "            return pdb_id_target_path \n",
    "        \n",
    "        log.none()  # no stdout spam\n",
    "        env = Environ()  # setup env for modelling\n",
    "        aln = Alignment(env)  # setup the alignment\n",
    "        mdl = Model(env)  # setup the model\n",
    "        #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "        # current working directory\n",
    "        #print(\"works until here step 1:\")\n",
    "        pdb_id_target = os.path.basename(pdb_id_target_path) #results in 2duk_A.pdb\n",
    "        pdb_id_chain = pdb_id_target[-5] #this corresponds to chain in single monomeric struc. We shall also pass this\n",
    "        #to our automodell derived child class.\n",
    "        #pdb_code_name is passed to modeller later.\n",
    "        pdb_code_name = pdb_id_target[:6] #2duk_A\n",
    "        pdb_4_digit_id = pdb_id_target[0:4]\n",
    "        #print(f\"this is {pdb_4_digit_id=}, {pdb_id_chain=}, {pdb_code_name=}\")\n",
    "        #print(\"works until here step 2:\")\n",
    "        #get uniprot_id second.\n",
    "        uniprot_id = self._return_uniprot_id_from_rcsb(pdb_4_digit_id)\n",
    "        #get associated fasta from uniprot id.\n",
    "        fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "    \n",
    "        #first we check what is the first ID in our struc. because otherwise we can miss structures that simply miss \n",
    "        #N terminus e.g gap 1-21 but they in fact start with residue 21 and are otherwise good or might just have small\n",
    "        # gaps to fix otherwhere in the structure.\n",
    "    \n",
    "\n",
    "        start_stop_dict = self._setup_and_prep_templates(template_paths)\n",
    "        start_struc_query, stop_struc_query = self._get_struc_stop(pdb_id_target_path) \n",
    "\n",
    "        #leave this as this adds the QUERY to the start_stop_dic\n",
    "        start_stop_dict[pdb_code_name] = ((start_struc_query, stop_struc_query, pdb_id_chain))\n",
    "\n",
    "        \n",
    "        #now the dict contains all paths and start stops.\n",
    "        #this is the only shift we care about!\n",
    "        shift = abs(1-start_struc_query)  #e.g 1- 8 abs means 7 shift.\n",
    "        \n",
    "        #if we find that the first end of gap corresponds to the start resi number of the pdb... we skip this gap.\n",
    "        #would be better to check if one of the templates might cover this gap. But currently not implemented.\n",
    "        #[(275, 284), (882, 889)]\n",
    "        #print(f\"this is {gap_list=}\")\n",
    "        \n",
    "        found_N_terminus, found_C_terminus, pdb_id_target_gaps = self._check_terminus_coverage(pdb_id_target_gaps, start_stop_dict)\n",
    "            \n",
    "        #print(f\"gaplist after cutting: {gap_list=}\")\n",
    "        \n",
    "        #lets grab the max range of gaps in our structure.\n",
    "        #max_gap_range = sorted([y - x for x, y in pdb_id_target_gaps], reverse=True)\n",
    "    \n",
    "        #check if we found suitable templates... IF NO and there are still only small gaps < 8 then we use the structure itself as template and still repair.\n",
    "        #if len(selected_templates) == 0 and max_gap_range[0] > 8:  \n",
    "            #this last part checks for the case we have\n",
    "            #no templates found but only small gaps of lenght < 8 and still want to repair.\n",
    "            #print(\"no suitable templates found.\")\n",
    "        #    return\n",
    "            \n",
    "        #elif len(selected_templates) == 0 and max_gap_range[0] < 8:\n",
    "            #nothing to do since we always append our own structure to repair ensemble. But in this case we can continue.\n",
    "            #print(\"we found gaps but they are small so we shall continue\")\n",
    "        #    pass\n",
    "        \n",
    "    \n",
    "        \"\"\"Modeller aln.salign treats the FIRST structure provided as QUERY. so we need to load this one FIRST.\"\"\"\n",
    "        #code to be passed to mdl.read\n",
    "    \n",
    "        #start struc query is already above computed at the beginning so we re use it again here.\n",
    "        #CONTINUE HERE\n",
    "    \n",
    "        \"\"\"model_segment specifies the range we look into. Ideally we look for start - stop based on majority vote. Chain is always the same in monomeric\"\"\"\n",
    "        mdl.read(file=pdb_code_name, model_segment=(f\"{start_struc_query}:{pdb_id_chain}\", f\"{stop_struc_query}:{pdb_id_chain}\"))\n",
    "        #append model object to alignment object.\n",
    "        aln.append_model(mdl, align_codes=pdb_code_name, atom_files=pdb_code_name)\n",
    "    \n",
    "        #now lets add all the templates.\n",
    "        #now we need to append all template strucs.\n",
    "        \n",
    "        for codes, (start_temp, stop_temp, chains) in start_stop_dict.items():\n",
    "            #dont add our query again to the stack.\n",
    "            if codes == pdb_code_name:\n",
    "                continue\n",
    "    \n",
    "            #rest of templates...add to the stack.\n",
    "            #print(f\"{start_temp}:{chains}\", f\"{stop_temp}:{chains}\")\n",
    "            mdl.read(file=codes, model_segment=(f\"{start_temp}:{chains}\", f\"{stop_temp}:{chains}\"))  \n",
    "            aln.append_model(mdl, align_codes=codes, atom_files=codes)\n",
    "    \n",
    "        #now add fasta sequence as last entry to the align model.\n",
    "        with open(f\"./{pdb_code_name}x.fasta\", \"w\") as fastaout:\n",
    "            fastaout.write(f\">{pdb_code_name}x\\n\")\n",
    "            fastaout.write(fasta_seq)\n",
    "    \n",
    "        aln_code = f\"{pdb_code_name}x\"\n",
    "        #align fasta file to our alignment object which contains now a fasta sequence and a structure object.\n",
    "        aln.append(file=f\"./{pdb_code_name}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "        \n",
    "        class MyModel(AutoModel):\n",
    "            def __init__(self, env, alnfile, knowns, sequence, gaps, chain, shift, **kwargs):\n",
    "                super(MyModel, self).__init__(env, alnfile, knowns, sequence, **kwargs)\n",
    "                self.gaps = gaps\n",
    "                self.chain = chain\n",
    "                self.shift = shift #shift because modeller always renumbers all stuff to be 1 based.\n",
    "                \n",
    "            def select_atoms(self):\n",
    "                selections = []\n",
    "                chain = self.chain\n",
    "    \n",
    "                #print(f\"this is self.gaps: {self.gaps}\")\n",
    "                for start, end in self.gaps:\n",
    "                    if start > 1:  #negative indx make problems so we start from 1 in this cases.\n",
    "                        #print(f\"Selecting atoms for range {start-self.shift}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start-self.shift}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                    else:\n",
    "                        #print(f\"Selecting atoms for range {start}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                \n",
    "                selected_atoms = Selection(*selections)\n",
    "                # Combine all selections into a single Selection object\n",
    "                #print(f\"Selected {len(selected_atoms)} atoms for optimization\")\n",
    "                return selected_atoms\n",
    "    \n",
    "    \n",
    "        #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "        env.io.atom_files_directory = [f'{self.work_dir}', f'../.']\n",
    "\n",
    "        #aln.malign3d(fit=True)\n",
    "        # Additional debugging: Print alignment content\n",
    "        #print(\"Alignment content before salign:\")\n",
    "        #for record in aln:\n",
    "            #print(record.code)\n",
    "        #align sequence to structure.\n",
    "        #overhang = 0 because we are confident the structures as templates are suitable candidates (seq id > 0.8)\n",
    "        # gap penalties are default.\n",
    "        # alignment_type = progressive : each template pairwise against query comparison.\n",
    "        #    \n",
    "        aln.salign(overhang=0, gap_penalties_1d=(-450, -50), alignment_type=\"progressive\", output=\"ALIGNMENT\")\n",
    "        #write out alignmentfile for automodell usage later\n",
    "        aln.write(file=f\"{pdb_code_name}.ali\")\n",
    "        #add template codes to the code list.\n",
    "        alternate_templates = [x for x in temp_codes]\n",
    "        # merge with our codes.\n",
    "        full_knowns = (pdb_code_name, *alternate_templates) #empty list from alternate_templates\n",
    "        #print(full_knowns)\n",
    "        #custom class\n",
    "        # selected atoms do not feel the neighborhood\n",
    "        #env.edat.nonbonded_sel_atoms = 2\n",
    "        #check chain if not A... take A and afterwards change it back to original chain. \n",
    "        pdb_original = None #this is considered false by default == 1 == False\n",
    "        if pdb_id_chain != \"A\":\n",
    "            #print(f\"we are inside pdb_id_chain != A: {pdb_id_chain=}\")\n",
    "            pdb_original = list(pdb_id_chain)\n",
    "            #we set it to A for repair.. but afterwards we swap it back!\n",
    "            pdb_id_chain = \"A\"\n",
    "    \n",
    "        a = MyModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code,\n",
    "                    gaps=pdb_id_target_gaps, shift=shift, chain=pdb_id_chain) #pdb_id_chain\n",
    "     \n",
    "\n",
    "        #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "        #a = AutoModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code)\n",
    "    \n",
    "        a.starting_model = 1\n",
    "        a.ending_model = 1\n",
    "    \n",
    "        # Thorough MD optimization:\n",
    "        #a.md_level = refine.slow\n",
    "    \n",
    "        # Repeat the whole cycle 2 times and do not stop unless obj.func. > 1E6\n",
    "        #a.repeat_optimization = 2\n",
    "    \n",
    "        #env.libs.topology.make(aln)\n",
    "        #env.libs.parameters.read(file='$(LIB)/par.lib')\n",
    "        #mdl.generate_topology(aln[pdb_code_name])\n",
    "        #a.generate_topology(aln[pdb_code_name])\n",
    "        # Assign the average of the equivalent template coordinates to MODEL:\n",
    "        #a.transfer_xyz(aln) #lets try this.\n",
    "        \n",
    "        # Get the remaining undefined coordinates from internal coordinates:\n",
    "        #a.build(initialize_xyz=True, build_method='INTERNAL_COORDINATES')\n",
    "        \n",
    "        try:\n",
    "            # Build the model(s)\n",
    "            a.make();\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during modeling: {e}\")\n",
    "    \n",
    "        #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            #remove artifacts.\n",
    "            self._remove_repair_artefacts(pdb_basep=pdb_id_target_path, pdb_code_name=pdb_code_name)\n",
    "    \n",
    "            #ok now we need to check what the updated start stop range is!\n",
    "    \n",
    "            #print(f\"{max_n_terminus_found=}, {max_n_terminus_found < start_struc_query=}\")\n",
    "            \n",
    "            #print(f\"{max_c_terminus_found=}, {max_c_terminus_found > start_struc_query=}\")\n",
    "    \n",
    "            #print(f\"{start_struc_query=}\")\n",
    "            #print(f\"{stop_struc_query=}\")\n",
    "            \n",
    "            \"\"\"\n",
    "            max_n_terminus_found=-2, max_n_terminus_found < start_struc_query=True\n",
    "            max_c_terminus_found=994, max_c_terminus_found > start_struc_query=True\n",
    "            start_struc_query=1\n",
    "            stop_struc_query=992\n",
    "            keep_start=-2, keep_stop=-2\n",
    "            \"\"\"\n",
    "    \n",
    "            if pdb_original:\n",
    "                #print(f\"this is inside pdb_original {pdb_id_chain=}\")\n",
    "                #rechain back and set pdb_id_chain to original pdb chain.\n",
    "                pdb_id_chain = self._rechain_back(path_to_pdb=pdb_id_target_path, pdb_original=pdb_original)\n",
    "                \n",
    "            #should take both chains and pdb path.. then rechain back and return back the new chain.\n",
    "    \n",
    "    \n",
    "            keep_start = start_struc_query\n",
    "            keep_stop = stop_struc_query\n",
    "            \n",
    "            if max_n_terminus_found and max_n_terminus_found < start_struc_query:\n",
    "                keep_start = max_n_terminus_found\n",
    "                    \n",
    "            if max_c_terminus_found and max_c_terminus_found > stop_struc_query:\n",
    "                keep_stop = max_c_terminus_found\n",
    "            \n",
    "            #print(f\"{keep_start=}, {keep_stop=}\")\n",
    "            #this should select only from the start to end and excludes potentially repaired N and C termini that would only introduce more noise.\n",
    "            self._select_correct_range(pdb_id_target_path, start=keep_start, stop=keep_stop)\n",
    "    \n",
    "    \n",
    "            monomeric_chain = \"\".join(pdb_id_chain)\n",
    "            #print(f\"{monomeric_chain=}\")\n",
    "            #not required! ?? lets see later if it is .\n",
    "            self._renumber_structure_monomeric(pdb_id_target_path, start=keep_start, chain=pdb_id_chain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        #if everyhing worked out we return the path!\n",
    "        return pdb_id_target_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _return_uniprot_id_from_rcsb(self, uniprot_id:str):\n",
    "    \n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        \n",
    "        searchp = f\"{link_path}/{uniprot_id}\"\n",
    "        #print(searchp)\n",
    "        resp = get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        \n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                return uniprot_id\n",
    "\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "    \n",
    "        #print(\"we are in get gene fasta\")\n",
    "        \"this is already overworked. should work.\"\n",
    "        #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "        \n",
    "        fields = \"sequence\"\n",
    "        \n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        \n",
    "        seq = \"\"\n",
    "        \n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "                #print(lines)\n",
    "            i += 1\n",
    "        \n",
    "        #print(seq)\n",
    "        return seq\n",
    "\n",
    "\n",
    "\n",
    "    def _setup_and_prep_templates(self, template_paths)->dict:\n",
    "\n",
    "        #print(\"works until here step 3:\")\n",
    "        start_stop_dict = defaultdict()\n",
    "    \n",
    "        temp_codes = []\n",
    "        temp_abs_paths = []\n",
    "        temp_chains = []\n",
    "        \n",
    "        for structure in set(template_paths):  #no duplicates here.\n",
    "            #print(f\"Structure: {structure} selected as template\")\n",
    "            temp_codes.append(structure.split(\"/\")[-1][:-4])\n",
    "            temp_abs_paths.append(structure)\n",
    "            temp_chains.append(structure.split(\"/\")[-1][-5]) #this is the chain we need.\n",
    "    \n",
    "        for temp_code, temp_paths, chains in zip(temp_codes, temp_abs_paths, temp_chains):\n",
    "            if len(temp_code) <= 6:\n",
    "                #append all start stop and shifts here.\n",
    "                start_struc_temp, stop_struc_temp = get_struc_stop(temp_paths)\n",
    "                start_stop_dict[temp_code] = ((start_struc_temp, stop_struc_temp, chains))\n",
    "    \n",
    "        return start_stop_dict\n",
    "\n",
    "    \n",
    "    def _get_struc_stop(self, path_to_pdb):\n",
    "    \n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"none\", path_to_pdb)\n",
    "        seq_ids = [x.get_id()[1] for x in structure.get_residues()]\n",
    "        seq_ids = sorted(seq_ids)\n",
    "        return seq_ids[0], seq_ids[-1] #this corresponds to the last residue.\n",
    "\n",
    "\n",
    "    def _remove_repair_artefacts(self, pdb_basep, pdb_code_name):\n",
    "    \n",
    "        pattern = f\"{pdb_code_name}\"\n",
    "        #print(pattern)\n",
    "        # Remove original_****_*.ali file\n",
    "        ali_files = glob.glob(f\"{pattern}.ali\")\n",
    "        for file in ali_files:\n",
    "            os.remove(file)\n",
    "        # Remove original_****_A.pdb\n",
    "        old_pdb_file = f\"{pattern}.pdb\"\n",
    "        if os.path.exists(old_pdb_file):\n",
    "            os.remove(old_pdb_file)\n",
    "            \n",
    "        # Rename original_****_Ax.B99990001.pdb to original_****_A.pdb\n",
    "        repaired_pdb_file = f\"{pattern}x.B99990001.pdb\"\n",
    "        if os.path.exists(repaired_pdb_file):\n",
    "            new_pdb_file = repaired_pdb_file.replace('x.B99990001', '')\n",
    "            shutil.move(repaired_pdb_file, new_pdb_file)\n",
    "    \n",
    "        # Remove original_****_*x.D00000001\n",
    "        d_files = glob.glob(f\"{pattern}x.D00000001\")\n",
    "        for file in d_files:\n",
    "            os.remove(file)\n",
    "    \n",
    "        # Remove other files\n",
    "        extensions_to_remove = ['.fasta', '.ini', '.rsr', '.sch', '.V99990001']\n",
    "        for ext in extensions_to_remove:\n",
    "            files = glob.glob(f\"{pattern}x{ext}\")\n",
    "            for file in files:\n",
    "                os.remove(file)\n",
    "                \n",
    "\n",
    "    def _check_terminus_coverage(self, pdb_id_target_gaps, start_stop_dict):\n",
    "        \"\"\"\n",
    "        Checks if the N-terminus and C-terminus gaps in the target structure can be covered by any of the templates.\n",
    "    \n",
    "        Args:\n",
    "        - pdb_id_target_gaps: A list of tuples representing the gaps in the target structure.\n",
    "        - start_stop_dict: A dictionary with template start and stop residue numbers and chains.\n",
    "    \n",
    "        Returns:\n",
    "        - A tuple of booleans indicating whether suitable templates for the N-terminus and C-terminus were found.\n",
    "        - Updated pdb_id_target_gaps after excluding uncovered terminus gaps.\n",
    "        \"\"\"\n",
    "        found_N_terminus = False\n",
    "        found_C_terminus = False\n",
    "    \n",
    "        n_terminus_range = range(pdb_id_target_gaps[0][0], pdb_id_target_gaps[0][1] + 1)\n",
    "        c_terminus_range = range(pdb_id_target_gaps[-1][0], pdb_id_target_gaps[-1][1] + 1)\n",
    "    \n",
    "        max_n_terminus_found = None\n",
    "        max_c_terminus_found = None\n",
    "    \n",
    "        for _, (start, stop, _) in start_stop_dict.items():\n",
    "            if int(start) < n_terminus_range[-1]:\n",
    "                found_N_terminus = True\n",
    "                max_n_terminus_found = max(max_n_terminus_found, start) if max_n_terminus_found is not None else start\n",
    "    \n",
    "            if int(stop) > c_terminus_range[0]:\n",
    "                found_C_terminus = True\n",
    "                max_c_terminus_found = max(max_c_terminus_found, stop) if max_c_terminus_found is not None else stop\n",
    "    \n",
    "        # Adjust the gaps list based on the terminus coverage\n",
    "        if not found_N_terminus and len(pdb_id_target_gaps) > 1:\n",
    "            pdb_id_target_gaps = pdb_id_target_gaps[1:]\n",
    "        elif not found_N_terminus:\n",
    "            return False, False, []\n",
    "    \n",
    "        if not found_C_terminus and len(pdb_id_target_gaps) > 1:\n",
    "            pdb_id_target_gaps = pdb_id_target_gaps[:-1]\n",
    "        elif not found_C_terminus:\n",
    "            return False, False, []\n",
    "    \n",
    "        return found_N_terminus, found_C_terminus, pdb_id_target_gaps\n",
    "\n",
    "\n",
    "    def _rechain_back(self, path_to_pdb:str, pdb_original:list)-> None:\n",
    "\n",
    "        #first check if its multichain or single chain.\n",
    "        new_chains = \"\".join(pdb_original)\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"default\", path_to_pdb)\n",
    "\n",
    "        for model in structure:\n",
    "            for idx, chain in enumerate(model):\n",
    "                if chain.id != new_chains[idx]:\n",
    "                    chain.id = new_chains[idx]\n",
    "    \n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(path_to_pdb)\n",
    "        return pdb_original\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _select_correct_range(self, path:str, start:int, stop:int):\n",
    "    \n",
    "        #sel only c_alpha\n",
    "        class CAlphaOnlyInCorrectRange(Select):\n",
    "            def __init__(self, start, stop, *args):\n",
    "                super().__init__(*args)\n",
    "                self.start = start\n",
    "                self.stop = stop\n",
    "\n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] >= self.start and residue.id[1] <= self.stop else 0    \n",
    "            \n",
    "        #filelst    path\n",
    "        #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        \n",
    "        #print(\"we are here\")\n",
    "        #print(fullpath)\n",
    "        structure = parser.get_structure(prot_name, path)\n",
    "        \n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(path, CAlphaOnlyInCorrectRange(start=start, stop=stop))\n",
    "\n",
    "\n",
    "\n",
    "    def _renumber_structure_monomeric(self, path_to_pdb:str, start:int, chain:str):\n",
    "\n",
    "        shiftres_location = f\"{self.work_dir}/pdb_shiftres_by_chain.py\"\n",
    "        bash_cmd = f\"python {shiftres_location} {path_to_pdb} {start-1} {chain}\"\n",
    "        bash_cmd_rdy = bash_cmd.split()\n",
    "        \n",
    "        with open(f\"{path_to_pdb}_tmp\", \"w\") as fh_tmp:\n",
    "            result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                 universal_newlines=True)\n",
    "        \n",
    "        #now replace the original one with the temp file.\n",
    "        os.replace(f\"{path_to_pdb}_tmp\", f\"{path_to_pdb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c52894-a149-4aca-8a54-502ba3784c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c90d2-edd3-4be3-8f57-4dbdd3942fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930e257-3936-493e-8403-7f588f530e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932ef9e-bd31-4b78-b032-8f0fd251f270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b2b3f-ce20-419f-975c-7a557cc3c8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9d0f5-5a1c-4885-a855-ae2646a7c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bda26-3394-40a8-b6e4-00360fce9c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c7472-aaed-41e6-8a9b-a27fb4492c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dbf07-2989-42fd-a806-0e226a51f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5b551-f8b4-4fae-acf6-631e935ea35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a8d1e-f1bb-4163-ac82-44fb0b82a786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8356b1-99e5-4718-8677-91b96ab98faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f85b3-1994-433e-9ebe-ff92658da97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88d07a-a796-4ebb-9fd0-ff73a334d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PdbEnsemble_meta = Downloader.meta_dict\n",
    "#PdbEnsemble_chains_seq_id = Downloader.chain_seqid_dict\n",
    "\n",
    "#Downloader.conservation(uniprot_id=\"Q9NZJ9\")\n",
    "#shifts = PdbEnsemble.parallel_shift_calculation()\n",
    "#Downloader.conservation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43495d81-2216-4a19-82ba-e0140cfceaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e2a62-c0e1-4f3c-a414-72b8b922d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_Cleaner = PDBCleaning(work_dir=work_dir, meta_dict=PdbEnsemble_meta, chain_seq_dict=PdbEnsemble_chains_seq_id)\n",
    "PDB_Cleaner.setup_cutoff(cutoff=3, apply_filter=True)  #apply filter to only include structures that are of good quality\n",
    "#PDB_Cleaner.parallel_shift_calculation()  # compute shift for each structure\n",
    "#PDB_Cleaner.parallel_renumbering()  # renumber based on shifts.\n",
    "#PDB_cleaned_ensemble.chain_dict\n",
    "\n",
    "#PDB_Cleaner.filtered_structures\n",
    "struct = PDB_Cleaner.filtered_structures\n",
    "#struct = PDB_Cleaner.get_unfiltered_strucs()\n",
    "#chain_seqid = PDB_Cleaner.chain_seq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e32ee-4977-414c-b264-ac0d0490ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(struct) #get_unfiltered_strucs() misses out on all files with multi chains\n",
    "#['3i7v_A', '6woh_A', '6wog_A', '4hfq_AB', '6wob_A', '3h95_AB', '7aui_A', '6woe_A', '6pcl_A', '2q9p_A', '7aul_A', '6wod_A', '6wo8_A', '6woa_A', '6wo9_A', '6wof_A', '3i7u_ABCD', '6pck_A', '7aup_A', '7auk_A', '7aus_A', '7aut_A', '2fvv_A', '6wo7_A', '7nnj_A', '7tn4_A', '6woi_A', '7aun_A', '6woc_A', '3mcf_A', '7aur_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b7c97-8a19-4cba-a3f6-e29c1e2ff3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chain_seqid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206e949-8850-4e5a-85a5-fd69b6f90125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDB_Builder = PDBBuilder(work_dir=work_dir, structures=struct, remove_intermediates=True) #structures that are filtered\n",
    "#PDB_Builder.build_assembly()\n",
    "#oligo_dict = PDB_Builder.oligodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c60a70-471f-4a6e-8425-f9c3a3c7cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(oligo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d2633-96a6-4a40-bc36-b451a7d5b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preper = PDBEnsemblePrep(work_dir=work_dir, oligo_dict=oligo_dict, chain_seq_dict=chain_seqid, main_prot_seq=None)\n",
    "#Preper.create_domain_boundaries()\n",
    "#Preper.get_oligostates(num_most_common_oligostates=3)\n",
    "#Preper.process_templates()\n",
    "#top_templates = Preper.top_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85adc600-e911-41b6-a8b0-0639335e0f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1429f1f-3a0a-454f-aa21-259e6722280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USAligner = USAlign(work_dir=work_dir, structure_seqid_dict=Preper.templates_for_oligos, \n",
    "#                    template_min_identity=0.1,\n",
    "#                   num_top_clusters_per_range=4)\n",
    "\n",
    "#USAligner.USAlign_run()\n",
    "#USAligner.filter_results(tm_cutoff=0.8,rmsd_min_cutoff=0.2, log_file=True)\n",
    "#USAligner.setup_oligo_directories()\n",
    "#result_dict = USAligner.result_dict\n",
    "#USAligner.run_multiseq_alignment(directory=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf31e48-db6e-4ed8-9803-2c3027b362ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repairstation = ModellerRepairEnsemble(ensemble_dict=result_dict, work_dir=work_dir)\n",
    "#print(Repairstation.repair_ensemble)\n",
    "#print(Repairstation.monomers)\n",
    "#Repairstation.repair_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e0a1a-b6fa-4a78-a902-e3a42426176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\"\n",
    "\n",
    "#work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203\"\n",
    "\n",
    "#fasta = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203/multiseq_fasta.fa\"\n",
    "\n",
    "#pca_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/pca_part\"\n",
    "\n",
    "#PCA_object = PrincipalComponentAnalysis(work_dir=work_dir, script_dir=script_dir, \n",
    "#                                        pca_dir=pca_dir,store_original=True)\n",
    "#PCA_object.run_PCA(work_dir)\n",
    "#PCA_object.prepare_ensemble()\n",
    "#PCA_object.run_PCA()  # works but is crap because of forced path locations for executables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614c4b0-b1c7-4069-b746-5bca97620321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA_object.multi_pdb_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81521527-400c-45d0-b49b-bec452fac237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bb0bb-7cfe-4ce1-b2d0-cde888f3029e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcdf9f-caaa-4d5e-9cdd-1ba317478f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad68230-5c43-4ca3-92f6-e9fcdb6c3a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2735516-ab1a-4c87-ae06-4a623907efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac38b3f-b54f-4051-b5b3-a8a2902af69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb825eb4-2ab6-4584-9c1d-8913a20c21fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910e871-c075-446b-8186-e051e0ea8d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ff7b5-72cf-44b5-b088-825ab691322e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8894f40-9733-44d6-bf9c-bb85a6b16544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e09dd-c2aa-45c1-abf3-6a22a6a95087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b374a-70f6-4562-b425-1342f3399170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b68735-2013-4f25-ad03-857ab72e9466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec6037-a0f3-4823-bc9d-fe3346d369b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4121fe-cc4e-437b-9ae6-76e07ad8e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
