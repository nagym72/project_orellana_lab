{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6266200-2843-4bff-ae79-088353476d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modules required for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c053508-7da1-4170-97f2-195f37728b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pdb-tools\n",
    "#!pip3 install atomium\n",
    "\n",
    "#!pip install sympy\n",
    "#!pip install UpSetPlot\n",
    "#!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba5f6a2-3f77-4a7e-a1f1-458b2c9a21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdb\n",
    "import requests\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil\n",
    "import hail as hl\n",
    "import glob\n",
    "import time\n",
    "import pytrimal\n",
    "# Import from installed package\n",
    "#from pypdb.clients.pdb.pdb_client import *\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import Bio\n",
    "import pandas as pd\n",
    "from Bio.PDB import PDBParser, PDBIO, Select, MMCIFParser\n",
    "from Bio.SeqIO import PirIO\n",
    "from Bio import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "#from Bio import pairwise2\n",
    "from Bio import Align, PDB\n",
    "from io import StringIO\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "from modeller.parallel import job, local_slave\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import logging\n",
    "import subprocess\n",
    "import shlex\n",
    "from subprocess import PIPE, run\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup  #required later to download SIFT files.\n",
    "import atomium\n",
    "from Bio.PDB import PDBParser, PDBIO, Structure\n",
    "from itertools import compress\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.gridspec as gridspe\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from bravado.client import SwaggerClient\n",
    "from Bio import AlignIO\n",
    "from pycanal import Canal\n",
    "#import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path\n",
    "from Bio.PDB import Superimposer, PDBParser\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from threading import Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from packman import molecule\n",
    "from packman.apps import predict_hinge\n",
    "#lot import generate_counts\n",
    "#from upsetplot import UpSet\n",
    "#from upsetplot import from_memberships\n",
    "#from upsetplot import from_contents\n",
    "#from upsetplot import from_indicators\n",
    "from scipy import stats\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "#logging.getLogger(\"requests\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a8716",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basehelper function:\n",
    "\n",
    "+ get_url\n",
    "+ get_gene_name_uniprot / get_gene_name redundant function.\n",
    "+ get_uniprot_id\n",
    "+ get_fasta_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3232,
   "id": "9114a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)  \n",
    "        if not response.ok:\n",
    "            print(response.text)\n",
    "    except:\n",
    "        response.raise_for_status()\n",
    "        #sys.exit()\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3233,
   "id": "ab87c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_name_uniprot(uniprot_id:str):\n",
    "    \n",
    "    fields = \"gene_primary\"\n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.text\n",
    "    #print(resp)\n",
    "    resp = resp.split(\"\\n\")\n",
    "    \n",
    "    #result_lst = [x.split(\"\\t\") for x in resp]\n",
    "    #result_lst = result_lst[0:-1]\n",
    "    #result_sort = sorted(result_lst, key= lambda x : x[2])\n",
    "    #resp = result_sort[0]\n",
    "    #print(resp)\n",
    "    return resp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3234,
   "id": "e8fb88c0-673b-45da-812e-84f7e8ee12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_name = get_gene_name_uniprot(\"1wio\")\n",
    "#print(gene_name)\n",
    "#get_prot_name = get_uniprot_id(gene_name)\n",
    "#print(get_prot_name)\n",
    "#print(get_gene_fasta(get_prot_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3235,
   "id": "3ce4f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniprot_id(uniprot_id:str):\n",
    "    fields = \"accession\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines()\n",
    "    for lines in resp:\n",
    "        lines = lines.decode()\n",
    "        if lines == \"Entry\":\n",
    "            continue\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3236,
   "id": "f5ec2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_fasta(uniprot_id:str):\n",
    "\n",
    "    #print(\"we are in get gene fasta\")\n",
    "    \"this is already overworked. should work.\"\n",
    "    #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "    \n",
    "    fields = \"sequence\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    seq = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    for lines in resp:\n",
    "        if i > 0:\n",
    "            seq += lines\n",
    "            #print(lines)\n",
    "        i += 1\n",
    "    \n",
    "    #print(seq)\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3237,
   "id": "a0c648c6-a378-4332-bddc-c9a841a882f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb_id_target= \"6lle\"\n",
    "#get_rcsb_fasta(pdb_id_target=pdb_id_target, chain=\"A\")\n",
    "#\n",
    "#query = \"P16615\"\n",
    "#main_prot_seq = get_gene_fasta(query)\n",
    "#print(main_prot_seq)\n",
    "#print(len(main_prot_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3238,
   "id": "cfbbb9d7-312d-42be-b604-13a8cb11fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_fasta_from_pdb_id(uniprot_id:str, chain:str):\n",
    "    \n",
    "    #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "    \n",
    "    fields = \"sequence\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    res = \"\"\n",
    "    i = 0\n",
    "    seq = 0\n",
    "    aln_code = []\n",
    "        \n",
    "    for lines in resp:\n",
    "        # if we encounter a new seq (e.g multi chain):\n",
    "        if lines[0] == \">\":\n",
    "            #we check if its the first seq. We make out custom header\n",
    "            continue\n",
    "        else:\n",
    "            res += lines\n",
    "            \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3239,
   "id": "6b17a7e9-4406-40ad-bf77-1f12519ccd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta_and_write(uniprot_id:str, path:str):\n",
    "    \n",
    "    \n",
    "    URL = f\"https://www.rcsb.org/fasta/entry/{uniprot_id}/display\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    #print(resp)\n",
    "    \n",
    "    res = \"\"\n",
    "    i = 0\n",
    "    seq = 0\n",
    "    chain = string.ascii_uppercase \n",
    "    \n",
    "    aln_code = []\n",
    "    \n",
    "    for lines in resp:\n",
    "        # if we encounter a new seq (e.g multi chain):\n",
    "        if lines[0] == \">\":\n",
    "            #we check if its the first seq. We make out custom header\n",
    "            origin_header = lines.split(\"|\")\n",
    "            if seq == 0:\n",
    "                res += f\">{uniprot_id}_{chain[i]}\\n\"\n",
    "                aln_code.append(f\"{uniprot_id}_{chain[i]}\")\n",
    "                i += 1\n",
    "                seq += 1\n",
    "                continue\n",
    "            #if its the nth seq, we add a \\n beforehand.\n",
    "            else:\n",
    "                \n",
    "                res += f\"\\n>{uniprot_id}_{chain[i]}\\n\"\n",
    "                aln_code.append(f\"{uniprot_id}_{chain[i]}\")\n",
    "                i += 1\n",
    "                continue\n",
    "        res += lines\n",
    "    \n",
    "    print(origin_header)\n",
    "    print(res)\n",
    "    #write to file and leave\n",
    "    #with open(f\"{path}x.fasta\", \"w\") as fasta_out:\n",
    "    #    fasta_out.write(res)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #return aln_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3240,
   "id": "caf42015-0f83-4005-b8e6-42d9b46c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hugo_name(uniprot_id:str):\n",
    "    \"\"\"\n",
    "    Retrieve the HUGO gene symbol from a Uniprot identifier.\n",
    "\n",
    "    Parameters:\n",
    "    - uniprot_id (str): The Uniprot identifier for the protein.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: The HUGO gene symbol corresponding to the Uniprot identifier.\n",
    "                  Returns None if no gene symbol is found. \"\"\"\n",
    "                  \n",
    "    \n",
    "    fields = \"gene_names\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    \n",
    "    resp = get_url(URL)\n",
    "    \n",
    "    resp = resp.text\n",
    "    \n",
    "    resp = resp.replace(\"Gene Names\", \"\")\n",
    "    \n",
    "    resp = resp.replace(\"\\n\", \"\")\n",
    "    \n",
    "    genes = resp.split(\" \")\n",
    "\n",
    "    #return None if we find nothing.\n",
    "    return genes[0] if len(genes) != 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3241,
   "id": "1eb0fffa-bbd3-4a47-9a3d-1181bed44081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbioportal_info(gene_name:str, study_id=\"msk_impact_2017\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve the mutation info from the cbioportal associated with our Uniprot ID.\n",
    "\n",
    "    Parameters:\n",
    "    - gene_name (str): The gene name identifier for the protein (HUGO format) Call first get_hugo_name(uniprot_id:str) to retrieve it.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    - pandas DataFrame or None: The HUGO gene symbol corresponding to the Uniprot identifier is used to search the cbioPortal API.\n",
    "                  Returns None if no gene symbol is found. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cbioportal = SwaggerClient.from_url('https://www.cbioportal.org/api/v2/api-docs',\n",
    "                                    config={\"validate_requests\":False,\"validate_responses\":False,\"validate_swagger_spec\": False})\n",
    "\n",
    "    for a in dir(cbioportal):\n",
    "        cbioportal.__setattr__(a.replace(' ', '_').lower(), cbioportal.__getattr__(a))\n",
    "    \n",
    "    muts = cbioportal.mutations.getMutationsInMolecularProfileBySampleListIdUsingGET(\n",
    "    molecularProfileId=f\"{study_id}_mutations\", # {study_id}_mutations gives default mutations profile for study \n",
    "    sampleListId=f\"{study_id}_all\", # {study_id}_all includes all samples\n",
    "    projection=\"DETAILED\").result()\n",
    "    \n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    mutation_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    gene_symbol_lst = []\n",
    "    mutation_type_lst = []\n",
    "    protein_change_lst = []\n",
    "    sample_id_lst = []\n",
    "    tumor_alt_count_lst = []\n",
    "    tumor_ref_count_lst = []\n",
    "    mutationStatus_lst = []\n",
    "    norm_alt_cnt_lst = []\n",
    "    norm_ref_cnt_lst = []\n",
    "    alleleSpecificCopyNumber_lst = []\n",
    "    driver_filt_lst = []\n",
    "    driver_filt_annot_lst = []\n",
    "    driver_tier_filt_lst = []\n",
    "    driver_tier_filt_annot_lst = []\n",
    "\n",
    "    \n",
    "    # Populate the DataFrame with mutation information\n",
    "    for mutation in muts:\n",
    "        #print(mutation)\n",
    "        gene_symbol = mutation.gene.hugoGeneSymbol\n",
    "        mutation_type = mutation.mutationType\n",
    "        protein_change = mutation.proteinChange\n",
    "        sample_id = mutation.sampleId\n",
    "        tumor_alt_count = mutation.tumorAltCount\n",
    "        tumor_ref_count = mutation.tumorRefCount\n",
    "        mutation_status = mutation.mutationStatus\n",
    "        norm_alt_cnt = mutation.normalAltCount\n",
    "        norm_ref_cnt = mutation.normalRefCount\n",
    "        alleleSpecificCopyNumber = mutation.alleleSpecificCopyNumber\n",
    "        driver_filt = mutation.driverFilter\n",
    "        driver_filt_annot = mutation.driverFilterAnnotation\n",
    "        driver_tier_filt = mutation.driverTiersFilter\n",
    "        driver_tier_filt_annot = mutation.driverTiersFilterAnnotation\n",
    "        \n",
    "# 'driverFilterAnnotation', 'driverTiersFilter', 'driverTiersFilterAnnotation',\n",
    "\n",
    "\n",
    "        \n",
    "        gene_symbol_lst.append(gene_symbol)\n",
    "        mutation_type_lst.append(mutation_type)\n",
    "        protein_change_lst.append(protein_change)\n",
    "        sample_id_lst.append(sample_id)\n",
    "        tumor_alt_count_lst.append(tumor_alt_count) \n",
    "        tumor_ref_count_lst.append(tumor_ref_count)\n",
    "        mutationStatus_lst.append(mutation_status)\n",
    "        norm_alt_cnt_lst.append(norm_alt_cnt)\n",
    "        norm_ref_cnt_lst.append(norm_ref_cnt)\n",
    "        alleleSpecificCopyNumber_lst.append(alleleSpecificCopyNumber)\n",
    "        driver_filt_lst.append(driver_filt)\n",
    "        driver_filt_annot_lst.append(driver_filt_annot)\n",
    "        driver_tier_filt_lst.append(driver_tier_filt)\n",
    "        driver_tier_filt_annot_lst.append(driver_filt_annot)\n",
    "        \n",
    "        \n",
    "    mutation_df[\"gene_symbol\"] = gene_symbol_lst\n",
    "    mutation_df[\"mutation_type\"] = mutation_type_lst\n",
    "    mutation_df[\"prot_change\"] = protein_change_lst\n",
    "    mutation_df[\"sample_id\"] = sample_id_lst\n",
    "    mutation_df[\"tumor_alt_count\"] = tumor_alt_count_lst\n",
    "    mutation_df[\"tumor_ref_count\"] = tumor_ref_count_lst\n",
    "    mutation_df[\"norm_alt_count\"] = tumor_alt_count_lst\n",
    "    mutation_df[\"norm_ref_count\"] = tumor_ref_count_lst\n",
    "    mutation_df[\"mutation_status\"] = mutationStatus_lst\n",
    "    mutation_df[\"alleleSpecificCopyNumber\"] = alleleSpecificCopyNumber_lst\n",
    "    mutation_df[\"driver_filt\"] = driver_filt_lst\n",
    "    mutation_df[\"driver_filt_annot\"] = driver_filt_annot_lst\n",
    "    mutation_df[\"driver_tier_filt\"] = driver_tier_filt_lst\n",
    "    mutation_df[\"driver_tier_filt_annot\"] = driver_filt_annot_lst\n",
    "\n",
    "    \n",
    "    \n",
    "    mutation_sub_df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        for mutation in mutation_df[\"gene_symbol\"]:\n",
    "            if mutation == gene_name:\n",
    "                mutation_sub_df = mutation_df[mutation_df[\"gene_symbol\"] == gene_name]\n",
    "                mutation_sub_df = mutation_sub_df[mutation_sub_df[\"mutation_type\"] == \"Missense_Mutation\"]\n",
    "    except:\n",
    "        print(f\"gene name: {gene_name} was not found\")\n",
    "    \n",
    "    return mutation_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3242,
   "id": "aec2a3c8-f7f7-4f90-a794-14dd028dc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#integrate get_rcsb_fasta into pipeline and substitute with get_gene_fasta!\n",
    "#ToBeDone\n",
    "\n",
    "def get_rcsb_fasta(pdb_id_target:str, chain:str):\n",
    "    \n",
    "    '''This function will take the uniprot id and the requiered chain and fetch ONLY the sequence of the\n",
    "    corresponding chain. This will be used for modeller repair as template in order to ONLY build\n",
    "    the required part of the protein (i.e part of the protein the structure covers) and not the fasta for the full length protein\n",
    "    which would lead to a full reconstruction of e.g EGFR 1-1xxx instead of maybe only repairing 300-460'''\n",
    "    \n",
    "    URL = f\"https://www.rcsb.org/fasta/entry/{pdb_id_target}/display\"\n",
    "    \n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    \n",
    "    #here we store each chain and their fasta respectively. If multiple chains are homo oligomer their fasta\n",
    "    #are stored together.\n",
    "    \n",
    "    fasta_per_chains = defaultdict()\n",
    "        \n",
    "    for lines in resp:\n",
    "        \n",
    "        if lines[0] == \">\":\n",
    "            #we store all headers given they contain the chain info.\n",
    "            #print(lines)\n",
    "            origin_header = lines.split(\"|\") #split by |\n",
    "            #print(origin_header)\n",
    "            chain_info = origin_header[1] #2nd entry is chain info\n",
    "            chain_info = chain_info[6:] # we are only interested in A, B , C , D not the keyword \"chains\" \n",
    "            #print(chain_info)\n",
    "            \n",
    "            #lets try to capture author annotated chains as well.\n",
    "            try:\n",
    "                \n",
    "                author_chain = chain_info[-2] #this is the authors chain info.\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                author_chain = \"Placeholder\"\n",
    "                \n",
    "            chain_info = re.sub('auth.[A-Z]', '', chain_info)  #if the authors have other chain labels we dont care\n",
    "            chain_info = chain_info.replace(\"[]\",\"\")  #this is removal from from the authors chain labels.\n",
    "            \n",
    "            chain_info = chain_info.replace(\",\",\"\") # remove , and merge them\n",
    "            chain_info = chain_info.replace(\" \",\"\") #make str out of those as key for a dict. \"ABCD\"\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            fasta_per_chains[chain_info] = lines\n",
    "            fasta_per_chains[author_chain] = lines\n",
    "            \n",
    "    \n",
    "    print(fasta_per_chains)\n",
    "    \n",
    "    #now lets look the prior names of the chains before renumbering.\n",
    "    \n",
    "    \n",
    "    #with open(f\"{path}/reports/chain_relabeling_protocol.csv\", \"r\") as chain_names_csv:\n",
    "    #    for lines in chain_names_csv:\n",
    "    #        splitted_csv = lines.split()\n",
    "    #        if splitted_csv[0] == pdb_id_target:\n",
    "    #            old = splitted_csv[1].split()\n",
    "    #            new = splitted_csv[2].split()\n",
    "                \n",
    "    \n",
    "    \n",
    "    for fasta_chains, seq in fasta_per_chains.items():\n",
    "        avail_chain = [x for x in fasta_chains]\n",
    "        #print(avail_chain)\n",
    "        if chain in avail_chain:\n",
    "            return seq\n",
    "            \n",
    "    \n",
    "    #try author chain annotation as well if not working.\n",
    "    \n",
    "    \n",
    "    \n",
    "    #return None if we dont find anything.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3243,
   "id": "8496ac2f-ef6f-4d88-a603-c84665a823ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb_id_target= \"6lle\"\n",
    "#get_rcsb_fasta(pdb_id_target=pdb_id_target, chain=\"A\")\n",
    "#\n",
    "#query = \"\"\n",
    "#main_prot_seq = get_gene_fasta(query)\n",
    "#print(len(main_prot_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6dedd3-f126-4c95-83ed-cac2b08fcece",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Helper functions need overview to check which is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c949e0-d64c-43bf-a6cf-9dd76a43f23f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Following functions:\n",
    "\n",
    "+ Setup directories and prep input files\n",
    "+ Prepare templates\n",
    "+ _write_report_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3244,
   "id": "02744083-db9e-4250-8656-6580d195c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories_prepare_input_files_1(gene_name:str, prot_fasta:str, template:list):\n",
    "    \n",
    "    #check if the directory is not already there.\n",
    "    #setup all required directories now.\n",
    "    \n",
    "    #hardcorded anchor location.\n",
    "    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "    \n",
    "    # Create directories if not already present\n",
    "    dirs_to_create = [\"reports\", \"shifts\", \"mutational_mapping\"]\n",
    "\n",
    "    for directory in dirs_to_create:\n",
    "        \n",
    "        dir_path = os.path.join(path, directory)\n",
    "        \n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "\n",
    "    # Create or update pdbfile.txt for batch download\n",
    "    \n",
    "    pdbfile_path = os.path.join(path, \"pdbfile.txt\")\n",
    "    \n",
    "    #rcsb_unique_ids = list(set(rcsbs[0:4] for vals in domain_dict.values() for rcsbs in vals))\n",
    "\n",
    "\n",
    "    chain_dict = defaultdict(list)\n",
    "\n",
    "    for pdb in template:\n",
    "        key = pdb[:4]  # Extract the first four characters\n",
    "        value = pdb[-1]  # Extract the character after the first four\n",
    "        chain_dict[key].append(value)\n",
    "    \n",
    "    unique_temp = list(set([x[0:4] for x in template]))\n",
    "\n",
    "    with open(pdbfile_path, \"w\") as pdb_tar:\n",
    "        pdb_tar.write(\",\".join(unique_temp))\n",
    "\n",
    "    \n",
    "    # Store the main fasta of the canonical isoform for later mapping\n",
    "    main_isoform_path = os.path.join(path, \"reports/main_isoform_fasta.txt\")\n",
    "    \n",
    "    with open(main_isoform_path, \"w\") as main:\n",
    "        main.write(prot_fasta)\n",
    "\n",
    "    \n",
    "    # Create chain_relabeling_protocol.csv\n",
    "    chain_relabeling_path = os.path.join(path, \"reports/chain_relabeling_protocol.csv\")\n",
    "    \n",
    "    with open(chain_relabeling_path, \"w\") as fh_chain:\n",
    "        fh_chain.write(\"pdb_file,previous_chains,new_chains\\n\")\n",
    "\n",
    "    \n",
    "    return chain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3245,
   "id": "8743bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_templates(gene_name_target:str,\n",
    "                      templates:list,\n",
    "                      seq_sim:list,\n",
    "                      query_start:list,\n",
    "                      query_end:list,\n",
    "                      temp_start:list,\n",
    "                      temp_end:list,\n",
    "                      path:str, oligodict:list):\n",
    "    \n",
    "    \"\"\"Make function out of making templates and reference structures.\"\"\"\n",
    "    #setup\n",
    "    \n",
    "    \n",
    "    #this retrieves the main len of the protein we are interested in\n",
    "    main_target_seq_len = get_seq_len(path=f\"{path}\", rcsb_id=\"None\", template=True)\n",
    "    #181 for NUD4B\n",
    "    \n",
    "    #gives back the length of each template.\n",
    "    temp_lengths = [f[0] - f[1] for f in zip(temp_end, temp_start)]\n",
    "    \n",
    "    homology_list = sorted(list(zip(templates,seq_sim, temp_lengths)), key= lambda x: x[1], reverse=True)\n",
    "    #print(homology_list)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        #gather target templates that will be used to compute tm scores and align scores\n",
    "        ref_list = []\n",
    "        #here we store human hits.\n",
    "        priority_list = []\n",
    "        #if we retrieve not as many hits, we take what we get\n",
    "        if len(homology_list) < 100:\n",
    "            \n",
    "            max_end = len(homology_list)\n",
    "            \n",
    "        #else we take max 100 potential templates\n",
    "        else:\n",
    "            \n",
    "            max_end = 100\n",
    "        #this will be used later to set a ref struc template.\n",
    "        \n",
    "        \n",
    "        for entry , seq, lengths in homology_list[0:max_end]:\n",
    "            \n",
    "            #this is the rcsb_id\n",
    "            prot_check = entry[0:4]\n",
    "            \n",
    "            #this retrieves gene_name\n",
    "            prot_name = get_gene_name(prot_check)\n",
    "            #this corresponds to the species the gene belongs to\n",
    "            if prot_name[-5:] == \"HUMAN\":\n",
    "                \n",
    "                #if its human, we take it as template and break.\n",
    "                reference_structure = entry+\".pdb\"\n",
    "                #temp score will help to find ideal target\n",
    "                temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                           seq_len=lengths,\n",
    "                           main_seq_len=main_target_seq_len)\n",
    "                #we store both seq similarity and potential .pdb file\n",
    "                priority_list.append((reference_structure, prot_name, temp_score_val, lengths, float(seq)))\n",
    "                continue\n",
    "                \n",
    "            #else we take as many hits we can and select the highest seq similarity out of those.\n",
    "            reference = entry+\".pdb\"\n",
    "            \n",
    "            #temp score will help to find ideal target\n",
    "            temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                       seq_len=lengths,\n",
    "                       main_seq_len=main_target_seq_len)\n",
    "            \n",
    "            ref_list.append((reference, prot_name, temp_score_val, lengths, float(seq))) \n",
    "            \n",
    "    except:\n",
    "        print(f\"We did not find structures.\\n selected template was:{prot_name}\")\n",
    "        print(f\"reference list contained: {ref_list}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(priority_list) != 0:\n",
    "        #if we found a human template we set it as reference structure.\n",
    "        priority_list_sorted = sorted(priority_list, key=lambda x : x[2], reverse=False)\n",
    "        print(f\"we found as reference structure(s): {priority_list[0][1]} -> {priority_list_sorted[0][0]}\")\n",
    "        \n",
    "        #little helper function to write out which template was choosen.\n",
    "        _write_report_template(path=path, \n",
    "                               template_gene_name=priority_list_sorted[0][1],\n",
    "                               template_rcsb=priority_list_sorted[0][0],\n",
    "                               template_length=priority_list_sorted[0][3],\n",
    "                               seq_id =priority_list_sorted[0][4],\n",
    "                               query_length=main_target_seq_len)\n",
    "        \n",
    "        return priority_list_sorted\n",
    "    \n",
    "    else:\n",
    "        #else we take the next best thing that has highest seq similarity hoping this is mouse or something.\n",
    "        ref_list_sorted = sorted(ref_list, key=lambda x : x[2], reverse=False)\n",
    "        print(f\"we found no human reference structure(s) but instead: {ref_list[0][1]} -> {ref_list_sorted[0][0]}\")\n",
    "        \n",
    "        #little helper function to write out which template was choosen.\n",
    "        _write_report_template(path=path, \n",
    "                               template_gene_name=ref_list_sorted[0][1],\n",
    "                               template_rcsb=ref_list_sorted[0][0],\n",
    "                               template_length=ref_list_sorted[0][3],\n",
    "                               seq_id =priority_list_sorted[0][4],\n",
    "                               query_length=main_target_seq_len,\n",
    "                               position_dir = position_dir,\n",
    "                               oligostate_dir=oligostate_dir)\n",
    "        \n",
    "        return ref_list_sorted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3246,
   "id": "eec5c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_report_template(path:str, template_gene_name:str, \n",
    "                           template_rcsb:str, template_length:str, \n",
    "                           query_length:str, seq_id:str,\n",
    "                           position_dir:str,\n",
    "                           oligostate_dir:str):\n",
    "    \n",
    "    \n",
    "    #the last dir corresponds to the gene_name of query.\n",
    "    #this gives the split dirs that make up path: e.g /home/micnag/b would give [\"home\", \"micnag\", \"b\"]\n",
    "    path_split = path.split(os.sep)\n",
    "    \n",
    "    \n",
    "    with open(f\"{path}/reports/selected_template_{oligostate_dir}_{position_dir}.tsv\", \"w\") as fh_report:\n",
    "        fh_report.write(\"Query_gene_name\\tTemplate_gene_name\\tTemplate_rcsb\\tTemplate_length\\tQuery_length\\tSequence_identity\")\n",
    "        fh_report.write(\"\\n\")\n",
    "        fh_report.write(f\"{path_split[-1]}\\t{template_gene_name}\\t{template_rcsb}\\t{template_length}\\t{query_length}\\t{seq_id}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3247,
   "id": "5fc41e57-2ec2-432d-834e-327f08537ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcsb_info(path:str):\n",
    "    \n",
    "    '''This function will take the uniprot id and the requiered chain and fetch ONLY the sequence of the\n",
    "    corresponding chain. This will be used for modeller repair as template in order to ONLY build\n",
    "    the required part of the protein (i.e part of the protein the structure covers) and not the fasta for the full length protein\n",
    "    which would lead to a full reconstruction of e.g EGFR 1-1xxx instead of maybe only repairing 300-460'''\n",
    "    \n",
    "    pdb_list = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    pdb_list = [f[0:4] for f in pdb_list if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    try:\n",
    "            \n",
    "        os.mkdir(f\"{path}/PCA_structures_info\")\n",
    "            \n",
    "    except Exception as error:\n",
    "            \n",
    "        print(error)\n",
    "    \n",
    "    \n",
    "    infodict = defaultdict()\n",
    "    \n",
    "    for pdb_id_target in pdb_list:\n",
    "        \n",
    "        URL = f\"https://www.rcsb.org/fasta/entry/{pdb_id_target}/display\"\n",
    "        \n",
    "        resp = get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "\n",
    "        #here we store each chain and their fasta respectively. If multiple chains are homo oligomer their fasta\n",
    "        #are stored together.\n",
    "            \n",
    "        for lines in resp:\n",
    "\n",
    "            if lines[0] == \">\":\n",
    "                #we store all headers given they contain the chain info.\n",
    "                lines_grp = lines.split(\"|\")\n",
    "                pdb_id = lines_grp[0].replace(\">\",\"\")\n",
    "                pdb_id = pdb_id[0:4].lower()\n",
    "                infodict[pdb_id] = lines_grp[1:]\n",
    "\n",
    "    with open(f\"{path}/PCA_structures_info/PCA_rcsb_headers.tsv\", \"w\") as fh_out:\n",
    "        for keys, vals in infodict.items():\n",
    "            fh_out.write(keys)\n",
    "            fh_out.write(\"\\t\")\n",
    "            for entries in vals:\n",
    "                fh_out.write(entries)\n",
    "                fh_out.write(\"\\t\")\n",
    "            fh_out.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3248,
   "id": "777cc5de-6935-4352-adec-b67afba67022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ADH1G_HUMAN/dimer/pos_1_375\"\n",
    "\n",
    "#get_rcsb_info(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f11170-a27c-46ef-8254-9b5a671e0424",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DOWNLOAD PDBS function\n",
    "\n",
    "requires:\n",
    "\n",
    "+ batch_download_modified.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3249,
   "id": "57df4a41-23ae-4d43-82ce-524be6f660cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(file_list, download_type, path):\n",
    "    \n",
    "    results = []\n",
    "    for file in file_list:\n",
    "        \n",
    "        bash_curl_cmd = f\"./batch_download_modified.sh -f {file} -o {path} -{download_type}\"\n",
    "        \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "        \n",
    "        result = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        results.append(result.stdout.split(\"\\n\")[:-1])  # Skip the last empty element\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3250,
   "id": "d5b08116-235f-4312-8c06-5c43758462a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_download_pdbs(gene_name):\n",
    "    \n",
    "    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "    \n",
    "    os.chdir(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs\")\n",
    "    \n",
    "   ## Setup directories\n",
    "   #check_path = f\"{path}/monomeric\"\n",
    "   #\n",
    "   #if not os.path.exists(check_path):\n",
    "   #    \n",
    "   #    os.makedirs(check_path)\n",
    "\n",
    "    # List of PDB and MMCIF files to download\n",
    "    pdbfile_list = [f\"{path}/pdbfile.txt\"]  # Adjust the number based on your requirements\n",
    "    \n",
    "    mmciffile_list = [f\"{path}/mmciffile.txt\"]\n",
    "\n",
    "    # ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit your tasks to the executor.\n",
    "        futures_pdb = [executor.submit(download_files, pdbfile_list, 'p', path)]\n",
    "\n",
    "        # Optionally, you can use as_completed to wait for and retrieve completed results.\n",
    "        for future in as_completed(futures_pdb):\n",
    "            result = future.result()\n",
    "\n",
    "    # ProcessPoolExecutor for MMCIF files\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit your tasks to the executor.\n",
    "        futures_pdb = [executor.submit(download_files, mmciffile_list, 'c', path)]\n",
    "\n",
    "        # Optionally, you can use as_completed to wait for and retrieve completed results.\n",
    "        for future in as_completed(futures_pdb):\n",
    "            result = future.result()\n",
    "\n",
    "\n",
    "    #print(f\"Dir has already some pdbs -> {len(os.listdir(check_path))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4881a01-7ee7-4b99-90dc-8cd8641d662f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f6b7d-2a04-490d-967c-186ea0b94ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa9143-1369-4ce8-9460-78ade5860e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79ce79db-d570-4649-b8c5-81910ac692e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper function / Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3251,
   "id": "d1483724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cleanup(path:str):\n",
    "        \n",
    "    #we dont need tmp pdb files given that all what we want is in dirs and no longer in this main dir.\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(\".pdb\")]\n",
    "    \n",
    "    for file in onlyfiles:\n",
    "        os.remove(f\"{path}/{file}\")\n",
    "    \n",
    "    #also remove the shifts tmp folder to save disc space.\n",
    "    try:\n",
    "        shutil.rmtree(f'{path}/shifts')\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698df52-b405-44b8-9275-3524665928dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reduce redundancy and split proteins into domain chunks / group similar structure ranges together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da7404-7a17-47eb-a79d-91ca46f99888",
   "metadata": {
    "tags": []
   },
   "source": [
    "## _Split domains + make_groups\n",
    "\n",
    "make_groups uses:\n",
    "\n",
    "+ <span style=\"color:blue\">_recursive_union_reduction</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3252,
   "id": "90bffdcb-831a-4d74-9126-8ca538c2f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_domains_1(templates:list, temp_start:list, temp_end:list, query_start:list, query_end:list):\n",
    "  \n",
    "    abs_seq_min = min(temp_start)\n",
    "    abs_seq_max = max(temp_end)\n",
    "    \n",
    "    abs_range = abs_seq_max - abs_seq_min\n",
    "    tolerance = 0.2 * abs_range\n",
    "\n",
    "    seq_ranges = list(zip(query_start, query_end, templates))\n",
    "    print(seq_ranges)\n",
    "    \n",
    "    # Find the majority vote\n",
    "    majority_range = _majority_vote(seq_ranges)\n",
    "\n",
    "    # Extend the winner's range by tolerance\n",
    "    winner_start, winner_end = majority_range\n",
    "    extended_start = max(abs_seq_min, winner_start - tolerance)\n",
    "    extended_end = min(abs_seq_max, winner_end + tolerance)\n",
    "    extended_range = (extended_start, extended_end)\n",
    "\n",
    "    # Filter sequences within the extended range\n",
    "    selected_ranges = [(start_q, end_q, template) for start_q, end_q, template in seq_ranges\n",
    "                       if extended_start <= start_q <= extended_end and extended_start <= end_q <= extended_end]\n",
    "\n",
    "    # Merge overlapping ranges\n",
    "    merged_ranges = merge_overlapping_ranges(selected_ranges)\n",
    "\n",
    "    print(f\"Winner's range: {extended_range}\")\n",
    "    print(f\"These are the merged ranges: {list(merged_ranges.keys())}\")\n",
    "\n",
    "    return merged_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3253,
   "id": "6194670c-3d81-47e5-ad02-f5f66586163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_groups(datalst:list, val_list:list):\n",
    "    \n",
    "    storage_dict = defaultdict(list)\n",
    "    \n",
    "    for ranges, vals in zip(datalst, val_list):\n",
    "        storage_dict[Interval(ranges[0],ranges[1])] = vals\n",
    "    \n",
    "    \n",
    "    #works here still\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    intervals = [Interval(begin, end) for (begin, end) in datalst]\n",
    "    \n",
    "    \n",
    "    #print(\"this are the intervals we deal with:\")\n",
    "    #print(intervals)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #good so far. all 34 structures are in here for case NUDT4\n",
    "    \n",
    "    interval_dict = defaultdict()\n",
    "    \n",
    "    for interv in intervals:\n",
    "        interval_dict[interv] = interv\n",
    "        \n",
    "\n",
    "    print(f\"length of dictionary before recursive_reduction: {len(interval_dict)}\")\n",
    "    if len(interval_dict) != 1:\n",
    "        result = _recursive_union_reduction(interval_dict, 0)\n",
    "        print(f\"length of dictionary after recursive_reduction: {len(result)}\\n\")\n",
    "    else:\n",
    "        result = interval_dict\n",
    "        #for keys, vals in result.items():\n",
    "        #    print(len(vals))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    interval_collection_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in result.items():\n",
    "        #for each key we need to find all associated intervals within the key range e.g all subsets.\n",
    "        \n",
    "        for interval_member in intervals:\n",
    "            condition = interval_member.is_subset(keys)\n",
    "            condition_2 = interval_member.measure > 0.7*keys.measure\n",
    "            if condition and condition_2:\n",
    "                interval_collection_dict[keys].append(interval_member)\n",
    "    \n",
    "    #print(len(intervals))\n",
    "    \n",
    "    rcsb_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in interval_collection_dict.items():\n",
    "        \n",
    "        if type(vals) == list:\n",
    "            rcsb_tmp = []\n",
    "            for val in vals:\n",
    "                tmp = storage_dict[val]\n",
    "                #print(f\"this is current key: {keys}\")\n",
    "                #print(f\"this is current structures: {storage_dict[val]}\")\n",
    "                #print(tmp)\n",
    "                for rcsbs in tmp:\n",
    "                    rcsb_tmp.append(rcsbs)\n",
    "            \n",
    "        else:\n",
    "            rcsb_tmp = []\n",
    "            tmp = storage_dict[val]\n",
    "            for rcsbs in tmp:\n",
    "                rcsb_tmp.append(rcsbs)\n",
    "        \n",
    "            rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        \n",
    "        \n",
    "    #for keys, vals in rcsb_dict.items():\n",
    "        #print(keys)\n",
    "        #print(vals)\n",
    "        #print(\"\\n\")\n",
    "    \n",
    "    return rcsb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3254,
   "id": "e9b03d84-8db2-4bf9-89e9-9dbae593b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_domains_pdb(pdb_ranges:dict, main_prot_seq:str):\n",
    "\n",
    "    #pdb ranges is a dict containing of key: path  val:(start, stop) \n",
    "    \n",
    "    domain_dict = _make_groups_pdb(pdb_ranges, main_prot_seq=main_prot_seq)\n",
    "\n",
    "    #print(domain_dict)\n",
    "    return domain_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if len(start_stop_list) != 1:\n",
    "        split_dictionary = _make_groups(start_stop_list, val_list)    \n",
    "    else:\n",
    "        split_dictionary = defaultdict(list)\n",
    "    \n",
    "    print(f\"These are the merged ranges: {[x for x in split_dictionary.keys()]}\")\n",
    "    \n",
    "    \n",
    "    single_interval_dict = defaultdict(list)\n",
    "    \n",
    "    #i = 0\n",
    "    #for vals in val_list:\n",
    "        #print(vals)\n",
    "        #print(Interval(start_stop_list[0][0], start_stop_list[0][1]))\n",
    "    #    single_interval_dict[Interval(start_stop_list[i][0], start_stop_list[i][1])] = vals\n",
    "    #    i += 1\n",
    "    \n",
    "    #this contains the domains we need to split and each set of structures associated per domain.\n",
    "    \n",
    "    split_return_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in split_dictionary.items():\n",
    "        #only care about more than 1 structure.\n",
    "        if len(vals) > 1: \n",
    "            split_return_dict[keys] = vals\n",
    "    \n",
    "    cnt = 0\n",
    "    for keys, vals in split_return_dict.items():\n",
    "        cnt += len(vals)\n",
    "        \n",
    "    return split_return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6c23e-f1ae-4dc5-8c4b-0bba4147b868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3255,
   "id": "f1838055-4ba6-4e13-afd3-bdc405a414ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_domains(templates:list, temp_start:list, temp_end:list, query_start:list, query_end:list):\n",
    "  \n",
    "    print(len(templates))\n",
    "    print(\"\\n\")\n",
    "    #print(temp_start)\n",
    "    #print(temp_end)\n",
    "    #print(query_start)\n",
    "    #print(query_end)\n",
    "    \n",
    "    #this will give the maximal possible range we will look into.\n",
    "    abs_seq_min = min(temp_start)\n",
    "    abs_seq_max = max(temp_end)\n",
    "    \n",
    "    abs_range = abs_seq_max - abs_seq_min\n",
    "    tolerance = 0.2 *abs_range\n",
    "    \n",
    "    seq_lengths = []\n",
    "    \n",
    "    for start, end in zip(query_start, query_end):\n",
    "        seq_lengths.append(end - start)\n",
    "\n",
    "    \n",
    "    seq_ranges = []\n",
    "    \n",
    "    testdict = defaultdict(list)\n",
    "    \n",
    "    for temp, start_q, end_q, length in zip(templates, query_start, query_end, seq_lengths):\n",
    "        testdict[(start_q, end_q)].append(temp)\n",
    "    \n",
    "    start_stop_list = []\n",
    "    val_list = []\n",
    "    \n",
    "    \n",
    "    for keys, values in testdict.items():\n",
    "        start_stop_list.append(keys)\n",
    "        val_list.append(values)\n",
    "    \n",
    "    print(f\"These are the ranges that we deal with: {start_stop_list} \\n\")\n",
    "    \n",
    "    if len(start_stop_list) != 1:\n",
    "        split_dictionary = _make_groups(start_stop_list, val_list)    \n",
    "    else:\n",
    "        split_dictionary = defaultdict(list)\n",
    "    \n",
    "    print(f\"These are the merged ranges: {[x for x in split_dictionary.keys()]}\")\n",
    "    \n",
    "    \n",
    "    single_interval_dict = defaultdict(list)\n",
    "    \n",
    "    #i = 0\n",
    "    #for vals in val_list:\n",
    "        #print(vals)\n",
    "        #print(Interval(start_stop_list[0][0], start_stop_list[0][1]))\n",
    "    #    single_interval_dict[Interval(start_stop_list[i][0], start_stop_list[i][1])] = vals\n",
    "    #    i += 1\n",
    "    \n",
    "    #this contains the domains we need to split and each set of structures associated per domain.\n",
    "    \n",
    "    split_return_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in split_dictionary.items():\n",
    "        #only care about more than 1 structure.\n",
    "        if len(vals) > 1: \n",
    "            split_return_dict[keys] = vals\n",
    "    \n",
    "    cnt = 0\n",
    "    for keys, vals in split_return_dict.items():\n",
    "        cnt += len(vals)\n",
    "        \n",
    "    return split_return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3256,
   "id": "16ac58f3-3242-4d87-970a-d87348817e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_union_reduction(dictionary_to_parse:dict, num_of_iterations:int):\n",
    "    \n",
    "    #print(\"this is dictionary to parse:\")\n",
    "    #print(dictionary_to_parse)\n",
    "    \n",
    "    if num_of_iterations < 20:\n",
    "        \n",
    "        #if everything merged together we dont need to continue\n",
    "        if len(dictionary_to_parse) == 1:\n",
    "            return dictionary_to_parse\n",
    "        \n",
    "        num_of_iterations += 1\n",
    "        val_1 = []\n",
    "        val_2 = []\n",
    "        \n",
    "        for keys, vals in dictionary_to_parse.items():\n",
    "            val_1.append(keys)\n",
    "            val_2.append(keys)\n",
    "        \n",
    "        union_dict = defaultdict(list)        \n",
    "        \n",
    "        for val in val_1:\n",
    "            union_new = False\n",
    "            for val_x in val_2:\n",
    "                if val != val_x:\n",
    "                    m_1 = val.measure\n",
    "                    m_2 = val_x.measure\n",
    "                    m_3 = val_x.intersect(val).measure\n",
    "                    \n",
    "                    #print(val, val_x)\n",
    "                    #print(m_1, m_2, m_3)\n",
    "                    condition_1 = m_3 <= m_1 and m_3 >= 0.8* m_1\n",
    "                    condition_2 = m_3 <= m_2 and m_3 >= 0.8* m_2\n",
    "                    #print(condition_1, condition_2)\n",
    "                    if condition_1 and condition_2:\n",
    "                        union_new = Union(val, val_x)\n",
    "                        val_tmp_1 = val\n",
    "                        val_tmp_2 = val_x\n",
    "            \n",
    "            if union_new:\n",
    "                union_dict[union_new].append(val_tmp_1)\n",
    "                union_dict[union_new].append(val_tmp_2)\n",
    "                \n",
    "            else:\n",
    "        \n",
    "                union_dict[val].append(val)\n",
    "\n",
    "        dictionary_to_parse = union_dict\n",
    "        \n",
    "        dictionary_to_parse = _recursive_union_reduction(dictionary_to_parse, num_of_iterations)\n",
    "    \n",
    "    \n",
    "    return dictionary_to_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17009c-b2f3-48f4-bb1a-16dfa92b6fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800ff84-3812-413e-a4d0-07b64f925b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3257,
   "id": "d810a640-8ea8-4346-8c32-15ce51cb54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _majority_vote(seq_ranges):\n",
    "    # Count occurrences of each range\n",
    "    count_dict = defaultdict(int)\n",
    "    for start_q, end_q, template in seq_ranges:\n",
    "        count_dict[(start_q, end_q)] += 1\n",
    "\n",
    "    # Find the range with the maximum count\n",
    "    majority_range = max(count_dict, key=count_dict.get)\n",
    "    \n",
    "    return majority_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3258,
   "id": "794f2de6-6990-4fd0-8c3b-9e551439589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_ranges(ranges):\n",
    "    sorted_ranges = sorted(ranges, key=lambda x: x[0])\n",
    "\n",
    "    merged_ranges = []\n",
    "    current_start, current_end, current_template = sorted_ranges[0]\n",
    "\n",
    "    for start, end, template in sorted_ranges[1:]:\n",
    "        if start <= current_end:\n",
    "            # Merge overlapping ranges\n",
    "            current_end = max(current_end, end)\n",
    "        else:\n",
    "            # Add the current merged range to the result\n",
    "            merged_ranges.append((current_start, current_end, current_template))\n",
    "            # Start a new merged range\n",
    "            current_start, current_end, current_template = start, end, template\n",
    "\n",
    "    # Add the last merged range\n",
    "    merged_ranges.append((current_start, current_end, current_template))\n",
    "\n",
    "    return dict(((start, end), templates) for start, end, templates in merged_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3259,
   "id": "b865832f-6194-4bc9-8771-3fe38389e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_values_for_overlapping_ranges(merged_ranges, original_dict):\n",
    "    merged_dict = defaultdict(list)\n",
    "\n",
    "    for merged_range in merged_ranges:\n",
    "        for key, value in original_dict.items():\n",
    "            if key[0] <= merged_range[0] and key[1] >= merged_range[1]:\n",
    "                merged_dict[tuple(merged_range)].extend(value)\n",
    "\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3260,
   "id": "cf8038a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_domains(templates:list, temp_start:list, temp_end:list, query_start:list, query_end:list):\n",
    "  \n",
    "    print(len(templates))\n",
    "    print(\"\\n\")\n",
    "    print(temp_start, temp_end)\n",
    "    #this will give the maximal possible range we will look into.\n",
    "    abs_seq_min = min(temp_start)\n",
    "    abs_seq_max = max(temp_end)\n",
    "    \n",
    "    abs_range = abs_seq_max - abs_seq_min\n",
    "    tolerance = 0.2 *abs_range\n",
    "    \n",
    "    seq_lengths = []\n",
    "    \n",
    "    for start, end in zip(query_start, query_end):\n",
    "        seq_lengths.append(end - start)\n",
    "\n",
    "    \n",
    "    seq_ranges = []\n",
    "    \n",
    "    testdict = defaultdict(list)\n",
    "    \n",
    "    for temp, start_q, end_q, length in zip(templates, query_start, query_end, seq_lengths):\n",
    "        testdict[(start_q, end_q)].append(temp)\n",
    "    \n",
    "    start_stop_list = []\n",
    "    val_list = []\n",
    "    \n",
    "    \n",
    "    for keys, values in testdict.items():\n",
    "        start_stop_list.append(keys)\n",
    "        val_list.append(values)\n",
    "    \n",
    "    print(f\"These are the ranges that we deal with: {start_stop_list} \\n\")\n",
    "    \n",
    "    if len(start_stop_list) != 1:\n",
    "        split_dictionary = _make_groups(start_stop_list, val_list)    \n",
    "    else:\n",
    "        split_dictionary = defaultdict(list)\n",
    "    \n",
    "    print(f\"These are the merged ranges: {[x for x in split_dictionary.keys()]}\")\n",
    "    \n",
    "    \n",
    "    single_interval_dict = defaultdict(list)\n",
    "    \n",
    "    #i = 0\n",
    "    #for vals in val_list:\n",
    "        #print(vals)\n",
    "        #print(Interval(start_stop_list[0][0], start_stop_list[0][1]))\n",
    "    #    single_interval_dict[Interval(start_stop_list[i][0], start_stop_list[i][1])] = vals\n",
    "    #    i += 1\n",
    "    \n",
    "    #this contains the domains we need to split and each set of structures associated per domain.\n",
    "    \n",
    "    split_return_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in split_dictionary.items():\n",
    "        #only care about more than 1 structure.\n",
    "        if len(vals) > 1: \n",
    "            split_return_dict[keys] = vals\n",
    "    \n",
    "    cnt = 0\n",
    "    for keys, vals in split_return_dict.items():\n",
    "        cnt += len(vals)\n",
    "        \n",
    "    return split_return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3261,
   "id": "9ab6b008-7088-496f-874a-de405591c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_groups_pdb(path_start_stop_dict:dict, main_prot_seq:str):\n",
    "    \n",
    "    \n",
    "    #first lets merge the same intervalls.\n",
    "\n",
    "    merged_dict = merge_paths_within_interval(path_start_stop_dict)\n",
    "\n",
    "    #print(\"this is merged_dict\")\n",
    "    #print(merged_dict)\n",
    "\n",
    "    print(f\"length of dictionary before recursive_reduction: {len(merged_dict)}\")\n",
    "    if len(merged_dict) != 1:\n",
    "        \n",
    "        #print(f\"before recursive call: {merged_dict}\")\n",
    "\n",
    "        tolerance = 0.2*len(main_prot_seq)\n",
    "\n",
    "        #2 rounds of merging.\n",
    "        \n",
    "        result = merge_overlapping_intervals(merged_dict, tolerance)\n",
    "\n",
    "        #print(f\"this is result {result}\")\n",
    "\n",
    "        result2 = merge_overlapping_intervals(result, tolerance)\n",
    "        #print(result2)\n",
    "\n",
    "        # Create a dictionary to store merged entries\n",
    "        merged_entries = {}\n",
    "\n",
    "        # Iterate through the original data\n",
    "        for (start, stop), values in result2.items():\n",
    "            values_as_tuple = tuple(sorted(values))  # Convert the set of values to a hashable tuple\n",
    "            if values_as_tuple not in merged_entries or (stop - start) > (merged_entries[values_as_tuple][1] - merged_entries[values_as_tuple][0]):\n",
    "                merged_entries[values_as_tuple] = (start, stop)\n",
    "\n",
    "        # Create a new merged dictionary with the original keys\n",
    "        result3 = {(span[0], span[1]): set(values) for values, span in merged_entries.items()}\n",
    "\n",
    "        # Create a Counter dictionary to count the length of each value set\n",
    "        count_dict = {key: len(value) for key, value in result3.items()}\n",
    "\n",
    "        print(count_dict)\n",
    "\n",
    "        return result3\n",
    "            \n",
    "        #print(f\"after recursive call: {result}\")\n",
    "    else:\n",
    "        result3 = merged_dict\n",
    "        #for keys, vals in result.items():\n",
    "        #    print(len(vals))\n",
    "            \n",
    "        return result3\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3262,
   "id": "920e20a6-ba73-4637-8737-ced3e70ea1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_groups(datalst:list, val_list:list):\n",
    "    \n",
    "    storage_dict = defaultdict(list)\n",
    "    \n",
    "    for ranges, vals in zip(datalst, val_list):\n",
    "        storage_dict[Interval(ranges[0],ranges[1])] = vals\n",
    "    \n",
    "    \n",
    "    #works here still\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    intervals = [Interval(begin, end) for (begin, end) in datalst]\n",
    "    \n",
    "    \n",
    "    #print(\"this are the intervals we deal with:\")\n",
    "    #print(intervals)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #good so far. all 34 structures are in here for case NUDT4\n",
    "    \n",
    "    interval_dict = defaultdict()\n",
    "    \n",
    "    for interv in intervals:\n",
    "        interval_dict[interv] = interv\n",
    "        \n",
    "\n",
    "    print(f\"length of dictionary before recursive_reduction: {len(interval_dict)}\")\n",
    "    if len(interval_dict) != 1:\n",
    "        result = _recursive_union_reduction(interval_dict, 0)\n",
    "        print(f\"length of dictionary after recursive_reduction: {len(result)}\\n\")\n",
    "    else:\n",
    "        result = interval_dict\n",
    "        #for keys, vals in result.items():\n",
    "        #    print(len(vals))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    interval_collection_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in result.items():\n",
    "        #for each key we need to find all associated intervals within the key range e.g all subsets.\n",
    "        \n",
    "        for interval_member in intervals:\n",
    "            condition = interval_member.is_subset(keys)\n",
    "            condition_2 = interval_member.measure > 0.7*keys.measure\n",
    "            if condition and condition_2:\n",
    "                interval_collection_dict[keys].append(interval_member)\n",
    "    \n",
    "    #print(len(intervals))\n",
    "    \n",
    "    rcsb_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in interval_collection_dict.items():\n",
    "        \n",
    "        if type(vals) == list:\n",
    "            rcsb_tmp = []\n",
    "            for val in vals:\n",
    "                tmp = storage_dict[val]\n",
    "                #print(f\"this is current key: {keys}\")\n",
    "                #print(f\"this is current structures: {storage_dict[val]}\")\n",
    "                #print(tmp)\n",
    "                for rcsbs in tmp:\n",
    "                    rcsb_tmp.append(rcsbs)\n",
    "            \n",
    "        else:\n",
    "            rcsb_tmp = []\n",
    "            tmp = storage_dict[val]\n",
    "            for rcsbs in tmp:\n",
    "                rcsb_tmp.append(rcsbs)\n",
    "        \n",
    "            rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        \n",
    "        \n",
    "    #for keys, vals in rcsb_dict.items():\n",
    "        #print(keys)\n",
    "        #print(vals)\n",
    "        #print(\"\\n\")\n",
    "    \n",
    "    return rcsb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3263,
   "id": "9fbc11e2-9b48-4a9a-8b8e-1070d965d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_paths_within_interval(path_start_stop_dict):\n",
    "    merged_dict = {}  # Create a new dictionary to store merged paths\n",
    "    for (pdb_path,chain), (start, stop) in path_start_stop_dict.items():\n",
    "        interval = (start, stop)\n",
    "        if interval in merged_dict:\n",
    "            merged_dict[interval].append((pdb_path, chain))\n",
    "        else:\n",
    "            merged_dict[interval] = [(pdb_path, chain)]\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3264,
   "id": "52c5e9a0-efea-4587-ab71-81ad694dbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(path_interval_dict, tolerance):\n",
    "    \n",
    "    intervals = list(path_interval_dict.keys())\n",
    "    merged_intervals = merge_intervals(intervals, tolerance)\n",
    "\n",
    "    merged_dict = {}\n",
    "\n",
    "    #print(f\"this is path_interval_dict : {path_interval_dict}\")\n",
    "    for merged_interval in merged_intervals:\n",
    "        merged_paths = []\n",
    "            \n",
    "        for interval, path_chain_list in path_interval_dict.items():\n",
    "            if is_within_tolerance(merged_interval, interval, tolerance):\n",
    "                for path, chain in path_chain_list: # list of lists consisting of tuples a (path / chain)\n",
    "                    merged_paths.append((path, chain))\n",
    "        merged_dict[merged_interval] = merged_paths\n",
    "\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3265,
   "id": "a9c37a19-14bb-4d98-babd-36c0220b08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_round_interval_merge(path_interval_dict, tolerance):\n",
    "    # First round of merging\n",
    "    merged_intervals_1 = merge_intervals(list(path_interval_dict.keys()), tolerance)\n",
    "\n",
    "    # Associate paths with merged intervals\n",
    "    merged_dict_1 = {}\n",
    "    for interval in merged_intervals_1:\n",
    "        merged_dict_1[interval] = [path for intv, paths in path_interval_dict.items() if\n",
    "                                    intv[0] - interval[1] <= tolerance or interval[0] - intv[1] <= tolerance\n",
    "                                    for path in paths]\n",
    "\n",
    "    # Second round of merging\n",
    "    merged_intervals_2 = merge_intervals(merged_intervals_1, tolerance)\n",
    "    \n",
    "    # Associate paths with merged intervals\n",
    "    merged_dict_2 = {}\n",
    "    for interval in merged_intervals_2:\n",
    "        merged_dict_2[interval] = [path for intv, paths in merged_dict_1.items() if\n",
    "                                    intv[0] - interval[1] <= tolerance or interval[0] - intv[1] <= tolerance\n",
    "                                    for path in paths]\n",
    "\n",
    "    return merged_dict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a82a4f-925e-42fe-90e1-ad40bdf6a1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3266,
   "id": "be03a77a-48cd-41e9-9467-3aff994d0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_within_tolerance(interval1, interval2, tolerance):\n",
    "    # Helper function to check if two intervals are within tolerance\n",
    "    return interval1[1] - interval2[0] <= tolerance or interval2[1] - interval1[0] <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3267,
   "id": "c11f298b-ee3b-4b17-9f62-3e812b79cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_intersection(interval1, interval2):\n",
    "    start1, stop1 = interval1\n",
    "    start2, stop2 = interval2\n",
    "    return max(start1, start2) <= min(stop1, stop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3268,
   "id": "d7abd9f7-2adb-46cd-b970-259dddd45b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_intervals(intervals, tolerance):\n",
    "\n",
    "    intervals.sort()  # Sort intervals by start value\n",
    "    merged = []\n",
    "\n",
    "    current_start, current_end = intervals[0]\n",
    "\n",
    "    for interval in intervals[1:]:\n",
    "        start, end = interval\n",
    "\n",
    "        if start - current_end <= tolerance and end - current_start <= tolerance:\n",
    "            # Merge overlapping intervals\n",
    "            current_end = max(current_end, end)\n",
    "        else:\n",
    "            # Interval does not overlap within tolerance, add the current interval\n",
    "            merged.append((current_start, current_end))\n",
    "            current_start, current_end = start, end\n",
    "\n",
    "    # Add the last merged interval\n",
    "    merged.append((current_start, current_end))\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3269,
   "id": "e054e932-e139-4aae-aab5-08643a0a5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_tolerance(interval1, interval2, tolerance):\n",
    "    \n",
    "    # Calculate the differences in starts and stops for both intervals\n",
    "    start_diff = abs(interval1[0] - interval2[0])\n",
    "    stop_diff = abs(interval1[1] - interval2[1])\n",
    "\n",
    "    # Check if both differences are within the tolerance\n",
    "    return start_diff <= tolerance and stop_diff <= tolerance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3270,
   "id": "b977a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_union_reduction(dictionary_to_parse:dict, num_of_iterations:int):\n",
    "    \n",
    "    #print(\"this is dictionary to parse:\")\n",
    "    #print(dictionary_to_parse)\n",
    "    \n",
    "    if num_of_iterations < 20:\n",
    "        \n",
    "        #if everything merged together we dont need to continue\n",
    "        if len(dictionary_to_parse) == 1:\n",
    "            return dictionary_to_parse\n",
    "        \n",
    "        num_of_iterations += 1\n",
    "        val_1 = []\n",
    "        val_2 = []\n",
    "        \n",
    "        for keys, vals in dictionary_to_parse.items():\n",
    "            val_1.append(keys)\n",
    "            val_2.append(keys)\n",
    "        \n",
    "        union_dict = defaultdict(list)        \n",
    "        \n",
    "        for val in val_1:\n",
    "            union_new = False\n",
    "            for val_x in val_2:\n",
    "                if val != val_x:\n",
    "                    m_1 = val.measure\n",
    "                    m_2 = val_x.measure\n",
    "                    m_3 = val_x.intersect(val).measure\n",
    "                    \n",
    "                    #print(val, val_x)\n",
    "                    #print(m_1, m_2, m_3)\n",
    "                    condition_1 = m_3 <= m_1 and m_3 >= 0.8* m_1\n",
    "                    condition_2 = m_3 <= m_2 and m_3 >= 0.8* m_2\n",
    "                    #print(condition_1, condition_2)\n",
    "                    if condition_1 and condition_2:\n",
    "                        union_new = Union(val, val_x)\n",
    "                        val_tmp_1 = val\n",
    "                        val_tmp_2 = val_x\n",
    "            \n",
    "            if union_new:\n",
    "                union_dict[union_new].append(val_tmp_1)\n",
    "                union_dict[union_new].append(val_tmp_2)\n",
    "                \n",
    "            else:\n",
    "        \n",
    "                union_dict[val].append(val)\n",
    "\n",
    "        dictionary_to_parse = union_dict\n",
    "        \n",
    "        dictionary_to_parse = _recursive_union_reduction(dictionary_to_parse, num_of_iterations)\n",
    "    \n",
    "    \n",
    "    return dictionary_to_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936621c-5061-47cc-8bac-17eb6c7309bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split based on oligomers.\n",
    "\n",
    "Input:\n",
    "\n",
    "1. <span style=\"color:green\">interval_dirs:dict</span>\n",
    "2. <span style=\"color:green\">oligostates:dict</span> \n",
    "3. <span style=\"color:green\">path:str</span> \n",
    "4. <span style=\"color:green\">rangedir:dict</span>\n",
    "\n",
    "Output:\n",
    "makes and sets up the required directories for the different oligomeric states.\n",
    "\n",
    "Helper functions:\n",
    "\n",
    "+ <span style=\"color:blue\">get_seq_len</span>\n",
    "+ <span style=\"color:blue\">get_gene_name</span>\n",
    "+ <span style=\"color:blue\">temp_score</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3271,
   "id": "c29a8923-eabb-4cb3-9bee-f71ad12f1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oligo_dirs1(pdb_ranges:dict, path_merged, domain_dict:dict):\n",
    "    \n",
    "    oligodirdict = {\n",
    "        \"1\": 'monomer',\n",
    "        \"2\": 'dimer',\n",
    "        \"3\": 'trimer',\n",
    "        \"4\": 'tetramer',\n",
    "        \"5\": 'pentamer',\n",
    "        \"6\": 'hexamer',\n",
    "        \"7\": 'heptamer',\n",
    "        \"8\": 'oktamer',\n",
    "        \"9\": 'nonamer',\n",
    "        \"10\": 'decamer',\n",
    "        \"11\": 'undecamer',\n",
    "        \"12\": 'dodecamer',\n",
    "        \"13\": 'tridecamer',\n",
    "        \"14\": 'tetradecamer',\n",
    "        \"15\": 'pentadecamer',\n",
    "        \"16\": 'hexadecamer',\n",
    "        \"17\": 'heptadecamer',\n",
    "        \"18\": 'oktadecamer',\n",
    "        \"19\": 'nonadecamer',\n",
    "        \"20\": 'eicosamer'\n",
    "    }\n",
    "\n",
    "\n",
    "    print(f\"this is pdb_ranges {pdb_ranges}\")\n",
    "\n",
    "    print(f\"this is domain_dict: {domain_dict}\")\n",
    "    \"\"\"\n",
    "    this is domain_dict: {(8, 147): \n",
    "    {('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woi_A.pdb',\n",
    "    'A')...\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    oligostates = []\n",
    "    for (path, chain), (_, value) in pdb_ranges.items():\n",
    "\n",
    "        oligomer = oligodirdict[str(len(chain))]        #chain e.g A ... len is therefore 1... oligodirdirct[1] will return monomer\n",
    "        if oligomer not in oligostates:\n",
    "            oligostates.append(oligomer)\n",
    "          \n",
    "    print(f\"this is oligostates: {oligostates}\") # this works.!\n",
    "    \n",
    "    #continue here\n",
    "    def create_directory_structure():\n",
    "\n",
    "        start_stop_list = list(domain_dict.keys())\n",
    "        \n",
    "        for (start, end) in start_stop_list:\n",
    "            pos_name = f\"pos_{start}_{end}\"\n",
    "            for oligostate in oligostates:\n",
    "                try:\n",
    "                    os.makedirs(os.path.join(path_merged, oligostate, pos_name))\n",
    "                except FileExistsError:\n",
    "                    pass\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "\n",
    "    \n",
    "\n",
    "    #now after dealing with the directory structure.. .we can parse through the files.\n",
    "    def copy_files():\n",
    "\n",
    "        for (start, stop), nested_set in domain_dict.items():\n",
    "            for (path, chain) in nested_set:\n",
    "                \n",
    "                try:\n",
    "                    oligomer = oligodirdict[str(len(chain))]\n",
    "                    directory, filename = os.path.split(path)\n",
    "                    position = f\"pos_{start}_{stop}\"\n",
    "                    savepath = os.path.join(directory, oligomer, position ,filename)\n",
    "                    \n",
    "                    #print(f\"we move {path} to {savepath}\")\n",
    "                    shutil.move(path, savepath)\n",
    "                except Exception as error:\n",
    "                    print(f\"Error moving {pat} to {savepath}: {error}\")\n",
    "\n",
    "            \n",
    "    create_directory_structure()\n",
    "    copy_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3272,
   "id": "33e65007-e6f4-4510-9554-d979d0dcc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oligo_dirs(interval_dirs, oligostates, path):\n",
    "    \n",
    "    oligodirdict = {\n",
    "        \"1\": 'monomer',\n",
    "        \"2\": 'dimer',\n",
    "        \"3\": 'trimer',\n",
    "        \"4\": 'tetramer',\n",
    "        \"5\": 'pentamer',\n",
    "        \"6\": 'hexamer',\n",
    "        \"7\": 'heptamer',\n",
    "        \"8\": 'oktamer',\n",
    "        \"9\": 'nonamer',\n",
    "        \"10\": 'decamer',\n",
    "        \"11\": 'undecamer',\n",
    "        \"12\": 'dodecamer',\n",
    "        \"13\": 'tridecamer',\n",
    "        \"14\": 'tetradecamer',\n",
    "        \"15\": 'pentadecamer',\n",
    "        \"16\": 'hexadecamer',\n",
    "        \"17\": 'heptadecamer',\n",
    "        \"18\": 'oktadecamer',\n",
    "        \"19\": 'nonadecamer',\n",
    "        \"20\": 'eicosamer'\n",
    "    }\n",
    "\n",
    "    intervallist = list(interval_dirs.keys())\n",
    "    dirs_to_make = set(oligostates.values())\n",
    "    \n",
    "    def create_directory_structure():\n",
    "        for (start, end) in intervallist:\n",
    "            pos_name = f\"pos_{start}_{end}\"\n",
    "            for dirs in dirs_to_make:\n",
    "                try:\n",
    "                    name = oligodirdict.get(str(dirs), 'unknown')\n",
    "                    os.makedirs(os.path.join(path, name, pos_name))\n",
    "                except FileExistsError:\n",
    "                    pass\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "    \n",
    "    def copy_files():\n",
    "        for pdb, oligostate in oligostates.items():\n",
    "            try:\n",
    "                previous_path = os.path.join(path, pdb)\n",
    "                oligo_dir = oligodirdict.get(str(oligostate), 'unknown')\n",
    "\n",
    "                for (start, end) in intervallist:\n",
    "                    val_pdb_set = interval_dirs.get((start, end), set())\n",
    "\n",
    "                    for paths in val_pdb_set:\n",
    "                        print(f\"we check paths for val_pdb_set for the pdb : {pdb} and oligostate: {oligostate}\")\n",
    "\n",
    "                        #('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo8.pdb', 'A')\n",
    "                        #print(paths)\n",
    "                        old_path = paths[0]\n",
    "                        val_pdb = paths[0].split(\"/\")[-1]\n",
    "                        chain = paths[1]\n",
    "                        \n",
    "                        print(pdb, val_pdb)\n",
    "                        if pdb == val_pdb:\n",
    "                            dir_name_to_use = f\"pos_{start}_{end}\"\n",
    "\n",
    "                            savepath = os.path.join(path, oligo_dir, dir_name_to_use, f\"{val_pdb[0:4]}_{chain}.pdb\")\n",
    "                        \n",
    "                            try:\n",
    "                                print(f\"we move {old_path} to {savepath}\")\n",
    "                                shutil.copy(old_path, savepath)\n",
    "                            except Exception as error:\n",
    "                                print(f\"Error moving {old_path} to {savepath}: {error}\")\n",
    "\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "\n",
    "    def check_leftover_files():\n",
    "\n",
    "        pdb_all = [f for f in os.listdir(merged_path) if os.path.isfile(os.path.join(merged_path, f)) and f.endswith(\".pdb\") and f[-5] != \"A\"]\n",
    "\n",
    "        print(f\"this is pdbs left {pdbs}\")\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        try:\n",
    "            for pdb in pdbs:\n",
    "\n",
    "                if pdb[0:-4] > 1:\n",
    "                    print(f\"pdb is multichain {pdb}\")\n",
    "                    #means its multichain\n",
    "                    continue\n",
    "                    \n",
    "                pdb_path = os.path.join(merged_path, pdb)\n",
    "    \n",
    "                structure = parser.get_structure(\"noname\", pdb_path)\n",
    "    \n",
    "                structure_len = [x.get_id()[1] for x in structure.get_residues()]\n",
    "    \n",
    "                start_struc, stop_struc = structure_len[0], structure_len[-1]\n",
    "    \n",
    "                search_dir = os.path.join(path, \"monomer\")\n",
    "                \n",
    "                directories = [d for d in os.listdir(search_dir) if os.path.isdir(os.path.join(search_dir, d))]\n",
    "    \n",
    "                ranges = []\n",
    "                for intervals in directories:\n",
    "                    print(f\"this is intervals {intervals}\")\n",
    "                    start_pos, stop_pos = intervals.split(\"_\")[1], intervals.split(\"_\")[2]\n",
    "                    ranges.append((int(start_pos), int(stop_pos)))\n",
    "    \n",
    "                dist_ranges = []\n",
    "    \n",
    "                for (start, stop) in ranges:\n",
    "                    #check where we should put our files.\n",
    "                    dist = abs((start - start_struc) + (stop-stop_struc))\n",
    "                    dist_ranges.append((start, stop, dist))\n",
    "    \n",
    "                \n",
    "                sorted_dist_ranges = sorted(dist_ranges, key=lambda x: x[2])\n",
    "                print(\"this is sorted_dist_ranges\")\n",
    "                #[(8, 147, 0), (1, 203, 49), (143, 312, 300)]\n",
    "                move_dir = f\"pos_{sorted_dist_ranges[0][0]}_{sorted_dist_ranges[0][1]}\"\n",
    "    \n",
    "                target_dir = os.path.join(search_dir, move_dir)\n",
    "                try:\n",
    "                    print(f\"we move {pdb_path} to {target_dir}\")\n",
    "                    shutil.move(pdb_path , target_dir)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(\"it happened inside check_leftover_files()\")\n",
    "            print(e)\n",
    "                \n",
    "            \n",
    "    create_directory_structure()\n",
    "    copy_files()\n",
    "\n",
    "    #merged_path = os.path.join(path, 'merged_cleaned_files')\n",
    "    #check_leftover_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14034152-5987-4158-b5ee-0b76180f8c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "903eee79-3433-468c-b783-bbb32bc9d60a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Atomium part / Biological assemblies\n",
    "\n",
    "### Major Subfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e80b6-1abd-44ce-a442-3b88911ea61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3273,
   "id": "a16785c3-f108-4be1-8a46-791b0a739438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biological_assemblies_atomium_2(path:str, gene_name, main_iso_seq, main_protein_seq):\n",
    "    \n",
    "    relevant_files = []\n",
    "\n",
    "    pdbs = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f[-4:] == \".pdb\"]\n",
    "\n",
    "    full_pdb_paths = [os.path.join(path, f) for f in pdbs]\n",
    "    \n",
    "    oligostates = defaultdict(str)\n",
    "\n",
    "    \n",
    "    print(pdbs)\n",
    "    #HERE WE NEED TO CONTINUE TOMMOROW\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Define your processing function, partially applied with gene_name and main_protein_seq\n",
    "        process_func = partial(_process_file2, gene_name=gene_name, main_protein_seq=main_protein_seq)\n",
    "\n",
    "        results = executor.map(process_func, full_pdb_paths)\n",
    "\n",
    "        for result in results:\n",
    "            print(f\"this is result: {result}\")\n",
    "            oligostates.update(result)\n",
    "\n",
    "    print(\"This is oligostates:\")\n",
    "    \n",
    "    print(oligostates)\n",
    "\n",
    "    return oligostates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3274,
   "id": "85be6d95-0cd0-4c48-b5b2-901f944f9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biological_assemblies_atomium_1(pdb_path_dict:dict, gene_name, main_iso_seq, main_protein_seq):\n",
    "    \n",
    "    relevant_files = []\n",
    "\n",
    "    #list of tuples consisting of path, chains always.\n",
    "    for path_chains in list(pdb_path_dict.values()):\n",
    "        for path, chains in path_chains: #this is a set object stored in a list\n",
    "            relevant_files.append((path, chains)) #0 is path 1 is chain\n",
    "            \n",
    "    \n",
    "    print(relevant_files)\n",
    "\n",
    "    oligostates = defaultdict(str)\n",
    "\n",
    "\n",
    "    #HERE WE NEED TO CONTINUE TOMMOROW\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Define your processing function, partially applied with gene_name and main_protein_seq\n",
    "        process_func = partial(_process_file, gene_name=gene_name, main_protein_seq=main_protein_seq)\n",
    "\n",
    "        results = executor.map(process_func, relevant_files)\n",
    "\n",
    "        for result in results:\n",
    "            oligostates.update(result)\n",
    "\n",
    "    print(\"This is oligostates:\")\n",
    "    \n",
    "    print(oligostates)\n",
    "\n",
    "    return oligostates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3275,
   "id": "a6d387cc-eec0-4ace-8e8c-36238c3c9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_file2(path, gene_name, main_protein_seq):\n",
    "    #helper function to split between nmr and xray / cryoem\n",
    "    try:\n",
    "        \n",
    "        pdb_file_name = path.split(\"/\")[-1]\n",
    "\n",
    "        base_path = os.path.dirname(path)\n",
    "        \n",
    "        pdb1 = atomium.open(path)\n",
    "        \n",
    "        model_len = pdb1.models\n",
    "\n",
    "        #print(pdb_file_name)\n",
    "        #print(base_path)\n",
    "        \n",
    "        if len(model_len) > 5:\n",
    "\n",
    "            print(\"we go into _NMR_ensemble\")\n",
    "            return _NMR_ensemble(path=base_path, files=pdb_file_name, gene_name=gene_name)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            return {pdb_file_name: _non_NMR_structures(path=base_path, files=pdb_file_name,\n",
    "                                                    gene_name=gene_name, main_protein_seq=main_protein_seq)}\n",
    "    \n",
    "    except Exception as error:\n",
    "        print(\"process file2 did not work\")\n",
    "        print(error)\n",
    "        \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3276,
   "id": "4e2d2370-64b9-42ee-8eb3-26299fbd4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_file(path_chain, gene_name, main_protein_seq):\n",
    "    #helper function to split between nmr and xray / cryoem\n",
    "    try:\n",
    "\n",
    "        path, chain = path_chain  #split tuple in 2\n",
    "        \n",
    "        pdb_file_name = path.split(\"/\")[-1]\n",
    "\n",
    "        base_path = os.path.dirname(path)\n",
    "        \n",
    "        pdb1 = atomium.open(path)\n",
    "        \n",
    "        model_len = pdb1.models\n",
    "\n",
    "        if len(model_len) > 5:\n",
    "\n",
    "            print(\"we go into _NMR_ensemble\")\n",
    "            return _NMR_ensemble(path=base_path, files=pdb_file_name, gene_name=gene_name)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            return {pdb_file_name: _non_NMR_structures(path=base_path, files=pdb_file_name,\n",
    "                                                    gene_name=gene_name, main_protein_seq=main_protein_seq)}\n",
    "    \n",
    "    except Exception as error:\n",
    "        print(\"process file did not work\")\n",
    "        print(error)\n",
    "        \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3277,
   "id": "1a63c892-563e-4d09-9b6b-114c0c779106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for XRAY and CRYO-EM ensembles.\n",
    "def _non_NMR_structures(path:str,\n",
    "                        files:str,\n",
    "                        gene_name:str,\n",
    "                        main_protein_seq:str):\n",
    "    \n",
    "    \"\"\"This function takes in the the pdb file that is xray or cryoem and rechains each chain. \n",
    "    Additionally, we merge the new labelled chains into a merged_pdb file for further use.\"\"\"\n",
    "    \n",
    "    #open the pdb file\n",
    "    \n",
    "    #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95.pdb\n",
    "    pdb1 = atomium.open(f'{path}/{files}')\n",
    "    \n",
    "    assemblies = [pdb1.generate_assembly(n + 1) for n in range(len(pdb1.assemblies))]\n",
    "    \n",
    "    #we take the first one(this is the biological unit built from the asymmetric unit)\n",
    "    assembly = assemblies[0]\n",
    "    \n",
    "    #<Model (2 chains, 6 ligands)>\n",
    "    print(f'{path}/{files}')\n",
    "\n",
    "    \n",
    "    print(assembly)\n",
    "    #this works for our purpose.\n",
    "    \n",
    "    seq_chains = []\n",
    "    \n",
    "    accepted_chains = []\n",
    "\n",
    "    for x in assembly.chains():\n",
    "        \n",
    "        seq = x.sequence #the sequence of each chain\n",
    "        \n",
    "        seq_len = len(seq) #the len of each chain aka its number of residues\n",
    "        \n",
    "        chain_label = x.id #the chain identifier\n",
    "        \n",
    "        seq_chains.append((chain_label,seq_len)) \n",
    "\n",
    "    \n",
    "    \n",
    "    sorted_lens = sorted(seq_chains, key= lambda x: x[1], reverse=True) #reverse = true :largest first.\n",
    "\n",
    "    acc_range = []\n",
    "\n",
    "    #this is sorted lens: [('A', 199), ('A', 199)]\n",
    "    print(f\"this is sorted lens: {sorted_lens}\")\n",
    "    for chains, lens in sorted_lens:\n",
    "        #first is largest and always accepted\n",
    "        if len(accepted_chains) == 0 or lens > 0.8 * min(acc_range):\n",
    "            #then we continue to append only if its similar size to prevent small peptides from interfering.\n",
    "            accepted_chains.append(chains)\n",
    "            acc_range.append(lens)\n",
    "\n",
    "\n",
    "\n",
    "    #this is accepted chains: ['A', 'A'], and this is accepted ranges: [199, 199]\n",
    "    print(f\"this is accepted chains: {accepted_chains}, and this is accepted ranges: {acc_range}\")\n",
    "\n",
    "    \"\"\"\n",
    "    if the chain contains more than 70% of the number of residues that the main fasta seq has we continue.\n",
    "    this part might become tricky if we have only partial structures of full length stuff available. might be\n",
    "    adjusted later.\n",
    "    \"\"\"\n",
    "   \n",
    "    oligostate = len(accepted_chains)  #this excludes small peptides ect from being mistaken as oligomers.\n",
    "            \n",
    "            \n",
    "    path_list = []\n",
    "            \n",
    "    seen_chains = []\n",
    "    \n",
    "    for idx, chains in enumerate(assembly.chains()):\n",
    "\n",
    "        chain_label = chains.id\n",
    "\n",
    "        #this is chain_label: A\n",
    "        print(f\"this is chain_label: {chain_label}\")\n",
    "        \n",
    "        if chain_label in accepted_chains:  #if its accepted we save it.\n",
    "\n",
    "            #A ['A', 'A']\n",
    "            print(chain_label, accepted_chains)\n",
    "\n",
    "            \n",
    "            #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_A_0.pdb\n",
    "            print(f\"{path}/{files[0:4]}_{chain_label}_{idx}.pdb\")\n",
    "\n",
    "            \n",
    "            chains.save(f\"{path}/{files[0:4]}_{idx}.pdb\")\n",
    "        \n",
    "            path_to_pdb = f\"{path}/{files[0:4]}_{idx}.pdb\"\n",
    "\n",
    "            print(\"we append now path\")\n",
    "            \n",
    "            path_list.append(path_to_pdb)\n",
    "            \n",
    "            seen_chains.append(chain_label)\n",
    "\n",
    "\n",
    "        \n",
    "        path_list = sorted(path_list, key=lambda x: int(x[-5]))\n",
    "        \n",
    "        print(path_list)\n",
    "        \n",
    "        unique_chains = list(set(seen_chains))\n",
    "\n",
    "        print(unique_chains)\n",
    "        \n",
    "        #for paths in path_list:\n",
    "\n",
    "        _merge_pdb_chains(path_list, pdb_name=files[0:4],\n",
    "                      gene_name=gene_name,\n",
    "                      seen_chains=seen_chains, \n",
    "                      unique_chains=unique_chains)\n",
    "    \n",
    "    \n",
    "    #we return the oligostate of this file and merge it into dict as return value.\n",
    "    return oligostate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3278,
   "id": "40fbc11d-1fc3-4e05-8e4f-5919229de5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN\"\n",
    "#files = \"4h1w.pdb\"\n",
    "#gene_name = \"serca\"\n",
    "\n",
    "\n",
    "#main_prot_seq = get_gene_fasta(\"O14983\")\n",
    "#_non_NMR_structures(path=path,\n",
    "#                        files=files,\n",
    "#                        gene_name=gene_name,\n",
    "#                        main_protein_seq=main_prot_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3279,
   "id": "6d27d57b-2d6f-422e-892b-62deeed4760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for NMR ensembles.\n",
    "def _NMR_ensemble(path:str, files:str, gene_name:str):\n",
    "\n",
    "    \n",
    "    \"\"\"This function takes in the NMR ensemble and splits each state into a respective PDB file.\"\"\"\n",
    "    \n",
    "    \n",
    "    #open the pdb file\n",
    "    print(f\"we currently open with atomium: {path}/{files}\")\n",
    "    pdb1 = atomium.open(f'{path}/{files}')\n",
    "  # this should be only 4yrg e.g \n",
    "    assembly = pdb1.generate_assembly(1)\n",
    "    \n",
    "    \n",
    "    oligostates = defaultdict()\n",
    "    \n",
    "    for idx, model in enumerate(pdb1.models):\n",
    "                        \n",
    "        path_list = []\n",
    "        seen_chains = []\n",
    "        \n",
    "        #here we save the structure.\n",
    "        model.save(f\"{path}/{files[0:4]}_{idx+1}.pdb\")\n",
    "        \n",
    "        #we append path\n",
    "        path_list.append(f\"{path}/{files[0:4]}_{idx+1}.pdb\")\n",
    "        #we append seen chains.\n",
    "        for chains in assembly.chains():\n",
    "            \n",
    "            #here we grab the chain ID\n",
    "            chain_label = chains.id\n",
    "            fasta_seq = chains.sequence\n",
    "            \n",
    "            oligostates[f\"{files[0:4]}_{idx+1}\"] = len(assembly.chains())\n",
    "            \n",
    "            seen_chains.append(chain_label)\n",
    "        \n",
    "            unique_chains = list(set(seen_chains))\n",
    "            \n",
    "        #for paths in path_list:\n",
    "        # we treat it like its a normal monomer (since NMR is single chain most of the time)\n",
    "        _merge_pdb_chains(path_list, pdb_name=f\"{files[0:4]}_{idx+1}\", \n",
    "                          gene_name=gene_name,\n",
    "                          seen_chains=seen_chains,\n",
    "                          unique_chains=unique_chains)\n",
    "        \n",
    "        \n",
    "    return oligostates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3280,
   "id": "bd753b2c-3e2d-4753-b1d5-7e961e3cc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/test/laura_test\"\n",
    "#files = \"2ro8.pdb\"\n",
    "#gene_name = \"NAC3_HUMAN\"\n",
    "\n",
    "#oligostate = _NMR_ensemble(path=path, \n",
    "#             files=files,\n",
    "#             gene_name=gene_name)\n",
    "#\n",
    "#print(oligostate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89138c41-459d-448b-ba8c-3d10ccd099ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testblock for atomium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3281,
   "id": "af6f232e-b6ba-4f5a-a9f9-58c0c5623ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for get_biological_assemblies_atomium\n",
    "\n",
    "#gene_name = \"CALM1_HUMAN\"\n",
    "#path = \"/home/micnag/bioinformatics/test/atomium_CALM1_testset\"\n",
    "\n",
    "#main_iso_seq = None\n",
    "\n",
    "#print(get_biological_assemblies_atomium(gene_name=gene_name,\n",
    "#                                  path=path,\n",
    "#                                 main_iso_seq=main_iso_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3282,
   "id": "0e0ba674-459c-419f-9c85-a41aa3cac991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hetero_atoms_2(pdb_path:str):\n",
    "    \"\"\"This function will grab the correct chains and overwrite the pdbs with only correct chains.\"\"\"\n",
    "    \n",
    "    #used to get rid of hetero atoms and wrong chain.\n",
    "    #inherit from Select, pass additional arg to __init__: correct_chain id\n",
    "    class NonHetAndCorrectChainSelect(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "            \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[0] == \" \" else 0\n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = \"none\"\n",
    "    \n",
    "    \n",
    "    structure = parser.get_structure(prot_name, pdb_path)\n",
    "    \n",
    "    non_canonical_aas = defaultdict()\n",
    "\n",
    "    canonical_aas = {'VAL', 'ILE', 'LEU', 'GLU', 'GLN',\n",
    "                     'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR',\n",
    "                     'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA',\n",
    "                     'GLY', 'PRO', 'CYS'}\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                curr_res = residue.get_resname()\n",
    "                curr_pos = residue.get_id()[1]\n",
    "\n",
    "                if curr_res not in canonical_aas:\n",
    "                    print(curr_res)\n",
    "                    non_canonical_aas[curr_pos] = (curr_res, chain.get_id())\n",
    "\n",
    "    \n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    \n",
    "    save_path = pdb_path\n",
    "    \n",
    "    io.save(save_path, NonHetAndCorrectChainSelect())\n",
    "    \n",
    "    return (non_canonical_aas, pdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3283,
   "id": "a92c9994-9068-4c9c-a0cd-1634c97899b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hetero_atoms_1(pdb_file:str, path:str):\n",
    "    \"\"\"This function will grab the correct chains and overwrite the pdbs with only correct chains.\"\"\"\n",
    "    \n",
    "    #used to get rid of hetero atoms and wrong chain.\n",
    "    #inherit from Select, pass additional arg to __init__: correct_chain id\n",
    "    class NonHetAndCorrectChainSelect(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "            \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[0] == \" \" else 0\n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    prot_name = pdb_file[0:4]\n",
    "    full_path = f\"{path}/{pdb_file}\"\n",
    "    \n",
    "    structure = parser.get_structure(prot_name, full_path)\n",
    "    \n",
    "    non_canonical_aas = defaultdict()\n",
    "\n",
    "    canonical_aas = {'VAL', 'ILE', 'LEU', 'GLU', 'GLN',\n",
    "                     'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR',\n",
    "                     'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA',\n",
    "                     'GLY', 'PRO', 'CYS'}\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                curr_res = residue.get_resname()\n",
    "                curr_pos = residue.get_id()[1]\n",
    "\n",
    "                if curr_res not in canonical_aas:\n",
    "                    print(curr_res)\n",
    "                    non_canonical_aas[curr_pos] = (curr_res, chain.get_id())\n",
    "    \n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    \n",
    "    save_path = full_path\n",
    "    \n",
    "    io.save(save_path, NonHetAndCorrectChainSelect())\n",
    "    \n",
    "    return non_canonical_aas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3284,
   "id": "88c78b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hetero_atoms(pdb_file:str, path:str):\n",
    "    \"\"\"This function will grab the correct chains and overwrite the pdbs with only correct chains.\"\"\"\n",
    "    \n",
    "    #used to get rid of hetero atoms and wrong chain.\n",
    "    #inherit from Select, pass additional arg to __init__: correct_chain id\n",
    "    class NonHet_and_correct_chain_Select(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "            \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[0] == \" \" else 0\n",
    "        \n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"{pdb_file[0:4]}\"\n",
    "    \n",
    "    fullpath = f\"{path}/{pdb_file}\"\n",
    "    \n",
    "    \n",
    "    print(\"we are here\")\n",
    "    print(fullpath)\n",
    "    structure = parser.get_structure(prot_name, fullpath)\n",
    "    \n",
    "    \n",
    "    non_canoncial_aas = defaultdict()\n",
    "    \n",
    "    \n",
    "    canonical_aas = ['VAL', 'ILE', 'LEU', 'GLU', 'GLN' ,\n",
    "                    'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR', \n",
    "                    'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA', \n",
    "                    'GLY', 'PRO', 'CYS']\n",
    "    \n",
    "    \n",
    "    for models in structure:\n",
    "        for chains in models:\n",
    "            \n",
    "            chain = chains.get_id()\n",
    "            \n",
    "            for residues in chains:\n",
    "                curr_res = residues.get_resname()\n",
    "                curr_pos = residues.get_id()[1] #the position\n",
    "                \n",
    "                \n",
    "                if curr_res not in canonical_aas:\n",
    "                    print(curr_res)\n",
    "                    non_canoncial_aas[curr_pos] = (curr_res, chain)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #io object to save structure object to file\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    \n",
    "    savepath = fullpath\n",
    "    \n",
    "    #we save all structures to the monomeric category.\n",
    "    io.save(savepath, NonHet_and_correct_chain_Select())\n",
    "    \n",
    "    return non_canoncial_aas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98605370-720a-4135-8f03-7b25d7d66285",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Function**:\n",
    "<b><span style=\"color:orange\">_merge_pdb_chains:</span></b>\n",
    "\n",
    "*Input:*\n",
    "1. <span style=\"color:green\">path_list:str</span> \n",
    "2. <span style=\"color:green\">pdb_name:str</span>\n",
    "3. <span style=\"color:green\">gene_name:str</span>\n",
    "4. <span style=\"color:green\">seen_chains:str</span>\n",
    "5. <span style=\"color:green\">unique_chains:str</span>\n",
    "\n",
    "<span style=\"color:green\"></span>\n",
    "\n",
    "calls internally following helper functions:\n",
    "\n",
    "+ <span style=\"color:blue\">_homomer_check</span>\n",
    "+ <span style=\"color:blue\">_pure_oligomer_rechaining</span>\n",
    "+ <span style=\"color:blue\">_mixed_oligomer_rechaining</span>\n",
    "+ <span style=\"color:blue\">_monomeric_rechaining</span>\n",
    "\n",
    "\n",
    "requires additionally:\n",
    "\n",
    "+ <span style=\"color:red\">pdb_merge.py</span> \n",
    "+ <span style=\"color:red\">pdb_tidy.py</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3285,
   "id": "a131aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_pdb_chains(path_list:str, pdb_name:str, gene_name:str,\n",
    "                     seen_chains:str, unique_chains:str):\n",
    "    \n",
    "    #needs to be adjusted how to handle hetero-mers.\n",
    "    outfile = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "    #for testing purpose we put this here : return\n",
    "    \n",
    "    letterdict = {1 : \"A\", 2 : \"B\", 3 : \"C\", 4 : \"D\",\n",
    "                 5 : \"E\", 6 : \"F\",7 : \"G\", 8 : \"H\",\n",
    "                 9 : \"I\", 10 : \"J\",11 : \"K\", 12 : \"L\",\n",
    "                 13 : \"M\", 14 : \"N\",15 : \"O\", 16 : \"P\",\n",
    "                 17 : \"Q\", 18 : \"R\",19 : \"S\", 20 : \"T\",\n",
    "                 21 : \"U\", 22 : \"V\",23 : \"W\", 24 : \"X\",\n",
    "                 25 : \"Y\", 26 : \"Z\" }\n",
    "    \n",
    "    #that means we have duplicates.\n",
    "    \n",
    "    #oligomeric heterodimer case first!\n",
    "    \n",
    "    #default setting\n",
    "    \n",
    "    oligohomomer = False\n",
    "    #print(f\"this is pathlist : {path_list}, and this is pdb name: {pdb_name}\")\n",
    "    \n",
    "    if len(seen_chains) != len(unique_chains):\n",
    "        #means its an oligomer.\n",
    "        oligomer = True\n",
    "        #now check if homo or hetero oligomer.\n",
    "        oligohomomer = _homomer_check(seen_chains, unique_chains)\n",
    "    else:\n",
    "        oligomer = False\n",
    "        \n",
    "    #means its an oligomer and a homo oligomer.\n",
    "    if oligohomomer:\n",
    "        \n",
    "        new_chains = _pure_oligomer_rechaining(seen_chains=seen_chains,\n",
    "                                   unique_chains=unique_chains,\n",
    "                                   path_list=path_list,\n",
    "                                   letterdict=letterdict)\n",
    "    \n",
    "    #means its an oligomer but a hetero oligomer.\n",
    "    if oligohomomer == False and oligomer:\n",
    "        \n",
    "        new_chains = _mixed_oligomer_rechaining(seen_chains=seen_chains,\n",
    "                                   unique_chains=unique_chains,\n",
    "                                   path_list=path_list,\n",
    "                                   letterdict=letterdict)\n",
    "    \n",
    "    #means its just either a single chain of the protein or some co crystallized binding complex protein as other chains.\n",
    "    if oligomer == False:\n",
    "\n",
    "        new_chains = _monomeric_rechaining(seen_chains=seen_chains,\n",
    "                                   unique_chains=unique_chains,\n",
    "                                   path_list=path_list,\n",
    "                                   letterdict=letterdict)\n",
    "     \n",
    "    #to be continued\n",
    "    \n",
    "    \n",
    "    #print(\"this is path list and chains\")\n",
    "    #print(path_list)\n",
    "    #print(seen_chains)\n",
    "    #print(new_chains)\n",
    "    \n",
    "    pdb_paths = \"\"\n",
    "    \n",
    "    for entries in path_list:\n",
    "        pdb_paths += f\"{entries} \"\n",
    "\n",
    "    #print(f\"this is pdb_paths:{pdb_paths}\")\n",
    "\n",
    "    #FIRST SAVE ALL SINGLE CHAIN CONSTRUCTS AS WELL IN MERGED cleaned files.\n",
    "    if len(path_list) > 1:\n",
    "        try:\n",
    "            #print(f\"pdb_paths inside save single chains: {pdb_paths}\")\n",
    "            save_single_chains(pdb_paths)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    #print(\"this is pdb_paths\")\n",
    "    \n",
    "    #print(pdb_paths)\n",
    "    \n",
    "    #print(f\"we start with bash_merge and save it at: {outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\")\n",
    "    \n",
    "    bash_merge = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_merge.py {pdb_paths}\"\n",
    "        \n",
    "    bash_curl_cmd_rdy = bash_merge.split()\n",
    "    \n",
    "    bash_tidy_cmd = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_tidy.py {outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\"\n",
    "    \n",
    "    bash_tidy_cmd_rdy = bash_tidy_cmd.split()\n",
    "    \n",
    "    #print(\"this is the merge command\")\n",
    "    \n",
    "    #print(bash_curl_cmd_rdy)\n",
    "    \n",
    "    with open(f\"{outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\", \"w\") as fh_out:\n",
    "        result_pdbs = run(bash_curl_cmd_rdy, stdout=fh_out, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "        \n",
    "    \n",
    "    #print(\"this is the tidy command thats breaking the merge.\")\n",
    "    #print(bash_tidy_cmd_rdy)\n",
    "    \n",
    "    with open(f\"{outfile}/merged_cleaned_files/{pdb_name}_{''.join(new_chains)}.pdb\", \"w\") as fh_out2:\n",
    "        results_tidy = run(bash_tidy_cmd_rdy, stdout=fh_out2, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "    \n",
    "    #we remove tmp intermediate files.\n",
    "    os.remove(f\"{outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\")\n",
    "    \n",
    "    with open(f\"{outfile}/reports/chain_relabeling_protocol.csv\", \"a\") as fh_chain:\n",
    "        outtext = f'{pdb_name},{\" \".join(sorted(seen_chains, reverse=False))},{\" \".join(new_chains)}'\n",
    "        fh_chain.write(outtext)\n",
    "        fh_chain.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3286,
   "id": "4e165749-f760-4722-9385-f9ebcc27b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_chains(pdb_paths:str):\n",
    "\n",
    "    #input is as string of type: \"path/to/a path/to/b path/to/c\"\n",
    "    \n",
    "    save_paths = pdb_paths.split(\" \")\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "        \n",
    "    prot_name = \"default\"\n",
    "\n",
    "    # Open the correct PDB and rechain it.\n",
    "    for paths in save_paths:\n",
    "        \n",
    "        structure = parser.get_structure(prot_name, paths)\n",
    "\n",
    "        name = paths.split(\"/\")[-1][0:6]\n",
    "        \n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        base_path = os.path.dirname(paths)\n",
    "        clean_loc = \"merged_cleaned_files\"\n",
    "\n",
    "        save_location = os.path.join(base_path, clean_loc,f\"{name}.pdb\")\n",
    "        print(f\"this is save location: {save_location}\")\n",
    "        #we save all structures to the monomeric category.\n",
    "        io.save(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3287,
   "id": "39e91712-7dd5-4b19-bf1e-966fdd6a2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _homomer_check(seen_chains:list, unique_chains:list):\n",
    "    \n",
    "    # ABCDEF    set: ABCDEF  vs unique:ABCDEF\n",
    "    \n",
    "    # AA   set : A vs unique A\n",
    "    \n",
    "    #if they both are the same set but the seen chain len is larger e.g [A A] vs [A] but as set\n",
    "    #they will both be [A] and [A] -> Means its a homodimer.\n",
    "    \n",
    "    \n",
    "    \n",
    "    #this means they differ and now it could be either a mixed or a homo oligomer.\n",
    "    #if len(unique chains == 1) this means the assymetric unit consists of 1 chain. and the\n",
    "    #biological unit is simply generated through means of symmetry operations.\n",
    "    \n",
    "    if len(seen_chains) != len(unique_chains) and len(unique_chains) == 1:\n",
    "        return True\n",
    "    \n",
    "    #this case covers the mixed oligomers. nevertheless they are oligomers!\n",
    "    if len(seen_chains) != len(unique_chains) and len(unique_chains) != 1:\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3288,
   "id": "c285ea6f-9ddf-4ed6-82ee-dbbca143297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pure_oligomer_rechaining(seen_chains:list,\n",
    "                                 unique_chains:list,\n",
    "                                 path_list:list,\n",
    "                                 letterdict:dict):    \n",
    "    \n",
    "    \n",
    "    seen_chains = sorted(seen_chains, reverse=False)\n",
    "\n",
    "    merged_chain_paths = zip(seen_chains, path_list)\n",
    "    \n",
    "    for idx, (chains, path_to_pdb) in enumerate(merged_chain_paths):        \n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "                \n",
    "        prot_name = f\"default\"\n",
    "                \n",
    "        #open the correct pdb and rechain it.\n",
    "        structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "        \n",
    "        new_chain = letterdict[idx+1]\n",
    "        \n",
    "        new_chain_seq = []\n",
    "        \n",
    "        for models in structure_template:\n",
    "            for chains in models:\n",
    "                \n",
    "                chains.id = \"_\"\n",
    "                \n",
    "                chains.id = new_chain\n",
    "                \n",
    "                new_chain_seq.append(new_chain)\n",
    "                \n",
    "                io = PDBIO()\n",
    "                \n",
    "                io.set_structure(structure_template)\n",
    "                \n",
    "                io.save(path_to_pdb)\n",
    "                \n",
    "    return new_chain_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3289,
   "id": "7a46a1ea-e77b-4ad9-a09a-a5ff94a0eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mixed_oligomer_rechaining(seen_chains:list,\n",
    "                               unique_chains:list,\n",
    "                               path_list:list,\n",
    "                               letterdict:dict):\n",
    "    \n",
    "    seen_chains = sorted(seen_chains, reverse=False)\n",
    "\n",
    "    merged_chain_paths = zip(seen_chains, path_list)\n",
    "    \n",
    "    chain_seq_len = len(seen_chains) #e.g 6\n",
    "    \n",
    "    shift = len(unique_chains) # e.g 3\n",
    "    \n",
    "    blocksize = int(chain_seq_len / shift) # e.g 2\n",
    "    \n",
    "    block_count = int(chain_seq_len/blocksize)\n",
    "    \n",
    "    # A A B B C C becomes A D B E C F\n",
    "    \n",
    "    # B B C C becomes A C B D \n",
    "    \n",
    "    #i = 1\n",
    "    \n",
    "    # A D \n",
    "    \n",
    "    # block 1 2 3 for A A B B C C \n",
    "    \n",
    "    # 0 2 1 3\n",
    "    \n",
    "    j = 0\n",
    "    new_chain_seq = []\n",
    "    for blocks in range(1, block_count+1):\n",
    "        # each block has 2 members:\n",
    "        #first iteration: 1 \n",
    "        # second iteration: 2\n",
    "        # third iteration: 3\n",
    "        for i in range(0, blocksize):\n",
    "            # first iteration A D\n",
    "            # second iteration B E\n",
    "            # third iteration C F\n",
    "            new_chain = letterdict[blocks+i*shift]\n",
    "            new_chain_seq.append(new_chain)\n",
    "            path_to_pdb = path_list[j]\n",
    "            #continue in path\n",
    "            j += 1\n",
    "            \n",
    "            #lets renumber the structure now.\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            \n",
    "            prot_name = f\"default\"\n",
    "            \n",
    "            #open the correct pdb and rechain it.\n",
    "            structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "            \n",
    "            for models in structure_template:\n",
    "                for chains in models:\n",
    "                    chains.id = \"_\"\n",
    "                    chains.id = new_chain\n",
    "            \n",
    "            io = PDBIO()\n",
    "            \n",
    "            io.set_structure(structure_template)\n",
    "            io.save(path_to_pdb)\n",
    "\n",
    "    return new_chain_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3290,
   "id": "4b1705f5-58e9-4c49-b227-02b316829b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _monomeric_rechaining(seen_chains:list,\n",
    "                          unique_chains:list,\n",
    "                          path_list:list,\n",
    "                          letterdict:dict):\n",
    "\n",
    "    print(\"we start monomeric rechain inside\")\n",
    "    new_chain_seq = []\n",
    "\n",
    "    for idx, (original_chains, path_to_pdb) in enumerate(zip(seen_chains, path_list)):\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        \n",
    "        prot_name = \"default\"\n",
    "\n",
    "        # Open the correct PDB and rechain it.\n",
    "        structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "\n",
    "        # Get the new chain ID\n",
    "        new_chain = letterdict[idx + 1]\n",
    "\n",
    "        for model in structure_template:\n",
    "            for original_chain in model:\n",
    "                print(\"Original chain ID:\", original_chain.id)\n",
    "                original_chain.id = \"_\"\n",
    "                original_chain.id = new_chain\n",
    "                print(\"New chain ID:\", original_chain.id)\n",
    "\n",
    "        # Save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure_template)\n",
    "\n",
    "        print(\"we save now:\")\n",
    "        io.save(path_to_pdb)\n",
    "\n",
    "        new_chain_seq.append(new_chain)\n",
    "\n",
    "    return new_chain_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8246c-5000-49f1-bff0-c2a266cd02da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# prepares references used for USAlign later downstream.\n",
    "\n",
    "### Major Subfunction.\n",
    "\n",
    "*Input:*\n",
    "\n",
    "1. <span style=\"color:green\">main_gene_name:str</span>\n",
    "2. <span style=\"color:green\">templates:list</span>\n",
    "3. <span style=\"color:green\">seq_sim:list</span>     \n",
    "4. <span style=\"color:green\">query_start:list</span>                      \n",
    "5. <span style=\"color:green\">query_end:list</span>\n",
    "6. <span style=\"color:green\">temp_start:list</span>                      \n",
    "7. <span style=\"color:green\">temp_end:list</span>                    \n",
    "8. <span style=\"color:green\">path:str</span>\n",
    "9. <span style=\"color:green\">oligodict:list</span>\n",
    "\n",
    "*Output*:\n",
    "\n",
    "<b><span style=\"color:brown\">template_dict:dir</span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3291,
   "id": "d3b8d387-7b3c-457a-ad55-0667936505e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files\"\n",
    "#main_protein_sequence = \"MMKFKPNQTRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFEQNQDRKHRTYVYVLTVTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLKLGCSPANGNSTVPSLPDNNALFVTAAQTSGLPSSVR\"\n",
    "#prepare_templates1(main_directory=merged_path, main_protein_sequence=main_protein_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3292,
   "id": "f0d66cab-faf6-460a-af1d-b0771a18bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdb(pdb_path, main_protein_sequence):\n",
    "    result_dict = _get_template_score(pdb_path, main_protein_sequence)\n",
    "    #max of values where each value is tuple (path, score) , we return both path and score.\n",
    "    return max(result_dict.items(), key=lambda item: item[1][1], default=(None, (None, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3293,
   "id": "53ab3c42-2cdf-4e54-9156-9626380d5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_processing(directories, main_protein_sequence):\n",
    "    template_score = 0  # initialize low\n",
    "    current_hit = None\n",
    "    \n",
    "    pdbs = [f for f in os.listdir(directories) if f.endswith(\".pdb\")]\n",
    "\n",
    "    pdb_files = [os.path.join(directories, pdb) for pdb in pdbs]\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Use the partial function to pass additional arguments to process_pdb\n",
    "        process_pdb_partial = partial(process_pdb, main_protein_sequence=main_protein_sequence)\n",
    "\n",
    "        # Process PDB files in parallel and find the highest template score\n",
    "        for pdb_path, (path, score) in executor.map(process_pdb_partial, pdb_files):\n",
    "            if score > template_score:\n",
    "                template_score = score\n",
    "                current_hit = path\n",
    "\n",
    "    return template_score, current_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3294,
   "id": "11f82f09-b775-4a81-9656-94eacc74b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_templates1(main_directory:str, main_protein_sequence:str):\n",
    "    \n",
    "    \"\"\"Make function out of making templates and reference structures.\"\"\"\n",
    "\n",
    "    directory_pdb_count = defaultdict()\n",
    "\n",
    "    # Use os.walk() to traverse the directory tree\n",
    "    for root, directories, _ in os.walk(main_directory):\n",
    "        # Check if the current root is exactly two levels deeper than the main directory\n",
    "        main_dir_components = len(os.path.normpath(main_directory).split(os.path.sep))\n",
    "        \n",
    "        current_dir_components = len(os.path.normpath(root).split(os.path.sep))\n",
    "        \n",
    "        if current_dir_components == main_dir_components + 2:\n",
    "            # Count .pdb files in the current directory\n",
    "            pdb_files = [file for file in os.listdir(root) if file.endswith(\".pdb\")]\n",
    "            \n",
    "            # Add the directory and count to the dictionary\n",
    "            directory_pdb_count[root] = len(pdb_files)\n",
    "    \n",
    "    #now lets parse through those dirs that contain more than 4 pdbs.\n",
    "    #accept those dirs that contain more than 4 pdbs.\n",
    "    accepted_dirs = []\n",
    "    \n",
    "    for directory, pdb_count in directory_pdb_count.items():\n",
    "        if pdb_count > 4:\n",
    "            #print(f\"this is a full dir! {directory} with pdb_count: {pdb_count}\")\n",
    "            accepted_dirs.append(directory)\n",
    "\n",
    "    #now we need to select a suitable template for all directories that contain pdbs\n",
    "    #here we store our hits.\n",
    "\n",
    "    winners = []\n",
    "    \n",
    "    for directories in accepted_dirs:\n",
    "        \n",
    "        template_score, current_hit = parallel_processing(directories, main_protein_sequence)\n",
    "\n",
    "        print(f\"Directory: {directories}, Best Hit: {current_hit}, Template Score: {template_score}\")\n",
    "        winners.append((directories, current_hit, template_score))\n",
    "        \n",
    "    return winners\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3295,
   "id": "1a8962c3-aca1-4aed-a691-7def0090bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb_path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/tetramer/pos_8_147/3i7u_ABCD.pdb\"\n",
    "\n",
    "#main_prot_sequence = \"MMKFKPNQTRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFEQNQDRKHRTYVYVLTVTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLKLGCSPANGNSTVPSLPDNNALFVTAAQTSGLPSSVR\"\n",
    "#main_prot_sequence = \"MKKEFSAGGVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKALKLKEKFKLMKKEFSAGGVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKALKLKEKFKLMKKEFSAGGVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKALKLKEKFKLMKKEFSAGGVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKALKLKEKFKL\"\n",
    "#_get_template_score(pdb_path, main_prot_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3296,
   "id": "0983ba43-c640-43ce-876e-e8b3a58e3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_template_score(pdb_path, main_protein_sequence):\n",
    "\n",
    "\n",
    "    path_score = []\n",
    "    \n",
    "    pdb_parser = Bio.PDB.PDBParser(QUIET=True)\n",
    "\n",
    "    structure = pdb_parser.get_structure(\"sample\", pdb_path)\n",
    "        \n",
    "    # Initialize an empty sequence string\n",
    "\n",
    "    chain_dict = defaultdict()\n",
    "\n",
    "    # Iterate through the structure and extract the sequence\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            query_sequence = \"\"\n",
    "            chain_id = chain.get_id()\n",
    "            \n",
    "            for residue in chain:\n",
    "                # Check if the residue is an amino acid (protein)\n",
    "                if residue.get_id()[0] == ' ':\n",
    "                    # Append the residue's three-letter code to the sequence\n",
    "                    query_sequence += residue.get_resname()\n",
    "\n",
    "            chain_dict[chain_id] = query_sequence\n",
    "\n",
    "    #now do alignment vs main_protein_sequence.\n",
    "    try:\n",
    "\n",
    "        resultdict = defaultdict()\n",
    "        \n",
    "        for chains, seq in chain_dict.items():\n",
    "            aligner = Align.PairwiseAligner()\n",
    "            alignments = aligner.align(main_protein_sequence, query_sequence)\n",
    "            #print(main_protein_sequence, query_sequence)\n",
    "        \n",
    "            start_time = time.time()\n",
    "            time_out = 5\n",
    "            scores = []\n",
    "\n",
    "            for a in alignments:\n",
    "                if time.time() - start_time > time_out:\n",
    "                    #print(f\"Timeout occurred for {pdb}\")\n",
    "                    break\n",
    "                scores.append(a.score)\n",
    "\n",
    "            score = max(scores)\n",
    "            \n",
    "            resultdict[chains] = (pdb_path, score)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return resultdict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3297,
   "id": "3373d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_references(main_gene_name:str,\n",
    "                      templates:list,\n",
    "                      seq_sim:list,\n",
    "                      query_start:list,\n",
    "                      query_end:list,\n",
    "                      temp_start:list,\n",
    "                      temp_end:list,\n",
    "                      path:str, oligodict:list,\n",
    "                      custom_template=False):\n",
    "    \n",
    "    \"\"\"Make function out of making templates and reference structures.\"\"\"\n",
    "    #setup\n",
    "    \n",
    "    print(\"we start prepare templates!!\")\n",
    "    \n",
    "    #this retrieves the main len of the protein we are interested in\n",
    "    main_target_seq_len = get_seq_len(path=f\"{path}\", rcsb_id=\"None\", template=True)\n",
    "    #181 for NUD4B\n",
    "    \n",
    "    #gives back the length of each template.\n",
    "    temp_lengths = [f[0] - f[1] for f in zip(temp_end, temp_start)]\n",
    "    \n",
    "    homology_list = sorted(list(zip(templates,seq_sim, temp_lengths)), key= lambda x: x[1], reverse=True)\n",
    "    \n",
    "    homology_dict = defaultdict(tuple)\n",
    "    \n",
    "    #homology dict will be used later to fetch corresponding templates for each oligomer and each domain.\n",
    "    for entries in homology_list:\n",
    "        homology_dict[entries[0][0:4]] = (entries[1], entries[2])\n",
    "    \n",
    "    \n",
    "    acceptable_dirnames = [\"monomer\",\n",
    "                           \"dimer\",\n",
    "                           \"trimer\",\n",
    "                           \"tetramer\",\n",
    "                           \"pentamer\",\n",
    "                           \"hexamer\",\n",
    "                           \"heptamer\",\n",
    "                           \"oktamer\",\n",
    "                           \"nonamer\",\n",
    "                           \"decamer\",\n",
    "                           \"undecamer\",\n",
    "                           \"dodecamer\",\n",
    "                           \"tridecamer\",\n",
    "                           \"tetradecamer\",\n",
    "                           \"pentadecamer\",\n",
    "                           \"hexadecamer\",\n",
    "                           \"heptadecamer\",\n",
    "                           \"oktadecamer\",\n",
    "                           \"nonadecamer\",\n",
    "                           \"eicosamer\"\n",
    "    ]\n",
    "    \n",
    "    intervaldirs = [ f for f in os.scandir(path) if f.is_dir() and f.name in acceptable_dirnames]\n",
    "    \n",
    "    intervalpathdict = defaultdict(list)\n",
    "    \n",
    "    for subentities in intervaldirs:\n",
    "        intervals = [ f for f in os.scandir(subentities.path) if f.is_dir()]\n",
    "        for interv in intervals:\n",
    "            onlyfiles = [f for f in os.listdir(interv.path) if os.path.isfile(os.path.join(interv.path, f))]\n",
    "            if len(onlyfiles) != 0:\n",
    "                intervalpathdict[interv.path] = onlyfiles\n",
    "                \n",
    "    template_dict = defaultdict(str)\n",
    "    \n",
    "    #print(\"this is intervalpathdict\")\n",
    "    #print(intervalpathdict)\n",
    "    \n",
    "    \n",
    "    for keys, vals in intervalpathdict.items():\n",
    "        \n",
    "        \n",
    "        fin_dir_list = keys.split(\"/\")\n",
    "        \n",
    "        #save position dir\n",
    "        position_dir = fin_dir_list[-1]\n",
    "        #and oligostate in order to write report later on.\n",
    "        oligostate_dir = fin_dir_list[-2]\n",
    "        \n",
    "        ref_list = []\n",
    "        #here we store human hits.\n",
    "        priority_list = []\n",
    "        #if we retrieve not as many hits, we take what we get\n",
    "        \n",
    "        if len(vals) < 100:\n",
    "                \n",
    "            max_end = len(vals)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            max_end = 100\n",
    "        \n",
    "        \n",
    "        print(f\"pos dir: {position_dir}, oligostate: {oligostate_dir}, val: {vals}\")\n",
    "        \n",
    "        exact_hits = []\n",
    "    \n",
    "        #group 2: other species same gene.  e.g KRS1_DROSOPHILA instead of KRS1_HUMAN\n",
    "        other_species_same_gene = []\n",
    "    \n",
    "        #group 3: other HUMAN but not exact gene. KRS2_HUMAN instead of KRS1\n",
    "        other_human_similar_gene = []\n",
    "    \n",
    "        #we dont find anything from the above.\n",
    "        suboptimal_templates = []\n",
    "        \n",
    "        for val in vals:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                prot_check = val[0:4] #pdb code without chain id\n",
    "            \n",
    "                prot_name = get_gene_name(prot_check)\n",
    "            \n",
    "                seq, lengths = homology_dict[prot_check][0], homology_dict[prot_check][1] \n",
    "            \n",
    "                #print(seq, lengths)\n",
    "                \n",
    "                #this means its human and our main prot is always human. but this should be extended.\n",
    "                #sometimes its better to have e.g MOUSE protein instead another less related human protein!\n",
    "                \n",
    "                \n",
    "                \n",
    "                #case 1\n",
    "                if prot_name == main_gene_name:\n",
    "                    temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                                                \n",
    "                    exact_hits.append((val, prot_name, temp_score_val))\n",
    "                    continue\n",
    "                \n",
    "                #case 2\n",
    "                if prot_name[:-6] == main_gene_name[:-6]:\n",
    "                    temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                                                \n",
    "                    other_species_same_gene.append((val, prot_name, temp_score_val))\n",
    "                    continue\n",
    "                \n",
    "                #case 3\n",
    "                if prot_name[-5:] == \"HUMAN\":\n",
    "                    temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                                                \n",
    "                    other_human_similar_gene.append((val, prot_name, temp_score_val))\n",
    "                    continue\n",
    "                \n",
    "                #if its none of the above.\n",
    "                temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                \n",
    "                suboptimal_templates.append((val, prot_name, temp_score_val))\n",
    "                \n",
    "            except Exception as error: \n",
    "                print(error)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #case 0 : custom_template = True which means we want a search based on a specific pdb template.\n",
    "        \n",
    "        pdb_shorts = [x[0:4] for x in vals]  #just interested in 4 letter pdb codes.\n",
    "        \n",
    "        if custom_template in pdb_shorts:\n",
    "            #we check if submitted struc is available at the current oligomer level.\n",
    "            try:\n",
    "                \n",
    "                prot_check = custom_template[0:4] #only pdb code without chain id\n",
    "                prot_name = get_gene_name(custom_template)\n",
    "                \n",
    "                #if we have it in the dict its fine. (it should be there if we did a proper seq search beforehand)\n",
    "                seq, lengths = homology_dict[prot_check][0], homology_dict[prot_check][1] \n",
    "                \n",
    "                #we calculate temp score \n",
    "                temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                        seq_len=lengths,\n",
    "                                        main_seq_len=main_target_seq_len)\n",
    "                \n",
    "                \n",
    "                #we count it as a perfect hit.\n",
    "                exact_hits.append((f\"{custom_template[0:4]}.pdb\", prot_name, 0))\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(exact_hits) != 0:\n",
    "            \n",
    "            exact_sorted = sorted(exact_hits, key= lambda x: x[2])\n",
    "            print(\"this is exact sorted\")\n",
    "            print(exact_sorted)\n",
    "            \n",
    "            with open(f\"{path}/mutational_mapping/{oligostate_dir}_{position_dir}.csv\", \"w\") as fh_out:\n",
    "                #we take it from here\n",
    "                #we only store the first potential structure for each range and oligomeric state!\n",
    "                previous_path = f\"{path}/{oligostate_dir}/{position_dir}/{exact_hits[0][0]}\"\n",
    "                #and store it here.\n",
    "                savepath = f\"{path}/mutational_mapping/{oligostate_dir}_{position_dir}_{exact_hits[0][0]}\"\n",
    "                shutil.copy(previous_path, savepath)\n",
    "                fh_out.write(exact_hits[0][0])\n",
    "                fh_out.write(\",\")\n",
    "                fh_out.write(exact_hits[0][1])\n",
    "                fh_out.write(\"\\n\")\n",
    "        \n",
    "        #print(exact_hits)\n",
    "        #print(other_species_same_gene)\n",
    "        #print(other_human_similar_gene)\n",
    "        #print(suboptimal_templates)\n",
    "        \n",
    "        \n",
    "        pos_oligomer = f'{oligostate_dir}/{position_dir}'\n",
    "        template_dict[pos_oligomer] = [exact_hits, other_species_same_gene, other_human_similar_gene,\n",
    "                                      suboptimal_templates]\n",
    "\n",
    "    #for keys, vals in template_dict.items():\n",
    "    #    print(keys)\n",
    "    #    print(vals)\n",
    "    #    print(\"\\n\")\n",
    "    \n",
    "    print(\"template dict is\")\n",
    "    print(template_dict)\n",
    "    \n",
    "    return template_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3298,
   "id": "88b1b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_len(path:str, rcsb_id:str, template=False):\n",
    "    \n",
    "    seq_len = 0\n",
    "\n",
    "    target = f\"{path}/reports/{rcsb_id}.fasta\"\n",
    "    \n",
    "    if template:\n",
    "        target = f\"{path}/reports/main_isoform_fasta\"\n",
    "    \n",
    "    try:\n",
    "        with open(target, \"r\") as fasta_file:\n",
    "            for lines in fasta_file:\n",
    "                if lines[0] != \">\":\n",
    "                    lines = lines.replace(\"\\n\",\"\") #get rid of newline char\n",
    "                    seq_len += len(lines) #add length of line to seq    \n",
    "    except:\n",
    "        #it did not work.\n",
    "        return 0 \n",
    "    \n",
    "    return seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3299,
   "id": "37e6dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_name(uniprot_id:str):\n",
    "    \n",
    "    fields = \"id\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.text\n",
    "    resp = resp.split(\"\\n\")\n",
    "    return resp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3300,
   "id": "52f4011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_score(seq_sim:float, seq_len:float, main_seq_len:float):\n",
    "    \n",
    "    \n",
    "    #seq_similarity * sequence len\n",
    "    tmp = (seq_sim*seq_len)\n",
    "    \n",
    "    #to prevent sqrt 0 for ideal matches.\n",
    "    eps = 1e-10\n",
    "    \n",
    "    #minimum squared diff between tmp and main_seq len is best\n",
    "    temp_score = math.sqrt((tmp - main_seq_len+eps)**2)\n",
    "    \n",
    "    return temp_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486406ae-0c7d-4a2e-84e3-e982276e5b03",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SEQ shift compared to UNIPROT + renumber whole structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd4008-c178-4edf-8a5e-1b8de6da3b89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### function *get_shifts*:\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">get_shift.sh</span>\n",
    "\n",
    "Output:\n",
    "\n",
    "<span style=\"color:brown\">shift_dict</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3301,
   "id": "e7939b47-38ac-44bd-b241-93005f7b3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shift(pdb, link_path):\n",
    "    shift_dict = defaultdict()\n",
    "\n",
    "    searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "    resp = get_url(searchp)\n",
    "    resp = resp.json()\n",
    "\n",
    "    for pdb_id, pdb_info in resp.items():\n",
    "        for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "            for mapping in uniprot_info['mappings']:\n",
    "                chain_id = mapping['chain_id']\n",
    "                unp_start = mapping['unp_start']\n",
    "                unp_end = mapping['unp_end']\n",
    "                \n",
    "                author_start = mapping['start']['author_residue_number']\n",
    "                author_end = mapping['end']['author_residue_number']\n",
    "\n",
    "                if author_start is None:\n",
    "                    author_start = unp_start\n",
    "                if author_end is None:\n",
    "                    author_end = unp_end\n",
    "\n",
    "                shift_start = unp_start - author_start\n",
    "                shift_end = unp_end - author_end\n",
    "\n",
    "                shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "\n",
    "    return shift_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3302,
   "id": "f99b2994-5aa2-4481-ad03-1aedcb066e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_shift_calculation(pdbfolder):\n",
    "    onlyfiles = [f for f in os.listdir(pdbfolder) if os.path.isfile(os.path.join(pdbfolder, f))]\n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "\n",
    "    link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "    shift_dict = defaultdict()\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Using partial to create a function with fixed parameters (link_path)\n",
    "        calculate_shift_partial = partial(calculate_shift, link_path=link_path)\n",
    "        # Map the calculation function to each pdb in parallel\n",
    "        results = executor.map(calculate_shift_partial, pdbs)\n",
    "\n",
    "        # Combine the results\n",
    "        for result in results:\n",
    "            for keys, vals in result.items():\n",
    "                shift_dict[keys] = vals\n",
    "\n",
    "    # Print or write the results\n",
    "    #for keys, vals in shift_dict.items():\n",
    "    #    print(keys, vals)\n",
    "\n",
    "    with open(f\"{pdbfolder}/shiftdict.txt\", \"w\") as shift_out:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            shift_out.write(keys)\n",
    "            shift_out.write(str(vals))\n",
    "            shift_out.write(\"\\n\")\n",
    "\n",
    "    return shift_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c08286-dd8b-4a5e-a849-7cb71d21eef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3303,
   "id": "8435b465-5ba3-4124-a516-642a0e1745eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_shifts(pdbfolder:str):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(pdbfolder) if os.path.isfile(os.path.join(pdbfolder, f))]\n",
    "    \n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "\n",
    "\n",
    "    link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "    #we will fetch the results through request.\n",
    "    shift_dict = defaultdict()\n",
    "    \n",
    "    for pdb in pdbs:\n",
    "        searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "        #print(searchp)\n",
    "        resp = get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        \n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                for mapping in uniprot_info['mappings']:\n",
    "                    chain_id = mapping['chain_id']\n",
    "                    unp_start = mapping['unp_start']\n",
    "                    unp_end = mapping['unp_end']\n",
    "                    #print(chain_id)\n",
    "                    #print(unp_start)\n",
    "                    #print(unp_end)\n",
    "                    author_start = mapping['start']['author_residue_number']\n",
    "                    author_end = mapping['end']['author_residue_number']\n",
    "\n",
    "                    #print(author_start)\n",
    "                    #print(author_end)\n",
    "\n",
    "                    if author_start == None:\n",
    "                        author_start = unp_start\n",
    "                    if author_end == None:\n",
    "                        author_end = unp_end\n",
    "\n",
    "                    shift_start = unp_start - author_start\n",
    "                    shift_end = unp_end - author_end\n",
    "\n",
    "                    \n",
    "                    shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "\n",
    "    for keys, vals in shift_dict.items():\n",
    "        print(keys, vals)\n",
    "\n",
    "    with open(f\"{pdbfolder}/shiftdict.txt\", \"w\") as shift_out:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            shift_out.write(keys)\n",
    "            shift_out.write(str(vals))\n",
    "            shift_out.write(\"\\n\")\n",
    "    \n",
    "    return shift_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3304,
   "id": "37ddf263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shifts(pdbfolder:str):\n",
    "    \n",
    "    savefolder = f\"{pdbfolder}/shifts\"\n",
    "    basepathlst = pdbfolder.split(\"/\")\n",
    "    #print(basepathlst)\n",
    "    \n",
    "    basepath = \"/\".join(basepathlst[1:5])\n",
    "    #print(basepath)\n",
    "    \n",
    "    os.chdir(f\"/{basepath}\")\n",
    "    \n",
    "    bash_correct_shift = f\"./get_shift.sh {pdbfolder} {savefolder}\"\n",
    "    \n",
    "    #print(bash_correct_shift)\n",
    "            \n",
    "    bash_command = bash_correct_shift.split()\n",
    "    \n",
    "    result = run(bash_command, stdout=PIPE, stderr=PIPE,\n",
    "                universal_newlines=True)\n",
    "\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "    #print(result.stdout)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        shift_dict = defaultdict()\n",
    "        \n",
    "        with open(f\"{pdbfolder}/shift_dict.txt\", \"r\") as shiftdic_fh:\n",
    "            for lines in shiftdic_fh:\n",
    "                key, val = lines.replace(\"\\n\",\"\").split(\":\")\n",
    "                shift_dict[key] = val\n",
    "                \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #write a report file as well\n",
    "    with open(f\"{pdbfolder}/reports/shift_reports.txt\", \"w\") as shiftout:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            shiftout.write(keys)\n",
    "            shiftout.write(\"\\t\")\n",
    "            shiftout.write(vals)\n",
    "            shiftout.write(\"\\n\")\n",
    "        \n",
    "    return shift_dict   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7650a-376e-4141-8724-3f96ad14ca38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### function *renumber_whole_structures*:\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">pdb_shiftres_by_chain.py</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3305,
   "id": "cc9fff5f-23a8-4313-8715-7df862b267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_structure(files, shift_dict, path):\n",
    "    \n",
    "    for keys, vals in shift_dict.items():\n",
    "        \n",
    "        if files == keys[0:4] and vals != str(0):\n",
    "            \n",
    "            chain = keys[-1]\n",
    "            shift = int(vals)\n",
    "\n",
    "            filepath = f\"{path}/{files}.pdb\"\n",
    "\n",
    "            # Should we really shift by shift + 1??? or just shift?\n",
    "            bash_cmd = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "\n",
    "            bash_cmd_rdy = bash_cmd.split()\n",
    "            \n",
    "            with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "            # Now replace the original one with the temp file.\n",
    "            os.replace(f\"{filepath}_tmp\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3306,
   "id": "b56ce4e4-cbfe-4c58-a18a-785248e70340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_renumbering(shift_dict, path):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    relevant_files = [f[0:4] for f in onlyfiles if f[-3:] == \"pdb\" or f[-3:] == \"cif\"]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # Using partial to create a function with fixed parameters (shift_dict, path)\n",
    "        renumber_structure_partial = partial(renumber_structure, shift_dict=shift_dict, path=path)\n",
    "        # Map the renumbering function to each relevant file in parallel\n",
    "        executor.map(renumber_structure_partial, relevant_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3307,
   "id": "dc1d4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_whole_structures(shift_dict:dict, path:str):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    relevant_files = [f[0:4] for f in onlyfiles if f[-3:] == \"pdb\" or f[-3:] == \"cif\"]\n",
    "    \n",
    "    #print(shift_dict)\n",
    "    for files in relevant_files:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            #print(files, keys[0:4])\n",
    "            if files == keys[0:4] and vals != str(0):\n",
    "                \n",
    "                chain = keys[-1]\n",
    "                shift = int(vals)\n",
    "                \n",
    "                filepath = f\"{path}/{files}.pdb\"\n",
    "                \n",
    "                #should we really shift by shift + 1??? or just shift?\n",
    "                \n",
    "                bash_cmd = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "                \n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "                \n",
    "                with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                    result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                         universal_newlines=True)\n",
    "                \n",
    "                #now replace the original one with the temp file.\n",
    "                os.replace(f\"{filepath}_tmp\", filepath)\n",
    "                            \n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3308,
   "id": "93807ef0-63f0-497d-9789-be855dc652e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/\"\n",
    "\n",
    "#shift_dict = get_shifts(pdbfolder=path)\n",
    "\n",
    "#renumber_repaired_single_chain(shift_dict=shift_dict, path = path, pdb_name=\"7nnj_B_0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c803330-9a48-43f1-a9c5-ced836ff7a07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# USALIGN + REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6aa3a5-a83b-414d-8790-6a0be4de0e11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### USAlign \n",
    "\n",
    "Input:\n",
    "\n",
    "1. <span style=\"color:green\">templates:dict</span>\n",
    "2. <span style=\"color:green\">gene_name:str</span>\n",
    "3. <span style=\"color:green\">path:str</span>\n",
    "4. <span style=\"color:green\"> **optional** report=True</span>\n",
    "\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">./USalign (.exe, C++)</span>\n",
    "\n",
    "\n",
    "helper_functions:\n",
    "\n",
    "+ <span style=\"color:blue\">_get_tm_scores_and_rmsd</span>\n",
    "+ <span style=\"color:blue\">us_report</span>\n",
    "+ <span style=\"color:blue\">multiple_seq_alignment_pre_pca_processing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3309,
   "id": "05532757-89f4-4557-b627-75d3f957ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_usalign(template, path_struc_1):\n",
    "    \n",
    "    bash_tm_and_rmsd_calc = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_struc_1} {template} -outfmt 2\"\n",
    "    bash_command = bash_tm_and_rmsd_calc.split()\n",
    "    \n",
    "    try:\n",
    "        result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        tm_2, rmsd = _get_tm_scores_and_rmsd(result.stdout)\n",
    "        \n",
    "        if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "            return path_struc_1, tm_2, rmsd\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3310,
   "id": "0167e190-0910-4855-aaaa-e0352b6f268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_usalign1_report(result_lst:list, outpath:str, outname:str):\n",
    "\n",
    "    #format of result_lst:  ((path_struc_1, template, tm_2, rmsd, oligomer))\n",
    "    def extract_base_path(full_path):\n",
    "        return os.path.basename(full_path) #we only care about the file name\n",
    "    \n",
    "    result_df = pd.DataFrame(result_lst, columns= [\"path_to_pdb\", \"template_used\", \"tm_2_score\", \"rmsd\", \"oligomeric_state\"])\n",
    "\n",
    "    result_df[\"path_to_pdb\"] = result_df[\"path_to_pdb\"].apply(extract_base_path)\n",
    "    result_df[\"template_used\"] = result_df[\"template_used\"].apply(extract_base_path)\n",
    "\n",
    "    output_path = os.path.join(outpath, outname)\n",
    "    result_df.to_csv(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3311,
   "id": "2142df58-ba7f-4894-a9b1-f2cef1733459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_usalign_oligomer(template, path_struc_1):\n",
    "\n",
    "    # -outfmt 2 : tsv outfile, \n",
    "    # -mol prot : only consider protein\n",
    "    # -ter 0:align all chains from all models (recommended for aligning biological assemblies, i.e. biounits)\n",
    "    # -mm 1: alignment of two multi-chain oligomeric structures\n",
    "\n",
    "    bash_tm_and_rmsd_calc = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_struc_1} {template} -outfmt 2 -mol prot -ter 0 -mm 1\"\n",
    "    bash_command = bash_tm_and_rmsd_calc.split()\n",
    "    \n",
    "    try:\n",
    "        result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        tm_2, rmsd = _get_tm_scores_and_rmsd(result.stdout)\n",
    "        \n",
    "        if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "            return path_struc_1, tm_2, rmsd\n",
    "        else:\n",
    "            print(f\"we remove {path_struc_1} because of tm: {tm_2} and rmsd: {rmsd}\")\n",
    "            os.remove(path_struc_1)\n",
    "            return path_struc_1, tm_2, rmsd\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3312,
   "id": "80890d6e-f042-4bd1-82bd-37bcb881c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_usalign_monomer(template, path_struc_1):\n",
    "    \n",
    "    bash_tm_and_rmsd_calc = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_struc_1} {template} -outfmt 2 -mol prot\"\n",
    "    bash_command = bash_tm_and_rmsd_calc.split()\n",
    "    \n",
    "    try:\n",
    "        result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        tm_2, rmsd = _get_tm_scores_and_rmsd(result.stdout)\n",
    "        \n",
    "        if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "            return path_struc_1, tm_2, rmsd\n",
    "        else:\n",
    "            print(f\"we remove {path_struc_1} because of tm: {tm_2} and rmsd: {rmsd}\")\n",
    "            os.remove(path_struc_1)\n",
    "            return path_struc_1, tm_2, rmsd\n",
    "            \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3313,
   "id": "fa95f4df-92a9-46ee-8522-6d822e299acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def USAlign1(template_list, main_prot_name , outpath_report:str):\n",
    "\n",
    "    #input template list\n",
    "    #((directories, current_hit, template_score))\n",
    "\n",
    "    #lets start USAlign for each directorie and pdb file that we have in there.\n",
    "    #directories are already /.../monomer/pos_x_y... so we can directly fetch all pdbs inside.\n",
    "    \n",
    "    try:\n",
    "        for directories, pdb_hit_path, template_score in template_list:\n",
    "\n",
    "            #collect all results here:\n",
    "\n",
    "            result_lst_usalign = []\n",
    "            \n",
    "            #this we will use as template\n",
    "            template = pdb_hit_path\n",
    "            \n",
    "            #list of pdbs.\n",
    "            pdbs = [os.path.join(directories, f) for f in os.listdir(directories) if os.path.isfile(os.path.join(directories, f))]\n",
    "            pos = os.path.basename(directories) #this will give the position\n",
    "            \n",
    "            oligomer =  os.path.basename(os.path.dirname(os.path.dirname(directories))) #this will give the oligomeric state.\n",
    "\n",
    "            out_report_name = f\"{oligomer}_{pos}_report.csv\"\n",
    "            \n",
    "            #now we go and use USAlign for each file against the template.\n",
    "            print(template)\n",
    "            print(pdbs)\n",
    "\n",
    "            if oligomer == \"monomer\":\n",
    "                with ProcessPoolExecutor() as executor:\n",
    "                    # Use executor.map to parallelize USAlign calls\n",
    "                    futures = [executor.submit(run_usalign_monomer, template, rcsb_file)  #we pass template full path and for the others also full path.\n",
    "                        for rcsb_file in pdbs\n",
    "                        if rcsb_file != template] #we exclude the template vs template comparison.\n",
    "        \n",
    "                    for future in as_completed(futures):\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            path_struc_1, tm_2, rmsd = result\n",
    "                            print(path_struc_1, tm_2, rmsd)\n",
    "                            result_lst_usalign.append((path_struc_1, template, tm_2, rmsd, oligomer))\n",
    "                        \n",
    "            else:\n",
    "\n",
    "                #else its an oligomer and we need to deal diffently with it.\n",
    "                with ProcessPoolExecutor() as executor:\n",
    "                    # Use executor.map to parallelize USAlign calls\n",
    "                    futures = [executor.submit(run_usalign_oligomer, template, rcsb_file)  #we pass template full path and for the others also full path.\n",
    "                        for rcsb_file in pdbs\n",
    "                        if rcsb_file != template] #we exclude the template vs template comparison.\n",
    "        \n",
    "                    for future in as_completed(futures):\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            path_struc_1, tm_2, rmsd = result\n",
    "                            print(path_struc_1, tm_2, rmsd)\n",
    "                            result_lst_usalign.append((path_struc_1, template, tm_2, rmsd, oligomer))\n",
    "\n",
    "            #if done we write a report file.\n",
    "            write_usalign1_report(result_lst=result_lst_usalign, outpath=outpath_report,\n",
    "                                 outname=out_report_name)\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"issue in USALIGN 1\")\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3314,
   "id": "3069431e-c48d-4ac8-9544-c5b70d9ccfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def USAlign(templates, gene_name, path, report=True):\n",
    "    \n",
    "    report_path = path\n",
    "    result_list = []\n",
    "    overall_result_list = []\n",
    "    template_dict = defaultdict()\n",
    "    min_seq_len_dict = defaultdict()\n",
    "\n",
    "    for keys, vals in templates.items():\n",
    "        found = False\n",
    "        for lists in vals:\n",
    "            for x in lists:\n",
    "                template_dict[keys] = x\n",
    "                found = True\n",
    "                break\n",
    "            if found:\n",
    "                break\n",
    "    \n",
    "    for keys, vals in template_dict.items():\n",
    "        pdb_id = vals[0]\n",
    "        template_score = vals[2]\n",
    "        basepath = keys.split(\"/\")\n",
    "        dir_path_for_usalign = f\"{path}/{basepath[0]}/{basepath[1]}\"\n",
    "\n",
    "        template = f\"{dir_path_for_usalign}/{pdb_id}\"\n",
    "\n",
    "        print(f\"this is dir_path_for_usalign: {dir_path_for_usalign} and this is dir_path for template: {template}\")\n",
    "        \n",
    "        pdb_files = [f for f in os.listdir(dir_path_for_usalign) if os.path.isfile(os.path.join(dir_path_for_usalign, f))]\n",
    "\n",
    "        if len(pdb_files) < 2:\n",
    "            continue\n",
    "\n",
    "        # Using ProcessPoolExecutor for parallelization\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Use executor.map to parallelize USAlign calls\n",
    "            futures = [\n",
    "                executor.submit(run_usalign, template, f\"{dir_path_for_usalign}/{rcsb_file}\")\n",
    "                for rcsb_file in pdb_files\n",
    "                if rcsb_file != pdb_id\n",
    "            ]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    path_struc_1, tm_2, rmsd = result\n",
    "                    us_report(path_struc_1, pdb_id, tm_2, rmsd, path, template_score=template_score, oligomer=basepath[0], pos=basepath[1])\n",
    "\n",
    "        \n",
    "        min_seq_len = multiple_seq_alignment_pre_pca_processing_version_2(path_to_pdbs=dir_path_for_usalign, template=pdb_id)\n",
    "        min_seq_len_dict[keys] = min_seq_len\n",
    "\n",
    "    print(\"this is min seq len dict\")\n",
    "    print(min_seq_len_dict)\n",
    "    return min_seq_len_dict, template_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d5294-f14f-4bfa-bb6d-8848f4d05802",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Us_report:\n",
    "\n",
    "simple helper function that writes out a report:\n",
    "\n",
    "sep=\"\\t\"\n",
    "\n",
    "Scheme:\n",
    "\n",
    "Query, Template, TM_2, RMSD, Template_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3315,
   "id": "b145f8e3-dbd3-4d06-8848-d675fe589ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_report(rcsb_file:str, pdb_id:str, tm_2:str, rmsd:str, path:str,\n",
    "             oligomer:str, pos:str, template_score:str):\n",
    "    \n",
    "    \n",
    "    path_to_assembly = f\"{path}/reports/{oligomer}_{pos}.csv\"\n",
    "    \n",
    "    if os.path.isfile(path_to_assembly) == False:\n",
    "        with open(path_to_assembly, \"w\") as fh_start:\n",
    "            fh_start.write(\"Query\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"Template\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"TM_2\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"RMSD\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"Template_score\")\n",
    "            fh_start.write(\"\\n\")\n",
    "    \n",
    "    with open(path_to_assembly, \"a\") as fh_out:\n",
    "        fh_out.write(str(rcsb_file))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(pdb_id))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(tm_2))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(rmsd))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(template_score))\n",
    "        fh_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2cf9c-4626-4078-8fe7-325b4e9e2066",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _get_tm_scores_and_rmsd:\n",
    "\n",
    "simple helper function that filters USAlign output and returns TM + RMSD vals as tuple:\n",
    "\n",
    "(TM_2, RMSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3316,
   "id": "b69486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tm_scores_and_rmsd(results:str):\n",
    "    \"\"\" helper function to retrieve tm scores and rmsd\n",
    "        We are only interested in TM_2 and RMSD.\n",
    "        IF RMSD is HIGH and TM_2 HIGH that means we have a conformer.\n",
    "        IF RMSD is LOW and TM_2 HIGH that means we have the same structure in the same conformer\n",
    "        IF RMSD is HIGH and TM_2 LOW that means the structures are not related.\"\"\"\n",
    "    \n",
    "    \"\"\"['#PDBchain1', 'PDBchain2', 'TM1', 'TM2', 'RMSD', 'ID1', 'ID2', 'IDali', 'L1', 'L2', 'Lali\\n/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/2duk_A.pdb:A', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/5ltu_A.pdb:A', \n",
    "    '0.8833', '0.9565', '0.94', '0.920', '1.000', '1.000', '138', '127', '127\\n']\"\"\"\n",
    "    \n",
    "    res_list = results.split(\"\\t\")\n",
    "    \n",
    "    #this one is from the mobile protein\n",
    "    tm_1 = res_list[12]\n",
    "    #this one belongs to the target protein (the one we superimpose the mobile protein onto)\n",
    "    tm_2 = res_list[13]\n",
    "    \n",
    "    #rmsd used to judge cutoff for trashing structures.\n",
    "    rmsd = res_list[14]\n",
    "    \n",
    "    #we return tm_2 and rmsd\n",
    "    return ((tm_2, rmsd))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6008e-3f99-4948-a4eb-c06d7b635939",
   "metadata": {
    "tags": []
   },
   "source": [
    "### multiple_seq_alignment_pre_pca_processing\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">./USalign (.exe, C++)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaa682-a3d9-4c9b-ad29-bf75480ffe55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3317,
   "id": "0ad396e5-2c7c-4ccf-ad8e-08422d15d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testpath = \"/home/micnag/bioinformatics/test/usaligntester\"\n",
    "\n",
    "#gap_dict = read_msa_file_version_1(path=testpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3318,
   "id": "9ba19ff8-eb32-4535-9955-facc6276b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/FAK1_HUMAN/monomer/pos_410_689\"\n",
    "#template_struc = \"2v7a.pdb\"\n",
    "#print(_remove_deviating_length_strucs(path_to_pdbs=path, template_struc=template_struc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271c63b-9871-4d54-9132-34ff9383bfc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA PROCESSING PIPELINE 2 (full length all chains 1 vs 1 alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3319,
   "id": "815dee0c-57a5-40ad-94b0-feabb44bb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_seq_alignment_pre_pca_processing_version_1(path_to_pdbs:str,\n",
    "                                                  template:str):\n",
    "    \n",
    "    \"\"\"-mm 1 -ter 0  for oligomers and all chains should be aligned.\"\"\"\n",
    "    \n",
    "    \n",
    "    min_seq_len = _remove_deviating_length_strucs(path_to_pdbs=path_to_pdbs, template_struc=template)\n",
    "    \n",
    "    \n",
    "    only_pdbs = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    \n",
    "    with open(f\"{path_to_pdbs}/chain_list.txt\", \"w\") as chain_out:\n",
    "        for pdbs in only_pdbs:\n",
    "            #only for pdb files.\n",
    "            if pdbs[-4:] == \".pdb\":\n",
    "                chain_out.write(pdbs)\n",
    "                chain_out.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_pdbs}/MSA_dir\")\n",
    "    \n",
    "    except:\n",
    "        print(\"already there\")\n",
    "    \n",
    "    \n",
    "    USalign_loc = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/\"\n",
    "    for pdbs in only_pdbs:\n",
    "        if pdbs != template:\n",
    "            \n",
    "            bash_cmd = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_to_pdbs}/{pdbs} {path_to_pdbs}/{template} -mm 1 -ter 0\"\n",
    "            \n",
    "            try:\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "                with open(f\"{path_to_pdbs}/MSA_dir/{template[0:4]}_{pdbs[0:4]}.txt\", \"w\") as msa_out:\n",
    "                    result = run(bash_cmd_rdy, stdout=msa_out, stderr=PIPE, \n",
    "                        universal_newlines=True)\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "    return min_seq_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3320,
   "id": "035865ee-0a45-4ae7-ab71-816e7d7affbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_seq_alignment_pre_pca_processing_version_3(path_to_pdbs:str,\n",
    "                                                  template:str):\n",
    "    \n",
    "    \"\"\"seq based alignment because we already did a structure based alignment.\"\"\"\n",
    "    \n",
    "    \n",
    "    #input files are already preselected based on structural alignment.. now we trimm them for further downstream pca.\n",
    "    \n",
    "    #to get min seq len\n",
    "    min_seq_len = _remove_deviating_length_strucs(path_to_pdbs=path_to_pdbs, template_struc=template)\n",
    "    \n",
    "    \n",
    "    only_pdbs = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    only_pdbs = [f for f in only_pdbs if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    with open(f\"{path_to_pdbs}/chain_list.txt\", \"w\") as chain_out:\n",
    "        for pdbs in only_pdbs:\n",
    "            #only for pdb files.\n",
    "            if pdbs[-4:] == \".pdb\":\n",
    "                chain_out.write(pdbs)\n",
    "                chain_out.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_pdbs}/MSA_dir\")\n",
    "     \n",
    "    except:\n",
    "        print(\"already there\")\n",
    "    \n",
    "    \n",
    "    USalign_loc = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/\"\n",
    "    for pdbs in only_pdbs:\n",
    "        if pdbs != template:\n",
    "            \n",
    "            print(pdbs, template)\n",
    "            bash_cmd = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_to_pdbs}/{pdbs} {path_to_pdbs}/{template} -byresi 5\"\n",
    "            \n",
    "            try:\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "                with open(f\"{path_to_pdbs}/MSA_dir/{template[0:4]}_{pdbs[0:4]}.txt\", \"w\") as msa_out:\n",
    "                    result = run(bash_cmd_rdy, stdout=msa_out, stderr=PIPE, \n",
    "                        universal_newlines=True)\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "    \n",
    "    \n",
    "    return min_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3321,
   "id": "215051c3-c736-40ad-9ea8-da90d42db9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_seq_alignment_pre_pca_processing_version_2(path_to_pdbs:str,\n",
    "                                                  template:str):\n",
    "    \n",
    "    \"\"\"seq based alignment because we already did a structure based alignment.\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    #to get min seq len\n",
    "    min_seq_len = _remove_deviating_length_strucs(path_to_pdbs=path_to_pdbs, template_struc=template)\n",
    "    \n",
    "    \n",
    "    only_pdbs = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    only_pdbs = [f for f in only_pdbs if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    with open(f\"{path_to_pdbs}/chain_list.txt\", \"w\") as chain_out:\n",
    "        for pdbs in only_pdbs:\n",
    "            #only for pdb files.\n",
    "            if pdbs[-4:] == \".pdb\":\n",
    "                chain_out.write(pdbs)\n",
    "                chain_out.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_pdbs}/MSA_dir\")\n",
    "     \n",
    "    except:\n",
    "        print(\"already there\")\n",
    "    \n",
    "    \n",
    "    USalign_loc = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/\"\n",
    "    for pdbs in only_pdbs:\n",
    "        if pdbs != template:\n",
    "            \n",
    "            print(pdbs, template)\n",
    "            bash_cmd = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_to_pdbs}/{pdbs} {path_to_pdbs}/{template} -byresi 5\"\n",
    "            \n",
    "            try:\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "                with open(f\"{path_to_pdbs}/MSA_dir/{template[0:4]}_{pdbs[0:4]}.txt\", \"w\") as msa_out:\n",
    "                    result = run(bash_cmd_rdy, stdout=msa_out, stderr=PIPE, \n",
    "                        universal_newlines=True)\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "    \n",
    "    \n",
    "    return min_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215fa7e-6267-4902-9a77-fd92d6c3f698",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Structure based alignment and cutting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3322,
   "id": "6c505624-399e-44f7-9d1d-38cd3a728277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_alignment(pdb: str, \n",
    "                     main_prot_seq: str, \n",
    "                     seq_to_check: str):\n",
    "\n",
    "    try:\n",
    "        aligner = Align.PairwiseAligner()\n",
    "        alignments = aligner.align(main_prot_seq, seq_to_check)\n",
    "\n",
    "        start_time = time.time()\n",
    "        time_out = 20\n",
    "        scores = []\n",
    "\n",
    "        for a in alignments:\n",
    "            if time.time() - start_time > time_out:\n",
    "                #print(f\"Timeout occurred for {pdb}\")\n",
    "                break\n",
    "            \n",
    "            scores.append(a.score)\n",
    "\n",
    "        score = max(scores)\n",
    "            \n",
    "        print(pdb, len(seq_to_check), score)\n",
    "\n",
    "        # Check if the sequence identity is above 80%\n",
    "        if score > 0.8 * len(seq_to_check):\n",
    "            return pdb, 'solid'\n",
    "        else:\n",
    "            return pdb, 'to_be_checked'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in {pdb}: {str(e)}\")\n",
    "        return pdb, 'to_be_checked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3323,
   "id": "ad1142ee-c29d-4995-866a-f2bdee841bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_prot_seq = \"MMKFKPNQTRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFEQNQDRKHRTYVYVLTVTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLKLGCSPANGNSTVPSLPDNNALFVTAAQTSGLPSSVR\"\n",
    "#seq_to_check = \"TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFET\"\n",
    "#aligner = Align.PairwiseAligner()\n",
    "#alignments = aligner.align(main_prot_seq, seq_to_check)\n",
    "#for a in alignments:\n",
    "#    score = a.score\n",
    "#\n",
    "#print(score)\n",
    "#\n",
    "#\n",
    "#if score > 0.8 * len(seq_to_check): \n",
    "#    print(\"we return solid\")\n",
    "#else:\n",
    "#    print(\"we return to be checked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3324,
   "id": "fe29a273-e04f-4b2c-ba69-410aa3c691a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_based_repair_1_wrapper(args):\n",
    "\n",
    "    #function for parallelization of modeller.\n",
    "    path_to_pdb, cutoff, pdb, min_len, max_len, structural_length, main_prot_seq, use_main = args\n",
    "    structure_based_repair_1(\n",
    "        path_to_pdb=path_to_pdb,\n",
    "        cutoff=cutoff,\n",
    "        pdb_id=pdb,\n",
    "        min_len=min_len,\n",
    "        max_len=max_len,\n",
    "        structural_length=structural_length,\n",
    "        main_prot_seq=main_prot_seq,\n",
    "        use_main=use_main\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3325,
   "id": "ea3c6a99-8758-430a-aa7c-5ca616200d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_based_cutting_1(path_to_pdbs:str, template:str, main_prot_seq:str):\n",
    "\n",
    "\n",
    "    print(\"Performing structure-based alignment\")\n",
    "    \n",
    "    \"\"\"This function will perform the structure based alignment based on the SUPERIMPOSER PDB.BIO class\n",
    "        \"\"\"\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    pdbs = [f for f in onlyfiles if f.endswith(\".pdb\")]\n",
    "\n",
    "    try:\n",
    "        new_directory_path = Path(path_to_pdbs) / \"PCA\" / \"clean_ensemble\"\n",
    "        \n",
    "        new_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    except Exception as error:\n",
    "        \n",
    "        print(f\"We could not make the directory: {new_directory_path}. Error: {error}\")\n",
    "\n",
    "    template_return = None  # Initialize template_return before the loop\n",
    "\n",
    "    pdb_info_dict = defaultdict()\n",
    "    \n",
    "    solid_ensemble = []\n",
    "    \n",
    "    for pdb in pdbs:\n",
    "        \n",
    "        pdb_path = os.path.join(path_to_pdbs, pdb)\n",
    "        \n",
    "        pdb_parser = Bio.PDB.PDBParser(QUIET=True)\n",
    "        \n",
    "        sample_structure = pdb_parser.get_structure(\"sample\", pdb_path)\n",
    "        \n",
    "        ref_chain = [x.get_id() for x in sample_structure.get_chains()]\n",
    "        \n",
    "        io = Bio.PDB.PDBIO()\n",
    "        \n",
    "        io.set_structure(sample_structure)\n",
    "        \n",
    "        io.save(os.path.join(path_to_pdbs, \"PCA\", f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"))\n",
    "\n",
    "        if pdb == template:\n",
    "            template_return = f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"\n",
    "\n",
    "        struc_start, struc_stop, sequence = select_c_alpha(os.path.join(path_to_pdbs, \"PCA\", f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"))\n",
    "        pdb_info_dict[f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"] = (struc_start, struc_stop, sequence)\n",
    "\n",
    "    #print(pdb_info_dict)\n",
    "    #print(\"still works after select_c_alpha\")\n",
    "    #len_ref = select_c_alpha(f\"{template}\")  #template path is full.\n",
    " \n",
    "    #pdb_lengths.append(len_ref) #we add it in the mix.\n",
    "    \n",
    "    \n",
    "    cnts_start = Counter(val[0] for val in pdb_info_dict.values())\n",
    "    cnts_stop = Counter(val[1] for val in pdb_info_dict.values())\n",
    "    \n",
    "    \n",
    "    highest_occ_start = cnts_start.most_common()  #grabs the highest frequency start of structures\n",
    "    highest_occ_stops = cnts_stop.most_common()  #grabs the highest frequency stop of structures\n",
    "    \n",
    "    print(highest_occ_start)\n",
    "    print(highest_occ_stops)\n",
    "    \n",
    "    min_len = highest_occ_start[0][0] # this corresponds to the majority vote and its associated starts.\n",
    "    max_len = highest_occ_stops[0][0] # this corresponds to the majority vote and its associated stops.\n",
    "    \n",
    "    print(min_len)\n",
    "    print(max_len)\n",
    "    \n",
    "    #so here we did majority voting (1) boundaries are established.\n",
    "    \n",
    "    #for keys, vals in pdb_info_dict.items():\n",
    "    #    print(keys, vals)    #val is: (start, stop, sequence(in full length!))\n",
    "    \n",
    "    # Solid Ensemble and To Be Checked lists\n",
    "    solid_ensemble_results = []\n",
    "    to_be_checked_results = []\n",
    "    \n",
    "    # Gather all pdb files\n",
    "    pdb_files = [f for f in os.listdir(os.path.join(path_to_pdbs, \"PCA\")) if f.endswith(\".pdb\")]\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        \n",
    "        \n",
    "        futures = [executor.submit(\n",
    "        perform_alignment, pdb, main_prot_seq, seq_to_check\n",
    "        ) for pdb, (start, stop, seq_to_check) in pdb_info_dict.items()]\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                pdb, category = result\n",
    "    \n",
    "                if category == 'solid':\n",
    "                    solid_ensemble_results.append(pdb)\n",
    "                elif category == 'to_be_checked':\n",
    "                    to_be_checked_results.append(pdb)\n",
    "                else:\n",
    "                    # Handle other error results if needed\n",
    "                    pass\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Solid Ensemble:\", solid_ensemble_results)\n",
    "    print(\"To Be Checked:\", to_be_checked_results)\n",
    "\n",
    "    \n",
    "    # Parallelized repair process.\n",
    "\n",
    "    # Create a list of arguments for the function\n",
    "    args_list = [(f\"{path_to_pdbs}/PCA\", 10, pdb, min_len, max_len,\n",
    "                  len(pdb_info_dict[pdb][2]),main_prot_seq, True) for pdb in solid_ensemble_results]\n",
    "\n",
    "    # Parallelize the calls using ProcessPoolExecutor\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        executor.map(structure_based_repair_1_wrapper, args_list)\n",
    "\n",
    "    solid_ensemble = [os.path.join(f\"{path_to_pdbs}/PCA/clean_ensemble\", pdb) for pdb in os.listdir(f\"{path_to_pdbs}/PCA/clean_ensemble\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/PCA/clean_ensemble\", pdb))]\n",
    "    \n",
    "   \n",
    "    #solidify ensemble by removing strong outliers.\n",
    "    \n",
    "    lst_ground_truth = [os.path.join(f\"{path_to_pdbs}/PCA/clean_ensemble\", pdb) for pdb in solid_ensemble_results]\n",
    "    \n",
    "    for pdb in solid_ensemble:\n",
    "        lst_ground_truth.append(f\"{pdb}\")\n",
    "    \n",
    "    #lets check the RMSD WITHIN the best hits to remove strong outliers.\n",
    "    \n",
    "    #lets return the average rmsd and use this as criterion! NEEDS TO BE DONE ATFER LUNCH\n",
    "    off_strucs, tot_mean, tot_sd = _calc_mean_tresholds_lst_1(lst_ground_truth)  #_1 is updated speed version\n",
    "    print(off_strucs)\n",
    "    \n",
    "    #remove off strucs. now compute mean rmsd against the ground truth assumed ensemble.\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_pdbs}/PCA/repaired_ensemble\")\n",
    "    \n",
    "    except:\n",
    "        print(f\"we could not make the dir : {path_to_pdbs}/PCA/repaired_ensemble\")\n",
    "\n",
    "    # Create a list of arguments for the function\n",
    "    args_list = [(f\"{path_to_pdbs}/PCA\", 10, pdb, min_len, max_len,\n",
    "                  len(pdb_info_dict[pdb][2]),main_prot_seq, False) for pdb in to_be_checked_results]\n",
    "\n",
    "    # Parallelize the calls using ProcessPoolExecutor\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        executor.map(structure_based_repair_1_wrapper, args_list)\n",
    "\n",
    "        \n",
    "    #after repair we check rmsd.\n",
    "    \n",
    "    lst_to_check = [f for f in os.listdir(f\"{path_to_pdbs}/PCA/repaired_ensemble\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/PCA/repaired_ensemble\", f))]\n",
    "    \n",
    "    print(\"we check now rmsd for repaired alternative strucs\")\n",
    "    \n",
    "    for pdb in lst_to_check:\n",
    "\n",
    "        try:\n",
    "            rmsd_avg = _calc_rmsd_dev(lst_to_pdbs=lst_ground_truth,\n",
    "                                  pdb_to_check=f\"{path_to_pdbs}/PCA/repaired_ensemble/{pdb}\")\n",
    "        \n",
    "            #if rmsd is low we add it to the ensemble.\n",
    "            if rmsd_avg < (tot_mean * 2 * tot_sd):\n",
    "                print(f\"we append {pdb} with rmsd avg: {rmsd_avg}, against {tot_mean * 2 * tot_sd} as threshold.\")\n",
    "                solid_ensemble.append(pdb)\n",
    "                #we move it to another dir.\n",
    "                shutil.copy(pdb, f\"{path_to_pdbs}/PCA/clean_ensemble/{os.path.basename(pdb)}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"we reject {pdb} with rmsd avg: {rmsd_avg}, against {tot_mean * 2 * tot_sd} as threshold.\")\n",
    "                os.remove(pdb)\n",
    "        \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "    \n",
    "    return template_return\n",
    "    #return pdbs to be fetched from the clean ensemble to another function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3326,
   "id": "a4fd9bec-74a4-4917-a9f9-fce5dc277ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_ensemble_list(work_dir, pdb_files):\n",
    "    #this function acts as an helper function in pca_laura_pipeline_1\n",
    "    with open(f\"{work_dir}/ensemble.txt\", \"w\") as esmbl:\n",
    "        for entry in pdb_files:\n",
    "            esmbl.write(entry[9:-4] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3327,
   "id": "91a85431-e9f7-4274-8850-064d3d6006af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_multi_pdb(work_dir, pdb_files):\n",
    "    #this function acts as an helper function in pca_laura_pipeline_1\n",
    "    ms = Structure.Structure(\"master\")\n",
    "    i = 0\n",
    "    for file in pdb_files:\n",
    "        location = os.path.join(work_dir, \"clean_ensemble\", file)\n",
    "        prot_name = file[:-4]\n",
    "        structure = PDBParser(QUIET=True).get_structure(prot_name, location)\n",
    "        for model in structure:\n",
    "            new_model = model.copy()\n",
    "            new_model.id = i\n",
    "            new_model.serial_num = i + 1\n",
    "\n",
    "            # Iterate through atoms and set B-factor to 0 because this causes issues with pcatoolS\n",
    "            for chain in new_model:\n",
    "                for residue in chain:\n",
    "                    for atom in residue:\n",
    "                        atom.set_bfactor(0)\n",
    "\n",
    "            i += 1\n",
    "            ms.add(new_model)\n",
    "\n",
    "    pdb_io = PDBIO()\n",
    "    pdb_io.set_structure(ms)\n",
    "    pdb_io.save(f\"{work_dir}/multi_ensemble.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3328,
   "id": "f5d67b39-734b-499d-8945-0d6042c8654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _move_executables(basepath, work_dir):\n",
    "    #this helper function is required for domenico and laura stuff to work.\n",
    "    try:\n",
    "        files_to_move = [f for f in os.listdir(basepath) if os.path.isfile(os.path.join(basepath, f))]\n",
    "        for file in files_to_move:\n",
    "            shutil.copy(os.path.join(basepath, file), work_dir)\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3329,
   "id": "45d30d05-954b-459d-836f-4b46ebed233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pdb_files(folder_path):\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Iterate over the files and remove those with the .pdb extension\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdb\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            os.remove(file_path)\n",
    "            #print(f\"Removed: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3330,
   "id": "15a65a40-10ca-418a-b632-29422f38a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_pca_pipeline(work_dir, template, protein, num_structures):\n",
    "\n",
    "    #here we run the pca pipeline.\n",
    "    os.chdir(work_dir)\n",
    "    _move_executables(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/PCA_pipeline\", work_dir)\n",
    "\n",
    "    # Part 1: PCA\n",
    "    bash_cmd1 = f\"./getpca.sh multi_ensemble.pdb {template} refpdb_ref\"\n",
    "    _run_command(bash_cmd1, \"Part 1\")\n",
    "    \n",
    "    # Part 2: Convert evec files\n",
    "    bash_cmd2 = f\"./convert.sh refpdb_ref_pca.evec\"\n",
    "    _run_command(bash_cmd2, \"Part 2\")\n",
    "\n",
    "    # Part 3: Projections\n",
    "    bash_cmd3 = f\"./getproj.sh {template} refpdb_ref_pca.evec {protein} multi_ensemble.pdb\"\n",
    "    _run_command(bash_cmd3, \"Part 3\")\n",
    "\n",
    "    return work_dir, protein, num_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3331,
   "id": "c6339fdc-6523-4c38-ac37-5f5fa4d37124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_command(bash_cmd, part_name):\n",
    "    try:\n",
    "        result = run(bash_cmd.split(), stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        print(f\"{part_name} output:\\n{result.stdout}\")\n",
    "        print(f\"{part_name} errors:\\n{result.stderr}\")\n",
    "    except Exception:\n",
    "        print(f\"Script did not work. Parameters: {bash_cmd.split()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3332,
   "id": "57e7e1a2-d274-4faa-90a3-d0e1bdf5ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_template_1(work_dir, pdb_files:str):\n",
    "\n",
    "    \n",
    "    clean_ensemble_dir = os.path.join(work_dir, \"clean_ensemble\")\n",
    "\n",
    "     # Define the paths for the source and destination files\n",
    "    source_path = os.path.join(clean_ensemble_dir, pdb_files[0])\n",
    "    destination_path = os.path.join(work_dir, pdb_files[0])\n",
    "\n",
    "    # Copy the first structure file to the working directory with the original filename\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    #we copied it into work dir.\n",
    "    return pdb_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3333,
   "id": "3a67a273-84ca-4b8b-b9ca-e030a79cb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#work_dir, protein, num_struc = pca_laura_pipeline_1(path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA\",\n",
    "#                    template=None, protein=\"SERCA\")\n",
    "#_plot_PCA_new(input_dir=work_dir, protein=protein, num_struc=num_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3334,
   "id": "2b3f23f2-c010-497e-b439-5df2d24d1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_laura_pipeline_1(path, template, protein):\n",
    "    \n",
    "    work_dir = path\n",
    "\n",
    "    clean_dir = f\"{path}/clean_ensemble\"\n",
    "    \n",
    "    os.chdir(work_dir)\n",
    "\n",
    "    pdb_files = [f for f in os.listdir(f\"{path}/clean_ensemble\") if os.path.isfile(os.path.join(f\"{path}/clean_ensemble\", f))]\n",
    "\n",
    "    print(\"we are now inside pca_laura_pipeline_1\")\n",
    "    #we store the required pdb file names here in save_ensemble_list\n",
    "    _save_ensemble_list(work_dir, pdb_files)\n",
    "\n",
    "    #first remove all previous pdbs\n",
    "    remove_pdb_files(path)\n",
    "    \n",
    "    #we need a multi_pdb file to work with laura code.\n",
    "    _create_multi_pdb(work_dir, pdb_files)\n",
    "\n",
    "\n",
    "    #we should choose a template from clean ensemble !!!\n",
    "\n",
    "    #TBD TOMORROW\n",
    "\n",
    "    #template is FULL PATH TO CLEAN ENSEMBLE.\n",
    "    template = select_template_1(work_dir, pdb_files)\n",
    "    print(f\"we select as template for pca: {template}\")\n",
    "    #workdir , template, protein, and len of pdb files aka how many are in pca.\n",
    "    return _run_pca_pipeline(work_dir, template, protein, len(pdb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3335,
   "id": "2cc6016a-38cf-4a40-9df8-5b2f4fed218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_PCA_new(input_dir:str, protein:str, num_struc:int):\n",
    "    \n",
    "    proj_dict = defaultdict()\n",
    "    \n",
    "    labels = []\n",
    "    with open(f\"{input_dir}/ensemble.txt\", \"r\") as label_in:\n",
    "        for lines in label_in:\n",
    "            lines = lines.replace(\"\\n\",\"\")\n",
    "            labels.append(lines[-6:])\n",
    "    \n",
    "    \n",
    "    var_list = []\n",
    "    \n",
    "    #they are already sorted.\n",
    "    with open(f\"{input_dir}/{protein}_evec3.dat\", \"r\") as var_readin:\n",
    "        for lines in var_readin:\n",
    "            line = lines.split()\n",
    "            if len(line) == 2:\n",
    "                var_list.append(float(line[1]))\n",
    "\n",
    "    expl_var = var_list / np.sum(var_list)\n",
    "    \n",
    "    with open(f\"{input_dir}/{protein}.mode_12.proj\", \"r\") as res_proj:\n",
    "        for lines in res_proj:\n",
    "            lines = lines.replace(\"\\n\",\"\")\n",
    "            #print(lines.split(\"      \"))  #6 spaces ??? \n",
    "            lines = lines.split(\"      \")\n",
    "            num = lines[1].replace(\" \",\"\")\n",
    "            \n",
    "            PC1 = float(lines[2].replace(\" \",\"\"))\n",
    "            PC2 = float(lines[3].replace(\" \",\"\"))\n",
    "            \n",
    "            proj_dict[str(num)] = (PC1, PC2)\n",
    "            \n",
    "    \n",
    "            \n",
    "    PC_1 = []\n",
    "    PC_2 = []\n",
    "    for keys, vals in proj_dict.items():\n",
    "        #print(vals)\n",
    "        PC_1.append(vals[0])\n",
    "        PC_2.append(vals[1])\n",
    "    \n",
    "    \n",
    "    plot_df = pd.DataFrame(columns=[\"PC1\", \"PC2\"])\n",
    "    plot_df[\"PC1\"] = PC_1\n",
    "    plot_df[\"PC1\"] = plot_df[\"PC1\"] * (-1)\n",
    "    plot_df[\"PC2\"] = PC_2\n",
    "    plot_df[\"labels\"] = labels\n",
    "    #michael9_test_df[\"PC2\"] = michael9_test_df[\"PC2\"] * (-1)\n",
    "    #michael9_test_df[\"labels\"] = labels\n",
    "    \n",
    "    fig = px.scatter(plot_df, x= \"PC1\",y= \"PC2\",\n",
    "                hover_data=[\"labels\"],labels={\"labels\": \"labels\"},\n",
    "                custom_data = [\"labels\"])\n",
    "\n",
    "    fig.update_traces(marker_size=10,\n",
    "                    hovertemplate=\"<b>RCSB: %{customdata[0]}</b>\",\n",
    "                    hoverlabel=dict(\n",
    "                    font_size=40),  # Set the hover label font size\n",
    "                        \n",
    "                        \n",
    "                    mode=\"markers+text\", selector=dict(type='scatter')    \n",
    "                        \n",
    "        )\n",
    "    \n",
    "    fig.update_layout(plot_bgcolor='lavender',\n",
    "                      paper_bgcolor='white', \n",
    "                     width = 800,\n",
    "                     height = 600,\n",
    "                     xaxis_title=f'PC 1 {expl_var[0]*100:.2f}%',  # Set the x-axis label\n",
    "                     yaxis_title=f'PC 2 {expl_var[1]*100:.2f}%',   # Set the y-axis label\n",
    "                     legend_title_text='Clusters',  # Set the legend title\n",
    "                     showlegend=True,  # Show the legend\n",
    "                     title={\n",
    "                     'text': f'PCA of {protein}: {num_struc} structures',  # Set the title text\n",
    "                     'x': 0.5,  # Center the title horizontally (0 to 1)\n",
    "                     'font': {'size': 30}  # Customize title font size and weight\n",
    "                     }\n",
    "                      \n",
    "                     )\n",
    "    \n",
    "    \n",
    "    pio.write_image(fig, f\"{input_dir}/{protein}_PC_plot.png\", format=\"png\")\n",
    "    \n",
    "    #return fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3336,
   "id": "5c431596-0922-44b9-a9d1-97357ebe954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file = \"/home/micnag/result_test_struc_align/PCA/SERCA.mode_12.proj\"\n",
    "#labels_file = \"/home/micnag/result_test_struc_align/PCA/ensemble.txt\"\n",
    "#var_file = \"/home/micnag/result_test_struc_align/PCA/SERCA_evec3.dat\"\n",
    "#protein = \"SERCA\"\n",
    "#num_struc = 86\n",
    "\n",
    "#_plot_PCA(input_file=input_file, \n",
    "#         labels_file=labels_file,\n",
    "#          explained_var_file=var_file,\n",
    "#         protein=protein,\n",
    "#         num_struc=num_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3337,
   "id": "351579e3-8cec-4e6d-a3c8-3f8fd3f4c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/result_test_struc_align/clean_ensemble\"\n",
    "#template = \"/home/micnag/result_test_struc_align/clean_ensemble/original_2c9m_A.pdb\"\n",
    "\n",
    "#pca_laura_pipeline(path=path, template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3338,
   "id": "9caf1ae1-fa40-4dd5-94de-908f03d0a5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### os.chdir(\"/home/micnag/\") #just because there is an error is we stuck within that makes problems\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#path = \"/home/micnag/result_test_struc_align\"\n",
    "#query = \"O14983\"\n",
    "#main_prot_seq = get_gene_fasta(query)\n",
    "#template = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/2c9m.pdb\"\n",
    "#structure_based_cutting(path_to_pdbs=path, template=template, main_prot_seq=main_prot_seq, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3339,
   "id": "e3a43b6b-7571-4059-91f9-f0973d42d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_proper_length_1(path:str,start:int, stop:int, use_main:bool):\n",
    "    \n",
    "    #sel only c_alpha\n",
    "    class RangeOnly(Select):\n",
    "        def __init__(self, start, stop):\n",
    "            super().__init__()\n",
    "            self.start = start\n",
    "            self.stop = stop\n",
    "            \n",
    "        def accept_atom(self, atom):\n",
    "            return 1 if atom.id == \"CA\" else 0\n",
    "            \n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if self.start <= residue.id[1] <= self.stop else 0 \n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    prot_name = f\"default\"\n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    structure_len = [x.get_id()[1] for x in structure.get_residues()]\n",
    "\n",
    "    if not use_main:\n",
    "        \n",
    "        start += structure_len[0] - 1  # Correct for overhang\n",
    "        \n",
    "    if len(structure_len) < (stop - start) + 1:\n",
    "        \n",
    "        print(f\"Removing {path}\")\n",
    "        \n",
    "        os.remove(path)\n",
    "        return False\n",
    "\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "\n",
    "    range_selector = RangeOnly(start, stop)\n",
    "    io.save(path, range_selector)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3340,
   "id": "5ce997c8-683f-4c40-bda3-c3a687cddfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_proper_length(path:str,start:int, stop:int, use_main:bool):\n",
    "    \n",
    "     #sel only c_alpha\n",
    "    class range_only(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "        \n",
    "        def accept_atom(self, atom):\n",
    "            return 1 if atom.id == \"CA\" else 0\n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[1] >= start and residue.id[1] <= stop else 0\n",
    "        \n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    \n",
    "    #now we load it again\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"default\"\n",
    "    \n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    structure_len = structure.get_residues()\n",
    "    \n",
    "    struc_len = [x.get_id()[1] for x in structure_len]\n",
    "    \n",
    "    #this is start\n",
    "    #1\n",
    "    #this is stop\n",
    "    #994\n",
    "    #/home/micnag/result_test_struc_align/original_4kyt_A.pdb\n",
    "    #992 994\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_main == False:    \n",
    "        \n",
    "        start = struc_len[0]\n",
    "        stop = stop + start -1 # this corresponds to correcting for the overhang!\n",
    "        \n",
    "    print(f\"we set as start: {start} and as stop: {stop}\")\n",
    "    \n",
    "    if len(struc_len) < (stop-start)+1:\n",
    "        \n",
    "        #print(path)\n",
    "        #print(struc_len,stop-start+1)\n",
    "        #print(struc_len)\n",
    "        #we remove it then.\n",
    "        #print(f\"we remove {path}\")\n",
    "\n",
    "        #os.remove(path)\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        io = PDBIO()\n",
    "            \n",
    "        io.set_structure(structure)\n",
    "    \n",
    "        #savepath needs to be changed.\n",
    "        io.save(path, range_only())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3341,
   "id": "fcc43c4f-9c19-4b85-8dc9-81e4469b3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/result_test_struc_align/original_5zmv_A.pdb\"\n",
    "#_select_proper_length(path=path, stop=994)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3342,
   "id": "7176052f-9bb6-4d25-acf5-2b45ab507a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_based_repair_1(path_to_pdb:str, pdb_id:str, min_len:int,\n",
    "                          max_len:int, \n",
    "                          structural_length:int, \n",
    "                          use_main:bool,\n",
    "                          main_prot_seq:str,\n",
    "                          cutoff=10):\n",
    "    \n",
    "    #print(\"we check gaps now\")\n",
    "    pdb_path = Path(path_to_pdb) / pdb_id\n",
    "    #print(f\"We check gaps now: {pdb_path}\")\n",
    "\n",
    "    #first we check the gaps\n",
    "    gaps = _gap_localization_1(f\"{path_to_pdb}/{pdb_id}\")\n",
    "\n",
    "    #[(1, 78), (85, 501), (508, 990), (1015, 1042)]\n",
    "    #print(gaps)\n",
    "\n",
    "\n",
    "    #we need to change to the homology modeller dir.\n",
    "\n",
    "    try:\n",
    "        os.chdir(path_to_pdb)\n",
    "        #print(os.getcwd())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Repair all that have less than 10 gaps.\n",
    "        gap_ranges = [y - x for x, y in gaps if (y < max_len and x >= min_len)]\n",
    "        gap_sorted = sorted(gap_ranges, reverse=True)\n",
    "\n",
    "        #print(gap_sorted)\n",
    "        #if there are no gaps but they dont match the full length e.g miss a bit in N or C termini OR have gaps < 10:\n",
    "        if not gap_sorted or gap_sorted[0] <= 10:\n",
    "            #print(f\"PDB is: {pdb_id}, use_main: {use_main}\")\n",
    "            mini_repair_residues_2(os.path.join(path_to_pdb, pdb_id), max_len, main_prot_seq, use_main)\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during repair: {e}\")\n",
    "        # Remove the file if repair fails\n",
    "        os.remove(os.path.join(path_to_pdb, pdb_id))\n",
    "        return\n",
    "    \n",
    "    if structural_length < max_len:\n",
    "        try:\n",
    "            mini_repair_residues_2(os.path.join(path_to_pdb, pdb_id), max_len, main_prot_seq, use_main)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during repair: {e}\")\n",
    "            os.remove(os.path.join(path_to_pdb, pdb_id))\n",
    "            return\n",
    "\n",
    "    \n",
    "    # If we deleted it, we don't need to continue\n",
    "    keep_struc = _select_proper_length_1(os.path.join(path_to_pdb, pdb_id), min_len, max_len, use_main)\n",
    "    if not keep_struc:\n",
    "        print(\"Removed the structure, returning!\")\n",
    "        try:\n",
    "            os.remove(os.path.join(path_to_pdb, pdb_id))\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "    non_canonical = remove_hetero_atoms_1(path=path_to_pdb, pdb_file=pdb_id)\n",
    "\n",
    "    if non_canonical:\n",
    "        for keys, vals in non_canonical.items():\n",
    "            _mutate_non_standard_aa_1(\n",
    "                os.path.join(path_to_pdb, pdb_id),\n",
    "                non_standard_residue=vals[0],\n",
    "                residue=keys,\n",
    "                chain=vals[1]\n",
    "            )\n",
    "\n",
    "    print(\"Shuffling repaired structures\")\n",
    "\n",
    "    dest_folder = \"clean_ensemble\" if use_main else \"repaired_ensemble\"\n",
    "    dest_path = os.path.join(path_to_pdb, dest_folder, pdb_id)\n",
    "    \n",
    "    print(f\"Copying from {os.path.join(path_to_pdb, pdb_id)} to : {dest_path}\")\n",
    "    shutil.copy(os.path.join(path_to_pdb, pdb_id), dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3343,
   "id": "2a2daee9-6e76-4ba6-a082-440e6aa0d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/home/micnag/bioinformatics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3344,
   "id": "3874cd5f-b08c-4ba5-a263-a082400573dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsd_1(pdb_path:str, template:str):\n",
    "    \n",
    "    aligner = Superimposer()\n",
    "    \n",
    "    pdb_parser = PDBParser(QUIET = True)\n",
    "        \n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", pdb_path)\n",
    "    template_structure = pdb_parser.get_structure(\"template\", template)\n",
    "    \n",
    "            \n",
    "    sample_model = list(sample_structure.get_atoms())\n",
    "    template_model = list(template_structure.get_atoms())\n",
    "            \n",
    "    aligner.set_atoms(template_model, sample_model)\n",
    "    aligner.apply(sample_model)\n",
    "    \n",
    "    return aligner.rms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3345,
   "id": "183dd3a9-d7b7-4c0d-80da-c7bb396a48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_rmsd_dev(lst_to_pdbs:list, pdb_to_check:str):\n",
    "    \n",
    "    avg_rms = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks to the thread pool\n",
    "        futures = [executor.submit(calculate_rmsd_parallel, pdb, pdb_to_check) for pdb in lst_to_pdbs]\n",
    "\n",
    "        # Collect results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                res = future.result()\n",
    "                avg_rms.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating RMSD: {e}\")\n",
    "\n",
    "    print(pdb_to_check, avg_rms)\n",
    "    # Return the mean RMSD\n",
    "    return np.mean(avg_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3346,
   "id": "92e422d6-d7ef-400d-a1cb-9ef2ceb3c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmsd_parallel(pdb, temp):\n",
    "    return get_rmsd_1(pdb_path=pdb, template=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3347,
   "id": "0b2dc23d-13ea-4fa2-85ce-1e9a62c6275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_mean_tresholds_lst_1(lst_to_pdbs):\n",
    "    avg_rms = defaultdict(list)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Iterate over unique pairs of structures\n",
    "        futures = []\n",
    "        for i, temp in enumerate(lst_to_pdbs[:-1]):\n",
    "            for pdb in lst_to_pdbs[i + 1:]:\n",
    "                future = executor.submit(calculate_rmsd_parallel, pdb, temp)\n",
    "                futures.append((pdb, future))\n",
    "\n",
    "        # Collect results\n",
    "        for pdb, future in futures:\n",
    "            try:\n",
    "                res = future.result()\n",
    "                avg_rms[pdb].append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating RMSD for {pdb}: {e}\")\n",
    "\n",
    "    total_means = [np.mean(vals) for vals in avg_rms.values()]\n",
    "    tot_mean = np.mean(total_means)\n",
    "    tot_sd = np.std(total_means)\n",
    "\n",
    "    off_strucs = [(keys, vals) for keys, vals in avg_rms.items() if np.mean(vals) > tot_mean + 2 * tot_sd]\n",
    "\n",
    "    print(off_strucs)\n",
    "    return off_strucs, tot_mean, tot_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3348,
   "id": "a2f80c6a-93e5-4dd7-8c36-affb700e67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_mean_tresholds_lst(lst_to_pdbs:list):\n",
    "    \n",
    "    avg_rms = defaultdict(list)\n",
    "    \n",
    "    #each against each\n",
    "    for temp in lst_to_pdbs:\n",
    "        #we compute the average rmsd for each struc against all others.\n",
    "        for pdb in lst_to_pdbs:\n",
    "            res = get_rmsd(pdb_path=pdb, \n",
    "                  template=temp)\n",
    "        \n",
    "            avg_rms[pdb].append(res)\n",
    "        \n",
    "\n",
    "    total_means = []\n",
    "\n",
    "    tot_mean_dict = defaultdict()\n",
    "    for keys, vals in avg_rms.items():\n",
    "        print(keys)\n",
    "    \n",
    "        tot = 0\n",
    "        for v in vals:\n",
    "            tot += v\n",
    "    \n",
    "        total_means.append(tot/len(vals))\n",
    "        print(tot/len(vals))\n",
    "        \n",
    "        tot_mean_dict[keys] = tot/len(vals) \n",
    "        \n",
    "    tot_mean = np.mean(total_means)\n",
    "    tot_sd = np.std(total_means)\n",
    "    \n",
    "    off_strucs = [(keys, vals) for keys, vals in tot_mean_dict.items() if vals > tot_mean + 2*tot_sd]\n",
    "    \n",
    "    print(off_strucs)\n",
    "    return (off_strucs, tot_mean, tot_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3349,
   "id": "9bba6ba1-611d-4ec1-a284-3cd752447c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pdb_test = \"/home/micnag/result_test_struc_align\"\n",
    "\n",
    "def _calc_mean_tresholds(path_to_pdbs:str):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "\n",
    "    avg_rms = defaultdict(list)\n",
    "    \n",
    "    #each against each\n",
    "    for temp in onlyfiles:\n",
    "        #we compute the average rmsd for each struc against all others.\n",
    "        for pdb in onlyfiles:\n",
    "            res = get_rmsd(pdb_path=f\"{path_to_pdbs}/{pdb}\", \n",
    "                  template=f\"{path_to_pdbs}/{temp}\")\n",
    "        \n",
    "            avg_rms[pdb].append(res)\n",
    "        \n",
    "\n",
    "    total_means = []\n",
    "\n",
    "    tot_mean_dict = defaultdict()\n",
    "    for keys, vals in avg_rms.items():\n",
    "        print(keys)\n",
    "    \n",
    "        tot = 0\n",
    "        for v in vals:\n",
    "            tot += v\n",
    "    \n",
    "        total_means.append(tot/len(vals))\n",
    "        print(tot/len(vals))\n",
    "        \n",
    "        tot_mean_dict[keys] = tot/len(vals) \n",
    "        \n",
    "    tot_mean = np.mean(total_means)\n",
    "    tot_sd = np.std(total_means)\n",
    "    \n",
    "    off_strucs = [(keys, vals) for keys, vals in tot_mean_dict.items() if vals > tot_mean + 2*tot_sd]\n",
    "    print(off_strucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3350,
   "id": "255fed41-2506-4d3d-a812-3d78c042273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_calc_mean_tresholds(path_to_pdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3351,
   "id": "dab714af-0211-4c38-98cd-b6f230dbce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gap_localization_1(pdb_path: str):\n",
    "    \"\"\"Helper function to compute the start and stops of gaps for later potential reconstruction.\"\"\"\n",
    "    \n",
    "    gap_ranges = []\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "    \n",
    "    test_res = test_structure.get_residues()\n",
    "    start = end = None\n",
    "\n",
    "    for res in test_res:\n",
    "        res_id = int(res.get_id()[1])\n",
    "        if end is None:\n",
    "            start = end = res_id\n",
    "        elif res_id == end + 1:\n",
    "            end = res_id\n",
    "        else:\n",
    "            if start != end:\n",
    "                gap_ranges.append((start, end))\n",
    "            start = end = res_id\n",
    "\n",
    "    if start is not None and start != end:\n",
    "        gap_ranges.append((start, end))\n",
    "\n",
    "    # Convert the list of gap ranges to a list of gap tuples\n",
    "    gap_tuples = [(start, end-1) for start, end in gap_ranges]\n",
    "    merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "    \n",
    "    return merged_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3352,
   "id": "462e4b47-61e2-4573-b330-0523ec08e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _structures_to_select_as_reference(structure_pdbs:str, gaps:list):\n",
    "    \n",
    "    np.random.seed(7) #lets make it random. we dont want to parse the whole ensemble.\n",
    "    \n",
    "    #either 10% of ensemble as template or min 5:\n",
    "    \n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(structure_pdbs) if os.path.isfile(os.path.join(structure_pdbs, f))]\n",
    "    \n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    struc_templates = []\n",
    "    \n",
    "    rng_num = np.unique(np.random.randint(0, len(structure_pdbs)-1, 3*len(onlyfiles)))  #we draw 60 samples and keep only uniques\n",
    "    \n",
    "    i = 0 #we start with first member of rng_num\n",
    "    \n",
    "    while len(struc_templates) < 4:\n",
    "        \n",
    "        #ifonlyfilesexhausted all potential templates we shall return None\n",
    "        if i == len(onlyfiles):\n",
    "            return None\n",
    "        \n",
    "        pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "            \n",
    "        struc_chosen = rng_num[i]\n",
    "        \n",
    "        test_structure = pdb_parser.get_structure(\"test\", f\"{structure_pdbs}/{pdbs[struc_chosen]}\")  #this choses the structure that has idx i from rng_num\n",
    "        \n",
    "        test_res = test_structure.get_residues()\n",
    "                                    \n",
    "        max_len = [x for x in test_res] #just for max len purpose.\n",
    "        \n",
    "        #need to reset generator!!!\n",
    "        test_res = test_structure.get_residues()\n",
    "        \n",
    "        \n",
    "        ext_gap_upper = gaps[1] + 5 if gaps[1] + 5 < len(max_len) else len(max_len) # if it goes beyong possible we stick with end range.\n",
    "        ext_gap_lower = gaps[0] - 5 if gaps[0] - 5 > 0 else gaps[0]  #min is pos 1. we cant go negative\n",
    "        \n",
    "        res_id_lst = [x.get_id()[1] for x in test_res if x.get_id()[1] >= ext_gap_lower and x.get_id()[1] <= ext_gap_upper] # +5 as buffer\n",
    "        \n",
    "        diff_required = ext_gap_upper - ext_gap_lower + 1\n",
    "        \n",
    "        \n",
    "        # this is borders ext:\n",
    "        #512 500\n",
    "        #this is required\n",
    "        #13\n",
    "\n",
    "        #print(\"this is borders ext:\")\n",
    "        #print(ext_gap_upper, ext_gap_lower)\n",
    "        #print(\"this is required\")\n",
    "        #print(diff_required)\n",
    "        \n",
    "        #print(gaps[1]+5 - gaps[0]- 5 + 1)\n",
    "        #implement that we grab CA_IDX GAPS[0] to GAPS[1] instead of idx of atomlist (which can have gaps and then is shifted)\n",
    "        \n",
    "        if len(res_id_lst) == (diff_required):\n",
    "            struc_templates.append(f\"{structure_pdbs}/{pdbs[struc_chosen]}\")\n",
    "            \n",
    "        #print(len(struc_templates))\n",
    "        i += 1\n",
    "        #reset if we get all templates for struc 1\n",
    "        if len(struc_templates) == 3:\n",
    "            i = 0\n",
    "        \n",
    "        \n",
    "    #these we use as templates.    \n",
    "    return struc_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3353,
   "id": "0a4ec20b-3651-4829-ba5d-c37f7411d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gap_alignment(structure_pdbs:str, gaps:list):\n",
    "    \n",
    "    \"\"\"This function will make an alignment locally around the gaps and compute RMSD.\n",
    "    IF LOW RMSD: we seal the gap.\n",
    "    IF HIGH RMSD: we cut the gap from all structures\"\"\"\n",
    "    \n",
    "    #structures are all aligned already. now we should do a local small alignment and then compute rmsd.\n",
    "    \n",
    "    gap_dict = defaultdict()\n",
    "    \n",
    "    for gap in gaps:\n",
    "    \n",
    "        ref_strucs = _structures_to_select_as_reference(structure_pdbs, gap)\n",
    "    \n",
    "        onlyfiles = [f for f in os.listdir(structure_pdbs) if os.path.isfile(os.path.join(structure_pdbs, f))]\n",
    "    \n",
    "        pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "        \n",
    "        for j, template in enumerate(ref_strucs):\n",
    "            \n",
    "            #here we store for each template the rmsd\n",
    "            template_rmsd = 0\n",
    "            \n",
    "            #print(template)\n",
    "            \n",
    "            pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "            \n",
    "        \n",
    "            template_structure = pdb_parser.get_structure(\"template\", f\"{template}\")\n",
    "            \n",
    "            template_model = template_structure.get_atoms()\n",
    "            \n",
    "            ext_gap_upper = gap[1] + 5\n",
    "            ext_gap_lower = gap[0] - 5\n",
    "            \n",
    "            \n",
    "            template_atoms = [x for x in template_model if x.get_full_id()[3][1] >= ext_gap_lower \n",
    "                              and x.get_full_id()[3][1] <= ext_gap_upper]\n",
    "            \n",
    "            template_ids = [x.get_full_id()[3][1] for x in template_atoms]\n",
    "            \n",
    "            \n",
    "            #print(template_atoms)\n",
    "            #print(template_ids)\n",
    "            \n",
    "            \n",
    "            template_model_list = [x for x in template_model]  #select atoms from start to finish but they have + 5 on both ends.\n",
    "            \n",
    "            #nested loop.\n",
    "            \n",
    "            #first gap all samples\n",
    "            for i, samples in enumerate(pdbs):\n",
    "                \n",
    "                pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "            \n",
    "                aligner = Bio.PDB.Superimposer()\n",
    "        \n",
    "                sample_structure = pdb_parser.get_structure(\"sample\", f\"{structure_pdbs}/{samples}\")\n",
    "            \n",
    "                sample_model = sample_structure.get_atoms()\n",
    "                \n",
    "                sample_atoms = [x for x in sample_model if x.get_full_id()[3][1] >= ext_gap_lower \n",
    "                        and x.get_full_id()[3][1] <= ext_gap_upper]\n",
    "                \n",
    "                if len(sample_atoms) == len(template_atoms):\n",
    "                    \n",
    "                    #if they are the same we proceed.\n",
    "                    \n",
    "                    #we start procedure of alignment if this is done.\n",
    "                    \n",
    "                    aligner.set_atoms(fixed=template_atoms, moving=sample_atoms)\n",
    "                    \n",
    "                    aligner.apply(sample_atoms)\n",
    "                    #print(template, samples)\n",
    "                    #print(aligner.rms)\n",
    "            \n",
    "                    template_rmsd += aligner.rms\n",
    "        #print(((template_rmsd/i))/j)\n",
    "                    \n",
    "                    \n",
    "        #outer most we compute the average rmsd between all template and all structs.\n",
    "        #j means we average over all templates.\n",
    "        #ref strucs 0 is just the first suitable template.\n",
    "        gap_dict[gap] = (template_rmsd/i)/j\n",
    "        \n",
    "        \n",
    "    #print(gap_dict)\n",
    "    return gap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3354,
   "id": "d00023c0-2b69-4662-848a-8929d9806b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_prot_seq = \"MKFKPNQTRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFENQDRKHRTYVYVLTVTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLKLGCSPTNGNSSVPSLPDNNALFVTAAPPSGVPSSIR\"\n",
    "#use_main = True\n",
    "#stop_pos = 176\n",
    "#path_to_pdb = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_181/PCA/original_2duk_A.pdb\"\n",
    "\n",
    "#mini_repair_residues_2(path_to_pdb=path_to_pdb, stop_pos=stop_pos, main_prot_seq=main_prot_seq, use_main=use_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3355,
   "id": "ba5030a6-ec58-43da-b42e-723673c104d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repair_artefacts(pdb_basep, pdb_code_name):\n",
    "    \n",
    "    pattern = f\"{pdb_code_name}\"\n",
    "    print(pattern)\n",
    "\n",
    "    # Remove original_****_*.ali file\n",
    "    ali_files = glob.glob(f\"{pattern}.ali\")\n",
    "    for file in ali_files:\n",
    "        os.remove(file)\n",
    "\n",
    "\n",
    "    # Remove original_****_A.pdb\n",
    "    old_pdb_file = f\"{pattern}.pdb\"\n",
    "    if os.path.exists(old_pdb_file):\n",
    "        os.remove(old_pdb_file)\n",
    "        \n",
    "    # Rename original_****_Ax.B99990001.pdb to original_****_A.pdb\n",
    "    repaired_pdb_file = f\"{pattern}x.B99990001.pdb\"\n",
    "    if os.path.exists(repaired_pdb_file):\n",
    "        new_pdb_file = repaired_pdb_file.replace('x.B99990001', '')\n",
    "        shutil.move(repaired_pdb_file, new_pdb_file)\n",
    "\n",
    "    # Remove original_****_*x.D00000001\n",
    "    d_files = glob.glob(f\"{pattern}x.D00000001\")\n",
    "    for file in d_files:\n",
    "        os.remove(file)\n",
    "\n",
    "    # Remove other files\n",
    "    extensions_to_remove = ['.fasta', '.ini', '.rsr', '.sch', '.V99990001']\n",
    "    for ext in extensions_to_remove:\n",
    "        files = glob.glob(f\"{pattern}x{ext}\")\n",
    "        for file in files:\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3356,
   "id": "1209e901-6a4d-4c34-8b18-c5fe0e6abec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147\")\n",
    "\n",
    "#path_to_pdb = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_A.pdb\"\n",
    "#start = 9\n",
    "#stop = 142\n",
    "#mini_repair_residues_monomeric_3(path_to_pdb=path_to_pdb, start=start, stop=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3357,
   "id": "70f437dc-ee23-410c-87d3-97a5e2b00747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_uniprot_id_from_rcsb(uniprot_id:str):\n",
    "    \n",
    "    link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "    \n",
    "    searchp = f\"{link_path}/{uniprot_id}\"\n",
    "    #print(searchp)\n",
    "    resp = get_url(searchp)\n",
    "    resp = resp.json()\n",
    "    \n",
    "    for pdb_id, pdb_info in resp.items():\n",
    "        for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "            return uniprot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3358,
   "id": "3c8e48f3-de16-4ea7-b467-2957ce780c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_repair_residues_oligomeric(path_to_pdb:str, start, stop):\n",
    "\n",
    "    \"\"\"\n",
    "    Function repairs structures with gaps less than 7 residues per gap.\n",
    "\n",
    "    Args:\n",
    "    - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "    - stop_pos (int): Stop position.\n",
    "    - main_prot_seq (str): Main protein sequence.\n",
    "    - use_main (bool): Whether to use the main protein.\n",
    "\n",
    "    Output:\n",
    "    Repaired structures.\n",
    "    \"\"\"\n",
    "    log.none()  # no stdout spam\n",
    "    env = Environ()  # setup env for modelling\n",
    "    aln = Alignment(env)  # setup the alignment\n",
    "    mdl = Model(env)  # setup the model\n",
    "\n",
    "    #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "    # current working directory\n",
    "    pdb_id_target = os.path.basename(path_to_pdb) #results in 2duk_A.pdb\n",
    "    \n",
    "    pdb_id_chain = pdb_id_target[5:-4] #this corresponds to chain in oligomeric struc.\n",
    "\n",
    "    first_chain, last_chain = pdb_id_chain[0], pdb_id_chain[-1]  #e.g A and B\n",
    "    \n",
    "    pdb_4_letter_code = pdb_id_target[0:4]\n",
    "\n",
    "    #if its a homo oligomer this works.\n",
    "    uniprot_id = return_uniprot_id_from_rcsb(pdb_4_letter_code)\n",
    "    \n",
    "    #get associated fasta from uniprot id.\n",
    "    fasta_seq = get_gene_fasta(uniprot_id)\n",
    "\n",
    "        \n",
    "    #code to be passed to mdl.read\n",
    "    #start = min(startlst)\n",
    "    \n",
    "    #stop = max(stoplst)\n",
    "\n",
    "    \n",
    "    fasta_seq = fasta_seq[(start-1):(stop+1)]\n",
    "    \n",
    "    #print(os.getcwd())\n",
    "    #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "\n",
    "    \n",
    "    merged_fasta = '/'.join([fasta_seq] * len(pdb_id_chain))  #this is our input for oligomers.\n",
    "    \n",
    "    #now we need to loop \n",
    "    \n",
    "    code = f\"{pdb_4_letter_code}_{pdb_id_chain}\"  #e.g 4hfq_AB\n",
    "    \n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{first_chain}\", f\"{stop}:{last_chain}\"))\n",
    "    \n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "    \n",
    "    with open(f\"./{pdb_4_letter_code}_{pdb_id_chain}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">{pdb_4_letter_code}_{pdb_id_chain}x\\n\")\n",
    "        fastaout.write(merged_fasta)\n",
    "        \n",
    "    aln_code = f\"{pdb_4_letter_code}_{pdb_id_chain}x\"\n",
    "    \n",
    "    aln.append(file=f\"./{pdb_4_letter_code}_{pdb_id_chain}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50), alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    aln.write(file=f\"{pdb_4_letter_code}_{pdb_id_chain}.ali\")\n",
    "\n",
    "    \n",
    "    a = AutoModel(env, alnfile=f\"{pdb_4_letter_code}_{pdb_id_chain}.ali\", knowns=f\"{pdb_4_letter_code}_{pdb_id_chain}\", sequence=aln_code)\n",
    "    \n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "\n",
    "    try:\n",
    "        # Build the model(s)\n",
    "        a.make();\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during modeling: {e}\")\n",
    "\n",
    "\n",
    "    pdb_code_name = f\"{pdb_4_letter_code}_{pdb_id_chain}\"\n",
    "\n",
    "    #try this later.\n",
    "    remove_repair_artefacts(pdb_basep=path_to_pdb, pdb_code_name=pdb_code_name)\n",
    "\n",
    "    #now lets select only c alpha\n",
    "    select_c_alpha(path_to_pdb)\n",
    "    \n",
    "    #renumber now based on old numbering.\n",
    "    renumber_structure_oligomeric(path_to_pdb, start=start, stop=stop,chain=pdb_id_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3359,
   "id": "2f4f42b1-460d-4fcb-acca-f9f845d117d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struc_stop(path_to_pdb):\n",
    "    \n",
    "    parser = PDBParser()\n",
    "\n",
    "    structure = parser.get_structure(\"none\", path_to_pdb)\n",
    "\n",
    "    seq_ids = [x.get_id()[1] for x in structure.get_residues()]\n",
    "\n",
    "    seq_ids = sorted(seq_ids)\n",
    "    \n",
    "    return seq_ids[0], seq_ids[-1] #this corresponds to the last residue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3360,
   "id": "33c26fcd-0492-402f-87da-40d17347b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_repair_residues_monomeric_3(path_to_pdb:str, start, stop):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function repairs structures with gaps less than 7 residues per gap.\n",
    "\n",
    "    Args:\n",
    "    - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "    - stop_pos (int): Stop position.\n",
    "    - main_prot_seq (str): Main protein sequence.\n",
    "    - use_main (bool): Whether to use the main protein.\n",
    "\n",
    "    Output:\n",
    "    Repaired structures.\n",
    "    \"\"\"\n",
    "    log.none()  # no stdout spam\n",
    "    env = Environ()  # setup env for modelling\n",
    "    aln = Alignment(env)  # setup the alignment\n",
    "    mdl = Model(env)  # setup the model\n",
    "\n",
    "\n",
    "    #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "    # current working directory\n",
    "    pdb_id_target = os.path.basename(path_to_pdb) #results in 2duk_A.pdb\n",
    "\n",
    "    \n",
    "    pdb_id_chain = pdb_id_target[5] #this corresponds to chain in single monomeric struc.\n",
    "\n",
    "    #pdb_code_name is passed to modeller later.\n",
    "    pdb_code_name = pdb_id_target[:6] #2duk_A\n",
    "\n",
    "    pdb_4_digit_id = pdb_id_target[0:4]\n",
    "\n",
    "    #get uniprot_id second.\n",
    "    uniprot_id = return_uniprot_id_from_rcsb(pdb_4_digit_id)\n",
    "\n",
    "    #get associated fasta from uniprot id.\n",
    "    fasta_seq = get_gene_fasta(uniprot_id)\n",
    "\n",
    "    #lets try this.\n",
    "    fasta_seq = fasta_seq[(start-1):(stop+1)]\n",
    "\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "\n",
    "    #code to be passed to mdl.read\n",
    "    code = f\"{pdb_code_name}\"\n",
    "\n",
    "    #here we pass out model to the mdl. object.\n",
    "\n",
    "    #safety_check.. we need to parse and obtain info if there is even this stop position in the structure.\n",
    "\n",
    "    start_struc, stop_struc = get_struc_stop(path_to_pdb)\n",
    "    print(stop_struc)\n",
    "    \n",
    "    if stop > stop_struc:\n",
    "        stop = stop_struc\n",
    "\n",
    "    if start < start_struc:\n",
    "        start = start_struc\n",
    "    \n",
    "    \"\"\"model_segment specifies the range we look into. Ideally we look for start - stop based on majority vote. Chain is always the same in monomeric\"\"\"\n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{pdb_id_chain}\", f\"{stop}:{pdb_id_chain}\"))\n",
    "\n",
    "    #append model object to alignment object.\n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "\n",
    "    \n",
    "    with open(f\"./{pdb_code_name}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">{pdb_code_name}x\\n\")\n",
    "        fastaout.write(fasta_seq)\n",
    "\n",
    "    aln_code = f\"{pdb_code_name}x\"\n",
    "\n",
    "\n",
    "    #align fasta file to our alignment object which contains now a fasta sequence and a structure object.\n",
    "    aln.append(file=f\"./{pdb_code_name}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "\n",
    "    # Additional debugging: Print alignment content\n",
    "    #print(\"Alignment content before salign:\")\n",
    "    #for record in aln:\n",
    "        #print(record.code)\n",
    "\n",
    "    #align sequence to structure.\n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50), alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    #print(\"Alignment content after salign:\")\n",
    "    #for record in aln:\n",
    "    #    print(record.code)\n",
    "\n",
    "    #write out alignmentfile for automodell usage later\n",
    "    aln.write(file=f\"{pdb_code_name}.ali\")\n",
    "\n",
    "    # Debugging: Print the contents of the alignment file\n",
    "    #with open(f\"original_{pdb_id_target}.ali\", \"r\") as ali_file:\n",
    "    #    print(\"Alignment file contents:\")\n",
    "    #    print(ali_file.read())\n",
    "    \n",
    "    # Debugging: Print ALIGN_CODES(1) and the sequence identifiers in the alignment file\n",
    "    #print(\"here we go\")\n",
    "\n",
    "    #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "\n",
    "    #print(\"Sequence identifiers in the alignment file:\")\n",
    "\n",
    "    #for record in aln:\n",
    "    #    print(record.code)\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    #print(f\"{pdb_id_target}.ali\")\n",
    "    #print(f\"{pdb_id_target}\")\n",
    "    #print(aln_code)\n",
    "    \n",
    "    a = AutoModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=f\"{pdb_code_name}\", sequence=aln_code)\n",
    "    \n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "\n",
    "    try:\n",
    "        # Build the model(s)\n",
    "        a.make();\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during modeling: {e}\")\n",
    "\n",
    "\n",
    "    #this worked.. now we need to clean all files that are no longer required\n",
    "\n",
    "\n",
    "    #try this later.\n",
    "    remove_repair_artefacts(pdb_basep=path_to_pdb, pdb_code_name=pdb_code_name)\n",
    "\n",
    "    #now lets select only c alpha\n",
    "    select_c_alpha(path_to_pdb)\n",
    "\n",
    "    #renumber now based on old numbering.\n",
    "    renumber_structure_monomeric(path_to_pdb, start=start, chain=pdb_id_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3361,
   "id": "e5a9bb38-31d9-4b38-9dcf-02f6c0acf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_structure_oligomeric(path_to_pdb:str, start:int, stop:int, chain:str):\n",
    "\n",
    "    shiftres_location = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py\"\n",
    "\n",
    "    shift = start\n",
    "    for chains in chain:\n",
    "        \n",
    "        bash_cmd = f\"python {shiftres_location} {path_to_pdb} {-(shift-1)} {chains}\"\n",
    "    \n",
    "        bash_cmd_rdy = bash_cmd.split()\n",
    "\n",
    "        with open(f\"{path_to_pdb}_tmp\", \"w\") as fh_tmp:\n",
    "            result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                 universal_newlines=True)\n",
    "    \n",
    "        #now replace the original one with the temp file.\n",
    "        os.replace(f\"{path_to_pdb}_tmp\", f\"{path_to_pdb}\")\n",
    "\n",
    "        shift += stop+1  #1 for each chain.  because we need to substract 1 for each additional chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3362,
   "id": "58ded363-50bb-4e7d-b59d-dc43a35b3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_structure_monomeric(path_to_pdb:str, start:int, chain:str):\n",
    "\n",
    "    shiftres_location = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py\"\n",
    "\n",
    "    bash_cmd = f\"python {shiftres_location} {path_to_pdb} {start-1} {chain}\"\n",
    "    \n",
    "    bash_cmd_rdy = bash_cmd.split()\n",
    "    \n",
    "    with open(f\"{path_to_pdb}_tmp\", \"w\") as fh_tmp:\n",
    "        result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "             universal_newlines=True)\n",
    "    \n",
    "    #now replace the original one with the temp file.\n",
    "    os.replace(f\"{path_to_pdb}_tmp\", f\"{path_to_pdb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3363,
   "id": "cba33cf7-9c74-46eb-a5b6-57488f9e76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_repair_residues_2(path_to_pdb:str, stop_pos:int, main_prot_seq:str, use_main:bool):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function repairs structures with gaps less than 7 residues per gap.\n",
    "\n",
    "    Args:\n",
    "    - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "    - stop_pos (int): Stop position.\n",
    "    - main_prot_seq (str): Main protein sequence.\n",
    "    - use_main (bool): Whether to use the main protein.\n",
    "\n",
    "    Output:\n",
    "    Repaired structures.\n",
    "    \"\"\"\n",
    "    log.none()  # no stdout spam\n",
    "    env = Environ()  # setup env for modelling\n",
    "    aln = Alignment(env)  # setup the alignment\n",
    "    mdl = Model(env)  # setup the model\n",
    "    \n",
    "    # current working directory\n",
    "    pdb_id_target = path_to_pdb.split(\"/\")[-1][9:-4]\n",
    "    \n",
    "    pdb_basep = \"/\".join(path_to_pdb.split(\"/\")[:-1])\n",
    "    #print(pdb_basep)\n",
    "\n",
    "    # this step needs to be done before calling this function! os.chdir(pdb_basep)\n",
    "    pdb_id_chain = pdb_id_target[5]\n",
    "\n",
    "    print(f\"Using main: {use_main}, PDB target: {pdb_id_target}\")\n",
    "\n",
    "    if use_main:\n",
    "        \n",
    "        fasta_seq = main_prot_seq\n",
    "        \n",
    "    else:\n",
    "\n",
    "        gene_name = get_gene_name_uniprot(f\"{pdb_id_target[0:4]}\")\n",
    "        \n",
    "        get_prot_name = get_uniprot_id(gene_name)\n",
    "        \n",
    "        fasta_seq = get_gene_fasta(get_prot_name)\n",
    "\n",
    "    #pdb_basep}/original_{pdb_id_target}.pdb  current dir .\n",
    "    \n",
    "    # start stop grab:\n",
    "    pdb_parser = PDBParser(QUIET=True)\n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", f\"{pdb_basep}/original_{pdb_id_target}.pdb\")\n",
    "    sample_res = sample_structure.get_residues()\n",
    "    sample_list = [x.get_id()[1] for x in sample_res]\n",
    "    start = sample_list[0]\n",
    "    stop = len(fasta_seq)\n",
    "\n",
    "    #print(start, stop)\n",
    "\n",
    "    #works here\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "    \n",
    "    code = f\"original_{pdb_id_target}\"\n",
    "    \n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{pdb_id_chain}\", f\"{stop}:{pdb_id_chain}\"))\n",
    "    \n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "\n",
    "    with open(f\"./original_{pdb_id_target}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">original_{pdb_id_target}x\\n\")\n",
    "        fastaout.write(fasta_seq)\n",
    "\n",
    "    aln_code = f\"original_{pdb_id_target}x\"\n",
    "    \n",
    "    #works\n",
    "    #print(\"it is still working here\")\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    \n",
    "    aln.append(file=f\"./original_{pdb_id_target}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "\n",
    "    # Additional debugging: Print alignment content\n",
    "    #print(\"Alignment content before salign:\")\n",
    "    #for record in aln:\n",
    "        #print(record.code)\n",
    "        \n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50), alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    #print(\"Alignment content after salign:\")\n",
    "    #for record in aln:\n",
    "    #    print(record.code)\n",
    "    \n",
    "    aln.write(file=f\"original_{pdb_id_target}.ali\")\n",
    "\n",
    "    # Debugging: Print the contents of the alignment file\n",
    "    #with open(f\"original_{pdb_id_target}.ali\", \"r\") as ali_file:\n",
    "    #    print(\"Alignment file contents:\")\n",
    "    #    print(ali_file.read())\n",
    "    \n",
    "    # Debugging: Print ALIGN_CODES(1) and the sequence identifiers in the alignment file\n",
    "    #print(\"here we go\")\n",
    "\n",
    "    #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "\n",
    "    #print(\"Sequence identifiers in the alignment file:\")\n",
    "\n",
    "    #for record in aln:\n",
    "    #    print(record.code)\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    #print(f\"{pdb_id_target}.ali\")\n",
    "    #print(f\"{pdb_id_target}\")\n",
    "    #print(aln_code)\n",
    "\n",
    "    a = AutoModel(env, alnfile=f\"original_{pdb_id_target}.ali\", knowns=f\"original_{pdb_id_target}\", sequence=aln_code)\n",
    "    \n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "\n",
    "    try:\n",
    "        # Build the model(s)\n",
    "        a.make();\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during modeling: {e}\")\n",
    "\n",
    "\n",
    "    #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    remove_repair_artefacts(pdb_basep=pdb_basep, pdb_id_target=pdb_id_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3364,
   "id": "dde0fbdb-9a76-4040-923c-81692a876e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_repair_residues_1(path_to_pdb:str, stop_pos:int, main_prot_seq:str, use_main:bool):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function repairs structures with gaps less than 7 residues per gap.\n",
    "\n",
    "    Args:\n",
    "    - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "    - stop_pos (int): Stop position.\n",
    "    - main_prot_seq (str): Main protein sequence.\n",
    "    - use_main (bool): Whether to use the main protein.\n",
    "\n",
    "    Output:\n",
    "    Repaired structures.\n",
    "    \"\"\"\n",
    "    \n",
    "    log.none()  # no stdout spam\n",
    "    env = Environ()  # setup env for modelling\n",
    "    aln = Alignment(env)  # setup the alignment\n",
    "    mdl = Model(env)  # setup the model\n",
    "    \n",
    "    # current working directory\n",
    "    current_pth = os.getcwd()\n",
    "\n",
    "    pdb_id_target = path_to_pdb.split(\"/\")[-1][9:-4]\n",
    "    pdb_basep = \"/\".join(path_to_pdb.split(\"/\")[:-1])\n",
    "\n",
    "    pdb_id_chain = pdb_id_target[5]\n",
    "\n",
    "    print(f\"Using main: {use_main}, PDB target: {pdb_id_target}\")\n",
    "\n",
    "    if use_main:\n",
    "        \n",
    "        fasta_seq = main_prot_seq\n",
    "        \n",
    "    else:\n",
    "\n",
    "        gene_name = get_gene_name_uniprot(f\"{pdb_id_target[0:4]}\")\n",
    "        \n",
    "        get_prot_name = get_uniprot_id(gene_name)\n",
    "        \n",
    "        fasta_seq = get_gene_fasta(get_prot_name)\n",
    "\n",
    "    #works here\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(f\"{pdb_basep}/{pdb_id_target}\")\n",
    "        \n",
    "    except FileExistsError:\n",
    "        \n",
    "        print(\"Directory already exists\")\n",
    "\n",
    "    os.chdir(f\"{pdb_basep}/{pdb_id_target}\")\n",
    "    \n",
    "    shutil.copy(f\"{pdb_basep}/original_{pdb_id_target}.pdb\", f\"{pdb_basep}/{pdb_id_target}/{pdb_id_target}.pdb\")\n",
    "    \n",
    "    # start stop grab:\n",
    "    pdb_parser = PDBParser(QUIET=True)\n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", f\"{pdb_basep}/{pdb_id_target}/{pdb_id_target}.pdb\")\n",
    "    sample_res = sample_structure.get_residues()\n",
    "    sample_list = [x.get_id()[1] for x in sample_res]\n",
    "    start = sample_list[0]\n",
    "    stop = len(fasta_seq)\n",
    "\n",
    "    print(start, stop)\n",
    "\n",
    "    #works here\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "    \n",
    "    code = f\"{pdb_id_target}\"\n",
    "    \n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{pdb_id_chain}\", f\"{stop}:{pdb_id_chain}\"))\n",
    "    \n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "\n",
    "    with open(f\"./{pdb_id_target}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">{pdb_id_target}x\\n\")\n",
    "        fastaout.write(fasta_seq)\n",
    "\n",
    "    aln_code = f\"{pdb_id_target}x\"\n",
    "    \n",
    "    #works\n",
    "    #print(\"it is still working here\")\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    \n",
    "    aln.append(file=f\"./{pdb_id_target}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "\n",
    "    # Additional debugging: Print alignment content\n",
    "    #print(\"Alignment content before salign:\")\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "        \n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50), alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    #print(\"Alignment content after salign:\")\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "    \n",
    "    aln.write(file=f\"{pdb_id_target}.ali\")\n",
    "\n",
    "    # Debugging: Print the contents of the alignment file\n",
    "    with open(f\"{pdb_id_target}.ali\", \"r\") as ali_file:\n",
    "        print(\"Alignment file contents:\")\n",
    "        print(ali_file.read())\n",
    "    \n",
    "    # Debugging: Print ALIGN_CODES(1) and the sequence identifiers in the alignment file\n",
    "    #print(\"here we go\")\n",
    "\n",
    "    #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "\n",
    "    #print(\"Sequence identifiers in the alignment file:\")\n",
    "\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    #print(f\"{pdb_id_target}.ali\")\n",
    "    #print(f\"{pdb_id_target}\")\n",
    "    #print(aln_code)\n",
    "\n",
    "    a = AutoModel(env, alnfile=f\"{pdb_id_target}.ali\", knowns=f\"{pdb_id_target}\", sequence=aln_code)\n",
    "    j = job(host='localhost')\n",
    "    for i in range(12):\n",
    "        j.append(local_slave())\n",
    "    a.use_parallel_job(j)\n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "\n",
    "    try:\n",
    "        # Build the model(s)\n",
    "        a.make()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during modeling: {e}\")\n",
    "\n",
    "    \n",
    "    os.chdir(current_pth)\n",
    "\n",
    "    pdbs_sorted = sorted([f for f in os.listdir(f\"{pdb_basep}/{pdb_id_target}\") if f.endswith(\".pdb\")], key=len)\n",
    "    \n",
    "    print(\"this is pdb sorted:\")\n",
    "    print(pdbs_sorted)\n",
    "    \n",
    "    print(f\"we copy now from {pdb_basep}/{pdb_id_target}/{pdbs_sorted[1]} to: {pdb_basep}/{pdb_id_target}/original_{pdbs_sorted[1]}\")\n",
    "    shutil.copy(f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[1]}\", f\"{pdb_basep}/{pdb_id_target}/original_{pdbs_sorted[1]}\")\n",
    "    \n",
    "    os.rename(f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[1]}\", f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[0]}\")\n",
    "\n",
    "    shutil.copy(f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[0]}\", f\"{pdb_basep}/original_{pdbs_sorted[0]}\")\n",
    "    os.chdir(current_pth)\n",
    "    print(f\"we remove : {pdb_basep}/{pdb_id_target}\")\n",
    "    shutil.rmtree(f\"{pdb_basep}/{pdb_id_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3365,
   "id": "d7a31025-3d2a-454a-b260-831b4d78489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _new_cut_nums(start:int, stop:int, gap_dict:dict):\n",
    "    \n",
    "    for keys, vals in gap_dict.items():\n",
    "        start_n = keys[0]\n",
    "        stop_n = keys[1]\n",
    "        \n",
    "        if start_n < start and not start_n < 0:\n",
    "            start = start_n\n",
    "        \n",
    "        if stop_n > stop:\n",
    "            stop = stop_n\n",
    "        \n",
    "        #in case its negative. we make it 1.\n",
    "        if start < 0:\n",
    "            start = 1\n",
    "    \n",
    "    print(start, stop)\n",
    "    return start, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3366,
   "id": "dff54110-4ab9-4267-ab19-27ada84d5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_pdb_test = \"/home/micnag/result_test_struc_align\"\n",
    "\n",
    "\n",
    "#gaps = _gap_localization(\"/home/micnag/result_test_struc_align/2oa0.pdb_aligned.pdb\")\n",
    "#_gap_alignment(path_to_pdb_test, gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3367,
   "id": "a9c35760-ed43-48f5-b129-273b1e35a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_pdbs = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "#template = f\"{path_to_pdbs}/3w5b_A_CA.pdb\"\n",
    "\n",
    "#path_to_pdbs = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/\"\n",
    "#template = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/2c88.pdb\"\n",
    "\n",
    "#structure_based_cutting(path_to_pdbs=path_to_pdbs, \n",
    "#                       template=template)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c392d7-c74f-4025-909c-7ee412905661",
   "metadata": {
    "tags": []
   },
   "source": [
    "# msa_file_readin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3368,
   "id": "e60c9425-c4d3-4531-887e-1f6272593ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_msa_file_version_1(path:str):\n",
    "    \n",
    "    \"\"\"Here we need to fetch information from N oligomer alignment files each containing info about 1 structure\n",
    "    being aligned towards the template structure.\"\"\"\n",
    "    \n",
    "    msa_dir = f\"{path}/MSA_dir\"\n",
    "    \n",
    "    only_msa = [f for f in os.listdir(msa_dir) if os.path.isfile(os.path.join(msa_dir, f))]\n",
    "    \n",
    "    #with this version, we need to analyze n - 1 alignment files for n structures with a designated template structure.\n",
    "    \n",
    "    seq_dict = defaultdict(list)\n",
    "    \n",
    "    for files in only_msa:\n",
    "        \n",
    "        headers = []\n",
    "        seqs = []\n",
    "        \n",
    "        #each files needs to be read out.\n",
    "        #each file contains fluffer + 2 sequences. ALL chains against all chains. First seq is always template.\n",
    "        \n",
    "        seq = False\n",
    "        \n",
    "        with open(f\"{msa_dir}/{files}\", \"r\") as msa: \n",
    "            i = 0\n",
    "            for lines in msa:\n",
    "                if lines[0:4] == \"Name\":\n",
    "            \n",
    "                    header_line = lines.split(\"/\")  #split at whitespace \n",
    "                    pdb_code = header_line[-1][0:4]\n",
    "                    \n",
    "                    #this looks good\n",
    "                    \n",
    "                    headers.append(pdb_code)\n",
    "                    \n",
    "                if lines[0:4] == '(\":\"':\n",
    "                    seq = True\n",
    "                    continue\n",
    "                #print(\"this is apparent correct line\")\n",
    "                #print(correct_lines)\n",
    "                \n",
    "                if seq:\n",
    "                    \n",
    "                    lines = lines.replace(\"*\",\"\")\n",
    "                    sequence = lines.replace(\"\\n\",\"\")\n",
    "                    \n",
    "                    tot_seq = len(sequence)\n",
    "                    removed_gap = sequence.replace(\"-\",\"\")\n",
    "                    remov_seq = len(removed_gap)\n",
    "                    \n",
    "                    seqs.append(sequence)\n",
    "                    \n",
    "               \n",
    "            #print(headers)\n",
    "            #print(seqs)\n",
    "            #ugly solution but only 0 and 2 are seq rest is filler. 2 will be the template 0 will be the structure thats superimposed onto\n",
    "            #template\n",
    "            seqs = [seqs[0], seqs[2]]\n",
    "            \n",
    "            #print(headers)\n",
    "            #print(seqs)\n",
    "            \n",
    "            for pdb_codes, seqs in zip(headers, seqs):\n",
    "                #print(pdb_codes, seqs)\n",
    "                seq_dict[pdb_codes].append(seqs)\n",
    "                \n",
    "            \n",
    "            \n",
    "    #print(seq_dict.keys())\n",
    "    \n",
    "\n",
    "    #seq dict will contain n times template seq and n-1 other sequences.\n",
    "    #we will use this dict to cut out proper positions.\n",
    "    \n",
    "    \n",
    "    '''check all gap positions.'''\n",
    "    \n",
    "    gaps_redundant = []\n",
    "    deletion_list = []\n",
    "\n",
    "    \n",
    "    for pdbs, seqs in seq_dict.items():\n",
    "        i = 0   #we count through the whole seq but only add gaps to the gaps_redundant list.\n",
    "        #print(pdbs, seqs)\n",
    "        print(len(seqs))\n",
    "        if len(seqs) > 1:  #thats our template. because all others will have 1 and seqs will have n seqs (corresponding to n comparisons\n",
    "            #between template and pdb)\n",
    "            #for seqlst in seqs:\n",
    "            #    seqlst_without_gaps = seqlst.replace(\"-\",\"\")\n",
    "            #    \n",
    "            #    gaplen = len(seqlst)-len(seqlst_without_gaps)\n",
    "            #    if gaplen > 0.01 * len(seqlst):\n",
    "            #        \n",
    "            #        #we remove the structure:\n",
    "            #        #print(pdbs)\n",
    "            #        #os.remove()\n",
    "            #        continue\n",
    "            #    i = 1\n",
    "            #    for seq in seqlst:\n",
    "            #        if seq == \"-\":\n",
    "            #            if i not in gaps_redundant:\n",
    "            #                gaps_redundant.append(i)\n",
    "            #            \n",
    "            #            i += 1\n",
    "            #            continue\n",
    "            #        else:\n",
    "            #            i += 1\n",
    "            continue\n",
    "                    \n",
    "        else:\n",
    "            #print(seqs)\n",
    "            for seqlst in seqs:\n",
    "                \n",
    "                gapless = seqlst.replace(\"-\",\"\")\n",
    "                withgap = len(seqlst)\n",
    "                withoutgap = len(gapless)\n",
    "                print(withgap, withoutgap)\n",
    "                if (withgap - withoutgap) > 0.10 * withoutgap: #here we can experiment how strict we want to be.\n",
    "                    print(\"this is too much gap\")\n",
    "                    print(pdbs)\n",
    "                    print(withgap, withoutgap)\n",
    "                    deletion_list.append(pdbs)\n",
    "                    continue\n",
    "                for aa in seqlst:\n",
    "                    if aa == \"-\":  #which means there is a gap.\n",
    "                        #print(aa)\n",
    "                        if i not in gaps_redundant:\n",
    "                            gaps_redundant.append(i)\n",
    "                        \n",
    "                        i += 1\n",
    "                        continue\n",
    "                    i += 1\n",
    "    \n",
    "    \n",
    "    gaps_to_remove = sorted(list(set(gaps_redundant)))  #set to make sure there are no duplicates.\n",
    "        \n",
    "    #print(\"apparent gaps to remove\")\n",
    "    #print(gaps_to_remove)\n",
    "    \n",
    "    return gaps_to_remove, deletion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3369,
   "id": "c4e47ca2-6ab0-46a4-8d33-06dd6aac8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#listlen = read_msa_file_version_1(path=path)\n",
    "#print(len(listlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3370,
   "id": "e48fdeb6-9e0f-4433-89f2-a821a3bbcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_deviating_length_strucs(path_to_pdbs:str, template_struc:str):\n",
    "    \n",
    "    \"\"\"This function will take in the pdbs in the directory of interest and remove those that are simply too large.\n",
    "    If they are too small this will also be removed.\n",
    "    All length criteria are based on the selected reference structure.\"\"\"\n",
    "    \n",
    "    \n",
    "    #first the template_struc to set length standard.\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    \n",
    "    prot_path = f\"{path_to_pdbs}/{template_struc}\"\n",
    "    \n",
    "    print(prot_path)\n",
    "    prot_name = \"default\"\n",
    "    \n",
    "    seq_length_ref = 0\n",
    "    try:\n",
    "        \n",
    "        structure = parser.get_structure(prot_name, prot_path)\n",
    "        \n",
    "        seq_length_ref = len([x.get_id()[1] for x in structure.get_residues()])\n",
    "\n",
    "            \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #now we parse through all of the rest and remove those that are less/more than 20% of the ref struc length.\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    \n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #we keep track of which is the shortest ref seq because we want to cut down later based on that size\n",
    "    #to end up with uniform lengths.\n",
    "    \n",
    "    shortes_seq_len = seq_length_ref\n",
    "    #now we remove all pdbs that dont fullfill the above criteria.\n",
    "    for pdb in pdbs:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            seq_len = 0\n",
    "            \n",
    "            prot_path = f\"{path_to_pdbs}/{pdb}\"\n",
    "            \n",
    "            prot_name = \"default\"\n",
    "            \n",
    "            structure = parser.get_structure(prot_name, prot_path)\n",
    "        \n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    for residue in chain:\n",
    "                        seq_len += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(pdb, seq_len)\n",
    "            #if this is the case, we remove it.\n",
    "            if seq_len < 0.9 * seq_length_ref or seq_len > 1.1 * seq_length_ref:  #here we can experiment how strict we want to be.\n",
    "                \n",
    "                print(f\"we remove {pdb} because its seq_len is: {seq_len} and the refsef is : {seq_length_ref}\")  \n",
    "                os.remove(f\"{path_to_pdbs}/{pdb}\")\n",
    "                \n",
    "            else:\n",
    "                #check if its shorter than the current shortest seq . This block only counts those that are not rejected\n",
    "                #i.e nothing thats 80% or less than refseq.\n",
    "                if seq_len < shortes_seq_len:\n",
    "                    shortes_seq_len = seq_len\n",
    "                \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "    return shortes_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3371,
   "id": "52c8e085-7c93-4443-ab54-4313f384357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mutate_non_standard_aa_1(path_to_pdb:str,\n",
    "                           non_standard_residue:str,\n",
    "                           residue:int,\n",
    "                           chain:str):\n",
    "    \n",
    "    \"\"\"Mutates a non-standard amino acid in a PDB file.\"\"\"\n",
    "    \n",
    "    path_to_script = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_mutate.py\"\n",
    "    path_to_error_script = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_delresname.py\"\n",
    "    path_to_tidy = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_tidy.py\"\n",
    "\n",
    "    input_file = f\"{path_to_pdb}_new\"\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, \"w\") as pdb_out:\n",
    "            bash_code = f\"python {path_to_script} {path_to_pdb} {chain} {residue} {non_standard_residue} ALA\"\n",
    "            run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        resulting_success = True\n",
    "\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(\"Attempting to delete non-standard residue...\")\n",
    "        \n",
    "        with open(input_file, \"w\") as pdb_out:\n",
    "            bash_code = f\"python {path_to_error_script} -{non_standard_residue} {path_to_pdb}\"\n",
    "            run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "    #now we tidy the file to adhere to the most common pdb standard\n",
    "    try:\n",
    "        with open(path_to_pdb, \"w\") as pdb_out:\n",
    "            bash_code = f\"python {path_to_tidy} {input_file}\"\n",
    "            run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        print(\"Cleaned the file. Outfile is at\", path_to_pdb)\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "    #we dont need the intermediate file that was only created to prevent read/write from same file.\n",
    "    try:\n",
    "        os.remove(input_file)  # Remove intermediate file\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    return resulting_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a56d3-df7b-44e6-ab8e-aa7b919c6f75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PRE PCA PROCESSING / FETCH PROPER RESIDUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3372,
   "id": "026065dd-3434-4f73-a810-2dae424226f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_structures_pca(remove_pos:list, path_to_strucs:str,\n",
    "                       max_seq_len:str, delete_pdbs:str):\n",
    "    \n",
    "    '''The kept positions are based on str position (not residue pos) so we need to\n",
    "    change each structure to 1 based residue counting \n",
    "    and then select ONLY those residues that are NOT in the remove positions range'''\n",
    "    \n",
    "    #renumber first all structures.\n",
    "    \n",
    "    pdbs = []\n",
    "    \n",
    "    with open(f\"{path_to_strucs}/chain_list.txt\", \"r\") as chain_file:\n",
    "        for lines in chain_file:\n",
    "            if lines != \"\\n\":\n",
    "                #this will contain files as \"4r3d.pdb\"\n",
    "                pdb = lines.replace(\"\\n\",\"\")\n",
    "                \n",
    "                if pdb[0:4] in delete_pdbs:\n",
    "                    print(f\"{path_to_strucs}/{pdb}\")\n",
    "                    os.remove(f\"{path_to_strucs}/{pdb}\")  # we remove it if its deemed bad.\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    pdbs.append(pdb)\n",
    "    \n",
    "    \n",
    "    #now we rechain copies.\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_strucs}/PCA\")\n",
    "    except Exception as error:\n",
    "        print(f\"{path_to_strucs}/PCA already exists\")\n",
    "    \n",
    "    \n",
    "    #loop over pdb list\n",
    "    \n",
    "    #seems to work.\n",
    "    \n",
    "    #required for the script to work.\n",
    "    \n",
    "    '''This block below needs to be tested.'''\n",
    "    \n",
    " \n",
    "    #first renumbering! because gap pos are 1 counted and start in all structures with 1 (e.g even if structure atom is normally positon 32 it will start with 1.\n",
    "    \n",
    "    pdb_reres_path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_reres.py\"\n",
    "    \n",
    "    for pdb_entries in pdbs:\n",
    "        \n",
    "        bash_cmd = f\"python {pdb_reres_path} -1 {path_to_strucs}/{pdb_entries}\"\n",
    "                \n",
    "        bash_cmd_rdy = bash_cmd.split()\n",
    "        \n",
    "        try:\n",
    "            with open(f\"{path_to_strucs}/PCA/{pdb_entries}\", \"w\") as fh_tmp:\n",
    "                result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                         universal_newlines=True)\n",
    "            \n",
    "            print(f\"we renumbered {pdb_entries}\")\n",
    "        \n",
    "        except Exception as error:\n",
    "            print(f\"we did not renumber {pdb_entries}\")\n",
    "        \n",
    "            print(error)\n",
    "    \n",
    "    #first remove the HET Atoms.\n",
    "    for pdb_entries in pdbs:\n",
    "        \n",
    "        non_canonical = remove_hetero_atoms_1(pdb_file=f\"{pdb_entries}\", path=f\"{path_to_strucs}/PCA\")\n",
    "        \n",
    "        #this means we have something special that needs to be taken care of.\n",
    "        \n",
    "        if non_canonical:\n",
    "            \n",
    "            #loop over dict that contains all positions that deviate from standard.\n",
    "            \n",
    "            #structure of dict is : key: position value: tuple: (aa_type, chain)\n",
    "            print(\"This is non canonical:\")\n",
    "            print(non_canonical)\n",
    "            \n",
    "            \n",
    "            remove_targets = []\n",
    "            fulllst = []\n",
    "            for keys, vals in non_canonical.items():\n",
    "                print(keys)\n",
    "                if keys in remove_targets:\n",
    "                    continue\n",
    "                else:\n",
    "                    remove_targets.append(keys)\n",
    "                    fulllst.append((keys, vals[0], vals[1]))\n",
    "\n",
    "            for (pos, hits, chain) in fulllst:\n",
    "                \n",
    "                print(pos, hits, chain)\n",
    "                #call function iteratively to replace step by step.\n",
    "                result = _mutate_non_standard_aa_1(path_to_pdb=f\"{path_to_strucs}/PCA/{pdb_entries}\",\n",
    "                       non_standard_residue=hits,residue=pos,chain = chain)\n",
    "                \n",
    "                if result == False:\n",
    "                    print(f\"we did not manage to delete and so we need to introduce a gap at {pos}\")\n",
    "                    #if we could not repair and were forced to delete it we append this and take it as a gap. otherwise its fine if it got \"repaired\" to ALA\n",
    "                    remove_pos.append(pos)\n",
    "                    \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(remove_pos)\n",
    "    \n",
    "\n",
    "    #here ends removal of het atoms.\n",
    "    \n",
    "    #pdb_reres_path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_reres.py\"\n",
    "    #\n",
    "    #for pdb_entries in pdbs:\n",
    "    #    \n",
    "    #    bash_cmd = f\"python {pdb_reres_path} -1 {path_to_strucs}/{pdb_entries}\"\n",
    "    #            \n",
    "    #    bash_cmd_rdy = bash_cmd.split()\n",
    "    #    \n",
    "    #    with open(f\"{path_to_strucs}/PCA/{pdb_entries}\", \"w\") as fh_tmp:\n",
    "    #        result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "    #                     universal_newlines=True)\n",
    "    #        \n",
    "            \n",
    "        \n",
    "    #now only select those residues that we need.\n",
    "    \n",
    "    \n",
    "    print(\"max seq len is:\")\n",
    "    print(max_seq_len)\n",
    "    \n",
    "    #we use this class and overwrite it for our purpose.\n",
    "    class ResidueSelect(Bio.PDB.Select):\n",
    "        def accept_residue(self, res):\n",
    "            #we accept all residues that are in the kept-position list\n",
    "            if res.id[1] in remove_pos or res.id[1] > max_seq_len:\n",
    "                return False\n",
    "            #but we reject those that are not shared between all structures.\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    \n",
    "    #now we select only those that we need and overwrite existing files.\n",
    "    for pdb_entries in pdbs:\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "                \n",
    "        prot_name = f\"default\"\n",
    "                \n",
    "        #open the correct pdb and rechain it.\n",
    "        structure_template = parser.get_structure(prot_name, \n",
    "                            f\"{path_to_strucs}/PCA/{pdb_entries}\")\n",
    "        \n",
    "        io = PDBIO()\n",
    "            \n",
    "        io.set_structure(structure_template)\n",
    "            \n",
    "        io.save(f\"{path_to_strucs}/PCA/{pdb_entries}\", ResidueSelect())\n",
    "        \n",
    "    #this return will contain the number of CAs required for the next step.\n",
    "    #second part of tuple contains the number of pdb structures in the ensemble.\n",
    "    \n",
    "    #pdbs 0 = ref struc.\n",
    "    \n",
    "    print(\"this is pdbs:\")\n",
    "    print(pdbs)\n",
    "    \n",
    "    #if its 0 we dont care any further.\n",
    "    if len(pdbs) == 0:\n",
    "        return None\n",
    "    \n",
    "    return pdbs[0] #else we return the template.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3373,
   "id": "121afd42-dbd4-4640-aa97-2beb94f0b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#max_seq_len = 10000\n",
    "#prep_structures_pca(remove_pos=[], path_to_strucs=path,\n",
    "#                       max_seq_len= max_seq_len, delete_pdbs=\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3374,
   "id": "e40a9941-604f-4a78-8924-ec01f675096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "#rem_pos = [1,2,3]\n",
    "#path_to_struc = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AACS_HUMAN/trimer/pos_16_671\"\n",
    "\n",
    "#max_seq_len = 1000\n",
    "\n",
    "\n",
    "#prep_structures_pca(path_to_strucs=path_to_struc,max_seq_len=max_seq_len, remove_pos=rem_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3375,
   "id": "5e57a63b-7a91-4c48-81ee-37a8282e18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_gaps = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 27, 28, 32, 33, 35, 36, 37, 38, 47, 49, 50, 51, 52, 53, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 97, 98, 99, 100, 101, 122, 123, 124, 125, 148, 149, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368]\n",
    "\n",
    "path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/FAK1_HUMAN/monomer/pos_410_689\"\n",
    "\n",
    "chains = \"A\"\n",
    "\n",
    "ref_struc_pdb = \"2v7a\"\n",
    "\n",
    "min_seq_len = 241\n",
    "                \n",
    "#then we will prepare those structures, cut gaps and will be left with uniform length\n",
    "#pdbs that have 1:1 correspondence between each residue against all pdbs.\n",
    "#ref_struc_pdb_id = prep_structures_pca(path_to_strucs=path, \n",
    "#                        remove_pos=remove_gaps,\n",
    "#                        max_seq_len = min_seq_len) #min_seq_len is length of shortes structure in the ensemble.\n",
    "                \n",
    "                \n",
    "#chains_of_ref = _get_chain_labels(path, ref_struc_pdb_id)\n",
    "                \n",
    "#dir_path_pca = f\"{path}/PCA/\"\n",
    "                \n",
    "                \n",
    "#PCA_domenico_new(path_to_pdbs=dir_path_pca,\n",
    "#                ref_struc_pdb_id=ref_struc_pdb_id[0:4],\n",
    "#                chains_of_reference_struc=chains_of_ref)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9ac14-2d1d-4ddc-abe6-921345a1b65a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA PLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3376,
   "id": "43b04234-095f-4c87-a344-fd6c80a394c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_domenico_new(path_to_pdbs:str, ref_struc_pdb_id:str,\n",
    "                    chains_of_reference_struc:str):\n",
    "    \n",
    "    \n",
    "    \"\"\"This function will take in the input directory with the pdb files that\n",
    "    are going to be used for PCA.\n",
    "    Domenicos pipeline consisting of 3 scripts needs to be MOVED around into the working dir\n",
    "    and then removed back after the job is done.\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    path_to_pdbs: the directory that contains the pdbs.\n",
    "    ref_struc_pdb_id: the 4 digit code OF THE REFERENCE STRUCTURE.\n",
    "    \n",
    "    exe are:\n",
    "    \n",
    "    + add_chain_label_ali\n",
    "    + run_pca_domenico\n",
    "    + write_CA\n",
    "    \n",
    "    the shellscript that combined them is:\n",
    "    + PCA_pipeline.sh\n",
    "    \n",
    "    Additional files required there are:\n",
    "    \n",
    "    + pdbs\n",
    "    + ensemble.txt\n",
    "    \n",
    "    All work needs to be done in the same directory (i.e the working dir) and we need to os.chdir() to this\n",
    "    dir and afterwords os.chdir() back.\n",
    "    \n",
    "    Execution:\n",
    "\n",
    "    bash PCA_pipeline.sh INPUT1 INPUT2\n",
    "\n",
    "    INPUT1 : pdb id (4 digits) of reference structure\n",
    "    INPUT2 : chains id (max 100 characters) of reference structure\n",
    "\n",
    "    INPUT1 and INPUT2 need to be the same as in the first row of the \"ensemble.txt\" file\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "          \n",
    "    print(\"we are inside PCA_domenico_new\")\n",
    "          \n",
    "        \n",
    "    \n",
    "    #check first which structures are present:\n",
    "    #get_rcsb_info(path=path_to_pdbs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"chains:{chains_of_reference_struc}\")\n",
    "    \n",
    "    print(f\"ref_struc_pdb:{ref_struc_pdb_id}\")\n",
    "          \n",
    "          \n",
    "    #first we need to make the working dir.\n",
    "    \n",
    "    basepath = \"/home/micnag/bioinformatics/domenico_pca\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        #move executables to this new location.\n",
    "        \n",
    "        os.chdir(f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/add_chain_label_ali\",f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/write_CA\",f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/run_pca_domenico\",f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/PCA_pipeline.sh\",f\"{path_to_pdbs}\")\n",
    "    \n",
    "        #change into new working dir.\n",
    "        os.chdir(f\"{path_to_pdbs}\")\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        os.chdir(f\"{path_to_pdbs}\")\n",
    "        \n",
    "    scriptcall = f\"./PCA_pipeline.sh\"\n",
    "    \n",
    "    \n",
    "    #first we remove all deviating CA files (in case there are any\n",
    "    _remove_deviating_ca(path_to_pdbs=path_to_pdbs,\n",
    "                        ref_struc_pdb_id=ref_struc_pdb_id)\n",
    "    \n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(f\"{path_to_pdbs}/\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/\", f))]\n",
    "    only_pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    \n",
    "    #we need to create ensemble.txt which is required by the pipeline.\n",
    "    #format : each line : <4 digit pdb> <chain(s)>\n",
    "    with open(f\"{path_to_pdbs}/ensemble.txt\", \"w\") as ensemble_txt:\n",
    "        for pdb_ids in only_pdbs:\n",
    "            pdb_id = pdb_ids[:-4]  #we just need the 4 digit ids. \n",
    "            chain = chains_of_reference_struc #this has to be the same as our reference structure.\n",
    "            \n",
    "            append_str = pdb_id + \" \" + chain + \"\\n\"\n",
    "            print('append string is:', append_str)\n",
    "            ensemble_txt.write(append_str)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(os.getcwd())\n",
    "    \n",
    "    print(\"these files are present in the directory:\")\n",
    "    \n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(f\"{path_to_pdbs}/\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/\", f))]\n",
    "    \n",
    "    print(onlyfiles)\n",
    "    \n",
    "    bash_cmd = f\"{scriptcall} {ref_struc_pdb_id} {chains_of_reference_struc}\"\n",
    "    \n",
    "    print(bash_cmd)\n",
    "    \n",
    "    bash_cmd_rdy = bash_cmd.split()\n",
    "    \n",
    "    try:\n",
    "        result = run(bash_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                 universal_newlines=True)\n",
    "            \n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "            \n",
    "    except:\n",
    "        print(f\"Script did not work. Here are the supplied parameters \\n{bash_cmd_rdy}\")\n",
    "        \n",
    "    #if it worked we restore the previous status quo:\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        #remove them from working dir back to normal storage dir.\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/add_chain_label_ali\")\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/write_CA\")\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/run_pca_domenico\")\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/PCA_pipeline.sh\")\n",
    "        \n",
    "        #change back into previous work dir.\n",
    "        os.chdir(f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs\")\n",
    "        \n",
    "    except Exception as error:\n",
    "        \n",
    "        os.chdir(f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs\")\n",
    "        print(\"Something did not work out at reshuffling .exe back into their former location.\")\n",
    "        print(error)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3377,
   "id": "82756aa8-4f9b-4e71-8f38-e382e15cccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_deviating_ca(path_to_pdbs:str, ref_struc_pdb_id:str):\n",
    "    \"\"\"helper function to remove deviating C-Alpha number files.\"\"\"\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(f\"{path_to_pdbs}/\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/\", f))]\n",
    "    only_pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    \n",
    "    print(f\"this is template path: {path_to_pdbs}/{ref_struc_pdb_id}.pdb\")\n",
    "    template_path = f\"{path_to_pdbs}/{ref_struc_pdb_id}.pdb\"\n",
    "        \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = \"noname\"\n",
    "    structure = parser.get_structure(prot_name, template_path)\n",
    "    \n",
    "    i = 0\n",
    "    for res in structure.get_atoms():\n",
    "        #we only count CA. if structures deviate from that.. remove them.\n",
    "        if res.get_id() == \"CA\":\n",
    "            i += 1\n",
    "    \n",
    "    \n",
    "    for pdbs in only_pdbs:\n",
    "        \n",
    "        try:\n",
    "            parser = PDBParser(QUIET=True)\n",
    "    \n",
    "            prot_name = \"noname\"\n",
    "            structure = parser.get_structure(prot_name, f\"{path_to_pdbs}/{pdbs}\")\n",
    "        \n",
    "            j = 0\n",
    "            for res in structure.get_atoms():\n",
    "            #we only count CA. if structures deviate from that.. remove them.\n",
    "                if res.get_id() == \"CA\":\n",
    "                    j += 1\n",
    "        \n",
    "            #this means it has different CA\n",
    "            if j != i:\n",
    "                print(pdbs)\n",
    "                print(j)\n",
    "                os.remove(f\"{path_to_pdbs}/{pdbs}\")\n",
    "                \n",
    "                \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "            #if we cant open it.. its faulty. remove it.\n",
    "            os.remove(f\"{path_to_pdbs}/{pdbs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3378,
   "id": "21139ed7-df47-4415-92a4-5bacd1426656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalize this plot function.\n",
    "\n",
    "def _plot_PCA(path_to_PCA:str):\n",
    "    \n",
    "    \"\"\"Quick function to plot the first 2 PC and their corresponding explained variances.\"\"\"\n",
    "    \n",
    "    expl_var = f\"{path_to_PCA}/pc_variances.txt\"\n",
    "    pca = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    structure_path = f\"{path_to_PCA}/ensemble.txt\"\n",
    "    \n",
    "    structures = []\n",
    "    \n",
    "    title_path = path_to_PCA.split(\"/\")\n",
    "    gene = title_path[-4]\n",
    "    oligomer = title_path[-3]\n",
    "    position = title_path[-2]\n",
    "    \n",
    "    \n",
    "    expl_var_vals = []\n",
    "    with open(expl_var, \"r\") as var:\n",
    "        for entries in var:\n",
    "            entries = entries.replace(\"\\n\",\"\")\n",
    "            expl_var_vals.append(float(entries))\n",
    "    \n",
    "    \n",
    "    with open(structure_path, \"r\") as struc_in:\n",
    "        for lines in struc_in:\n",
    "            pdb = lines.replace(\"\\n\",\" \")\n",
    "            pdb = pdb[0:4]\n",
    "            structures.append(pdb)\n",
    "            \n",
    "    #we compute the first 10.\n",
    "    PCA = defaultdict(list)\n",
    "    \n",
    "    with open(pca, \"r\") as pc_read:\n",
    "        for lines in pc_read:\n",
    "            #lets collect each column as a separate list\n",
    "            \n",
    "            #each member in the list will correspond to a structure and its associated PC value.\n",
    "            for i in range(0,10):\n",
    "                PCA[i+1].append(float(lines.split()[i]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot(PCA[1], PCA[2], \"o\")\n",
    "    \n",
    "    zipped_PCA = list(zip(PCA[1],PCA[2]))\n",
    "    \n",
    "    \n",
    "    accept_lst = [\"3w5b\", \"2oa0\", \"6hxb\"]\n",
    "\n",
    "    for i, (x,y) in enumerate(zipped_PCA):\n",
    "        \n",
    "        label = \"{}\".format(structures[i])\n",
    "        \n",
    "        if label in accept_lst:\n",
    "            print(label)\n",
    "            plt.annotate(label, # this is the text\n",
    "                 (x, y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(-10,-20), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "            print(x,y)\n",
    "            plt.plot(x,y , \"o\",color=\"red\")\n",
    "            \n",
    "    plt.title(f\"{gene}\\n{oligomer} {position}\")\n",
    "    plt.xlabel(f\"PCA 1 (variance {expl_var_vals[0]}%)\")\n",
    "    plt.ylabel(f\"PCA 2 (variance {expl_var_vals[1]}%)\")\n",
    "    \n",
    "    \n",
    "    plt.savefig(f\"{path_to_PCA}/PC_plot2.png\")\n",
    "    plt.clf()\n",
    "\n",
    "#path_to_PCA = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"   \n",
    "    \n",
    "#_plot_PCA(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3379,
   "id": "58eb5226-42c5-4ec8-b738-702299b13e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_PCA = \"/home/micnag/bioinformatics/test/test_pca\"   \n",
    "    \n",
    "#_plot_PCA(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3380,
   "id": "376712de-5844-463a-8b4b-b6ba2adf2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porcupine_plot(path_to_PCA:str):\n",
    "    \"\"\"Function to create porcupine plots\"\"\"\n",
    "    \n",
    "    \n",
    "    path_to_pca_comp = f\"{path_to_PCA}/PCs.txt\"\n",
    "    \n",
    "    path_to_pca_proj = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    pca_components = np.loadtxt(path_to_pca_comp, usecols=[0,1])\n",
    "    pca_scores = np.loadtxt(path_to_pca_proj, usecols=[0,1])\n",
    "    \n",
    "    normalized_pca_components = pca_components / np.linalg.norm(pca_components, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    #print(normalized_pca_components[0:10])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_scores[:, 0], pca_scores[:, 1], color='blue', label='Data Points')\n",
    "\n",
    "    for i in range(len(pca_scores)):\n",
    "        x0, y0 = pca_scores[i]\n",
    "        dx, dy = normalized_pca_components[i]\n",
    "        plt.arrow(x0, y0, dx, dy, color='red', width=0.02, head_width=0.1, length_includes_head=True)\n",
    "    \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Porcupine Plot - PCA Data')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3381,
   "id": "ab61ac4d-aa0d-4d44-be4a-a2a8d5beba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "\n",
    "#porcupine_plot(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df1de9-a981-4092-8400-57b425971544",
   "metadata": {
    "tags": []
   },
   "source": [
    "# REPAIR STATION MODELLER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3382,
   "id": "0827c39c-34ce-4eb8-bb49-51151a9e7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursive helper function to measure gaps\n",
    "def _catchup(idx:int, resseq:int, start:int):\n",
    "    \n",
    "    '''This helper function will start iteratively catching up to the gap by incrementing\n",
    "    each iteration by 1 until the idx matches the current resseq\n",
    "    The amount of iterations needed to get back to the resseq is returned together\n",
    "    with the start and end of the gap.'''\n",
    "    \n",
    "    #if we have a gap.\n",
    "    if idx != resseq:\n",
    "        #close gapsize by 1 and recall function recursively.\n",
    "        idx += 1\n",
    "        return _catchup(idx,resseq, start) #needs function call otherwise return:None\n",
    "    \n",
    "    #if we sealed the gap we return the report.\n",
    "    return(idx, resseq, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3383,
   "id": "fa8add00-b42d-4a9a-82dc-a354dd8e90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gaps_single_struc(path:str, report=True)-> dict:\n",
    "    ''' This function should take all pdbs (already removed from het atoms)\n",
    "        and sort them according to a fixed max_gap parameter.\n",
    "        If this gap length is exceeded, we trash the structure.\n",
    "        Otherwise, we separate in two folders:\n",
    "        structures that are completely intact and those that need repair\n",
    "        but have gaps smaller than max_gap.\n",
    "        If report = True we will also write out a report documenting the gaps.'''\n",
    "    \n",
    "    #setup\n",
    "    #set path to folderpath\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_path = path.split(sep=\"/\")\n",
    "    \n",
    "    #last entry of path is 4r23_A_0.pdb. so we need 4r23_A_0 \n",
    "    prot_name = prot_path[-1][:-4]\n",
    "    \n",
    "    '''main part'''\n",
    "    #print(onlyfiles)\n",
    "    \n",
    "    gap_dict = defaultdict(str)\n",
    "    \n",
    "\n",
    "    fullpath = f\"{path}\"   # ../P0633/1c0f.pdb\n",
    "    \n",
    "    try:\n",
    "        structure = parser.get_structure(prot_name, fullpath)\n",
    "        \n",
    "        #we will return the gaps list at the end of function (explained there at return)\n",
    "        gaps = []\n",
    "        start_res = []\n",
    "        '''Just used to grab the first residue in each chain!'''\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                chains = chain.get_residues()\n",
    "                for residue in chains:\n",
    "                    tmp = residue.get_full_id()[3][1]\n",
    "                    start_res.append(tmp)\n",
    "                    break\n",
    "       \n",
    "        i = 0 #we iterate through the list of start_res given that we can have many chains!\n",
    "        # e.g each chain can start with a different residue... e.g pos 5 pos 10 ect.\n",
    "        for model in structure:\n",
    "            \n",
    "            for chain in model:\n",
    "                \n",
    "                #we store the chain id for later purpose of repairing\n",
    "                chain_id = chain.get_full_id()[2]\n",
    "                \n",
    "                idx = start_res[i] #get first residue position\n",
    "                \n",
    "                i += 1 #shift pointer by 1 to get next start residue pos in next chain iteration.\n",
    "                \n",
    "                \"\"\"THIS PART NEEDS MAJOR OVERHAUL!\"\"\" #done mostly\n",
    "                for res in chain.get_residues():\n",
    "                \n",
    "                    resseq = res.get_full_id()[3][1]\n",
    "                    #print(idx, resseq)\n",
    "                    \n",
    "                    if idx != resseq: #this means we have a gap!\n",
    "\n",
    "                        if idx > resseq:\n",
    "                            continue\n",
    "                        #store start of gap for later return\n",
    "                        start = idx\n",
    "                    \n",
    "                        #we try to catch it then\n",
    "                        res = _catchup(idx,resseq, start) #recursive call\n",
    "                    \n",
    "                        diffval = res[0]-res[2] #diff between end of gap and start of gap = gaplength\n",
    "                        \n",
    "                        gaps.append((chain_id, res[2]-1, res[0], diffval)) # end - start\n",
    "                        #print(gaps)\n",
    "                        idx = res[0]+1 #update to bring idx back to current resseq\n",
    "                        continue\n",
    "                    \n",
    "                    idx += 1\n",
    "                    if idx < resseq:\n",
    "                        print(f\"idx is {idx}, resseq is {resseq}\")\n",
    "                        break\n",
    "                        \n",
    "            gap_dict[prot_name] = gaps\n",
    "    except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "        \n",
    "    return gap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3384,
   "id": "ce323029-996e-42c4-80f9-54c57550b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_viability(gap_dict:dict, max_gap=7):\n",
    "    \n",
    "    \"\"\"Quick check if the pdb can be salvaged or if it should be left as it is.\"\"\"\n",
    "    \n",
    "    gap_lengths = []\n",
    "    \n",
    "    for pdb, gaps in gap_dict.items():\n",
    "        \n",
    "        if len(gaps) == 0:\n",
    "            return False\n",
    "        for gap in gaps:\n",
    "            single_gap = gap[3]\n",
    "            gap_lengths.append(single_gap)\n",
    "            \n",
    "    gap_lengths_sort = sorted(gap_lengths, reverse=True)\n",
    "    \n",
    "    #if we find that the max gap is too large we return False else True\n",
    "    return False if gap_lengths_sort[0] > max_gap else True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d033d-9543-4ac3-abdf-bda1e48b7a50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## REPAIR\n",
    "\n",
    "+ build model from fasta files\n",
    "+ rechain new model \n",
    "+ cut according to residues that we need (i.e remove N and C-terminal overhangs, which is only spaghetti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3385,
   "id": "b112a4b6-b1f3-4852-8d49-e28cfc357ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _start_stop_fasta(pdb_id_target:str, path:str):\n",
    "    \n",
    "    \"\"\"This function reads in a structure and returns start / end of it to limit the seq which \n",
    "    will be used to rebuild gapped structures\"\"\"\n",
    "    \n",
    "\n",
    "    fullpath = f\"{path}/{pdb_id_target}.pdb\"\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    protname = pdb_id_target\n",
    "    chain_corr = pdb_id_target[5:6]\n",
    "    \n",
    "    #chain_corr = \"A\"  #THIS IS ALWAYS A BECAUSE MODELLER MAKES ALL RESULT FILES BE CHAIN A. DONT CHANGE IT TO WHATEVER CHAIN IT REALLY IS.\n",
    "   \n",
    "    structure = parser.get_structure(protname, fullpath)\n",
    "    \n",
    "    positions = []\n",
    "    seq = []\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            if chain.get_id() == chain_corr:\n",
    "                for residues in chain:\n",
    "                    res_num = residues.get_id()[1]\n",
    "                    if res_num > 0:\n",
    "                        positions.append(res_num) #we discard negative entries\n",
    "                        seq.append(residues.get_resname())\n",
    "    \n",
    "    start, stop = positions[0], positions[-1]\n",
    "    #gives back start and stop.\n",
    "    return start, stop, chain_corr\n",
    "\n",
    "#_start_stop_fasta(pdb_id_target=\"2duk_A_0.pdb\", path=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3386,
   "id": "45d83b5c-eba9-476f-acc5-e825bf1235a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _renumber_struc(path_to_struc:str,\n",
    "                    start_num:str, \n",
    "                    chain:str):\n",
    "    \n",
    "    #renumbering fresh struc:\n",
    "    \n",
    "    shiftres_location = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py\"\n",
    "    \n",
    "    #rechain again.\n",
    "    parser = PDBParser(QUIET=True)\n",
    "                \n",
    "    prot_name = f\"default\"\n",
    "                \n",
    "    #open the correct pdb and rechain it.\n",
    "    structure_template = parser.get_structure(prot_name, path_to_struc)\n",
    "        \n",
    "    new_chain = chain \n",
    "        \n",
    "    for models in structure_template:\n",
    "        for chains in models:\n",
    "                \n",
    "            chains.id = \"_\"\n",
    "            \n",
    "            chains.id = new_chain\n",
    "            \n",
    "            io = PDBIO()\n",
    "            \n",
    "            io.set_structure(structure_template)\n",
    "            \n",
    "            io.save(path_to_struc)\n",
    "    \n",
    "    #we continue\n",
    "    \n",
    "    print(\"This is path, start num and chain\")\n",
    "    print(f\"{path_to_struc}\")\n",
    "    \n",
    "    print(f\"{start_num}\")\n",
    "    print(f\"{chain}\")\n",
    "    \n",
    "    bash_cmd = f\"python {shiftres_location} {path_to_struc} {start_num} {chain}\"\n",
    "    \n",
    "    bash_cmd_rdy = bash_cmd.split()\n",
    "    \n",
    "    with open(f\"{path_to_struc}_tmp\", \"w\") as fh_tmp:\n",
    "        result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "             universal_newlines=True)\n",
    "    \n",
    "    #now replace the original one with the temp file.\n",
    "    os.replace(f\"{path_to_struc}_tmp\", f\"{path_to_struc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3387,
   "id": "562c1588-2bc4-46e9-8368-fd9715671362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut_proper_models(path_to_struc:str,\n",
    "                   start_num:str, end_num:str,\n",
    "                       chain:str):\n",
    "    \n",
    "    \n",
    "    class ResidueSelect(Bio.PDB.Select):\n",
    "        def accept_residue(self, res):\n",
    "            #we accept all residues that are within start and stop \n",
    "            if res.id[1] >= int(start_num) and res.id[1] <= int(end_num) and res.parent.id == new_chain:  #i changed chain to new_chain\n",
    "                return True\n",
    "            #but we reject those that are before (N-terminal overhang) or after (C-terminal overhang)\n",
    "            #these are anyhow just spaghetti model.\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    #rechain again.\n",
    "    parser = PDBParser(QUIET=True)\n",
    "                \n",
    "    prot_name = f\"default\"\n",
    "                \n",
    "    #open the correct pdb and rechain it.\n",
    "    structure_template = parser.get_structure(prot_name, path_to_struc)\n",
    "        \n",
    "    new_chain = chain \n",
    "        \n",
    "    for models in structure_template:\n",
    "        for chains in models:\n",
    "                \n",
    "            chains.id = \"_\"\n",
    "            \n",
    "            chains.id = new_chain\n",
    "            \n",
    "            io = PDBIO()\n",
    "            \n",
    "            io.set_structure(structure_template)\n",
    "            \n",
    "            io.save(path_to_struc, ResidueSelect())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea38450-c87e-4dd4-93dc-807798d450f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### WE NEED TO UPDATE THE FASTA PROCESSING IN ORDER TO INCLUDE ALL CHAINS PROPERLY.\n",
    "\n",
    "Needs to take chain A and B separately in order to not mess up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0fb59-c874-479d-a54c-827604d7b5fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ALL OLIGOMER PIPELINE \n",
    "(PCA prep + PCA + PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3388,
   "id": "663fc75a-6000-4851-ad4d-548b93c8b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_oligomer_runs(path:str, min_seq_len_dict:dict):\n",
    "    \n",
    "    \"\"\"This function basically will call a bunch of other functions \n",
    "    for each potential oligomeric state and each position within them.\"\"\"\n",
    "    \n",
    "    oligodirectories = [\"monomer\",\n",
    "                        \"dimer\",\n",
    "                        \"trimer\",\n",
    "                        \"tetramer\",\n",
    "                        \"pentamer\",\n",
    "                        \"hexamer\",\n",
    "                        \"heptamer\",\n",
    "                        \"oktamer\",\n",
    "                        \"nonamer\",\n",
    "                        \"decamer\",\n",
    "                        \"undecamer\",\n",
    "                        \"dodecamer\",\n",
    "                        \"tridecamer\",\n",
    "                        \"tetradecamer\",\n",
    "                        \"pentadecamer\",\n",
    "                        \"hexadecamer\",\n",
    "                        \"heptadecamer\",\n",
    "                        \"oktadecamer\",\n",
    "                        \"nonadecamer\",\n",
    "                        \"eicosamer\"\n",
    "    ]\n",
    "    \n",
    "    relevant_dirs = []\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        if file in oligodirectories:\n",
    "            d = os.path.join(path, file)\n",
    "            relevant_dirs.append(d)\n",
    "            \n",
    "    \n",
    "    print(\"this is relevant dirs\")\n",
    "    #now we go through each of those oligomer dirs and for each position:\n",
    "    \n",
    "    \n",
    "    #lets see here why we dont go into some higher oligomers pca.\n",
    "    print(relevant_dirs)\n",
    "    \n",
    "    dir_dictionary = defaultdict(list)\n",
    "    \n",
    "    for dirs in relevant_dirs:\n",
    "\n",
    "        dir_dictionary[dirs] = os.listdir(dirs)\n",
    "    \n",
    "    \n",
    "    print(\"this is dir dictionary\")\n",
    "    print(dir_dictionary)\n",
    "    \n",
    "    path_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in dir_dictionary.items():\n",
    "        subdir_list = []\n",
    "        for subdir in vals:\n",
    "            new_path = keys + \"/\" + subdir\n",
    "            subdir_list.append(new_path)\n",
    "\n",
    "        path_dict[keys] = subdir_list \n",
    "\n",
    "    \n",
    "    for keys, vals in path_dict.items():\n",
    "        print(\"this is path_dict.items\")\n",
    "        print(keys, vals)\n",
    "        for dir_paths in vals:\n",
    "            \n",
    "            #now we need to parse through ALL of these directories:\n",
    "            \n",
    "            onlyfiles = [f for f in os.listdir(dir_paths) if os.path.isfile(os.path.join(dir_paths, f))]\n",
    "            \n",
    "            onlypdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "            \n",
    "            print(\"This is onlypdbs from within the run_all_oligomers\")\n",
    "            print(onlypdbs)\n",
    "            if len(onlypdbs) < 5:\n",
    "                #if thats the case that we dont have more than 2 structures \n",
    "                #we go to the next dir.\n",
    "                continue\n",
    "            \n",
    "            # '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/monomer/pos_31_97'\n",
    "            path_split = dir_paths.split(\"/\")\n",
    "            oligostate = path_split[-2] #this corresponds to monomer e.g\n",
    "            position_ = path_split[-1] #thi sis pos_31_97 e.g\n",
    "            \n",
    "            search_oligo_pos = f\"{oligostate}>{position_}\"\n",
    "            \n",
    "            print(\"this is min seq len dict at search oligo pos\")\n",
    "            print(min_seq_len_dict[search_oligo_pos])\n",
    "            min_seq_len = min_seq_len_dict[search_oligo_pos]\n",
    "            print(min_seq_len)\n",
    "            \n",
    "            \n",
    "            #first we grab which positions are gap and whats the max seq length will be.\n",
    "            remove_pos, delete_pdbs = read_msa_file_version_1(dir_paths)\n",
    "            \n",
    "            \n",
    "            print(\"this is remove pos\")\n",
    "            print(remove_pos)\n",
    "            print(\"this is delete_pdbs\")\n",
    "            print(delete_pdbs)\n",
    "            \n",
    "            #then we will prepare those structures, cut gaps and will be left with uniform length\n",
    "            #pdbs that have 1:1 correspondence between each residue against all pdbs.\n",
    "            \n",
    "            ref_struc_pdb_id = prep_structures_pca(path_to_strucs=dir_paths, \n",
    "                                remove_pos=remove_pos,delete_pdbs=delete_pdbs,\n",
    "                                max_seq_len = min_seq_len) #min_seq_len is length of shortes structure in the ensemble.\n",
    "            \n",
    "            if ref_struc_pdb_id == None:\n",
    "                continue #means we are done here.\n",
    "            \n",
    "            print(\"we enter chains_of ref\")\n",
    "            print(\"this is dir path pca and ref_struc_pdb_id\")\n",
    "            print(dir_paths, ref_struc_pdb_id)\n",
    "            chains_of_ref = _get_chain_labels(dir_paths, ref_struc_pdb_id)\n",
    "            \n",
    "            \n",
    "            print(\"we enter now domenico_new\")\n",
    "            \n",
    "            dir_path_pca = f\"{dir_paths}/PCA\"\n",
    "            \n",
    "            \n",
    "            print(dir_path_pca, ref_struc_pdb_id[:-4], chains_of_ref)\n",
    "            \n",
    "            #ref_struc [0:4] = 4 digit code\n",
    "            \n",
    "            #grep -a because temp output file in domenicos code after gromacs alignment contained\n",
    "            #binary stuff.\n",
    "            \n",
    "            #try:\n",
    "            #    PCA_domenico_new(path_to_pdbs=dir_path_pca,\n",
    "            #                 ref_struc_pdb_id=ref_struc_pdb_id[:-4],\n",
    "            #                 chains_of_reference_struc=chains_of_ref)\n",
    "\n",
    "            #\n",
    "    \n",
    "            #    #plot results as well:\n",
    "            #\n",
    "            #    #here we need dir_path_pca but above in PCA_domenico_new\n",
    "            #    #we only need dir paths!!!!!\n",
    "            #    #_plot_PCA(path_to_PCA=dir_path_pca)\n",
    "            #    \n",
    "            #except Exception as error:\n",
    "            #    print(error)\n",
    "            #    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3389,
   "id": "2e87af28-40a4-46e7-b937-69fadb2efd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_chain_labels(dir_paths:str, ref_struc_pdb_id:str):\n",
    "    \n",
    "    \"\"\"Helper function to extract chain label info\"\"\"\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "                \n",
    "    prot_name = f\"default\"\n",
    "               \n",
    "    path_to_pdb = f\"{dir_paths}/{ref_struc_pdb_id}\"\n",
    "    \n",
    "    #open the correct pdb and rechain it.\n",
    "    structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "    \n",
    "    \n",
    "    chains_str = \"\"\n",
    "    for models in structure_template:\n",
    "        for chains in models:\n",
    "            chains_str += chains.id\n",
    "    \n",
    "    \n",
    "    return chains_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3390,
   "id": "bb7b20cd-16d2-4fce-a921-b2cee8ef33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/\"\n",
    "#all_oligomer_runs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3391,
   "id": "5cc9bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report_gaps(report_file_loc:str, result_gaps:dict):\n",
    "    \"\"\"needs documentation.\"\"\"\n",
    "    with open(report_file_loc, \"w\") as fh_report:\n",
    "        for keys, vals in result_gaps.items():\n",
    "            fh_report.write(str(keys[0:6]))\n",
    "            fh_report.write(\",\")\n",
    "            #split list into string sep = ;\n",
    "            if len(vals) == 0: #means we have no gaps\n",
    "                fh_report.write(\"No_gap\")\n",
    "                fh_report.write(\"\\n\")\n",
    "                continue\n",
    "            #start and endgap are the positions that are still PRESENT in the structure. so first missing res is start+1\n",
    "            for entries in vals: #is a tuple containing (CHAIN, STARTGAP, ENDGAP, LEN GAP)\n",
    "                for k in entries:\n",
    "                    fh_report.write(str(k)+\" \")  #k = entry of tuple\n",
    "                fh_report.write(\";\")    \n",
    "            fh_report.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135d292-0dfc-447f-bb70-b2ba313f85d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mutational analysis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3392,
   "id": "41f90bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mutations_for_mapping(directory_to_search:str, prot_name:str):\n",
    "    \n",
    "    \"\"\"NEEDS TO BE WORKED ON / IS JUST A TEMPLATE NOW.\"\"\"\n",
    "    \n",
    "    path=\"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "    \n",
    "    #fetches the primary gene name that is used to store mutations in csv folder.\n",
    "    gene_name = get_gene_name_uniprot(uniprot_id=prot_name)\n",
    "    \n",
    "    #print(gene_name)\n",
    "    # Read all files from a directory, and read your input argument\n",
    "    files = os.listdir(directory_to_search)\n",
    "    \n",
    "    # Sort file names by name\n",
    "    files = sorted(files) \n",
    "\n",
    "    all_iso_forms = []\n",
    "    \n",
    "    #seems to work\n",
    "    for file_name in files:\n",
    "\n",
    "        if file_name.startswith(gene_name):\n",
    "            all_iso_forms.append(file_name)\n",
    "    \n",
    "    \n",
    "    #print(all_iso_forms)\n",
    "\n",
    "    mutation_pos_count_dict = defaultdict(int)\n",
    "    \n",
    "    for isoform in all_iso_forms:\n",
    "        mutation_iso_dict = defaultdict(int)\n",
    "        with open(f\"{path}/{isoform}\", \"r\") as fh_test:\n",
    "            for entries in fh_test:\n",
    "                mut_list = entries.split(\",\")\n",
    "            \n",
    "    \n",
    "            #this should do the trick. sorts based on position in increasing order.\n",
    "            mut_sorted = sorted(mut_list, key=lambda x: int(x[1:-1]), reverse=False)\n",
    "    \n",
    "            #for entries in mut_sorted:\n",
    "            #    print(entries)\n",
    "    \n",
    "            #count mutations per position\n",
    "            for entries in mut_sorted:\n",
    "                pos = int(entries[1:-1])\n",
    "                mutation_iso_dict[pos] += 1\n",
    "                \n",
    "        mutation_pos_count_dict[isoform] = mutation_iso_dict\n",
    "    #print(mutation_pos_count_dict)\n",
    "\n",
    "    #for keys, vals in mutation_pos_count_dict.items():\n",
    "        #print(keys, vals)\n",
    "        #for isoforms, muts in keys.items():\n",
    "        #    print(isoforms)\n",
    "        #    print(muts)\n",
    "            \n",
    "    ##test purpose\n",
    "    #with open(\"/home/micnag/bioinformatics/test/RNASET2_muts.csv\", \"w\") as fh_out:\n",
    "    #    for keys, vals in mutation_pos_count_dict.items():\n",
    "    #        fh_out.write(str(keys))\n",
    "    #        fh_out.write(\",\")\n",
    "    #        fh_out.write(str(vals))\n",
    "    #        fh_out.write(\"\\n\")\n",
    "    \n",
    "    #print(mutation_pos_count_dict)\n",
    "    return mutation_pos_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3393,
   "id": "a8bcc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surr_mutations(pdb_file:str, mutation_dict:dict,  \n",
    "                   outpath:str, cutoff=8, protname=\"default\"):\n",
    "    \n",
    "    \n",
    "    #print(mutation_dict)\n",
    "    \n",
    "    mutated_neighbours = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for keys, vals in mutation_dict.items():\n",
    "        tmp = []\n",
    "        for position, freq in vals.items():\n",
    "            tmp.append(position)\n",
    "        \n",
    "        mutated_neighbours[keys] = tmp\n",
    "        \n",
    "    with open(pdb_file, \"r\") as pdbfile1:\n",
    "\n",
    "        # first we need to make extract all atoms from our pdb file.\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        \n",
    "        structure = parser.get_structure(protname, pdbfile1)\n",
    "\n",
    "        # Selection.unfold_entities(<structure object>, <level of information that you want>)\n",
    "        # other levels are \"C\" for chain, \"R\" for residue, \"A\" for atom and so on.\n",
    "        \n",
    "\n",
    "        atom_list = Bio.PDB.Selection.unfold_entities(structure, \"A\")\n",
    "        \n",
    "        # lets get the coordinates of all atoms now\n",
    "        \n",
    "        #for each atom we store his parent residue\n",
    "        atom_coords = [(atom.get_coord(), atom.get_parent()) for atom in atom_list]\n",
    "        \n",
    "        for atoms in atom_list:\n",
    "            print(atoms.get_id())\n",
    "        # we provide as argument here the Selection.unfold.entities object which has all atoms.\n",
    "        ns = Bio.PDB.NeighborSearch(atom_list)  # this class object has the .search() method defined in its __init__\n",
    "        \n",
    "        \n",
    "        \n",
    "        #here we store all neighbours\n",
    "        general_neighbours = defaultdict(list)\n",
    "        \n",
    "        for atoms in atom_coords:\n",
    "            \n",
    "            f'''For each atom we will make a search for all surrounding atoms that are within {cutoff} A radius.'''\n",
    "            parent_res = atoms[1].get_id()[1]  #returns the residue number.\n",
    "            \n",
    "            proximal_atoms = ns.search(atoms[0], 8, \"R\")\n",
    "            \n",
    "            # I SET HERE search for atoms[0] because atoms is a tuple containing of coordinates\n",
    "            # and parent residue name see line 75 + 76 #print(atom_coords[0])\n",
    "\n",
    "            f\"\"\"Synthax: ns.search(<target object>, <Cutoff to be searched for>, \n",
    "            <type of information level that should be returned>\n",
    "            R means we dont want the single atoms that are within {cutoff}A \n",
    "            found but instead their corresponding residues. For all atoms we would set <A> instead of <R>\"\"\"\n",
    "\n",
    "            # this function searches through a target (in our case each atom as we loop through all available atoms)\n",
    "            # and returns a list with all atoms within specified atoms .\n",
    "            \n",
    "                \n",
    "            for residues in proximal_atoms:  # we go through all residues that were found within cutoff A\n",
    "            \n",
    "                id_x = residues.get_id()[1]\n",
    "                # get_id gives us a tuple with shape (\"\", \"residue number\", \"optinal flag\").\n",
    "                # Out of this tuple we want the residue id which is [1]\n",
    "                # we only want residues that we dont have already in the list.\n",
    "                # Makes no sense to add stuff that is already in there\n",
    "                general_neighbours[parent_res].append(id_x)\n",
    "                \n",
    "        \n",
    "        # if we have all we append the whole list to the dictionary. we take the atoms parent residue name as a key.\n",
    "        non_redundant_neighbours = defaultdict(int)\n",
    "        \n",
    "        \n",
    "        for keys, vals in general_neighbours.items():\n",
    "            unique_hits = list(set([x for x in vals if x != keys]))\n",
    "            \n",
    "            non_redundant_neighbours[keys] = unique_hits\n",
    "        \n",
    "        \n",
    "        \n",
    "        #now lets check how many mutations are within them.\n",
    "        \n",
    "        for isoform, mutations in mutated_neighbours.items():\n",
    "            print(isoform)\n",
    "            print(mutations)\n",
    "            with open(f\"{outpath}/{isoform[:-4]}_mutated_surroundings.tsv\", \"w\") as fh_out:\n",
    "                for position, neighbours in non_redundant_neighbours.items():\n",
    "                \n",
    "                    #make sets of both (just to use intersection)\n",
    "                    neighbours = set(neighbours)\n",
    "                    mutations = set(mutations)\n",
    "                    \n",
    "                    shared_mut_pos = neighbours.intersection(mutations)\n",
    "                    shared_mut_lst = list(sorted(list(shared_mut_pos)))\n",
    "                    print(position)\n",
    "                    print(shared_mut_lst)\n",
    "                    \n",
    "                    fh_out.write(str(position))\n",
    "                    fh_out.write(\"\\t\")\n",
    "                    for hits in shared_mut_lst:\n",
    "                        \n",
    "                        fh_out.write(str(hits))\n",
    "                        fh_out.write(\" \")\n",
    "                    \n",
    "                    fh_out.write(\"\\t\")\n",
    "                    fh_out.write(str(len(shared_mut_lst)))\n",
    "                    fh_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3394,
   "id": "98a5562b-a102-4e0e-83fb-3764b24937e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surr_mutations_calpha_1(pdb_file:str, mutation_dict:dict,  \n",
    "                   outpath:str, NMA_info:dict, cutoff=8, protname=\"default\"):\n",
    "    \n",
    "    #print(mutation_dict)\n",
    "    mutated_neighbours = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in mutation_dict.items():\n",
    "        tmp = []\n",
    "        for position, freq in vals.items():\n",
    "            tmp.append(position)\n",
    "        \n",
    "        mutated_neighbours[keys] = tmp\n",
    "        \n",
    "    \n",
    "    #print(mutated_neighbours)\n",
    "    with open(pdb_file, \"r\") as pdbfile1:\n",
    "\n",
    "        # first we need to make extract all atoms from our pdb file.\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        \n",
    "        structure = parser.get_structure(protname, pdbfile1)\n",
    "\n",
    "        # Selection.unfold_entities(<structure object>, <level of information that you want>)\n",
    "        # other levels are \"C\" for chain, \"R\" for residue, \"A\" for atom and so on\n",
    "        atom_list = Bio.PDB.Selection.unfold_entities(structure, \"A\")\n",
    "        \n",
    "        # lets get the coordinates of all atoms now\n",
    "        #only CALPHA\n",
    "        atom_coords = [atom for atom in atom_list if atom.get_id() == \"CA\"]\n",
    "    \n",
    "        # we provide as argument here the Selection.unfold.entities object which has all atoms.\n",
    "        ns = Bio.PDB.NeighborSearch(atom_coords)  # this class object has the .search() method defined in its __init__\n",
    "        \n",
    "        #here we store all neighbours\n",
    "        general_neighbours = defaultdict(list)\n",
    "        \n",
    "        for atoms in atom_coords:\n",
    "            \n",
    "            f'''For each atom we will make a search for all surrounding atoms that are within {cutoff} A radius.'''\n",
    "            parent_res = atoms.get_parent().get_id()[1]  #returns the residue number.\n",
    "            \n",
    "            proximal_atoms = ns.search(atoms.get_coord(), 8, \"A\")\n",
    "            \n",
    "            # I SET HERE search for atoms[0] because atoms is a tuple containing of coordinates\n",
    "            # and parent residue name see line 75 + 76 #print(atom_coords[0])\n",
    "\n",
    "            f\"\"\"Synthax: ns.search(<target object>, <Cutoff to be searched for>, \n",
    "            <type of information level that should be returned>\n",
    "            R means we dont want the single atoms that are within {cutoff}A \n",
    "            found but instead their corresponding residues. For all atoms we would set <A> instead of <R>\"\"\"\n",
    "\n",
    "            # this function searches through a target (in our case each atom as we loop through all available atoms)\n",
    "            # and returns a list with all atoms within specified atoms .\n",
    "            \n",
    "            for atms in proximal_atoms:  # we go through all residues that were found within cutoff A\n",
    "                \n",
    "                id_x = atms.get_id()\n",
    "                # get_id gives us a tuple with shape (\"\", \"residue number\", \"optinal flag\").\n",
    "                # Out of this tuple we want the residue id which is [1]\n",
    "                # we only want residues that we dont have already in the list.\n",
    "                # Makes no sense to add stuff that is already in there\n",
    "                if id_x == \"CA\":\n",
    "                    general_neighbours[parent_res].append(atms.get_parent().get_id()[1])\n",
    "                \n",
    "        \n",
    "        # if we have all we append the whole list to the dictionary. we take the atoms parent residue name as a key.\n",
    "        non_redundant_neighbours = defaultdict(int)\n",
    "        \n",
    "        for keys, vals in general_neighbours.items():\n",
    "            unique_hits = list(set([x for x in vals if x != keys]))\n",
    "            \n",
    "            non_redundant_neighbours[keys] = unique_hits\n",
    "        \n",
    "        \n",
    "        #now lets check how many mutations are within them.\n",
    "        \n",
    "        for isoform, mutations in mutated_neighbours.items():\n",
    "            #print(mutations)\n",
    "            \n",
    "            if NMA_info:\n",
    "                \n",
    "                NMA_dict = defaultdict()\n",
    "                NMA_scores = [x[1] for x in NMA_info] # we grab the scores knowing that it contains for each residue from start to finish a score.\n",
    "                #these scores are RENUMBERED FROM POS 1 even if POS 1 in the protein has resnum 134 e.g\n",
    "            else:\n",
    "                NMA_scores = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            df_mutation = pd.DataFrame()\n",
    "            \n",
    "                \n",
    "            #will contain all the neighbouring CA of each position as a list of lists.\n",
    "            neighbour_list = []\n",
    "            #these are the res nums of the CAs\n",
    "            prot_idx = []\n",
    "            \n",
    "            #surrounding mutated res list\n",
    "            mutated_res_neighbours = []\n",
    "            \n",
    "            #number of mutated neighbours\n",
    "            num_mutated_neighbours = []\n",
    "            \n",
    "            #NMA score list\n",
    "            NMA_score_attached_vals = []\n",
    "            \n",
    "            \n",
    "            i = 0 \n",
    "            for position, neighbours in non_redundant_neighbours.items():\n",
    "                \n",
    "                #make sets of both (just to use intersection)\n",
    "                \n",
    "                neighbours = set(neighbours)\n",
    "                \n",
    "                neighbour_list.append(list(neighbours))\n",
    "                prot_idx.append(position)\n",
    "            \n",
    "                mutations = set(mutations)\n",
    "                \n",
    "                shared_mut_pos = neighbours.intersection(mutations)\n",
    "                shared_mut_lst = list(sorted(list(shared_mut_pos)))\n",
    "                \n",
    "                mutated_res_neighbours.append(shared_mut_lst)\n",
    "                \n",
    "                num_mutated_neighbours.append(len(shared_mut_lst))\n",
    "                \n",
    "                if len(NMA_scores) != 0:\n",
    "                    \n",
    "                    NMA_score_attached_vals.append(NMA_scores[i])\n",
    "                    \n",
    "                    i += 1\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    NMA_score_attached_vals.append(np.NaN)\n",
    "                    i += 1\n",
    "            \n",
    "            #print(neighbour_list)\n",
    "            \n",
    "            df_mutation[\"neighbour_CAs\"] = [x for x in list(neighbour_list)]\n",
    "            df_mutation[\"mutated_neighbours_CAs\"] = [x for x in list(mutated_res_neighbours)] if len(mutated_res_neighbours) > 0 else np.NaN\n",
    "            df_mutation[\"num_mutated_neighbours\"] = [int(x) for x in list(num_mutated_neighbours)]\n",
    "            df_mutation[\"NMA_scores\"] = NMA_score_attached_vals\n",
    "            df_mutation.index = prot_idx\n",
    "            #print(pdb_file)\n",
    "            df_mutation.to_csv(outpath, sep=\"\\t\")\n",
    "            #print(df_mutation.head())\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3395,
   "id": "2a66ee7b-f6a7-49db-8afd-d3547e9426fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mutational_mapping_new(dir_to_proteins:str, main_prot_name:str, include_NMA=True):\n",
    "    \n",
    "     #good candidate to put all of them into 1 function. Next time wrap it up.\n",
    "    \"\"\"mutational mapping part\"\"\"\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{dir_to_proteins}/Mutational_mapping\")\n",
    "    \n",
    "    except Exception as error:\n",
    "        #print(error)\n",
    "        pass\n",
    "    \n",
    "    #this dir path needs to be hardcoded because it is constant in every case.4\n",
    "    dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "    \n",
    "    #path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{main_prot_name}\"\n",
    "\n",
    "    if include_NMA:\n",
    "        \n",
    "        NMA_dir = f\"{dir_to_proteins}/NMA\"\n",
    "        try:\n",
    "            dirs_to_check = [d for d in os.listdir(NMA_dir) if os.path.isdir(os.path.join(NMA_dir, d))]\n",
    "            \n",
    "            #print(dirs_to_check)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        \n",
    "        NMA_dict = defaultdict()\n",
    "        \n",
    "        for dirs in dirs_to_check:\n",
    "            path_to_check = f\"{dirs[:-4]}.ca.pdb_bfactor_res.txt\"\n",
    "            try:\n",
    "                #we append a dic as val to key from NMA_dict\n",
    "                NMA_scores_per_struc = []\n",
    "                \n",
    "                with open(f\"{NMA_dir}/{dirs}/{path_to_check}\", \"r\") as fh_nma_in:\n",
    "                    for lines in fh_nma_in:\n",
    "                        lines = lines.split()\n",
    "                        pos, score = lines[0], lines[1]\n",
    "                        NMA_scores_per_struc.append((int(pos), float(score)))\n",
    "                    \n",
    "                    #we append the new dict to a global dict.\n",
    "                    NMA_dict[dirs] = NMA_scores_per_struc\n",
    "                        \n",
    "            except Exception as error:\n",
    "                #print(f\"we could not open {NMA_dir}/{dirs}/{path_to_check}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "    #for strucs, combo_scores in NMA_dict.items():\n",
    "    #    print(strucs)\n",
    "    #    print(combo_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_file_input = [f for f in os.listdir(dir_to_proteins) if os.path.isfile(os.path.join(dir_to_proteins, f))]\n",
    "    #grab only pdbs\n",
    "    pdb_mapping_inputs = [f for f in all_file_input if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #grab corresponding mutational dictionary for each gene\n",
    "    mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=main_prot_name)\n",
    "    \n",
    "    #now lets map on each structure for each oligomer.\n",
    "    #check if there are pdb structures to map\n",
    "    if len(pdb_mapping_inputs) != 0:\n",
    "        #if yes map for each position and oligomer.\n",
    "        for pdb_files in pdb_mapping_inputs:\n",
    "            \n",
    "            \n",
    "            if include_NMA:\n",
    "                try:\n",
    "                    NMA_info = NMA_dict[pdb_files]\n",
    "                except:\n",
    "                    NMA_info = False\n",
    "            else:\n",
    "                NMA_info = False\n",
    "            \n",
    "            \n",
    "            #only CA-based. 8 A cutoff.\n",
    "            \n",
    "            pdb_file_path = f\"{dir_to_proteins}/{pdb_files}\"\n",
    "            \n",
    "            surr_mutations_calpha_1(pdb_file=pdb_file_path, mutation_dict=mut_dictionary, protname=main_prot_name,  \n",
    "                      outpath=f\"{dir_to_proteins}/Mutational_mapping/{pdb_files[:-4]}.tsv\", cutoff=8,\n",
    "                                 NMA_info=NMA_info)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3396,
   "id": "c2ff15f2-316b-4005-8d91-d25c4f7b46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test new block mutational mapping\n",
    "\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/monomer/pos_1_181\"\n",
    "#run_mutational_mapping_new(dir_to_proteins=path, main_prot_name=\"NUD4B_HUMAN\", include_NMA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3397,
   "id": "6fd82b42-dbde-4fba-9380-6def64fca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing mutational mapping block.\n",
    "\n",
    "#dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "\n",
    "#pdb_file = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/mutational_mapping/3w5b.pdb\"\n",
    "\n",
    "#outpath = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/mutational_mapping\"\n",
    "\n",
    "#mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=\"AT2A1_HUMAN\")\n",
    "#surr_mutations_calpha(pdb_file=pdb_file, mutation_dict=mut_dictionary, protname=\"AT2A1_HUMAN\",  \n",
    "#                   outpath=outpath, cutoff=8)\n",
    "#mutational_freq_hist(mut_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811c359-4cd7-4b28-8ced-3742180c461c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3398,
   "id": "7522d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutational_freq_hist(mutational_dict:dict):\n",
    "    \n",
    "    #print(mutational_dict)\n",
    "    \n",
    "    tmp_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in mutational_dict.items():\n",
    "        tmp_list = []\n",
    "        for key, val in vals.items():\n",
    "            tmp_list.append((key,val))\n",
    "        tmp_sort = sorted(tmp_list, key=lambda x: x[1], reverse=True)\n",
    "        tmp_dict[keys] = tmp_sort\n",
    "    \n",
    "\n",
    "    all_isoform_dicts = defaultdict(list)\n",
    "    only_muts = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in tmp_dict.items():\n",
    "        min_pos = 1\n",
    "        max_pos = max([x[0] for x in vals])\n",
    "        \n",
    "        for (pos, mut) in vals:\n",
    "            all_isoform_dicts[keys].append((pos, mut))\n",
    "            only_muts[keys].append((pos, mut))\n",
    "            \n",
    "        for i in range(min_pos, max_pos):\n",
    "            if i not in [x[0] for x in vals]:\n",
    "                all_isoform_dicts[keys].append((i, 0))\n",
    "    \n",
    "    \n",
    "    overlap_mutations = defaultdict(int)\n",
    "    \n",
    "    for keys, vals in only_muts.items():\n",
    "        \n",
    "        for muts in vals:\n",
    "            \n",
    "            overlap_mutations[muts[0]] += muts[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    shared_list = []\n",
    "    \n",
    "    #for keys, vals in all_isoform_dicts.items():\n",
    "    #    #setup min max to find the boundaries for our mutations on the protein.\n",
    "    #    min_pos = min([int(x[1]) for x in vals])\n",
    "    #    max_pos = max([int(x[1]) for x in vals])\n",
    "    #    #width = (max_pos-min_pos)\n",
    "    #\n",
    "    #    mutations_list = [x[1] for x in vals]\n",
    "    #    avg_mut = np.average(mutations_list)\n",
    "    #    #print(min_pos, max_pos, avg_mut)\n",
    "    #    shared_list.append([x for x in vals])\n",
    "        \n",
    "    #print(shared_list)\n",
    "        \n",
    "    #result = set(shared_list[0]).intersection(*shared_list)\n",
    "    #print(result)\n",
    "\n",
    "\n",
    "    \"\"\"MIGHT BE NEEDED LATER\"\"\"\n",
    "    fig = plt.figure(figsize=(100, 50))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    #threshold_avg = avg_mut\n",
    "    #threshold_1_sd = avg_mut + 1*np.std(mutations_list)\n",
    "    #threshold_2_sd = avg_mut + 2*np.std(mutations_list)\n",
    "    #threshold_3_sd = avg_mut + 3*np.std(mutations_list)\n",
    "    \n",
    "    \n",
    "    #below_avg = []\n",
    "    #sd_1 = []\n",
    "    #sd_2 = []\n",
    "    #sd_3 = []\n",
    "    \n",
    "    #for keys, vals in mutational_dict.items():\n",
    "    #    if vals > threshold_3_sd:\n",
    "    #        sd_3.append((keys, vals))\n",
    "    #        continue\n",
    "    #    if vals > threshold_2_sd:\n",
    "    #        sd_2.append((keys, vals))\n",
    "    #        continue\n",
    "    #    if vals > threshold_1_sd:\n",
    "    #        sd_1.append((keys, vals))\n",
    "    #        continue\n",
    "    #    if vals < threshold_avg:\n",
    "    #        below_avg.append((keys, vals))\n",
    "    #        continue\n",
    "    #\n",
    "    #x_below_avg = [x[0] for x in below_avg]\n",
    "    #y_below_avg = [x[1] for x in below_avg]\n",
    "    #\n",
    "    #x_sd_1 = [x[0] for x in sd_1]\n",
    "    #y_sd_1 = [x[1] for x in sd_1]\n",
    "    #\n",
    "    #x_sd_2 = [x[0] for x in sd_2]\n",
    "    #y_sd_2 = [x[1] for x in sd_2]\n",
    "    #\n",
    "    #x_sd_3 = [x[0] for x in sd_3]\n",
    "    #y_sd_3 = [x[1] for x in sd_3]\n",
    "    \n",
    "    #ax.bar(x_below_avg,y_below_avg, color = 'grey', width=1,label='Below avg')\n",
    "    #ax.bar(x_sd_1,y_sd_1, color = 'yellow', width=1, label='Above 1 SD') \n",
    "    #ax.bar(x_sd_2,y_sd_2, color = 'orange', width=1, label='Above 2 SD')\n",
    "    #ax.bar(x_sd_3,y_sd_3, color = 'red', label='Above 3 SD')\n",
    "    \n",
    "    ax.bar(overlap_mutations.keys(), overlap_mutations.values())\n",
    "    \n",
    "    #ax.bar(mutational_dict.keys(), mutational_dict.values(), color=[\"red\", \"blue\"])\n",
    "    #ax.set_title(f\"Number of mutations in 8A vicinity\", {'fontsize': 120})\n",
    "    #ax.set_xlabel('Position', fontsize=80)\n",
    "    #ax.set_ylabel('Mutations in Neighbourhood [Mutations / 8A surroundings]', fontsize=80)\n",
    "    #this does not take into account if the protein is actually longer.  needs improvement if this is important.\n",
    "    #ax.hlines(avg_mut, xmin=0, xmax=max_pos, color=\"grey\", linestyles=\"dotted\",linewidth=3)\n",
    "    #ax.hlines(avg_mut+2*np.std(mutations_list), xmin=0, xmax=max_pos, color=\"r\", linestyles=\"dashed\",linewidth=4)\n",
    "    #ax.hlines(avg_mut+3*np.std(mutations_list), xmin=0, xmax=max_pos, color=\"r\", linestyles=\"dashed\",linewidth=5)\n",
    "    #ax.tick_params(labelsize=60)\n",
    "    #ax.xaxis.set_ticks(np.arange(0, max_pos, 10))\n",
    "    #ax.legend(fontsize=80, borderpad=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3399,
   "id": "46cd7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "\n",
    "#mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=\"RNT2_HUMAN\")\n",
    "#mutational_freq_hist(mut_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3400,
   "id": "89999e9c-bb86-4669-a8f0-32f29f3bf760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to cluster PCA automatically\n",
    "\n",
    "\n",
    "def PCA_clustering(path_to_PCA:str):\n",
    "    \n",
    "    \"\"\"Function will take in the PCA data and perform affinity propagation clustering.\"\"\"\n",
    "    \n",
    "    \n",
    "    expl_var = f\"{path_to_PCA}/pc_variances.txt\"\n",
    "    pca = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    \n",
    "    #centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "    #X, labels_true = make_blobs(\n",
    "    #        n_samples=300, centers=centers, cluster_std=0.5, random_state=0)\n",
    "    \n",
    "    \n",
    "    #read in the first 2 PC from PCA file.\n",
    "    pca_array = np.loadtxt(pca, dtype=\"float\",\n",
    "                           usecols=[0,1])\n",
    "    \n",
    "    pca_mean = np.mean(pca_array)\n",
    "    pca_std = np.std(pca_array)\n",
    "    print(pca_mean)\n",
    "    print(pca_std)\n",
    "    \n",
    "    pca_normalized = (pca_array-pca_mean)/pca_std\n",
    "    \n",
    "    #print(pca_normalized)\n",
    "    #normalize array\n",
    "    #pca_array_normalized = stats.zscore(pca_array, axis=None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    af = AffinityPropagation(random_state=0,max_iter=1000,damping=0.7).fit(pca_normalized)\n",
    "    \n",
    "    cluster_centers_indices = af.cluster_centers_indices_\n",
    "    labels = af.labels_\n",
    "    \n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    \n",
    "    \n",
    "    print(cluster_centers_indices)\n",
    "    print(labels)\n",
    "    \n",
    "    cnt_dict = defaultdict()\n",
    "    \n",
    "    hits = set(labels)\n",
    "    \n",
    "    for x in hits:\n",
    "        cnt_dict[x] = 0\n",
    "        \n",
    "    for x in labels:\n",
    "        cnt_dict[x] += 1\n",
    "        \n",
    "    \n",
    "    highest_groups = []\n",
    "    for keys, vals in cnt_dict.items():\n",
    "        highest_groups.append((keys, vals))\n",
    "        \n",
    "    \n",
    "    highest_sort = sorted(highest_groups, key=lambda x : x[1], reverse=True)\n",
    "    \n",
    "    print(highest_sort)\n",
    "    #print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    #print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "    #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "    #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "    #print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\n",
    "    #print(\n",
    "    #    \"Adjusted Mutual Information: %0.3f\"\n",
    "    #    % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
    "    #)\n",
    "    #print(\n",
    "    #    \"Silhouette Coefficient: %0.3f\"\n",
    "    #    % metrics.silhouette_score(X, labels, metric=\"sqeuclidean\")\n",
    "    #)\n",
    "\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    \n",
    "    colors = plt.cycler(\"color\", plt.cm.viridis(np.linspace(0, 1, 4)))\n",
    "    \n",
    "    for k, col in zip(range(n_clusters_), colors):\n",
    "        class_members = labels == k\n",
    "        cluster_center = pca_array[cluster_centers_indices[k]]\n",
    "        plt.scatter(\n",
    "            pca_array[class_members, 0], pca_array[class_members, 1], color=col[\"color\"], marker=\".\"\n",
    "        )\n",
    "        plt.scatter(\n",
    "            cluster_center[0], cluster_center[1], s=14, color=col[\"color\"], marker=\"o\"\n",
    "        )\n",
    "        for x in pca_array[class_members]:\n",
    "            plt.plot(\n",
    "                [cluster_center[0], x[0]], [cluster_center[1], x[1]], color=col[\"color\"]\n",
    "            )\n",
    "    \n",
    "    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3401,
   "id": "302d876f-5dbd-42c5-8db5-889b8091453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_PCA = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "#PCA_clustering(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3402,
   "id": "824a24f1-9887-4438-908c-66e8fc2968eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_clustering_2(path_to_PCA:str):\n",
    "    \n",
    "    \"\"\"Function will take in the PCA data and perform affinity propagation clustering.\"\"\"\n",
    "    \n",
    "    \n",
    "    expl_var = f\"{path_to_PCA}/pc_variances.txt\"\n",
    "    pca = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    \n",
    "    np.random.seed(0)\n",
    "    #centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "    #X, labels_true = make_blobs(\n",
    "    #        n_samples=300, centers=centers, cluster_std=0.5, random_state=0)\n",
    "    \n",
    "    \n",
    "    #read in the first 2 PC from PCA file.\n",
    "    pca_array = np.loadtxt(pca, dtype=\"float\",\n",
    "                           usecols=[0,1])\n",
    "    \n",
    "    pca_mean = np.mean(pca_array)\n",
    "    pca_std = np.std(pca_array)\n",
    "    \n",
    "    #normalize\n",
    "    pca_normalized = (pca_array-pca_mean)/pca_std\n",
    "    \n",
    "    \n",
    "    clust = OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.05)\n",
    "    \n",
    "    # Run the fit\n",
    "    clust.fit(pca_normalized)\n",
    "    labels_050 = cluster_optics_dbscan(\n",
    "    reachability=clust.reachability_,\n",
    "    core_distances=clust.core_distances_,\n",
    "    ordering=clust.ordering_,\n",
    "    eps=0.5)\n",
    "    \n",
    "    labels_200 = cluster_optics_dbscan(\n",
    "    reachability=clust.reachability_,\n",
    "    core_distances=clust.core_distances_,\n",
    "    ordering=clust.ordering_,\n",
    "    eps=2)\n",
    "\n",
    "    space = np.arange(len(pca_array))\n",
    "    reachability = clust.reachability_[clust.ordering_]\n",
    "    labels = clust.labels_[clust.ordering_]\n",
    "    \n",
    "    print(\"these are the number of conformers we found\")\n",
    "    print(len(set([x for x in labels if x >= 0])))\n",
    "    print(set(labels))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    G = gridspec.GridSpec(2, 3)\n",
    "    ax1 = plt.subplot(G[0, :])\n",
    "    ax2 = plt.subplot(G[1, 0])\n",
    "    ax3 = plt.subplot(G[1, 1])\n",
    "    ax4 = plt.subplot(G[1, 2])\n",
    "    \n",
    "    # Reachability plot\n",
    "    colors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\n",
    "    for klass, color in zip(range(0, 5), colors):\n",
    "        Xk = space[labels == klass]\n",
    "        Rk = reachability[labels == klass]\n",
    "        ax1.plot(Xk, Rk, color, alpha=0.3)\n",
    "    ax1.plot(space[labels == -1], reachability[labels == -1], \"k.\", alpha=0.3)\n",
    "    ax1.plot(space, np.full_like(space, 2.0, dtype=float), \"k-\", alpha=0.5)\n",
    "    ax1.plot(space, np.full_like(space, 0.5, dtype=float), \"k-.\", alpha=0.5)\n",
    "    ax1.set_ylabel(\"Reachability (epsilon distance)\")\n",
    "    ax1.set_title(\"Reachability Plot\")\n",
    "    \n",
    "    # OPTICS\n",
    "    colors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\n",
    "    for klass, color in zip(range(0, 5), colors):\n",
    "        Xk = pca_array[clust.labels_ == klass]\n",
    "        ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\n",
    "    ax2.plot(pca_array[clust.labels_ == -1, 0], pca_array[clust.labels_ == -1, 1], \"k+\", alpha=0.1)\n",
    "    ax2.set_title(\"Automatic Clustering\\nOPTICS\")\n",
    "    \n",
    "    # DBSCAN at 0.5\n",
    "    colors = [\"g.\", \"r.\", \"b.\", \"c.\", \"y.\", \"m.\"]\n",
    "    for klass, color in zip(range(0, 4), colors):\n",
    "        Xk = pca_array[labels_050 == klass]\n",
    "        ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\n",
    "    ax3.plot(pca_array[labels_050 == -1, 0], pca_array[labels_050 == -1, 1], \"k+\", alpha=0.1)\n",
    "    ax3.set_title(\"Clustering at 0.5 epsilon cut\\nDBSCAN\")\n",
    "    \n",
    "    # DBSCAN at 2.\n",
    "    colors = [\"g.\", \"m.\", \"y.\", \"c.\"]\n",
    "    for klass, color in zip(range(0, 4), colors):\n",
    "        Xk = pca_array[labels_200 == klass]\n",
    "        ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\n",
    "    ax4.plot(pca_array[labels_200 == -1, 0], pca_array[labels_200 == -1, 1], \"k+\", alpha=0.1)\n",
    "    ax4.set_title(\"Clustering at 2.0 epsilon cut\\nDBSCAN\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3403,
   "id": "98ca3558-74c9-494c-89e3-609803bd2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_PCA = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "#PCA_clustering_2(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3404,
   "id": "13e78d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutational_upsetplot(mutational_dict:dict, save_file=True):\n",
    "    \n",
    "    #print(mutational_dict)\n",
    "    \n",
    "    tmp_dict = defaultdict()\n",
    "    \n",
    "    isoforms_names = []\n",
    "    for keys, vals in mutational_dict.items():\n",
    "        tmp_list = []\n",
    "        #store names of isoforms.\n",
    "        if not isoforms_names:\n",
    "            isoforms_names.append(keys)\n",
    "        for key, val in vals.items():\n",
    "            tmp_list.append((keys,key,val))\n",
    "        tmp_sort = sorted(tmp_list, key=lambda x: x[1], reverse=True)\n",
    "        tmp_dict[keys] = tmp_sort\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_isoform_dicts = defaultdict(list)\n",
    "    only_muts = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in tmp_dict.items():\n",
    "        min_pos = 1\n",
    "        max_pos = max([x[1] for x in vals])\n",
    "        \n",
    "        for (isoform, pos, mut) in vals:\n",
    "            all_isoform_dicts[keys].append((isoform[:-4],pos, mut))\n",
    "            only_muts[keys].append((pos, mut))\n",
    "            \n",
    "        for i in range(min_pos, max_pos):\n",
    "            if i not in [x[0] for x in vals]:\n",
    "                all_isoform_dicts[keys].append((isoform[:-4], i, 0))\n",
    "    \n",
    "\n",
    "    tmp_ = []\n",
    "    for keys, vals in all_isoform_dicts.items():\n",
    "        for x in vals:\n",
    "            tmp_.append(x)\n",
    "    \n",
    "    \n",
    "    sorted_full_isoforms = sorted(tmp_, key=lambda x: x[1], reverse=False)\n",
    "    \n",
    "    max_val = sorted_full_isoforms[-1][1]\n",
    "      \n",
    "    muts = defaultdict(int)\n",
    "    isoforms = defaultdict(list)\n",
    "    \n",
    "    for entries in sorted_full_isoforms:\n",
    "        if entries[2] == 0:\n",
    "            continue\n",
    "        muts[(entries[1])] += entries[2]\n",
    "        isoforms[str(entries[1])].append(entries[0])\n",
    "        \n",
    "    datalst = []\n",
    "    members = []\n",
    "    \n",
    "    for i in range(1, max_pos):\n",
    "        datalst.append(muts[i])\n",
    "        members.append(isoforms[str(i)])\n",
    "\n",
    "    #mut_freq_overlap = from_memberships(members[0:5], data=data[0:5])\n",
    "    \n",
    "    example = from_memberships(\n",
    "    members, data=datalst)\n",
    "    \n",
    "    \n",
    "    \n",
    "    UpSet.plot(example, subset_size=\"sum\", orientation=\"horizontal\",\n",
    "        sort_by=\"degree\", sort_categories_by=\"cardinality\",\n",
    "        facecolor=\"navy\",shading_color=\"lightgray\",\n",
    "        show_counts=True,other_dots_color=.2)\n",
    "    \n",
    "    title = min(isoforms_names)[:-4]\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    #if save_file:\n",
    "        #plt.savefig(\"/home/micnag/bioinformatics/test/myplot.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3405,
   "id": "ff5f449e-5edd-463d-aba4-56a11a7e8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_sequence_from_struc(pdb):\n",
    "\n",
    "    lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                    ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                    ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                    ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "    \n",
    "    canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    pdb_name = os.path.basename(pdb)\n",
    "    prot_name = f\"default\"\n",
    "\n",
    "    \n",
    "    #print(\"we are here\")\n",
    "    #print(fullpath)\n",
    "    structure = parser.get_structure(prot_name, pdb)\n",
    "    struc_seq = [canonical_aas[x.get_resname()] for x in structure.get_residues()]\n",
    "    struc_seq = \"\".join(struc_seq)\n",
    "\n",
    "    return (struc_seq, pdb_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3406,
   "id": "42b8cd13-cdb7-4cdc-937d-6e455d2e0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_c_alpha(path:str):\n",
    "    \n",
    "    #sel only c_alpha\n",
    "    class C_alpha_only(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "            \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_atom(self, atom):\n",
    "            return 1 if atom.id == \"CA\" else 0\n",
    "        \n",
    "        #overloaded to only accept positive residue numbering.\n",
    "        def accept_residue(self, residue):      \n",
    "            return 1 if residue.id[1] > 0 else 0    \n",
    "        \n",
    "    lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                    ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                    ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                    ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "    \n",
    "    canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"default\"\n",
    "    \n",
    "    #print(\"we are here\")\n",
    "    #print(fullpath)\n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    # Select C-alpha atoms and save the modified structure\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(path, C_alpha_only())\n",
    "    \n",
    "    # Reload the modified structure for further processing\n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    # Extract sequence and range information\n",
    "    struc_full = [canonical_aas[x.get_resname()] for x in structure.get_residues()]\n",
    "    struc_full = \"\".join(struc_full)\n",
    "    \n",
    "    struc_range = [x.get_id()[1] for x in structure.get_residues()]\n",
    "    struc_start, struc_stop = struc_range[0], struc_range[-1]\n",
    "    \n",
    "    return struc_start, struc_stop, struc_full, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3407,
   "id": "fcfbbdc6-5f71-4fa8-876a-4005866741e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_c_alpha(\"/home/micnag/pdb_neg_test.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3408,
   "id": "3f8cfde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "\n",
    "#mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=\"RNT2_HUMAN\")\n",
    "#mutational_upsetplot(mut_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e1297-ba0f-4ea6-a655-201631e8b352",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normal mode analysis block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3409,
   "id": "d99d3134-9de1-43f6-9e7f-453865a8b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMA_pipeline(path:str):\n",
    "    \n",
    "    baselocation = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/NMA_pipeline\"\n",
    "        \n",
    "    #we work in a clean work dir.\n",
    "    try:\n",
    "        os.mkdir(f\"{path}/NMA\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #fetch all file names that we need to shuffle.\n",
    "    files_to_shuffle = [f for f in os.listdir(baselocation) if os.path.isfile(os.path.join(baselocation, f))]\n",
    "    \n",
    "    #we move all required files to the work dir.\n",
    "    for files in files_to_shuffle:\n",
    "        #copy to target folder\n",
    "        shutil.copy(f\"{baselocation}/{files}\", f\"{path}/NMA\")\n",
    "        \n",
    "    #lets start NMA for all structure\n",
    "    os.chdir(f\"{path}/NMA\")\n",
    "    \n",
    "    result_files = [\"evec.dat\", \"constants.dat\", \"hessian.dat\"]\n",
    "\n",
    "    complete_struc_path = f\"{path}/PCA/clean_ensemble\"\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(complete_struc_path) if os.path.isfile(os.path.join(complete_struc_path, f)) and f[-4:] == \".pdb\"]\n",
    "\n",
    "    #set b factor to 0 otherwise this interfers with memory allocation if the columns are not separated (occ. and b fac)\n",
    "    \n",
    "    \n",
    "    for pdbs in onlyfiles:\n",
    "        print(\"we run now nma with\")\n",
    "        print(pdbs)\n",
    "\n",
    "        #first we set b fac to 0\n",
    "        _set_b_factors_0(path=f\"{complete_struc_path}/{pdbs}\")\n",
    "        \n",
    "        run_nma = f\"./get_nma.sh {complete_struc_path}/{pdbs}\"\n",
    "        \n",
    "        bash_cmd = run_nma.split()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            #lets keep it here to be save\n",
    "            os.chdir(f\"{path}/NMA\")\n",
    "            result = run(bash_cmd, stdout=PIPE, stderr=PIPE,\n",
    "                universal_newlines=True)\n",
    "        \n",
    "            print(result.stdout)\n",
    "            print(result.stderr)\n",
    "            \n",
    "            os.mkdir(f\"{path}/NMA/{pdbs}\")\n",
    "            \n",
    "            #we move the CA-only structure also for representation.\n",
    "            \n",
    "            shutil.move(f\"{complete_struc_path}/{pdbs[0:4]}.ca.pdb\", f\"{path}/NMA/{pdbs}\")\n",
    "            \n",
    "            #we move the result files to another dir where we work later on for visualization.\n",
    "            for resultf in result_files:\n",
    "                shutil.move(f\"{complete_struc_path}/{resultf}\", f\"{path}/NMA/{pdbs}\")\n",
    "            \n",
    "            #now we call prep nma scores to compute the b factor associated values.\n",
    "            prep_nma_scores(dir_with_normal_modes=f\"{path}/NMA/{pdbs}\", nm=10)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3410,
   "id": "1c12f74e-cdf2-4ccc-bcb5-a110666dfca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_nma_scores(dir_with_normal_modes:str, nm=10):\n",
    "    \n",
    "    \n",
    "    #./ptraj_evec_bfactor_cum.pl nma 1kju.ca.pdb evec.dat 10 testbfactorpdb\n",
    "    pdb_list= [f for f in os.listdir(dir_with_normal_modes) if os.path.isfile(os.path.join(dir_with_normal_modes, f)) and f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #from here we grab our executables and scripts\n",
    "    baselocation = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/NMA_pipeline\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        shutil.copy(f\"{baselocation}/ptraj_evec_bfactor_cum.pl\", f\"{dir_with_normal_modes}\")\n",
    "        \n",
    "        #so we execute the script from here.\n",
    "        os.chdir(dir_with_normal_modes)\n",
    "        \n",
    "        run_perl = f\"./ptraj_evec_bfactor_cum.pl nma {pdb_list[0]} evec.dat {nm} {pdb_list[0][:-4]}\"\n",
    "    \n",
    "        bash_cmd = run_perl.split()\n",
    "        \n",
    "        with open(f\"{dir_with_normal_modes}/{pdb_list[0]}_bfactor_res.txt\", \"w\") as bfout:\n",
    "        \n",
    "            result = run(bash_cmd, stdout=bfout, stderr=PIPE,\n",
    "                universal_newlines=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3411,
   "id": "6873db3c-3a52-4e99-808d-d3150b2613b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range_parallelized(pdb_path:str):\n",
    "\n",
    "    hits = []\n",
    "    \n",
    "    try:\n",
    "        parser = PDBParser()\n",
    "        \n",
    "        structure = parser.get_structure(\"none\", pdb_path)\n",
    "\n",
    "        chains_residues = []\n",
    " \n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "\n",
    "                chain_id = chain.id\n",
    "        \n",
    "                # Filter the residues within the chain\n",
    "                residues_in_chain = [res for res in chain if res.get_id()[0] == \" \"]\n",
    "        \n",
    "                result = [res.get_id()[1] for res in residues_in_chain]\n",
    "\n",
    "                chains_residues.append((chain_id, result))\n",
    "\n",
    "        #now we have all chains and their ids\n",
    "        \n",
    "        if len(chains_residues) == 1: #means its monomer\n",
    "            \n",
    "            for chains, resids in chains_residues:\n",
    "                \n",
    "                min_num, max_num = min(resids), max(resids)\n",
    "\n",
    "                #print(pdb_path, min_num, max_num)\n",
    "                hits.append((pdb_path, chains, min_num, max_num))\n",
    "                \n",
    "        elif len(chains_residues) > 1: # oligomer.\n",
    "\n",
    "            #real chains is in the name of the pdb.\n",
    "            #'/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/\n",
    "            #NUDT4B/merged_cleaned_files/3i7u_ABCD.pdb'\n",
    "            \n",
    "\n",
    "            chain_name = os.path.basename(pdb_path)[5:-4]  #this should be ABCD in the case above\n",
    "            \n",
    "            min_max_list = []\n",
    "            \n",
    "            for chains, resids in chains_residues:\n",
    "                \n",
    "                chains += chains\n",
    "                \n",
    "                min_num, max_num = min(resids), max(resids)\n",
    "                \n",
    "                min_max_list.append((min_num, max_num))\n",
    "\n",
    "            lowest_start = min([x[0] for x in min_max_list])\n",
    "            highest_stop = max([x[1] for x in min_max_list])\n",
    "\n",
    "\n",
    "            hits.append((pdb_path, chain_name, lowest_start, highest_stop))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3412,
   "id": "6907940f-edea-4957-a418-200723989359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_ranges(path:str):\n",
    "\n",
    "    real_range_dict = defaultdict()\n",
    "\n",
    "    pdbs = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.pdb')]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(executor.map(get_range_parallelized, pdbs))\n",
    "    # Process the results\n",
    "\n",
    "    for hit in results:\n",
    "        for pdb_path, chains, min_num, max_num in hit:\n",
    "            real_range_dict[(pdb_path, chains)] = (min_num, max_num)\n",
    "    \n",
    "    return real_range_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3413,
   "id": "a46b90d5-b40e-4427-80b3-c9dc464dc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oligostates = {'3i7v.pdb': 1, '2q9p.pdb': 1, '3mcf.pdb': 1, \n",
    "#    '3h95.pdb': 2, '2duk.pdb': 1, '6woa.pdb': 1, '6woi.pdb': 1, \n",
    "#    '4hfq.pdb': 2, '6wod.pdb': 1, '6pck.pdb': 1, '6wob.pdb': 1, \n",
    "#    '6wof.pdb': 1, '6wo9.pdb': 1, '5ltu.pdb': 2, '7nnj.pdb': 1, \n",
    "#    '6wo8.pdb': 1, '6wo7.pdb': 1, '3i7u.pdb': 4, '2fvv.pdb': 1, \n",
    "#    '6woc.pdb': 1, '6pcl.pdb': 1, '6wog.pdb': 1, '7tn4.pdb': 1, \n",
    "#    '6woh.pdb': 1, '6woe.pdb': 1}\n",
    "#\n",
    "#remove_artifacts_and_rename(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files\", \n",
    "#                            oligostates= oligostates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3414,
   "id": "4d925e79-2c03-442d-b6b7-43d3e57479f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_artifacts_and_rename(path_merged:str, oligostates:dir):\n",
    "\n",
    "    #defaultdict(<class 'str'>, {'3i7v.pdb': 1, '2q9p.pdb': 1, '3mcf.pdb': 1, \n",
    "    #'3h95.pdb': 2, '2duk.pdb': 1, '6woa.pdb': 1, '6woi.pdb': 1, \n",
    "    #'4hfq.pdb': 2, '6wod.pdb': 1, '6pck.pdb': 1, '6wob.pdb': 1, \n",
    "    #'6wof.pdb': 1, '6wo9.pdb': 1, '5ltu.pdb': 2, '7nnj.pdb': 1, \n",
    "    #'6wo8.pdb': 1, '6wo7.pdb': 1, '3i7u.pdb': 4, '2fvv.pdb': 1, \n",
    "    #'6woc.pdb': 1, '6pcl.pdb': 1, '6wog.pdb': 1, '7tn4.pdb': 1, \n",
    "    #'6woh.pdb': 1, '6woe.pdb': 1})\n",
    "\n",
    "    \"\"\"\n",
    "    We need to parse through the whole dictionary and search \n",
    "    for all files that start with each 4 letter pdb code.\n",
    "\n",
    "    Afterwards we need to delete intermediate oligomers that are not real and only keep monomer chain length and\n",
    "    the real oligomer e.g in the case of 3i7u we need to delete 3i7u AB and ABC but NOT ABCD and also keep A B C D.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for pdb_code, oligomer in oligostates.items():\n",
    "    \n",
    "        matching_files = [f for f in os.listdir(path_merged) if f.startswith(pdb_code[:-4]) and f.endswith(\".pdb\")]\n",
    "\n",
    "        #print(matching_files)\n",
    "        for pdb_file in matching_files:\n",
    "        \n",
    "            full_path = os.path.join(path_merged, pdb_file)\n",
    "\n",
    "            #print(pdb_file)\n",
    "            # Extract the chain identifier (e.g., \"A\", \"AB\") from the filename\n",
    "            chain_identifier = pdb_file[5:-4]  # Remove \".pdb\" from the end\n",
    "            \n",
    "            # Compare the length of the chain with the oligomer value\n",
    "            if len(chain_identifier) == oligomer or len(chain_identifier) == 1:\n",
    "                \n",
    "                #print(f\"Matched: {pdb_file}, Chain: {chain_identifier}, Oligomer: {oligomer}\")\n",
    "\n",
    "                _rename_according_to_chain(full_path, pdb_file, chain_identifier, path_merged)\n",
    "            \n",
    "            else:\n",
    "                #print(f\"PDB: {pdb_file} is an artifact. , Chain: {chain_identifier}, Oligomer: {oligomer}\")\n",
    "                os.remove(full_path)\n",
    "        \n",
    "    \n",
    "    #now afterwards we need to remove the artifacts.\n",
    "    leftover_clean = [f for f in os.listdir(path_merged)]\n",
    "\n",
    "    for pdbs in leftover_clean:\n",
    "\n",
    "        #print(pdbs)\n",
    "        \n",
    "        full_path = os.path.join(path_merged, pdbs)\n",
    "\n",
    "        chain = pdbs[5:-4]\n",
    "        \n",
    "        #print(f\"this is leftover chain: {chain}\")\n",
    "        \n",
    "        if chain.isdigit():\n",
    "      \n",
    "            os.remove(full_path)\n",
    "            #print(f\"Removed leftover file: {full_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3415,
   "id": "008b78bf-3164-4263-9e2c-c5e6d9ec0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rename_according_to_chain(full_path:str, pdb_file:str, chain_identifier:str,\n",
    "                              path_merged:str):\n",
    "\n",
    "    parser = PDBParser()\n",
    "\n",
    "    structure = parser.get_structure(\"none\", full_path)\n",
    "\n",
    "    seen_chains = []\n",
    "    for models in structure:\n",
    "        for chains in models:\n",
    "                \n",
    "            seen_chains.append(chains.id)\n",
    "\n",
    "    \n",
    "    #print(seen_chains, chain_identifier)\n",
    "\n",
    "    if chain_identifier not in seen_chains or len(chain_identifier) != len(seen_chains):\n",
    "        \n",
    "        #print(\"problematic\")\n",
    "        #print(pdb_file)\n",
    "        io = PDBIO()\n",
    "\n",
    "        new_chain = \"\".join(sorted(seen_chains))  #get AB eg\n",
    "        \n",
    "        io.set_structure(structure)\n",
    "        \n",
    "        new_name = f\"{pdb_file[:5]}{new_chain}.pdb\"\n",
    "        \n",
    "        new_save_path = os.path.join(path_merged, new_name)\n",
    "        #print(new_save_path)\n",
    "        #save as new file with proper filename\n",
    "        io.save(new_save_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3416,
   "id": "170b1c2b-bd54-4aec-bc36-f1c7ec159358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/nma_test/NMA/1kju.pdb\"\n",
    "#prep_nma_scores(dir_with_normal_modes=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3417,
   "id": "101d4459-5b6a-4f44-b8e3-e5dba53a02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/nma_test\"\n",
    "\n",
    "#NMA_pipeline(path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb1b6c-478d-4cde-b851-9b88d2df6e74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main function body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3418,
   "id": "76caadbe-1fcd-47cc-9092-455ccf712fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_struc_1(query:str, templates:str, seq_sim:str, query_start:str, query_end:str,\n",
    "                temp_start:str, temp_end:str,\n",
    "                run_NMA:bool, run_PCA:bool)-> None:\n",
    "    \n",
    "    ''' Input: dictionary containing:\n",
    "        \n",
    "        Key: Gene_name / Uniprot ID\n",
    "        \n",
    "        Values: [0]: All rcsb ids of homologs and their respective chains e.g 8DGD_A for chain A of 8DGD\n",
    "                [1]: Seq similarity [0,1]\n",
    "                \n",
    "        We parse through all mmseq2 hit rcsb ids and retrieve associated structures.\n",
    "        First retrieved structure will be used as REFERENCE structure.\n",
    "        This makes sense because mmseqs2 hits are sorted by seq similarity and highest seq\n",
    "        similarity is listed first (assuming this seq similarity hit corresponds to the best \n",
    "        structural similarity hit).\n",
    "        Then we select the correct chain and compute RMSD / TM score for each respective pair against\n",
    "        the Reference structue. We use the first entry corresponding to the highest seq similarity as template to align other structures against.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #go through all steps and check logic and redundancy.\n",
    "    \n",
    "    uniprot_id = query\n",
    "    #Used to store protein name for matching with cosmic_results later downstream \n",
    "    main_prot_name = get_gene_name_uniprot(query) \n",
    "    \n",
    "    #domain dict will be used to create sub directories later.\n",
    "    #domain_dict = _split_domains(templates, temp_start, temp_end, query_start, query_end)\n",
    "\n",
    "    \n",
    "    #print(\"this is the output of split domains\")\n",
    "    #print(domain_dict)\n",
    "    \n",
    "    \n",
    "    #print(main_prot_name)\n",
    "    \n",
    "    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{main_prot_name}\"\n",
    "\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if not os.path.exists(os.path.join(path, \"merged_cleaned_files\")):\n",
    "        \n",
    "            os.mkdir(os.path.join(path, \"merged_cleaned_files\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    #here we store the protein original fasta\n",
    "    main_prot_seq = get_gene_fasta(query)\n",
    "    \n",
    "    #print(main_prot_seq)\n",
    "    \n",
    "    #setup directories and makes batch download pdb input file\n",
    "    chain_dict = setup_directories_prepare_input_files_1(gene_name = main_prot_name,\n",
    "                                          prot_fasta = main_prot_seq,\n",
    "                                          template=templates)\n",
    "    \n",
    "\n",
    "    #print(f\"we work currently in the directory: {main_prot_name}\")\n",
    "    #print(\"we start download\")\n",
    "    #we download both pdbs as well as mmcif files here.\n",
    "\n",
    "    #step 1 download\n",
    "    batch_download_pdbs(gene_name=main_prot_name)\n",
    "    \n",
    "    #print(\"we start shifts\")\n",
    "    #correct eventual shifts\n",
    "    #step 2 correct shifts\n",
    "    shift_dict = parallel_shift_calculation(pdbfolder=path)\n",
    "    \n",
    "    #make new dirs within gene directory for higher oligomers if available.\n",
    "\n",
    "    #get_single_chains_pdbs(pdb_dict=chain_dict, path=path, \n",
    "    #                       domain_dict=domain_dict, shift_dict=shift_dict)\n",
    "    \n",
    "    #print(\"we start renumber whole strucs\")\n",
    "    #try to renumber whole structures before building oligomers\n",
    "\n",
    "    #step 3 renumber \n",
    "    parallel_renumbering(shift_dict=shift_dict, path=path)\n",
    "    \n",
    "    #step 4 rechain everything.\n",
    "    oligostates = get_biological_assemblies_atomium_2(path=path,\n",
    "                                        gene_name=main_prot_name,\n",
    "                                        main_iso_seq=main_prot_seq,\n",
    "                                        main_protein_seq=main_prot_seq)\n",
    "\n",
    "    print(oligostates)\n",
    "\n",
    "    #now lets rename 0 1 2 3 4 files to A B C D E etc.. and also remove those oligomers that are artefacts.\n",
    "\n",
    "    try:\n",
    "        \n",
    "        path_merged = os.path.join(path, \"merged_cleaned_files\")\n",
    "\n",
    "        remove_artifacts_and_rename(path_merged=path_merged, oligostates=oligostates)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"error in remove_artifacts_and_rename\")\n",
    "        print(e)\n",
    "\n",
    "    \n",
    "    #before going to oligostates we should now try to check the real ranges of structures.\n",
    "    #step 5 get all ranges.\n",
    "\n",
    "    \n",
    "    \n",
    "    pdb_ranges = get_real_ranges(path=path_merged)\n",
    "\n",
    "    for keys, vals in pdb_ranges.items():\n",
    "        print(keys, vals)\n",
    "\n",
    "\n",
    "    #maybe first rechain !\n",
    "\n",
    "    domain_dict = _split_domains_pdb(pdb_ranges, main_prot_seq)\n",
    "\n",
    "\n",
    "    \n",
    "    for keys, vals in domain_dict.items():\n",
    "        print(keys)\n",
    "        for s in vals:\n",
    "            print(s)\n",
    "\n",
    "    #lets make the required directories.\n",
    "    make_oligo_dirs1(pdb_ranges, path_merged, domain_dict)\n",
    "    \n",
    "    \n",
    "    #maybe clean up pdbs in path..\n",
    "    _cleanup(path=path)\n",
    "\n",
    "    #works until here..\n",
    "    \n",
    "    #prepare templates for each domain and each oligomeric state respectively.\n",
    "    template_lst = prepare_templates1(main_directory=path_merged, \n",
    "                                      main_protein_sequence=main_prot_seq)  \n",
    "\n",
    "    \n",
    "    #works until here..\n",
    "\n",
    "    #next we need to go through all found template_lists directories and do the rest of the pipeline.\n",
    "    \n",
    "    #compute tm scores and rmsd\n",
    "\n",
    "    \n",
    "    #here we should make sure to get multiple templates for each domain range that is interesting.\n",
    "\n",
    "\n",
    "    #this is where we store a report of tm scores and rmsd for each structure against template.\n",
    "    report_path = os.path.join(path, \"reports\")\n",
    "    \n",
    "    #this should also cover oligomers!\n",
    "    USAlign1(template_list=template_lst,\n",
    "             main_prot_name=main_prot_name, \n",
    "             outpath_report=report_path)\n",
    "\n",
    "    #lets run mmseq now to gather info about conservation.\n",
    "    #this block is out commented because it works and we dont need to retest it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #run mmseq to find homolog sequences based on the majority path.\n",
    "\n",
    "        out_dir = f\"{path}/mmseq_conservation\"\n",
    "        #print(\"we start mmseq now!\")\n",
    "        mmseq_fasta_result = mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=out_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "\n",
    "        #print(\"we got a df!\")\n",
    "        conserv_df.to_csv(f\"{out_dir}/conservation_df.csv\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \"\"\"\n",
    "\n",
    "    #template_lst [((directories, current_hit, template_score))] *only parse through that directories because others will be < 5 struc.\n",
    "    \n",
    "    pca_path = run_pca_for_all_oligomers1(pdb_and_template_lst=template_lst,\n",
    "                                          main_prot_seq=main_prot_seq)\n",
    "\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"DO A MAJORITY VOTE TO DECIDE ON THE RELEVANT OLIGOMERIC STATE. WE carry on for all cases where we have 5 + structures.\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    #oligo_state_to_check = path/to/most/structures\n",
    "    #print(\"this is relevant oligomers input\")\n",
    "    #print(path, templates_new_dict)\n",
    "    check_paths_with_templates = _relevant_oligomers_1(path=path, templates_new_dict=templates_new_dict)\n",
    "    # FUNCTION CALL (ALL_DIRS_WITH_STRUCS):\n",
    "    # RETURN : MAJORITY CLASS \n",
    "    # CONTINUE with majority class ONLY.\n",
    "    print(\"this is check path with templates\")\n",
    "    print(check_paths_with_templates)\n",
    "\n",
    "\n",
    "    return\n",
    "    try:\n",
    "        #run mmseq to find homolog sequences based on the majority path.\n",
    "\n",
    "        out_dir = f\"{path}/mmseq\"\n",
    "        #print(\"we start mmseq now!\")\n",
    "        mmseq_fasta_result = mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=out_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "\n",
    "        #print(\"we got a df!\")\n",
    "        conserv_df.to_csv(f\"{out_dir}/conservation_df.csv\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #lets stop here.\n",
    "\n",
    "    if run_PCA:\n",
    "\n",
    "        for oligo_path, template in check_paths_with_templates:\n",
    "            pca_path = run_pca_for_all_oligomers(path=oligo_path, \n",
    "                              template=template,\n",
    "                              main_prot_seq=main_prot_seq, \n",
    "                              prot_name=main_prot_name)\n",
    "\n",
    "            try:\n",
    "                #print(f\"this is pca_path: {pca_path}\")\n",
    "                path_ensemble = os.path.join(pca_path, \"ensemble.txt\")\n",
    "                #print(f\"this is path_ensemble: {path_ensemble}\")\n",
    "                save_path =  os.path.join(pca_path, \"cluster_plot.png\")\n",
    "        \n",
    "                #print(f\"this is save_path: {save_path}\")\n",
    "                #print(f\"this is main_prot_name: {main_prot_name}\")\n",
    "                pca_12_proj_path = os.path.join(pca_path, f\"{main_prot_name}.mode_12.proj\")\n",
    "                \n",
    "                #print(f\"this is pca_12_proj_path: {pca_12_proj_path}\")\n",
    "                \n",
    "                #get from PCA the different clusters in order to select structures that are used in downstream NMA.\n",
    "                pca_df = get_cluster_df(PCA_12_proj_file=pca_12_proj_path,\n",
    "                                       prot_names=path_ensemble,\n",
    "                                       save_path=save_path,\n",
    "                                   ensemble_name=main_prot_name)\n",
    "    \n",
    "                #print(f\"this is pca_df: {pca_df.shape}\")\n",
    "                clean_dir = os.path.join(pca_path, \"clean_ensemble\")\n",
    "    \n",
    "                #print(f\"this is clean_dir: {clean_dir}\")\n",
    "                #get all structures that are assigned -1 as label (i.e not assigned to any cluster) and from each assigned cluster 1 representative structure\n",
    "                #that represents the closest to the main ensemble.\n",
    "                \n",
    "                representative_df = get_representative_file_paths(clean_dir=clean_dir, \n",
    "                                                  representative_df=pca_df)\n",
    "        \n",
    "                #get nma for selected structures\n",
    "    \n",
    "                #print(\"we start get_nma_domenico_here: \")\n",
    "                \n",
    "                work_dir = get_nma_domenico(representative_df, pca_path)\n",
    "    \n",
    "        \n",
    "                lst_of_dirs_to_parse = get_all_directories(work_dir)\n",
    "                # lets fetch now all b facs that are calculated from domenicos nma \n",
    "                #print(lst_of_dirs_to_parse)\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    executor.map(process_b_fac_parallel, lst_of_dirs_to_parse)\n",
    "    \n",
    "                #extracts all the relevant pdb files that we need for downstream analysis (comp and exp b factor files: i.e 2x the number of original_pdbs)\n",
    "                orig_b_fac , comp_b_fac = get_pdb_paths_for_hinges(work_dir=work_dir)\n",
    "    \n",
    "                \n",
    "                #both calls will compute hinges based on NMA b facs and experimental b factors.\n",
    "                hinges_orig_b_fac = hinges_parallelized_detection(path_to_pdbs=orig_b_fac)\n",
    "                hinges_comp_b_fac = hinges_parallelized_detection(path_to_pdbs=comp_b_fac)\n",
    "    \n",
    "                try:\n",
    "                    #we save both dictionaryies in the normal NMA folder which is 1 level above the individual files (i.e where the summary over all files belong)\n",
    "                    save_original_b_fac = os.path.join(work_dir, f\"{main_prot_name}_exp_b_fac_hinges.json\")\n",
    "                    save_comp_b_fac = os.path.join(work_dir, f\"{main_prot_name}_comp_b_fac_hinges.json\")\n",
    "    \n",
    "                    save_dict_to_json(hinges_orig_b_fac, save_original_b_fac)\n",
    "                    save_dict_to_json(hinges_comp_b_fac, save_comp_b_fac)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \n",
    "                #this merged dictionary will also be saved.\n",
    "                merged_hinge_dict = merge_hinge_dicts(dict1=hinges_orig_b_fac, dict2=hinges_comp_b_fac)\n",
    "    \n",
    "                #save merged dict.\n",
    "                try:\n",
    "                    save_merged_b_fan_hinges = os.path.join(work_dir, f\"{main_prot_name}_merged_hinges.json\")\n",
    "                    save_dict_to_json(merged_hinge_dict, save_merged_b_fan_hinges)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    #here is second stop point.\n",
    "    \n",
    "    return\n",
    "    # Initialize variables to avoid errors\n",
    "    updated_clinvar_df = cbioport_df = cosmic_df = gnomad_df = clinvar_df = gnomad_mut_dict = gnomad_mutation_dict = None\n",
    "\n",
    "    \n",
    "    # Step 1: Cosmic mutations\n",
    "    try:\n",
    "        cosmic_df = get_cosmic_mutations(gene_name=main_prot_name)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    #we save it in the folder for the protein outside of monomer / pos at the base level. \n",
    "    save_dataframe_to_csv(cosmic_df, path, \"cosmic_mutations\")\n",
    "\n",
    "    # Step 2: Map gnomad mutations\n",
    "    try:\n",
    "        gnomad_table_path = map_gnomad(Gene_name=main_prot_name, outpath=oligo_state_to_check)\n",
    "        gnomad_df, gnomad_mutation_dict = gnomad_to_pandas(Gene_name=main_prot_name, path_to_tsv=gnomad_table_path, fasta_seq=main_prot_seq)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(gnomad_df, path, \"gnomad_mutations\")\n",
    "\n",
    "    # Step 3: Gather mutations from clinvar\n",
    "    try:\n",
    "        clinvar_df = map_clinvar(Gene_name=main_prot_name)\n",
    "        clinvar_map_outpath = f\"{path}/clinvar_intermediate.csv\"\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(clinvar_df, path, \"clinvar_intermediate\")\n",
    "\n",
    "    # Step 4: Map clinvar to gnomad\n",
    "    try:\n",
    "        list_to_be_searched, clinvar_df = map_clinvar_to_gnomad_1(Gene_name=main_prot_name, clinvar_df=clinvar_df,\n",
    "                                                                  gnomad_mut_dict=gnomad_mutation_dict, clinvar_mapped_df_path=clinvar_map_outpath)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        # Update clinvar muts that were found 1 step before.\n",
    "        try:\n",
    "            updated_clinvar_df = update_clinvar_muts_based_on_gnomad(clinvar_df=clinvar_df, gnomad_dict=gnomad_mutation_dict)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(updated_clinvar_df, path, \"clinvar_mutations\")\n",
    "\n",
    "    # Step 5: Fetch additional info from cbioportal\n",
    "    try:\n",
    "        gene_name = get_hugo_name(uniprot_id)\n",
    "        print(f\"This is gene name in hugo: {gene_name}\")\n",
    "        cbioport_df = get_cbioportal_info(gene_name=gene_name)\n",
    "        save_dataframe_to_csv(cbioport_df, path, \"cbioport_mutations\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    # Print shapes (if available)\n",
    "    dataframes = [cosmic_df, updated_clinvar_df, gnomad_df, cbioport_df]\n",
    "    for df in dataframes:\n",
    "        try:\n",
    "            print(f\"This is df shape: {df.shape}\")\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3419,
   "id": "2e913e73-84ad-4a11-933e-4a702afc2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to handle the common task of saving a DataFrame to CSV\n",
    "def save_dataframe_to_csv(df, path, filename):\n",
    "    try:\n",
    "        df.to_csv(f\"{path}/{filename}.csv\", index=False)\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3420,
   "id": "70e13565-0393-4f26-a3d7-f71103ff291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cosmic = get_cosmic_mutations(gene_name=\"NUDT4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3421,
   "id": "4c687669-cb03-4b1e-8ff4-4188e8aa12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cosmic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1bc97-83c5-4855-a4a0-679cd504eff7",
   "metadata": {},
   "source": [
    "# MUTATIONAL MAPPING NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3422,
   "id": "03ba8836-89b3-4301-becf-4ea7976ae9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosmic_mutations(gene_name:str):\n",
    "\n",
    "    path = \"/home/micnag/bioinformatics/cosmic/Cosmic_GenomeScreensMutant_v98_GRCh38.tsv\"\n",
    "    \n",
    "    usecols=['GENE_SYMBOL',\n",
    "         'MUTATION_AA', 'MUTATION_DESCRIPTION',\n",
    "       'MUTATION_ZYGOSITY', 'LOH', 'CHROMOSOME', 'GENOME_START', 'GENOME_STOP']\n",
    "\n",
    "    df = dd.read_csv(path, sep=\"\\t\", dtype={'CHROMOSOME': 'object',\n",
    "       'MUTATION_ZYGOSITY': 'object', 'GENOME_START': 'float64',\n",
    "       'GENOME_STOP': 'float64',\n",
    "       'LOH': 'object'}, usecols=usecols)\n",
    "\n",
    "    #we need to switch these tuples and then map the 1letter aa code to 3letter aa for later compatibility.\n",
    "    lst =  [('Val',\"V\"), ('Ile',\"I\"), ('Leu',\"L\"), ('Glu',\"E\"), ('Gln',\"Q\"),\n",
    "                    ('Asp',\"D\"), ('Asn',\"N\"), ('His',\"H\"), ('Trp',\"W\"), ('Phe',\"F\"), ('Tyr',\"Y\"), \n",
    "                    ('Arg',\"R\"), ('Lys',\"K\"), ('Ser',\"S\"), ('Thr',\"T\"), ('Met',\"M\"), ('Ala',\"A\"), \n",
    "                    ('Gly',\"G\"), ('Pro',\"P\"), ('Cys',\"C\")]\n",
    "\n",
    "    lst = [(y, x) for x, y in lst]\n",
    "\n",
    "    canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "\n",
    "    df_re = df[df[\"MUTATION_DESCRIPTION\"].str.contains(\"missense\")]\n",
    "    \n",
    "    df_re = df_re[df_re[\"GENE_SYMBOL\"] == f\"{gene_name}\"]\n",
    "\n",
    "    meta = ('Gene name', 'str') \n",
    "    df_re['CHROMOSOME'] = df_re['CHROMOSOME'].astype('object')\n",
    "    df_re['WT_AA'] = df_re['MUTATION_AA'].str[2].apply(lambda x: canonical_aas[x], meta=meta)\n",
    "    df_re['MUTATION_POSITION'] = df_re['MUTATION_AA'].str[3:-1]\n",
    "    df_re['MUTATED_AA'] = df_re['MUTATION_AA'].str[-1].apply(lambda x: canonical_aas[x], meta=meta)\n",
    "\n",
    "    df_re = df_re.drop(\"MUTATION_AA\", axis=1)\n",
    "    \n",
    "    cosmic_df = df_re.compute()\n",
    "\n",
    "    cosmic_df[\"GENOME_START\"] = cosmic_df[\"GENOME_START\"].astype(int)\n",
    "    cosmic_df[\"GENOME_STOP\"] = cosmic_df[\"GENOME_STOP\"].astype(int)\n",
    "    \n",
    "    return cosmic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3423,
   "id": "cc889c2f-22c3-4aaa-b69e-f72e4646a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gnomad(Gene_name:str, outpath:str):\n",
    "\n",
    "    outpath = f\"{outpath}/gnomad_datatable.txt\"\n",
    "    #outpath=\"/home/micnag/bioinformatics/hail_trials/newline_test.tsv\"\n",
    "    \n",
    "    path = '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/gnomad_data.mt'\n",
    "    #path1 = \"/home/micnag/bioinformatics/gnomad_raw_data/gnomad_data_2.mt\"\n",
    "    mt = hl.read_matrix_table(path)\n",
    "    \n",
    "    substring1 = Gene_name\n",
    "    substring2 = \"missense\"\n",
    "    \n",
    "    \n",
    "    mt = mt.annotate_rows(Gene_names=mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[3]) ,\n",
    "                      type_of_change = mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[1]) , \n",
    "                      AA_change = mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[11]) , \n",
    "                      ENST_identifier= mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[6])\n",
    "\n",
    "    ) \n",
    "             \n",
    "    filtered_mt_2 = mt.filter_rows(\n",
    "    \n",
    "    #hl.any(lambda x: hl.str(x).contains(substring3), mt.AA_change)\n",
    "    hl.any(lambda x: hl.str(x).contains(substring1), mt.info.vep) &\n",
    "    hl.any(lambda x: hl.str(x).contains(substring2), mt.info.vep)\n",
    "    \n",
    "    )\n",
    "                     \n",
    "    filtered_mt_3 = filtered_mt_2.annotate_rows(\n",
    "        Allele_count_int = filtered_mt_2.info.AC,\n",
    "        Allele_frequency_float = filtered_mt_2.info.AF,\n",
    "        Allele_number_int = filtered_mt_2.info.AN,\n",
    "        Gene_name_str = _replace_empty(filtered_mt_2.Gene_names), \n",
    "        Mutation_change_str = _replace_empty(filtered_mt_2.AA_change),\n",
    "        Type_of_change_str = _replace_empty(filtered_mt_2.type_of_change))\n",
    "    \n",
    "    \n",
    "    \n",
    "    rows_to_keep = [\"Gene_name_str\", \"Mutation_change_str\", \"Type_of_change_str\", \"Allele_count_int\",\n",
    "                \"Allele_frequency_float\", \"Allele_number_int\"]\n",
    "\n",
    "\n",
    "    selected_rows = filtered_mt_3.select_rows(\n",
    "        Allele_count_int=filtered_mt_3.Allele_count_int,\n",
    "        Allele_frequency_float=filtered_mt_3.Allele_frequency_float,\n",
    "        Allele_number_int=filtered_mt_3.Allele_number_int,\n",
    "        Gene_name_str=hl.str(filtered_mt_3.Gene_name_str),\n",
    "        Mutation_change_str=hl.str(filtered_mt_3.Mutation_change_str),\n",
    "        Type_of_change_str=hl.str(filtered_mt_3.Type_of_change_str)\n",
    "            )\n",
    "\n",
    "    save_buffer = selected_rows.select_rows(*rows_to_keep)\n",
    "    \n",
    "    select_rows_out = save_buffer.rows()\n",
    "    \n",
    "    select_rows_out.export(outpath)\n",
    "    \n",
    "    #this is the location where we save the results.\n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3424,
   "id": "bd5de7fc-51bf-45af-bf9b-843945eb3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnomad_to_pandas(Gene_name:str,path_to_tsv:str, fasta_seq:str):\n",
    "    \n",
    "    df = pd.read_csv(path_to_tsv, sep=\"\\t\")\n",
    "    pd.set_option('display.float_format', '{:.2e}'.format)\n",
    "    \n",
    "    #modifying and streamlining input for further downstream compatibility.\n",
    "    df['Allele_frequency_float'] = df['Allele_frequency_float'].str.strip('[]').astype(float)\n",
    "    df['Allele_count_int'] = df['Allele_count_int'].str.strip('[]').astype(int)\n",
    "    \n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].astype(\"string\")\n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].str.replace(\"[\", \"\")\n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].str.replace(\"]\", \"\")\n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].str.replace(\"\\\"\", \"\")\n",
    "    \n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].astype(\"string\")\n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].str.replace(\"[\", \"\")\n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].str.replace(\"]\", \"\")\n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].str.replace(\"\\\"\",\"\")\n",
    "    \n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].astype(\"string\")\n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].str.replace(\"[\", \"\")\n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].str.replace(\"]\", \"\")\n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].str.replace(\"\\\"\",\"\")\n",
    "\n",
    "    #required to parse the info.\n",
    "    lst =  [('Val',\"V\"), ('Ile',\"I\"), ('Leu',\"L\"), ('Glu',\"E\"), ('Gln',\"Q\"),\n",
    "                    ('Asp',\"D\"), ('Asn',\"N\"), ('His',\"H\"), ('Trp',\"W\"), ('Phe',\"F\"), ('Tyr',\"Y\"), \n",
    "                    ('Arg',\"R\"), ('Lys',\"K\"), ('Ser',\"S\"), ('Thr',\"T\"), ('Met',\"M\"), ('Ala',\"A\"), \n",
    "                    ('Gly',\"G\"), ('Pro',\"P\"), ('Cys',\"C\")]\n",
    "    \n",
    "    canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "\n",
    "    Mut_dict = defaultdict()\n",
    "\n",
    "    for x, y, z, allele_cnt, allele_freq, allele_num \\\n",
    "        in zip(df[\"Gene_name_str\"], df[\"Mutation_change_str\"], df[\"Type_of_change_str\"],\n",
    "          df[\"Allele_count_int\"], df[\"Allele_frequency_float\"], df[\"Allele_number_int\"]):\n",
    "        \n",
    "        tmp = x.split(\",\")\n",
    "        tmp2 = y.split(\",\")\n",
    "        tmp3 = z.split(\",\")\n",
    "        #print(tmp, tmp2, tmp3)\n",
    "        #print(allele_cnt, allele_freq, allele_num)\n",
    "        for x1, y1, z1 in zip(tmp, tmp2, tmp3):\n",
    "            \n",
    "            #print(x1, y1, z1)\n",
    "            if x1 == Gene_name and y1[0:4] == \"ENSP\":\n",
    "                gene_code = x1\n",
    "                mutation_code = y1[20:]\n",
    "                type_change = z1\n",
    "                \n",
    "                result = _check_proper_transcript(fasta_seq=fasta_seq,\n",
    "                                                  mutation_code=mutation_code, \n",
    "                                                  dict_to_check=canonical_aas, allele_count=allele_cnt)\n",
    "                \n",
    "                #print(gene_code, mutation_code, type_change)\n",
    "                if result:\n",
    "                    Mut_dict[mutation_code] = (allele_cnt, allele_freq, allele_num)\n",
    "        \n",
    "    \n",
    "    #we return the dictionary with out mutations and their respective allele_cnts, allele_freqs, allele_nums\n",
    "    return df, Mut_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3425,
   "id": "61b7338c-1d57-4650-b3a4-61850e74884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clinvar(Gene_name:str):\n",
    "    \n",
    "    path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/clinvar_database/variant_summary.txt\"\n",
    "\n",
    "    use_cols = [\"Type\", \"Name\", \"GeneSymbol\",\n",
    "           \"ClinicalSignificance\", \"PhenotypeList\",\n",
    "           \"Assembly\", \"ChromosomeAccession\", \n",
    "           \"Chromosome\", \"Start\", \"Stop\"]\n",
    "\n",
    "    column_data_types = {\n",
    "    \"Type\": str,\n",
    "    \"Name\": str,\n",
    "    \"GeneSymbol\": str,\n",
    "    \"ClinicalSignificance\": str,\n",
    "    \"PhenotypeList\": str,\n",
    "    \"Assembly\": str,\n",
    "    \"ChromosomeAccession\": str,\n",
    "    \"Chromosome\": str,\n",
    "    \"Start\": int,\n",
    "    \"Stop\": int\n",
    "    }\n",
    "\n",
    "    df_work = pd.read_csv(path, sep=\"\\t\", usecols=use_cols, dtype=column_data_types)\n",
    "    \n",
    "    df_work.loc[:, \"AA_change\"] = df_work[\"Name\"].str.split().str.get(-1)\n",
    "    df_work.loc[:, \"AA_change\"] = df_work[\"AA_change\"].str.replace(\"(\", \"\")\n",
    "    df_work.loc[:, \"AA_change\"] = df_work[\"AA_change\"].str.replace(\")\", \"\")\n",
    "    \n",
    "    df_work.loc[:,\"Original_AA\"] = df_work[\"AA_change\"].str[2:5]\n",
    "    df_work.loc[:,\"Modified_AA\"] = df_work[\"AA_change\"].str[-3:]\n",
    "    df_work['Position'] = pd.to_numeric(df_work['AA_change'].str[5:-3], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN values in the 'Position' column\n",
    "    df_work.dropna(subset=['Position'], inplace=True)\n",
    "    df_work['Position'] = df_work['Position'].astype(int)\n",
    "    \n",
    "    df_work[\"Genomic_location\"] = df_work[\"Chromosome\"] + \":\" + df_work[\"Start\"].astype(str)\n",
    "    df_work[\"gnomad_aa_change\"] = \"p.\" + df_work[\"Original_AA\"] + df_work[\"Position\"].astype(str) + df_work[\"Modified_AA\"]\n",
    "    \n",
    "    df_work = df_work.drop(\"AA_change\", axis=1)\n",
    "    df_work = df_work.drop(\"Name\", axis=1)\n",
    "    df_work = df_work.drop(\"Chromosome\", axis=1)\n",
    "    df_work = df_work.drop(\"Start\", axis=1)\n",
    "    df_work = df_work.drop(\"Stop\", axis=1)\n",
    "    \n",
    "    df_work[\"Allele_count\"] = [np.nan] * len(df_work)\n",
    "    df_work[\"Allele_number\"] = [np.nan] * len(df_work)\n",
    "    df_work[\"Allele_frequency\"] = [np.nan] * len(df_work)\n",
    "    \n",
    "    \n",
    "    accepted_residues = [\"Ala\", \"Gly\", \"Ser\", \"Leu\", \"Pro\",\n",
    "                    \"Ile\", \"Val\", \"Phe\", \"Tyr\", \"Trp\",\n",
    "                     \"His\", \"Thr\", \"Asn\", \"Gln\", \"Asp\", \n",
    "                     \"Glu\",\"Cys\", \"Met\", \"Lys\", \"Arg\"]\n",
    "    \n",
    "    \n",
    "    #filtering based on our Gene name.\n",
    "    df_filtered = df_work[(df_work[\"Type\"] == \"single nucleotide variant\") & \n",
    "        (df_work[\"GeneSymbol\"] == Gene_name) &\n",
    "        (df_work[\"Assembly\"] == \"GRCh37\") & \n",
    "        (df_work['Original_AA'].isin(accepted_residues)) &\n",
    "        (df_work['Modified_AA'].isin(accepted_residues)) ]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3426,
   "id": "28b8b170-5a1c-484d-b9ec-83eafb7f1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clinvar_to_gnomad_1(Gene_name: str, clinvar_df: pd.DataFrame,\n",
    "                         gnomad_mut_dict: dict,\n",
    "                         clinvar_mapped_df_path: str):\n",
    "\n",
    "    path = '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/gnomad_data.mt'\n",
    "    mt = hl.read_matrix_table(path)\n",
    "\n",
    "    hits = set(gnomad_mut_dict.keys())\n",
    "\n",
    "    found = defaultdict(tuple)\n",
    "    to_be_searched = []\n",
    "\n",
    "    for x, y in zip(clinvar_df[\"gnomad_aa_change\"], clinvar_df[\"Genomic_location\"]):\n",
    "        if x[2:] in hits:\n",
    "            found[y] = gnomad_mut_dict[x[2:]]\n",
    "        else:\n",
    "            to_be_searched.append((x, y))\n",
    "        \n",
    "\n",
    "    # now we try to search for those hits that were not found in the gnomad dataset.\n",
    "    hit_test = [x[1] for x in to_be_searched]\n",
    "\n",
    "    if not hit_test:\n",
    "        return None\n",
    "\n",
    "    loci_to_filter_hail = hl.set(hit_test)\n",
    "\n",
    "    mt = mt.annotate_rows(Gene_names=mt.info.vep.map(\n",
    "        lambda x: x.split(\"|\")[3]),\n",
    "        type_of_change=mt.info.vep.map(\n",
    "            lambda x: x.split(\"|\")[1]),\n",
    "        AA_change=mt.info.vep.map(\n",
    "            lambda x: x.split(\"|\")[11]),\n",
    "        ENST_identifier=mt.info.vep.map(\n",
    "            lambda x: x.split(\"|\")[6]),\n",
    "        Allele_count_int=mt.info.AC,\n",
    "        Allele_frequency_float=mt.info.AF,\n",
    "        Allele_number_int=mt.info.AN\n",
    "    )\n",
    "\n",
    "    filtered_mt_2 = mt.filter_rows(\n",
    "        loci_to_filter_hail.contains(mt.locus)\n",
    "    )\n",
    "\n",
    "    selected_row_fields = [\"Allele_count_int\", \"Allele_number_int\", \"Allele_frequency_float\"]\n",
    "\n",
    "    selected_rows = filtered_mt_2.rows()\n",
    "\n",
    "    sel_output = selected_rows.select(*selected_row_fields)\n",
    "\n",
    "    # Convert Hail table to Pandas DataFrame\n",
    "    sel_output_df = sel_output.to_pandas()\n",
    "\n",
    "    # Merge the DataFrame with the original clinvar_df\n",
    "    merged_df = pd.merge(clinvar_df, sel_output_df, left_on=[\"gnomad_aa_change\", \"Genomic_location\"], \n",
    "                         right_on=[\"Allele_count_int\", \"Allele_number_int\"], how=\"left\")\n",
    "\n",
    "    # Export the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(clinvar_mapped_df_path, index=False)\n",
    "\n",
    "    # we return the outpath for further downstream work\n",
    "    return to_be_searched, clinvar_mapped_df_path\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3427,
   "id": "2ecbd603-ea49-4368-b579-22506be2a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clinvar_to_gnomad(Gene_name:str, clinvar_df:pd.DataFrame,\n",
    "                         gnomad_mut_dict:dict,\n",
    "                         clinvar_mapped_df_path:str):\n",
    "    \n",
    "    path = '/home/micnag/bioinformatics/gnomad_raw_data/gnomad_data.mt'\n",
    "    #path1 = \"/home/micnag/bioinformatics/gnomad_raw_data/gnomad_data_2.mt\"\n",
    "    mt = hl.read_matrix_table(path)\n",
    "    \n",
    "    hits = []\n",
    "\n",
    "    to_be_searched = []\n",
    "    found = defaultdict(tuple)\n",
    "\n",
    "    for keys, vals in gnomad_mut_dict.items():\n",
    "        #print(keys, vals)\n",
    "        hits.append(keys)\n",
    "\n",
    "    for x, y in zip(clinvar_df[\"gnomad_aa_change\"], clinvar_df[\"Genomic_location\"]):\n",
    "        if x[2:] in hits:\n",
    "            found[y] = (gnomad_mut_dict[x[2:]])\n",
    "        else:\n",
    "            to_be_searched.append((x, y))\n",
    "            #print(x,\"not found\")\n",
    "\n",
    "    #now we try to search for those hits that were not found in the gnomad dataset.\n",
    "    hit_test = [x[1] for x in to_be_searched]\n",
    "\n",
    "\n",
    "    if len(hit_test) == 0:\n",
    "        return None\n",
    "        \n",
    "    #print(len(hit_test)) #126 but only 26 annotated in gnomad.\n",
    "    loci_to_filter_hail = [hl.parse_locus(locus) for locus in hit_test]\n",
    "\n",
    "    #we annotate these rows that bare information that is interesting for us.\n",
    "    mt = mt.annotate_rows(Gene_names=mt.info.vep.map(\n",
    "        \n",
    "        lambda x: x.split(r\"|\")[3]) ,\n",
    "                          \n",
    "                      type_of_change = mt.info.vep.map(\n",
    "        lambda x: x.split(r\"|\")[1]) , \n",
    "                          \n",
    "                      AA_change = mt.info.vep.map(\n",
    "        lambda x: x.split(r\"|\")[11]) , \n",
    "                          \n",
    "                      ENST_identifier= mt.info.vep.map(\n",
    "        lambda x: x.split(r\"|\")[6]),\n",
    "        \n",
    "                      Allele_count_int = mt.info.AC,\n",
    "                          \n",
    "                      Allele_frequency_float = mt.info.AF,\n",
    "                          \n",
    "                      Allele_number_int = mt.info.AN\n",
    "    ) \n",
    "\n",
    "    \n",
    "    filtered_mt_2 = mt.filter_rows(\n",
    "    \n",
    "        #hl.any(lambda x: hl.str(x).contains(substring3), mt.AA_change)\n",
    "        #hl.any(lambda x: hl.str(x).contains(substring1), mt.info.vep) &\n",
    "        #hl.any(lambda x: hl.str(x).contains(substring2), mt.info.vep) &\n",
    "        hl.literal(loci_to_filter_hail).contains(mt.locus)\n",
    "    \n",
    "    ) \n",
    "    \n",
    "    selected_row_fields = [\"Allele_count_int\", \"Allele_number_int\", \"Allele_frequency_float\"]\n",
    "\n",
    "    #selected_rows = selected_rows.rows()\n",
    "\n",
    "    selected_row_field_1 = filtered_mt_2.select_rows(*selected_row_fields)\n",
    "\n",
    "    sel_output = selected_row_field_1.rows()\n",
    "\n",
    "    sel_output.export(clinvar_mapped_df_path)\n",
    "    \n",
    "    #we return the outpath for further downstream work\n",
    "    return to_be_searched, clinvar_mapped_df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3428,
   "id": "08d6f3d6-ac77-4a77-bb52-295c4a2a1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_clinvar_muts_based_on_gnomad(clinvar_df:pd.DataFrame,\n",
    "                                       gnomad_dict:dict):\n",
    "\n",
    "    clinvar_rev_path = \"/home/micnag/bioinformatics/hail_trials/reverse_mapping_test.tsv\"\n",
    "    \n",
    "    df_reload = pd.read_csv(clinvar_rev_path,\n",
    "                            sep=\"\\t\")\n",
    "\n",
    "    #df_reload.shape\n",
    "\n",
    "    #df_reload.columns\n",
    "    df_reload['Allele_frequency_float'] = df_reload['Allele_frequency_float'].str.strip('[]').astype(float)\n",
    "    df_reload['Allele_count_int'] = df_reload['Allele_count_int'].str.strip('[]').astype(int)\n",
    "\n",
    "    for x, allele_c, allele_num, allele_freq in zip(df_reload[\"locus\"],\n",
    "                                                    df_reload[\"Allele_count_int\"],\n",
    "                                                    df_reload[\"Allele_number_int\"],\n",
    "                                                    df_reload['Allele_frequency_float']):\n",
    "        #print(x, allele_c, allele_freq, allele_num)\n",
    "        if x not in gnomad_dict.keys():\n",
    "            gnomad_dict[x] = (allele_c, allele_freq, allele_num)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    #setup dummies for mapping.\n",
    "    clinvar_df[\"Allele_count\"] = [np.nan] * len(clinvar_df)\n",
    "    clinvar_df[\"Allele_number\"] = [np.nan] * len(clinvar_df)\n",
    "    clinvar_df[\"Allele_frequency\"] = [np.nan] * len(clinvar_df)\n",
    "    \n",
    "    for k, v in gnomad_dict.items():\n",
    "    \n",
    "        #k = chr:position e.g 16:17464758  and v = (Allele_cnt, Allele_freq, Allele_num)\n",
    "        clinvar_df.loc[clinvar_df[\"Genomic_location\"] == k, \"Allele_count\"] = int(v[0])\n",
    "        clinvar_df.loc[clinvar_df[\"Genomic_location\"] == k, \"Allele_frequency\"] = v[1]\n",
    "        clinvar_df.loc[clinvar_df[\"Genomic_location\"] == k, \"Allele_number\"] = int(v[2])\n",
    "    \n",
    "    \n",
    "    return clinvar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3429,
   "id": "364578b7-0b1e-4b82-9d21-0734512e1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_proper_transcript(fasta_seq:str, mutation_code:str,\n",
    "                             dict_to_check:dict, allele_count:int):\n",
    "    \n",
    "    #print(mutation_code)\n",
    "    try:\n",
    "        position = int(mutation_code[3:-3])\n",
    "        \n",
    "    except Exception as error:\n",
    "        return False\n",
    "    \n",
    "    #quality filter to return ONLY variants that passed the filter criteria.\n",
    "    if allele_count == 0:\n",
    "        return False\n",
    "    \n",
    "    aa_according_to_gnomad = mutation_code[0:3]\n",
    "    \n",
    "    aa_to_check = dict_to_check[aa_according_to_gnomad]\n",
    "    \n",
    "    aa_according_to_fasta = fasta_seq[position-1] #-1 because of python starting with 0\n",
    "    \n",
    "    #print(aa_to_check, aa_according_to_fasta)\n",
    "    if aa_to_check == aa_according_to_fasta:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3430,
   "id": "26077e38-3d9a-4512-9efa-6f337546f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_empty(arr):\n",
    "    return hl.map(lambda x: hl.if_else(x == \"\", \"empty\", x), arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271790be-a65f-43b5-affd-f8117565b80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5031d-9884-438d-a2a9-8c6fea5c58d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3431,
   "id": "5e07c586-4cf7-4e7e-a86c-fec78325fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relevant_oligomers_1(path:str, templates_new_dict:dict):\n",
    "    \n",
    "    \"\"\"This function basically will call a bunch of other functions \n",
    "    for each potential oligomeric state and each position within them.\"\"\"\n",
    "    \n",
    "    oligodirectories = [\"monomer\",\n",
    "                        \"dimer\",\n",
    "                        \"trimer\",\n",
    "                        \"tetramer\",\n",
    "                        \"pentamer\",\n",
    "                        \"hexamer\",\n",
    "                        \"heptamer\",\n",
    "                        \"oktamer\",\n",
    "                        \"nonamer\",\n",
    "                        \"decamer\",\n",
    "                        \"undecamer\",\n",
    "                        \"dodecamer\",\n",
    "                        \"tridecamer\",\n",
    "                        \"tetradecamer\",\n",
    "                        \"pentadecamer\",\n",
    "                        \"hexadecamer\",\n",
    "                        \"heptadecamer\",\n",
    "                        \"oktadecamer\",\n",
    "                        \"nonadecamer\",\n",
    "                        \"eicosamer\"\n",
    "    ]\n",
    "    \n",
    "    relevant_dirs = [os.path.join(path, file) for file in oligodirectories if os.path.isdir(os.path.join(path, file))]\n",
    "    \n",
    "    dir_dictionary = {dirs: os.listdir(dirs) for dirs in relevant_dirs}\n",
    "\n",
    "    path_to_check = [os.path.join(dirs, subdir) for dirs, vals in dir_dictionary.items() for subdir in vals]\n",
    "    \n",
    "    path_majority_vote = [\n",
    "        (check_paths, len(os.listdir(check_paths))) for check_paths in path_to_check if os.path.isdir(check_paths)\n",
    "    ]\n",
    "\n",
    "    majority_vote = sorted(path_majority_vote, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"this is majority vote\")\n",
    "    print(majority_vote)\n",
    "\n",
    "    # we keep all where we have at least 5 strucs.\n",
    "\n",
    "    \n",
    "    #[('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_149', 14),\n",
    "\n",
    "\n",
    "\n",
    "    #[('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_8_147', 23), \n",
    "    # ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/dimer/pos_8_147', 1), \n",
    "    # ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/dimer/pos_143_312', 1), \n",
    "    # ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/dimer/pos_1_203', 1), \n",
    "    # ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/tetramer/pos_8_147', 1), \n",
    "    # ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_143_312', 0)]\n",
    "\n",
    "    check_paths_with_templates = []\n",
    "    \n",
    "    for path, counts in majority_vote:\n",
    "        if counts >= 5:\n",
    "            accepted_path = path\n",
    "            key_parts = accepted_path.split('/')\n",
    "            relevant_key = f\"{key_parts[-2]}/{key_parts[-1]}\"\n",
    "            print(f\"this is relevant_key: {relevant_key} and this its templates_new_dict: {templates_new_dict}\")\n",
    "            template = templates_new_dict.get(relevant_key, [None])[0]    \n",
    "            check_paths_with_templates.append((path, template))\n",
    "\n",
    "    return check_paths_with_templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3432,
   "id": "1251b2af-2a99-4fa5-ab9e-1b7ee95322a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_b_factors_0(path:str):\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"test\"\n",
    "    \n",
    "    fullpath = f\"{path}\"\n",
    "    \n",
    "    structure = parser.get_structure(prot_name, fullpath)\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    atom.set_bfactor(0.0)\n",
    "    \n",
    "    \n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    \n",
    "    savepath = fullpath\n",
    "        \n",
    "    #we save all structures to the monomeric category.\n",
    "    io.save(savepath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3433,
   "id": "9e9bfea2-8af9-4a7b-b1d2-070d78b252f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_based_on_majority(pdbs, majority_start, majority_stop):\n",
    "\n",
    "    #pdbs is a list of abs paths\n",
    "    #maj start is the start where we shoudl accept residues.\n",
    "    #maj stop is the end where we shoudl accept residues.\n",
    "\n",
    "    class range_only(Select):\n",
    "        \n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "        \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[1] >= majority_start and residue.id[1] <= majority_stop else 0\n",
    "        \n",
    "    parser = PDBParser()\n",
    "    #lets cut all based on majority.\n",
    "    for pdb in pdbs:\n",
    "\n",
    "        structure = parser.get_structure(\"none\", pdb)\n",
    "        \n",
    "        io = PDBIO()\n",
    "        \n",
    "        io.set_structure(structure)\n",
    "    \n",
    "        save_path = pdb\n",
    "    \n",
    "        io.save(save_path, range_only())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3434,
   "id": "acf76059-a3ce-4d58-862c-90aa965e71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_for_all_oligomers1(pdb_and_template_lst:list, main_prot_seq:str):\n",
    "\n",
    "    #lets go through all templates.\n",
    "    #pdb_template_lst((directory, template_abs_path, template_score_michael))\n",
    "    \n",
    "    for directory, template_abs_path, template_score in pdb_and_template_lst:\n",
    "\n",
    "        pdbs = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "        oligostate = os.path.basename(os.path.dirname(directory))\n",
    "\n",
    "        print(f\"this is oligostate: {oligostate}\")\n",
    "        #we check if there are at least 4 strucs in there.\n",
    "        if len(pdbs) >= 4:\n",
    "            \n",
    "            #step 0 different pipeline for monomers vs oligomers..\n",
    "\n",
    "            #if else condition for monomer vs oligomer\n",
    "\n",
    "            if oligostate == \"monomer\":\n",
    "\n",
    "                #step 1.0 select only c-alpha for all strucs.\n",
    "\n",
    "                result_lengths_seqs = defaultdict()\n",
    "                #call select_c_alpha parallelized.\n",
    "\n",
    "                try:\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        \n",
    "                        # Map the function\n",
    "                        result = list(executor.map(select_c_alpha, pdbs))\n",
    "                        #struc_start, struc_stop, struc_full = return from select_c_alpha\n",
    "    \n",
    "                    #concurrent.futures.wait(result)\n",
    "    \n",
    "                    for struc_start, struc_stop, struc_full_seq, path_returned in result:\n",
    "                        #print(\"this is the result of c alpha partial\")\n",
    "                        #print(struc_start, struc_stop, struc_full_seq)\n",
    "                        result_lengths_seqs[path_returned] = (struc_start, struc_stop, struc_full_seq)\n",
    "                    \n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                    # Parallelize both functions using ProcessPoolExecutor\n",
    "                        remove_het_results = list(executor.map(remove_hetero_atoms_2, pdbs))\n",
    "                    \n",
    "    \n",
    "                    for non_canonical, pdb_path in remove_het_results:\n",
    "                        \n",
    "                        if non_canonical:\n",
    "                            \n",
    "                            for keys, vals in non_canonical.items():\n",
    "                                _mutate_non_standard_aa_1(pdb_path,\n",
    "                                non_standard_residue=vals[0],\n",
    "                                residue=keys,\n",
    "                                chain=vals[1])\n",
    "\n",
    "\n",
    "                    #for keys, vals in result_lengths_seqs.items():\n",
    "                    #    print(keys)\n",
    "                    #    print(vals)\n",
    "\n",
    "                    # here works.\n",
    "\n",
    "                    # now after we get seq length and removed heteroatoms and modified residues are transformed to alanine.\n",
    "            \n",
    "\n",
    "                    pdbs_left = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "                    print(f\"this is pdbs left: {pdbs_left}\")\n",
    "                    #grab majority ranges\n",
    "                    majority_start, majority_stop = vote_majority_start_stop(result_lengths_seqs=result_lengths_seqs)\n",
    "                    \n",
    "                    #we cut on the leftover files.\n",
    "                    cut_based_on_majority(pdbs_left, majority_start, majority_stop)\n",
    "\n",
    "                    #this is needed for modeller...\n",
    "                    os.chdir(directory)\n",
    "                    \n",
    "                    try:\n",
    "                        #now we repair all files.\n",
    "\n",
    "                        repair_pdb_partial = partial(repair_pdb_monomer, start=majority_start, stop=majority_stop)\n",
    "                        \n",
    "                        with ProcessPoolExecutor() as executor:\n",
    "                            executor.map(repair_pdb_partial, pdbs_left)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    \n",
    "                    #afterwards grab the reconstructed sequence!\n",
    "                    try:\n",
    "                        with ProcessPoolExecutor() as executor:\n",
    "                    \n",
    "                            sequences_and_codes = list(executor.map(grab_sequence_from_struc, pdbs_left))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        #write out fasta file.\n",
    "                        fasta_path = os.path.join(directory, \"multifasta.fasta\")\n",
    "                        \n",
    "                        write_multifasta(sequences_and_codes=sequences_and_codes,\n",
    "                                                 outfile=fasta_path)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "\n",
    "                    #lets see if this works.\n",
    "                    #try:\n",
    "                    #    call_clustal()\n",
    "\n",
    "                    #except Exception as e:\n",
    "                    #    print(e)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"error in the double parallelized steps.\")\n",
    "                    print(e)\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            else: #its an oligomer\n",
    "\n",
    "                result_lengths_seqs = defaultdict()\n",
    "\n",
    "                try:\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        \n",
    "                        # Map the function\n",
    "                        result = list(executor.map(select_c_alpha, pdbs))\n",
    "                        #struc_start, struc_stop, struc_full = return from select_c_alpha\n",
    "    \n",
    "                    #concurrent.futures.wait(result)\n",
    "    \n",
    "                    for struc_start, struc_stop, struc_full_seq, path_returned in result:\n",
    "                        print(\"this is the result of c alpha partial\")\n",
    "                        print(struc_start, struc_stop, struc_full_seq)\n",
    "                        result_lengths_seqs[path_returned] = (struc_start, struc_stop, struc_full_seq)\n",
    "                    \n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                    # Parallelize both functions using ProcessPoolExecutor\n",
    "                        remove_het_results = list(executor.map(remove_hetero_atoms_2, pdbs))\n",
    "                    \n",
    "    \n",
    "                    for non_canonical, pdb_path in remove_het_results:\n",
    "                        \n",
    "                        if non_canonical:\n",
    "                            \n",
    "                            for keys, vals in non_canonical.items():\n",
    "                                _mutate_non_standard_aa_1(pdb_path,\n",
    "                                non_standard_residue=vals[0],\n",
    "                                residue=keys,\n",
    "                                chain=vals[1])\n",
    "\n",
    "\n",
    "                    for keys, vals in result_lengths_seqs.items():\n",
    "                        print(keys)\n",
    "                        print(vals)\n",
    "\n",
    "                    \n",
    "                    pdbs_left = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "                    \n",
    "                    #grab majority ranges\n",
    "                    majority_start, majority_stop = vote_majority_start_stop(result_lengths_seqs=result_lengths_seqs)\n",
    "                    \n",
    "                    #we cut on the leftover files.\n",
    "                    cut_based_on_majority(pdbs_left, majority_start, majority_stop)\n",
    "\n",
    "                    #needed for Modeller.\n",
    "                    os.chdir(directory)\n",
    "                    \n",
    "                    try:\n",
    "                        #now we repair all files.\n",
    "                        repair_pdb_partial_oligo = partial(repair_pdb_oligomer, start=start, stop=stop)\n",
    "                        \n",
    "                        with ProcessPoolExecutor() as executor:\n",
    "                            executor.map(repair_pdb_partial_oligo, pdbs_left)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    #afterwards grab the reconstructed sequence!\n",
    "                    try:\n",
    "                        with ProcessPoolExecutor() as executor:\n",
    "                    \n",
    "                            sequences_and_codes = list(executor.map(grab_sequence_from_struc, pdbs_left))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        #write out fasta file.\n",
    "                        fasta_path = os.path.join(directory, \"multifasta.fasta\")\n",
    "                        \n",
    "                        write_multifasta(sequences_and_codes=sequences_and_codes,\n",
    "                                                 outfile=fasta_path)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "\n",
    "    #here we will return the repaired directories.\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "    \n",
    "            #monomer steps:\n",
    "            \n",
    "            #step 1 bring pdbs to uniform min max length based on majority vote.\n",
    "\n",
    "                #ALTERNATIVE TRY\n",
    "                #step 2 : make multiple sequence alignment\n",
    "\n",
    "                #step 3 : sort based on alignment. Find structures that have more than n gaps. If less < n gaps:\n",
    "                #proceed to fill gaps with MODELLER. IF gaps > n : Consider removing residues in other structures IF USEFUL. ELSE TRASH.\n",
    "            \n",
    "            #MAIN TRY:\n",
    "            \n",
    "            #step 2 repair small gaps and thrash structures that are too defective\n",
    "\n",
    "            #step 3 ... \n",
    "\n",
    "            #oligomer steps:\n",
    "\n",
    "            #step 1: bring pdbs to uniform min max length per chain based on majority vote.\n",
    "\n",
    "            #step 2: make multiple sequence alignment based on all chains. \n",
    "\n",
    "            #step 3: analyze if gaps can be filled. if NOT . consider removing residues in all corresponding chains. IF useful. ELSE THRASH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3435,
   "id": "f605ba55-ab92-45e8-858d-001355e27f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_pdb_oligomer(pdb_path, start, stop):\n",
    "    try:\n",
    "        mini_repair_residues_oligomeric(path_to_pdb=pdb_path, start=start, stop=stop)\n",
    "    except Exception as e:\n",
    "        print(f\"Error repairing {pdb_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3436,
   "id": "a171eaaf-703b-485d-8b1c-af900db943dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_pdb_monomer(pdb_path, start, stop):\n",
    "    try:\n",
    "        mini_repair_residues_monomeric_3(path_to_pdb=pdb_path, start=start, stop=stop)\n",
    "    except Exception as e:\n",
    "        print(f\"Error repairing {pdb_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3437,
   "id": "873d20e1-874f-40e4-b4c0-764b5fa9006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_multifasta(sequences_and_codes:tuple, outfile:str):\n",
    "\n",
    "    sequence_records = []\n",
    "    \n",
    "    for sequence, code in sequences_and_codes:\n",
    "        #print(sequence, code)\n",
    "        #print(Seq(\"GHDG\"))\n",
    "        seq_record = SeqRecord(Bio.Seq.Seq(sequence), id=code)\n",
    "        sequence_records.append(seq_record)\n",
    "\n",
    "    with open(outfile, \"w\") as fasta_out:\n",
    "        Bio.SeqIO.write(sequence_records, fasta_out, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3438,
   "id": "adcea038-7d5c-48f2-96ce-3b491dcf2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_majority_start_stop(result_lengths_seqs:dict):\n",
    "\n",
    "    cnts_start = Counter(val[0] for val in result_lengths_seqs.values())\n",
    "    cnts_stop = Counter(val[1] for val in result_lengths_seqs.values())\n",
    "    \n",
    "    highest_occ_start = cnts_start.most_common()  #grabs the highest frequency start of structures\n",
    "    highest_occ_stops = cnts_stop.most_common()  #grabs the highest frequency stop of structures\n",
    "    \n",
    "    min_len = highest_occ_start[0][0] # this corresponds to the majority vote and its associated starts.\n",
    "    max_len = highest_occ_stops[0][0] # this corresponds to the majority vote and its associated stops.\n",
    "    \n",
    "    return (min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3439,
   "id": "c38f4622-48d6-4aa3-8692-a33dfbd89c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_for_all_oligomers(path:str, \n",
    "                             template:str,\n",
    "                             main_prot_seq:str,\n",
    "                             prot_name:str):\n",
    "    \n",
    "        \n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #if there are more than 5 structures we do PCA\n",
    "    if len(onlyfiles) >= 5:\n",
    "        #structure_based_cutting_1(path_to_pdbs:str, template:str, main_prot_seq:str):\n",
    "        \n",
    "        template_new = structure_based_cutting_1(path_to_pdbs=path, \n",
    "                               template=template,\n",
    "                               main_prot_seq=main_prot_seq)\n",
    "        \n",
    "        print(template_new)\n",
    "        #now if it worked we can run PCA\n",
    "        work_dir, protein, num_struc = pca_laura_pipeline_1(path=f\"{path}/PCA\",\n",
    "                           template=template_new,\n",
    "                           protein=prot_name)\n",
    "        \n",
    "        try:\n",
    "            _plot_PCA_new(input_dir=work_dir, protein=protein, num_struc=num_struc)\n",
    "        except Exception as error:\n",
    "            \n",
    "            print(\"we could not run _plot_PCA_new\")\n",
    "            print(error)\n",
    "\n",
    "\n",
    "    return work_dir\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3440,
   "id": "34f3e653-0194-400f-a30d-34ad0c9250c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_pca_for_all_oligomers(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ADH1G_HUMAN/dimer\",\n",
    "#                         template_dict=None,\n",
    "#                         main_prot_seq=None,\n",
    "#                         prot_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9b1e9-860d-4ed8-87f7-c873a6e70cce",
   "metadata": {},
   "source": [
    "1. map_gnomad()  #we map all gnomad mutations to a hailtable and select only those relevant gene muts.\n",
    "2. gnomad_to_pandas()  #we convert the found mutations of gnomad to a pandas df.\n",
    "3. map_clinvar()     #we search for all clinvar mutations based on the same gene_name. \n",
    "4. map_clinvar_to_gnomad()  # we search for those mutations that dont already have associated frequencies if they might be present in gnomad\n",
    "5. update_clinvar_muts_based_on_gnomad() #now we update our previous clinvar df with those new found frequencies (if found)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72c337-8e25-49fe-96cb-b34797567f36",
   "metadata": {},
   "source": [
    "# testblock to retrieve mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3441,
   "id": "d42cd5d3-ff4f-4c71-b2aa-3be827874a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 gather mutations from cosmic:\n",
    "\n",
    "#main_prot_name = \"FLNC\"\n",
    "#main_prot_seq = get_gene_fasta(main_prot_name)\n",
    "#\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#oligo_state_to_check = path\n",
    "#\n",
    "#clinvar_mapped_df_path = f\"{path}/clinvar_map.txt\"\n",
    "#\n",
    "#cosmic_df = get_cosmic_mutations(gene_name=main_prot_name) \n",
    "#\n",
    "#cosmic_df.head()\n",
    "#\n",
    "#try:\n",
    "#    cosmic_df.to_csv(f\"{path}/cosmic_mutations.csv\")\n",
    "#except Exception as error:\n",
    "#    print(error)\n",
    "##    \n",
    "##step 2 map gnomad mutations.\n",
    "#\n",
    "#gnomad_table_path = map_gnomad(Gene_name=main_prot_name, outpath=oligo_state_to_check)\n",
    "##\n",
    "###step 3 extract mutations from gnomad\n",
    "#gnomad_df, gnomad_mutation_dict = gnomad_to_pandas(Gene_name=main_prot_name,\n",
    "#             path_to_tsv=gnomad_table_path, \n",
    "#             fasta_seq=main_prot_seq)\n",
    "##\n",
    "##\n",
    "###print(gnomad_mutation_dict)\n",
    "#gnomad_df.head()\n",
    "#try:\n",
    "#    gnomad_df.to_csv(f\"{path}/gnomad_mutations.csv\")\n",
    "#except Exception as error:\n",
    "#    print(error)\n",
    "##\n",
    "##step 4 gather mutations from clinvar\n",
    "#clinvar_df = map_clinvar(Gene_name=main_prot_name)\n",
    "### Search for mutations in gnomad if they are present so we obtain info about allele freq ect.\n",
    "#list_to_be_searched, clinvar_mapped_df_path = map_clinvar_to_gnomad(Gene_name=main_prot_name, clinvar_df=clinvar_df,\n",
    "#                     gnomad_mut_dict=gnomad_mutation_dict,clinvar_mapped_df_path=clinvar_mapped_df_path)\n",
    "#\n",
    "#\n",
    "#clinvar_df.head()\n",
    "#\n",
    "### Update clinvar muts that were found 1 step before.\n",
    "#updated_clinvar_df = update_clinvar_muts_based_on_gnomad(clinvar_df=clinvar_df, gnomad_dict=gnomad_mutation_dict)\n",
    "##try:\n",
    "##    updated_clinvar_df.to_csv(f\"{path}/clinvar_mutations.csv\")\n",
    "##except Exception as error:\n",
    "##    print(error)\n",
    "#\n",
    "#updated_clinvar_df.head()\n",
    "#\n",
    "##print(f\"This is cosmic df: {cosmic_df.shape}\")\n",
    "##\n",
    "##print(f\"This is updated_clinvar df: {updated_clinvar_df.shape}\")\n",
    "##\n",
    "##print(f\"This is gnomad df: {gnomad_df.shape}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe44db-72f8-4c00-8c7a-99526917540a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3442,
   "id": "8aed7a0d-be53-461d-b95e-2b29cbc0af61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#_for_all_oligomers(path=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\", template_dict=template_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0033513-b595-4f5b-a561-f923ee000ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3443,
   "id": "df9d1f14-4d11-4b14-9158-5e80404eabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repair flnc strucs\n",
    "\n",
    "def FLNC_repair(path_to_pdb:str, stop_pos:int, main_prot_seq:str):\n",
    "    \"\"\"Function should take only those proteins that have\n",
    "    a) gaps with less than 7 residues missing per gap.\n",
    "    \n",
    "    Input:\n",
    "    path to pdb_folder\n",
    "    gap_dict file\n",
    "    \n",
    "    Output:\n",
    "    produces repaired structures\n",
    "    \"\"\"\n",
    "\n",
    "    #onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    #pdbs = [corr_file for corr_file in onlyfiles if corr_file[-4:] == \".pdb\"]\n",
    "    \n",
    "    log.none() #no stdout spam\n",
    "    env = Environ() #setup env for modelling\n",
    "    \n",
    "    aln = Alignment(env) # setup alignment within the env.\n",
    "    \n",
    "    mdl = Model(env)  #setup the model object where we will append our pdbs into.\n",
    "     \n",
    "    current_pth = os.getcwd()\n",
    "    \n",
    "    #print(\"we are inside mini repair\")\n",
    "    #print(pdb_basep)\n",
    "\n",
    "    #['', 'home', 'micnag', 'result_test_struc_align', 'original_7e7s_A.pdb']\n",
    "    #print(pdb_basep)\n",
    "    #/home/micnag/result_test_struc_align\n",
    "    #print(pdb_id_target)\n",
    "    #7e7s_A\n",
    "    #print(pdb_id_chain)\n",
    "    #A\n",
    "    \n",
    "    # we need this for downstream split.\n",
    "    pdb_id_target = \"FLNc_ABD\"\n",
    "    \n",
    "    pdb_id_chain = \"A\"\n",
    "    \n",
    "    \n",
    "    #this is the new version which grabs correct seq per chain.\n",
    "    \n",
    "    #if original ensemble: we fetch the right uniprot sequence else we fetch whatever other homolog protein it is.\n",
    "\n",
    "    #either we grab this!\n",
    "    fasta_seq = main_prot_seq\n",
    "        \n",
    "    print(fasta_seq)\n",
    "    print(len(fasta_seq))\n",
    "    \n",
    "    os.chdir(\"/home/micnag/bioinformatics/FLNC_project/new_structures_modeller_repaired\")\n",
    "    print(os.getcwd())\n",
    "    \n",
    "    \n",
    "    #start stop grab:\n",
    "    \n",
    "    pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "        \n",
    "    #sample struc\n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", path_to_pdb)\n",
    "    \n",
    "    sample_res = sample_structure.get_residues()\n",
    "    \n",
    "    sample_list = [x.get_id()[1] for x in sample_res]\n",
    "    \n",
    "    start = sample_list[0] #first residue.\n",
    "    stop = len(fasta_seq) #max amount of fasta seq.\n",
    "    \n",
    "    print(start, stop)\n",
    "    \n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "\n",
    "\n",
    "    #write fasta\n",
    "    #grab start + end pos.\n",
    "    \n",
    "    #start, stop, chain = _start_stop_fasta(pdb_id_target=pdb_id_target, \n",
    "    #                                                   path=f\"{pdb_basep}/{pdb_id_target}\")\n",
    "    \n",
    "    code = f\"{pdb_id_target}\"\n",
    "    \n",
    "    #print(\"this will be start and stop\")\n",
    "    #print(start, stop)\n",
    "    \n",
    "    \n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{pdb_id_chain}\", f\"{stop}:{pdb_id_chain}\"))\n",
    "    \n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "    \n",
    "    #print(\"this is pdb_id_target and chain\")\n",
    "    #print(f\"{pdb_id_target[0:4]}\", chain)\n",
    "    \n",
    "    #this is the old version which grabs full fasta.\n",
    "    #fasta_seq = get_gene_fasta_from_pdb_id(f\"{pdb_id_target[0:4]}\", chain=chain)\n",
    "    \n",
    "    #print(\"this is fasta seq\")\n",
    "    #print(fasta_seq)\n",
    "    \n",
    "    with open(f\"./{pdb_id_target}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">{pdb_id_target}x\\n\")\n",
    "        fastaout.write(fasta_seq)\n",
    "    \n",
    "    \n",
    "    aln_code = f\"{pdb_id_target}x\"\n",
    "\n",
    "\n",
    "    aln.append(file=f\"./{pdb_id_target}x.fasta\", align_codes=aln_code, \n",
    "               alignment_format=\"fasta\")\n",
    "    \n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50),\n",
    "    alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    \n",
    "    ##aln.malign(gap_penalties_1d=(-500, -300))\n",
    "    ##aln.malign3d(gap_penalties_3d=(0.0, 2.0))\n",
    "    aln.write(file=f\"{pdb_id_target}.ali\")\n",
    "\n",
    "    #lets model\n",
    "    \n",
    "    #we will fish one out of the pdb list and align it against all others.\n",
    "    \n",
    "    a = AutoModel(env,\n",
    "                  alnfile = f\"{pdb_id_target}.ali\",\n",
    "                  knowns = f\"{pdb_id_target}\",\n",
    "                  sequence = aln_code)\n",
    "    \n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "    a.make()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3444,
   "id": "aacd8c9f-9f6a-43fc-a774-dc84b8e9a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_prot_seq = \"MMNNSGYSDAGLGLGDETDEMPSTEKDLAEDAPWKKIQQNTFTRWCNEHLKCVGKRLTDLQRDLSDGLRLIALLEVLSQKRMYRKFHPRPNFRQMKLENVSVALEFLEREHIKLVSIDSKAIVDGNLKLILGLIWTLILHYSISMPMWEDEDDEDARKQTPKQRLLGWIQNKVPQLPITNFNRDWQDGKALGALVDNCAPGLCPDWEAWDPNQPVENAREAMQQADDWLGVPQVIAPEEIVDPNVDEHSVMTYLSQFPKAKL\"\n",
    "\n",
    "#path = \"/home/micnag/bioinformatics/FLNC_project/new_structures_modeller_repaired/FLNc_ABD.pdb\"\n",
    "\n",
    "#FLNC_repair(path_to_pdb=path, stop_pos=262, main_prot_seq=main_prot_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a9ea1-8db2-465a-86a3-31e78d70eb6e",
   "metadata": {},
   "source": [
    "# integrate cbioportal for patient information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3445,
   "id": "ca2af421-2722-4f56-9feb-59ae0ab058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_name = get_hugo_name(\"P00533\")\n",
    "#print(gene_name)\n",
    "#result_df = get_cbioportal_info(gene_name=gene_name)\n",
    "#result_df.head(10)\n",
    "\n",
    "#print(result_df.shape)\n",
    "#['alleleSpecificCopyNumber', 'aminoAcidChange', 'center', 'chr', 'driverFilter', \n",
    "# 'driverFilterAnnotation', 'driverTiersFilter', 'driverTiersFilterAnnotation', \n",
    "# 'endPosition', 'entrezGeneId', 'gene', 'keyword', 'molecularProfileId', 'mutationStatus', \n",
    "# 'mutationType', 'namespaceColumns', 'ncbiBuild', 'normalAltCount', 'normalRefCount', \n",
    "# 'patientId', 'proteinChange', 'proteinPosEnd', 'proteinPosStart', 'referenceAllele', \n",
    "# 'refseqMrnaId', 'sampleId', 'startPosition', 'studyId', 'tumorAltCount', 'tumorRefCount', \n",
    "# 'uniquePatientKey', 'uniqueSampleKey', 'validationStatus', 'variantAllele', 'variantType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3446,
   "id": "f11e0c8f-1955-4279-9bf1-a65a604a0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq alignment through mmseq2 and retrieval of multi pdb fasta.\n",
    "\n",
    "def mmseq_multi_fasta(uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "\n",
    "    \"\"\"\n",
    "    uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "\n",
    "    outdir: location where result files will be stored.\n",
    "\n",
    "    sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "\n",
    "    filter_msa = 0 default. if 1 hits are stricter.\n",
    "\n",
    "    query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #we blast with this fasta as query.\n",
    "    trgt_fasta_seq = get_gene_fasta(uniprot_id)\n",
    "\n",
    "\n",
    "    #Make outdir for all required files.\n",
    "    try:\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    #we need to write it out to file.\n",
    "    with open(f\"{outdir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "        fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "        fasta_out.write(trgt_fasta_seq)\n",
    "    \n",
    "    #fetch pre downloaded database from a parent folder.\n",
    "\n",
    "    msa_file = None\n",
    "    new_location = None\n",
    "    \n",
    "    try:\n",
    "        DB_storage_location = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/swissprot_DB\"\n",
    "        #shutil.copy(previous_path, savepath)\n",
    "        \n",
    "        bash_curl_cmd = f\"mmseqs createdb {outdir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\"\n",
    "    \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "\n",
    "        print(bash_curl_cmd)\n",
    "        print(\"we start now\")\n",
    "        \n",
    "        #run first cmd which setups query database based on our input fasta file\n",
    "        result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "\n",
    "        print(\"this worked\")\n",
    "        print(result_setup_query_db.stderr)\n",
    "\n",
    "        bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"\n",
    "    \n",
    "        bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "        \n",
    "        #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "        result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "\n",
    "        print(\"this worked as well\")\n",
    "        print(result_setup_blast_db.stderr)\n",
    "        #mmseqs result2flat resultDB resultDB_flat --threads 4\n",
    "        #mmseqs result2fasta pdb70_mmseqs2.fasta resultDB_flat resultDB.fasta --threads 4\n",
    "\n",
    "        # mmseqs convertalis queryDB targetDB resultDB result.fasta\n",
    "\n",
    "        #mmseqs result2flat queryDB targetDB resultDB result_flat.txt\n",
    "\n",
    "        #bash_curl_cmd_4 = f\"mmseqs result2flat {DB_storage_location}/result_DB {DB_storage_location}/result_DB_flat --use-fasta-header TRUE\"\n",
    "\n",
    "        #bash_curl_cmd_rdy_4 = bash_curl_cmd_4.split()\n",
    "\n",
    "        #result_setup_flat_convert = run(bash_curl_cmd_rdy_4, stdout=PIPE, stderr=PIPE, \n",
    "        #                     universal_newlines=True)\n",
    "        \n",
    "        #print(\"here works still\")\n",
    "        #bash_curl_cmd_3 = f\"mmseqs convertalis {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_result.m8\"\n",
    "        #mmseqs convertalis query_fastaDB PDB_DB result_DB \"$input_fasta\"_result.m8\n",
    "        #bash_curl_cmd_rdy_3 = bash_curl_cmd_3.split()\n",
    "        \n",
    "        #result_setup_flat_convert = run(bash_curl_cmd_rdy_3, stdout=PIPE, stderr=PIPE, \n",
    "        #                     universal_newlines=True)\n",
    "\n",
    "        #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "        bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "\n",
    "        bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "\n",
    "        result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "        #print(result_setup_flat_convert.stderr)\n",
    "\n",
    "        #delete last line.. required.\n",
    "        sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'\n",
    "        \n",
    "        bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "\n",
    "        #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "        with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "            result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "\n",
    "        # Specify the path to your MSA file\n",
    "        msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "\n",
    "\n",
    "        #transfer the msa file to another location and delete useless files.\n",
    "        # we need to delete : all uniprot* files. \n",
    "        # all query*. All result* \n",
    "\n",
    "        new_location = f\"{outdir}/{uniprot_id}.fasta\"\n",
    "\n",
    "        \n",
    "        shutil.copy(msa_file, new_location)\n",
    "\n",
    "        remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "    #we want the path to msa_file for downstream analysis.\n",
    "    return new_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3447,
   "id": "31abe1c3-0a7d-484c-98ff-60c4a6f9e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_and_dirs_msa(directory, uniprot_id):\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Check if the item is a file and matches the specified patterns\n",
    "        if os.path.isfile(file_path) and (filename.startswith(\"query\") or filename.startswith(\"result\") or filename.startswith(uniprot_id)):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed file: {file_path}\")\n",
    "        \n",
    "        # Check if the item is a directory and has the name \"tmp\"\n",
    "        elif os.path.isdir(file_path) and filename == \"tmp\":\n",
    "            shutil.rmtree(file_path)\n",
    "            print(f\"Removed directory and its contents: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3448,
   "id": "dd87c061-9d6a-4426-a96e-5f6e05818fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conservation(path_to_msa:str):\n",
    "\n",
    "    '''\n",
    "    path_to_msa:str    path to the multiple sequence alignment file. The first entry is expected to be used as reference.\n",
    "    canal: object that contains the msa seq. Will be used with canal.analysis to compute conservation scores.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "    result_cons = canal.analysis(method=\"all\")\n",
    "    #this is a pandas df that contains 3 cols : 3 diff conservation score computations.\n",
    "    return result_cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f168fe3-7cc7-415c-a990-08115ab7c8d4",
   "metadata": {},
   "source": [
    "# steps for conservation\n",
    "\n",
    "1) mmseq_multi_fasta\n",
    "2) remove_files_and_dirs_msa\n",
    "3) get_conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3449,
   "id": "d70ab2a5-fe83-41ea-b75f-b653ec72be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outdir = \"/home/micnag/bioinformatics/mmseq_dir_pipe_test\"\n",
    "#uniprot_id = \"Q14315\"\n",
    "\n",
    "#out_path_msa = mmseq_multi_fasta(uniprot_id =uniprot_id, \n",
    "#                outdir=outdir)\n",
    "\n",
    "#get_conservation(path_to_msa=out_path_msa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e936d-78b5-4c48-8bdd-3f447a82f5f5",
   "metadata": {},
   "source": [
    "# PCA cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3450,
   "id": "0878241e-9988-4aa9-ab52-ce3c1ef759b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_df(PCA_12_proj_file:str, prot_names:str, save_path:str, ensemble_name:str):\n",
    "    \n",
    "    '''\n",
    "\n",
    "    Input:\n",
    "\n",
    "    PCA_12_proj_file: Path to the file that contains the projected PC1 and PC2 values as column entries.\n",
    "\n",
    "    prot_names: The path to the file that contains the names of the associated proteins (4 digit pdb code) that are used in the PCA.\n",
    "\n",
    "    Output:\n",
    "\n",
    "    pandas.DataFrame ready for downstream applications. \n",
    "\n",
    "    Cols: prot_name , PC1, PC2\n",
    "    \n",
    "    '''\n",
    "\n",
    "    PC_lst = []\n",
    "    prot_lst = []\n",
    "    #lets store the PC 1 2 coords as a list consisting of tuples [(PC1, PC2), (PC1, PC2),.....n] for all n structures.\n",
    "    with open(PCA_12_proj_file, \"r\") as pca_input:\n",
    "        for lines in pca_input:\n",
    "            lines = lines.split()\n",
    "            PC_lst.append((lines[2], lines[3]))\n",
    "\n",
    "    #here we have the associated names + chains so we also add them to a list for later retrieval.\n",
    "    with open(prot_names, \"r\") as prot_input:\n",
    "        for pdb_chain in prot_input:\n",
    "            pdb_chain = pdb_chain.replace(\"\\n\",\"\")\n",
    "            prot_lst.append(pdb_chain)\n",
    "\n",
    "    pca_df = pd.DataFrame()\n",
    "    pca_df[\"prot_name\"] = prot_lst\n",
    "    pca_df[\"PC1\"] = [float(x[0]) for x in PC_lst]\n",
    "    pca_df[\"PC2\"] = [float(x[1]) for x in PC_lst]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    PC1 = pca_df['PC1'].values\n",
    "    PC2 = pca_df['PC2'].values\n",
    "    X = np.column_stack((PC1, PC2))\n",
    "    #y = pca_df.iloc[:,0]\n",
    "    \n",
    "    PC_normalized = scaler.fit_transform(X)\n",
    "    #print(PC_normalized)\n",
    "    pca_df[\"PC1_norm\"] = PC_normalized[:, 0]\n",
    "    pca_df[\"PC2_norm\"] = PC_normalized[:, 1]\n",
    "    \n",
    "     # Perform DBSCAN\n",
    "    clusters = cluster_pca_dbscan(pca_df)\n",
    "    pca_df[\"clusters\"] = clusters\n",
    "    \n",
    "    #this df contains : all -1 labels and from each other label 1 mean structure (corresponding to the structure closest to the mean of PC1 and PC2 over the whole cluster)\n",
    "    representative_df = extract_representative_strucs(pca_df)\n",
    "\n",
    "    \n",
    "    unique_clusters = pca_df['clusters'].unique()\n",
    "    custom_cmap = plt.get_cmap('Set1', len(unique_clusters))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(pca_df['PC1_norm'], pca_df['PC2_norm'], c=pca_df['clusters'], cmap=custom_cmap, s=50)\n",
    "    \n",
    "    # Add labels with jitter\n",
    "    jitter_amount = 0.02  # Adjust as needed\n",
    "    for i, row in representative_df.iterrows():\n",
    "        x_jitter = np.random.uniform(-jitter_amount, jitter_amount)\n",
    "        y_jitter = np.random.uniform(-jitter_amount, jitter_amount)\n",
    "        plt.text(row['PC1_norm'] + x_jitter, row['PC2_norm'] + y_jitter, row['prot_name'], alpha=1)\n",
    "\n",
    "    plt.title(f'DBSCAN automated PCA clustering of {ensemble_name}')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    \n",
    "\n",
    "     # Set colorbar ticks to only include used cluster labels\n",
    "    colorbar = plt.colorbar(scatter, ticks=unique_clusters, label='Cluster Labels')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    \n",
    "    return representative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3451,
   "id": "d79fe3a6-1919-40a2-b724-3181fc8c2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_pca_dbscan(pca_df:pd.DataFrame):\n",
    "    # Assuming pca_df has columns 'PC1' and 'PC2'\n",
    "    X = pca_df[['PC1_norm', 'PC2_norm']].values\n",
    "    y = pca_df.iloc[:,0]\n",
    "\n",
    "    dbscan_model = DBSCAN(eps=0.5, min_samples=5)  # You may need to adjust eps and min_samples\n",
    "    dbscan_labels = dbscan_model.fit_predict(X)\n",
    "    \n",
    "    return dbscan_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3452,
   "id": "3a84c5d5-95fd-415e-9def-51fcd8ef9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_representative_strucs(pca_df:pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "   # Extract rows where 'clusters' column is not -1\n",
    "    non_minus_one_df = pca_df[pca_df['clusters'] != -1].copy()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the representative structures\n",
    "    representative_df = pd.DataFrame()\n",
    "\n",
    "    # Calculate mean values for each label group\n",
    "    mean_values = non_minus_one_df.groupby('clusters')[['PC1', 'PC2']].mean()\n",
    "\n",
    "    # Iterate over unique labels and find the row closest to the mean for each label\n",
    "    for label in non_minus_one_df['clusters'].unique():\n",
    "        label_rows = non_minus_one_df[non_minus_one_df['clusters'] == label]\n",
    "        label_mean = mean_values.loc[label]\n",
    "\n",
    "        # Calculate the distance to the mean for each row in the label group\n",
    "        distances = np.linalg.norm(label_rows[['PC1', 'PC2']].values - label_mean.values, axis=1)\n",
    "\n",
    "        # Find the row with the minimum distance\n",
    "        closest_row_index = distances.argmin()\n",
    "        closest_row = label_rows.iloc[closest_row_index:closest_row_index + 1].copy()\n",
    "\n",
    "        # Append the closest row to the representative DataFrame\n",
    "        representative_df = pd.concat([representative_df, closest_row], ignore_index=True)\n",
    "\n",
    "    # Append all rows with label -1 to the representative DataFrame\n",
    "    label_minus_one_rows = pca_df[pca_df['clusters'] == -1].copy()\n",
    "    representative_df = pd.concat([representative_df, label_minus_one_rows], ignore_index=True)\n",
    "\n",
    "    return representative_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3453,
   "id": "c522f622-ffb4-4e3f-8af5-5911f3e2bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy_files(representative_df:pd.DataFrame, basepath:str):\n",
    "\n",
    "    try:\n",
    "        os.mkdir(f\"{basepath}/NMA\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    file_destination = f'{basepath}/NMA/'\n",
    "    \n",
    "    for filep in representative_df.loc[representative_df['file_path_exists'], 'file_path']:\n",
    "        try:\n",
    "            shutil.copy(filep, file_destination)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return file_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3454,
   "id": "70949a2e-38e7-4ff0-b75e-a207de825295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement domenico NMA for those structures that are found representative for the ensemble.\n",
    "\n",
    "def get_nma_domenico(representative_df:pd.DataFrame, basepath:str):\n",
    "    #print(os.getcwd())\n",
    "    #os.chdir(\"/home/micnag/bioinformatics/domenico_nma\")\n",
    "\n",
    "    work_dir = _copy_files(representative_df, basepath)\n",
    "\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    print(os.getcwd())\n",
    "\n",
    "    #copy executable from baselocation to this folder.\n",
    "    \n",
    "    try:\n",
    "        baselocation = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/ENM_NMA_general\"\n",
    "        shutil.copy(baselocation, work_dir)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    nma_commands = []\n",
    "    \n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(work_dir):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(work_dir, filename)\n",
    "        \n",
    "        # Check if the path is a regular file and has a \".pdb\" extension\n",
    "        if os.path.isfile(file_path) and filename.endswith('.pdb'):\n",
    "            # Your code to process each \".pdb\" file goes here\n",
    "            pdb_without_ending = filename[:-4]\n",
    "\n",
    "            pdb_chain = pdb_without_ending.split(\"_\")[-1]\n",
    "\n",
    "            print(f\"we append now {pdb_without_ending, pdb_chain}\")\n",
    "            nma_commands.append((pdb_without_ending, pdb_chain, work_dir))\n",
    "\n",
    "    print(nma_commands)\n",
    "    # Using concurrent processing to parallelize the execution\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "         # Use as_completed to wait for completion and print results\n",
    "        futures = {executor.submit(run_nma_command, *args): args for args in nma_commands}\n",
    "        for future in as_completed(futures):\n",
    "            args = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Exception for {args}: {e}\")\n",
    "    \n",
    "\n",
    "    return work_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3455,
   "id": "56c54ace-89e9-475b-bde4-3307b69b94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nma_command(pdb_code, chain, work_dir):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        bash_curl_cmd = f\"./ENM_NMA_general {pdb_code} {chain} 10 50\"\n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "    \n",
    "        #print(\"we start now\")\n",
    "        print(bash_curl_cmd_rdy)\n",
    "        result = subprocess.run(bash_curl_cmd_rdy, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "    \n",
    "        print(f\"Results for {pdb_code}_{chain}:\")\n",
    "        print(result.stderr)\n",
    "        print(result.stdout)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    try:\n",
    "        base_filename = f\"{pdb_code}_{chain}\"\n",
    "\n",
    "        new_dir = f\"{work_dir}/{pdb_code}\"\n",
    "        os.mkdir(new_dir)\n",
    "\n",
    "        res_lst = [\"bfact.txt\", \"evecs.txt\", \"freq_GHz.txt\"]\n",
    "        for results in res_lst:\n",
    "            move_path = os.path.join(work_dir, f\"{base_filename}_{results}\")\n",
    "            shutil.move(move_path, new_dir)\n",
    "\n",
    "        \n",
    "        shutil.move(os.path.join(work_dir, f\"{pdb_code}.pdb\"), new_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3456,
   "id": "2276eca0-884f-4cf3-a0a8-58ce4660b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bfactor_projections(pdb_file_path:str):\n",
    "\n",
    "    target_files = \"bfact.txt\"\n",
    "    \n",
    "    pdb_name = os.path.basename(pdb_file_path).split(\".\")[0]\n",
    "    \n",
    "    pdb_basep = os.path.dirname(pdb_file_path)\n",
    "    \n",
    "    pdb_suffix = pdb_name.split(\"_\")[1:]\n",
    "    \n",
    "    chain = pdb_name.split(\"_\")[-1] #this fetches the whole chains.\n",
    "    \n",
    "    file_bfac = os.path.join(pdb_basep,f\"{pdb_name}_{chain}_{target_files}\")\n",
    "    #'/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa7_A/_A_bfact.txt'\n",
    "    #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa7_A/_/A/_bfact.txt'\n",
    "    result_df = _retrieve_b_fac(file_bfac, pdb_file_path)\n",
    "    \n",
    "    outf = os.path.join(pdb_basep, f\"{pdb_name}.csv\")\n",
    "    \n",
    "    result_df.to_csv(outf)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3457,
   "id": "5d18419d-b4ab-48d4-b834-0055d0ff0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get bfactor projections for a given PDB file\n",
    "def process_b_fac_parallel(dir_path:list):\n",
    "    \n",
    "    try:\n",
    "        pdb_file = next((f for f in os.listdir(dir_path) if f.endswith(\".pdb\")), None)\n",
    "        if pdb_file:\n",
    "            pdb_path = os.path.join(dir_path, pdb_file)\n",
    "            print(pdb_path)\n",
    "            get_bfactor_projections(pdb_path)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing directory {dir_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3458,
   "id": "86f661d4-601b-4b4e-87f1-95a2c91d4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_b_fac(path_to_b_fac:str, pdb_path:str):\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    exp_bf = []\n",
    "    comp_bf = []\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    with open(path_to_b_fac, \"r\") as pdb_bfac:\n",
    "        for lines in pdb_bfac:\n",
    "            if i > 0:\n",
    "                if lines[0] != \" \":\n",
    "                    break\n",
    "                lines = lines.replace(\"\\n\", \" \")\n",
    "                lines = lines.split(\" \")\n",
    "                # Flatten the list of lists\n",
    "                flat_data = [item for item in lines if item]  # Remove empty strings\n",
    "                try:\n",
    "                    exp_bf.append(float(flat_data[0]))\n",
    "                    comp_bf.append(float(flat_data[1]))\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #print(lines)\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "    result_df[\"exp_bf\"] = exp_bf\n",
    "    result_df[\"comp_bf\"] = comp_bf\n",
    "    \n",
    "\n",
    "    copy_struc_for_mod_b_facs(pdb_path=pdb_path, comp_bf=comp_bf)\n",
    "\n",
    "    result_df[\"RMSF_comp_bfac\"] = np.sqrt(result_df.loc[:, \"comp_bf\"] / (8*np.pi))\n",
    "    \n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3459,
   "id": "62d23860-74d1-4f1b-b042-049813dd1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_file_paths(clean_dir:str, representative_df:pd.DataFrame):\n",
    "\n",
    "    #print(representative_df)\n",
    "    # Assuming 'prot_name' is the column containing protein names\n",
    "    representative_df['file_path'] = f'{clean_dir}/original_' + representative_df['prot_name'] + '.pdb'\n",
    "\n",
    "    # Check if the files exist\n",
    "    representative_df['file_path_exists'] = representative_df['file_path'].apply(os.path.exists)\n",
    "\n",
    "    return representative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3460,
   "id": "fa7fa857-b94d-49e3-a5c9-0d42933501e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_struc_for_mod_b_facs(pdb_path:str, comp_bf:list):\n",
    "    #this function will simply just change the experimental B factors with modified B factors computed from NMA.\n",
    "\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    structure = parser.get_structure(\"default\", pdb_path)\n",
    "\n",
    "    # Extract file information\n",
    "    pdb_name = os.path.basename(pdb_path)\n",
    "    pdb_basep = os.path.dirname(pdb_path)\n",
    "    pdb_suffix = pdb_name.split(\"_\")[1:]\n",
    "\n",
    "    # Modify B-factors\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue, new_bfactor in zip(chain, comp_bf):\n",
    "                for atom in residue:\n",
    "                    atom.set_bfactor(new_bfactor)\n",
    "\n",
    "    # Save the modified structure to a new PDB file\n",
    "    output_path = os.path.join(pdb_basep, f\"comp_bfac_{'_'.join(pdb_suffix)}\")\n",
    "    io = PDB.PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3461,
   "id": "112214c9-9d6e-4a06-9243-5898b43b738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_directories(directory):\n",
    "    \n",
    "    try:\n",
    "        # Get a list of all entries in the directory\n",
    "        all_entries = os.listdir(directory)\n",
    "\n",
    "        # Filter out only directories and return their full paths\n",
    "        all_directories = [os.path.join(directory, entry) for entry in all_entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "\n",
    "        return all_directories\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting directories: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3462,
   "id": "c265210b-3528-461a-8701-5b55090d298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hinge_parallelize(file_path):\n",
    "    \n",
    "    try:\n",
    "        result_dict = {}\n",
    "        \n",
    "        total_hinge_res_local = []\n",
    "        \n",
    "        pdb_name = file_path.split(\"_\")[-1]\n",
    "        \n",
    "        chain = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "        hinges = hinge_pred(path_to_pdb=file_path, chain=chain)\n",
    "\n",
    "        result_dict[pdb_name] = hinges\n",
    "        total_hinge_res_local.extend(hinges)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    \n",
    "    return result_dict, total_hinge_res_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3463,
   "id": "6bd8bc2f-459a-42ae-a2a2-ca7550a1f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_pred(path_to_pdb:str, chain:str):\n",
    "\n",
    "\n",
    "    hinge_lst = []\n",
    "\n",
    "    mol = molecule.load_structure(path_to_pdb)\n",
    "\n",
    "    backbone = [j for i in mol[0][chain].get_backbone() for j in i if j is not None]\n",
    "\n",
    "    alpha_start, alpha_stop, step_size = 2, 8, 0.5\n",
    "\n",
    "    for i in np.arange(alpha_start, alpha_stop, step_size):\n",
    "        i = np.around(i, decimals=1)\n",
    "        try:\n",
    "            # here we can either store output in a sep file or we just trash it.\n",
    "            predict_hinge(backbone, Alpha=i, outputfile=open(f\"{path_to_pdb}_{i}.txt\", \"w\"))\n",
    "            hinge_res = mol[0][chain].get_hinges()\n",
    "\n",
    "            for a in hinge_res:\n",
    "                if a.get_pvalue() < 0.05:\n",
    "                    hits = [hinge_res.get_id() for hinge_res in a.get_elements()]\n",
    "                    hinge_lst.extend(hits)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "    sorted_hinges = sorted(set(hinge_lst), reverse=False)\n",
    "    return sorted_hinges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3464,
   "id": "c60b46bb-bbfe-40da-b917-7414696c5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_json(my_dict, file_path):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - my_dict: The dictionary to be saved.\n",
    "    - file_path: The path to the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert defaultdict to dict because we use them extensively.\n",
    "    if isinstance(my_dict, defaultdict):\n",
    "        my_dict = dict(my_dict)\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(my_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3465,
   "id": "afd172cb-f0b6-4bae-a193-9be8c8e47ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdb_paths_for_hinges(work_dir:str):\n",
    "\n",
    "    subdirectories = [d for d in os.listdir(work_dir) if os.path.isdir(os.path.join(work_dir, d))]\n",
    "\n",
    "    pdb_paths = []\n",
    "\n",
    "    # Loop through each subdirectory\n",
    "    for subdirectory in subdirectories:\n",
    "        subdirectory_path = os.path.join(work_dir, subdirectory)\n",
    "\n",
    "        # Get the list of PDB files in the subdirectory\n",
    "        pdb_files = [f for f in os.listdir(subdirectory_path) if f.endswith(\".pdb\")]\n",
    "\n",
    "        # Add the paths to the PDB files to the result list\n",
    "        pdb_paths.extend([os.path.join(subdirectory_path, pdb_file) for pdb_file in pdb_files])\n",
    "\n",
    "    orig_b_fac = []\n",
    "    comp_b_fac = []\n",
    "\n",
    "    for pdb_path in pdb_paths:\n",
    "        final_p = pdb_path.split(\"/\")[-1]\n",
    "        if final_p.startswith(\"original\"):\n",
    "            orig_b_fac.append(pdb_path)\n",
    "        else:\n",
    "            comp_b_fac.append(pdb_path)\n",
    "\n",
    "    #lists containing the pdb locations of the experimental strucs and the same strucs but with B factors computed from NMA.\n",
    "    return orig_b_fac , comp_b_fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3466,
   "id": "fbcd86fb-7794-4824-9704-13be59283432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinges_parallelized_detection(path_to_pdbs:list):\n",
    "\n",
    "    \"\"\"We compute with the PACKMAN hinge detection package for all alpha values between 2 and 8 in steps of 0.5 all potential \n",
    "    hinges. Needs more documentation.\"\"\"\n",
    "    \n",
    "    result_dicts = []\n",
    "    total_hinge_res = []\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            # Using as_completed for more flexibility in handling results\n",
    "            futures = [executor.submit(process_hinge_parallelize, pdb) for pdb in path_to_pdbs]\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                # Try-except inside to catch potential errors here; required because low alpha values result in infinite graphs.\n",
    "                try:\n",
    "                    result_dict, hinge_res_local = future.result()\n",
    "                    result_dicts.append(result_dict)\n",
    "                    total_hinge_res.append(hinge_res_local)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrieving result: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "\n",
    "    print(\"this is total hinge_res:\")\n",
    "    print(total_hinge_res)\n",
    "    \n",
    "    flattened_total_hinge_res = [item for sublist in total_hinge_res for item in sublist]\n",
    "\n",
    "    # Count the frequency of each value\n",
    "    frequency_dict = Counter(flattened_total_hinge_res)\n",
    "\n",
    "    # Normalize the Counter by dividing each count by the number of structures\n",
    "    normalized_frequency_dict = {key: count / len(total_hinge_res) for key, count in frequency_dict.items()}\n",
    "\n",
    "    # Sort the frequency dictionary by values in descending order\n",
    "    sorted_frequency = dict(sorted(normalized_frequency_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    print(sorted_frequency)\n",
    "\n",
    "    return sorted_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3467,
   "id": "6e3e9e24-2028-49a9-aa85-4004b59afdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hinge_dicts(dict1:dict, dict2:dict)-> dict:\n",
    "\n",
    "    ## Find common keys\n",
    "    common_keys = set(dict1.keys()) & set(dict2.keys())\n",
    "    \n",
    "    # Create a new dictionary for the overlap\n",
    "    overlap_dict = {}\n",
    "    \n",
    "    # Iterate over common keys\n",
    "    for key in common_keys:\n",
    "        value1 = dict1[key]\n",
    "        value2 = dict2[key]\n",
    "        \n",
    "        # Check if both dictionaries have the entry\n",
    "        if key in dict1 and key in dict2:\n",
    "            overlap_dict[key] = (value1, value2)\n",
    "        elif key in dict1:\n",
    "            overlap_dict[key] = (value1,)\n",
    "        elif key in dict2:\n",
    "            overlap_dict[key] = (value2,)\n",
    "    \n",
    "    # Step 1: Sort the keys based on the sum of values\n",
    "    sorted_keys_step1 = sorted(overlap_dict.keys(), key=lambda key: sum(overlap_dict[key]), reverse=True)\n",
    "\n",
    "    #we store result in this dict:\n",
    "\n",
    "    merged_dict = defaultdict()\n",
    "    \n",
    "    # Step 2: Sort the keys within each group\n",
    "    for key in sorted_keys_step1:\n",
    "        values = overlap_dict[key]\n",
    "        sorted_keys_within_group = sorted(values, reverse=True)\n",
    "        merged_dict[key] = sorted_keys_within_group\n",
    "        #print(f\"{key}: {sorted_keys_within_group}\")\n",
    "\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3ed0c-2676-49d6-8292-96a8f4bed1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436e6080-7408-4438-b942-5ea7cc7c6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial of pytrimal\n",
    "path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/multifasta_aln.fasta\"\n",
    "ali = pytrimal.Alignment.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9d67ed-f000-4459-8df3-ebcff032c8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment(names=[b'3i7v_A.pdb', b'3i7u_C.pdb', b'6woh_A.pdb', b'6wog_A.pdb', b'6wob_A.pdb', b'6woe_A.pdb', b'6pcl_A.pdb', b'2q9p_A.pdb', b'6wod_A.pdb', b'6wo8_A.pdb', b'6woa_A.pdb', b'3i7u_A.pdb', b'6wo9_A.pdb', b'2duk_A.pdb', b'6wof_A.pdb', b'5ltu_A.pdb', b'6pck_A.pdb', b'2fvv_A.pdb', b'6wo7_A.pdb', b'7nnj_A.pdb', b'7tn4_A.pdb', b'6woi_A.pdb', b'5ltu_B.pdb', b'6woc_A.pdb', b'3i7u_B.pdb', b'3mcf_A.pdb', b'3i7u_D.pdb'], sequences=['--------------GVLF---KDGEVLLI--KTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEV-----KDAKFFPIKEAKKLLKYKGDKE-IFEKALKLKEKFKL', '--------------GVLF---KDGEVLLI--KTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEV-----KDAKFFPIKEAKKLLKYKGDKE-IFEKALKLKEKFKL', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', '--------------GVLF---KDGEVLLI--KTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEV-----KDAKFFPIKEAKKLLKYKGDKE-IFEKALKLKEKFKL', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', '-RTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFE-----NQDRKHRTYVYVLT---VTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLK-------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFE-----NQDRKHRTYVYVLT---VTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFE-----NQDRKHRTYVYVLT---VTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', 'TRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFE-----NQDRKHRTYVYVLT---VTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKL--------', 'TRTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFE-----NQERKHRTYVYVLI---VTEVLEDWEDSVNIGRKREWFKIEDAIKVLQYHKPVQASYFETL--------', '--------------GVLF---KDGEVLLI--KTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEV-----KDAKFFPIKEAKKLLKYKGDKE-IFEKALKLKEKFKL', '-RTYDPEGFKKRAACLCFRSEREDEVLLVSSSRYPDRWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGVFEQ----NQDPKHRTYVYVLT---VTELLEDWEDSVSIGRKREWFKVEDAIKVLQCHKPVHAEYLEKL--------', '--------------GVLF---KDGEVLLI--KTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHYWYTLKGERIFKTVKYYLMKYKEGEPRPSWEV-----KDAKFFPIKEAKKLLKYKGDKE-IFEKALKLKEKFKL'])\n"
     ]
    }
   ],
   "source": [
    "print(ali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb12694a-17bf-4469-af91-32d53c19a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = pytrimal.ManualTrimmer(gap_threshold=0.9, conservation_percentage=60)\n",
    "trimmed = trimmer.trim(ali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2dc2bf7-65cd-4899-9912-4b993254d640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3i7v_A.pdb 110\n",
      "3i7u_C.pdb 110\n",
      "6woh_A.pdb 110\n",
      "6wog_A.pdb 110\n",
      "6wob_A.pdb 110\n",
      "6woe_A.pdb 110\n",
      "6pcl_A.pdb 110\n",
      "2q9p_A.pdb 110\n",
      "6wod_A.pdb 110\n",
      "6wo8_A.pdb 110\n",
      "6woa_A.pdb 110\n",
      "3i7u_A.pdb 110\n",
      "6wo9_A.pdb 110\n",
      "2duk_A.pdb 110\n",
      "6wof_A.pdb 110\n",
      "5ltu_A.pdb 110\n",
      "6pck_A.pdb 110\n",
      "2fvv_A.pdb 110\n",
      "6wo7_A.pdb 110\n",
      "7nnj_A.pdb 110\n",
      "7tn4_A.pdb 110\n",
      "6woi_A.pdb 110\n",
      "5ltu_B.pdb 110\n",
      "6woc_A.pdb 110\n",
      "3i7u_B.pdb 110\n",
      "3mcf_A.pdb 110\n",
      "3i7u_D.pdb 110\n"
     ]
    }
   ],
   "source": [
    "for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "    print(name.decode().rjust(6), len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f8cce75-b803-4d89-bcbe-3477a2e43b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer2 = pytrimal.OverlapTrimmer(residue_overlap=0.6, sequence_overlap=75)\n",
    "trimmed2 = trimmer.trim(ali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c262afc-a77b-4260-9665-41c663a9b747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3i7v_A.pdb 110\n",
      "3i7u_C.pdb 110\n",
      "6woh_A.pdb 110\n",
      "6wog_A.pdb 110\n",
      "6wob_A.pdb 110\n",
      "6woe_A.pdb 110\n",
      "6pcl_A.pdb 110\n",
      "2q9p_A.pdb 110\n",
      "6wod_A.pdb 110\n",
      "6wo8_A.pdb 110\n",
      "6woa_A.pdb 110\n",
      "3i7u_A.pdb 110\n",
      "6wo9_A.pdb 110\n",
      "2duk_A.pdb 110\n",
      "6wof_A.pdb 110\n",
      "5ltu_A.pdb 110\n",
      "6pck_A.pdb 110\n",
      "2fvv_A.pdb 110\n",
      "6wo7_A.pdb 110\n",
      "7nnj_A.pdb 110\n",
      "7tn4_A.pdb 110\n",
      "6woi_A.pdb 110\n",
      "5ltu_B.pdb 110\n",
      "6woc_A.pdb 110\n",
      "3i7u_B.pdb 110\n",
      "3mcf_A.pdb 110\n",
      "3i7u_D.pdb 110\n"
     ]
    }
   ],
   "source": [
    "for name, seq in zip(trimmed.names, trimmed2.sequences):\n",
    "    print(name.decode().rjust(6), len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8da77329-7d1f-44b1-9c96-751f13a2e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3i7v_A.pdb GVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHKGERIFKTVKYYLMEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKAL\n",
      "3i7u_C.pdb GVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHKGERIFKTVKYYLMEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKAL\n",
      "6woh_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6wog_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6wob_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6woe_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6pcl_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "2q9p_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6wod_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6wo8_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6woa_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "3i7u_A.pdb GVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHKGERIFKTVKYYLMEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKAL\n",
      "6wo9_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "2duk_A.pdb CLCFQEDEVLLVSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFENQDRKHRTYVYVLTVTEILEDWEDRKREWFKVEDAIKVLQCHKPVHEYLEKL\n",
      "6wof_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "5ltu_A.pdb CLCFQEDEVLLVSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFENQDRKHRTYVYVLTVTEILEDWEDRKREWFKVEDAIKVLQCHKPVHEYLEKL\n",
      "6pck_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "2fvv_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6wo7_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "7nnj_A.pdb CLCFQEDEVLLVSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFENQDRKHRTYVYVLTVTEILEDWEDRKREWFKVEDAIKVLQCHKPVHEYLEKL\n",
      "7tn4_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "6woi_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "5ltu_B.pdb CLCFQEDEVLLVSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFENQDRKHRTYVYVLTVTEILEDWEDRKREWFKVEDAIKVLQCHKPVHEYLEKL\n",
      "6woc_A.pdb CLCFSEEEVLLVSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIFENQERKHRTYVYVLIVTEVLEDWEDRKREWFKIEDAIKVLQYHKPVQSYFETL\n",
      "3i7u_B.pdb GVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHKGERIFKTVKYYLMEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKAL\n",
      "3mcf_A.pdb CLCFREDEVLLVSRYPDRWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGVFENQDPKHRTYVYVLTVTELLEDWEDRKREWFKVEDAIKVLQCHKPVHEYLEKL\n",
      "3i7u_D.pdb GVLFKDGEVLLIKTPSNVWSFPKGNIEPGEKPEETAVREVWEETGVKGEILDYIGEIHKGERIFKTVKYYLMEGEPRPSWEVKDAKFFPIKEAKKLLKYKGDKEIFEKAL\n"
     ]
    }
   ],
   "source": [
    "trimmer3 = pytrimal.ManualTrimmer(gap_threshold=1, conservation_percentage=20)\n",
    "trimmed3 = trimmer3.trim(ali)\n",
    "\n",
    "for name, seq in zip(trimmed3.names, trimmed3.sequences):\n",
    "    print(name.decode().rjust(6), seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04c54947-84a1-44d1-973a-af0946bf9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmer_function_aln(path_to_multifasta:str):\n",
    "\n",
    "    #trimmer function to get optimal alignment.\n",
    "    outdir = os.path.basename(path_to_multifasta) # this is the relevant dir.\n",
    "    #load multi fasta\n",
    "    ali = pytrimal.Alignment.load(path_to_multifasta)\n",
    "    \n",
    "    \"\"\"Remove all positions in the alignment with gaps in 10% or more of the sequences, \n",
    "    unless this leaves less than 60% of original alignment. \n",
    "    In such case, print the 60% best (with less gaps) positions\"\"\"\n",
    "    \n",
    "    trimmer = pytrimal.ManualTrimmer(gap_threshold=0.9, conservation_percentage=60)\n",
    "    trimmed = trimmer3.trim(ali)\n",
    "    \n",
    "    sequence_records = []\n",
    "    \n",
    "    for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "        code = name.decode().rjust(6)\n",
    "        seq_record = SeqRecord(Bio.Seq.Seq(seq), id=code)\n",
    "        with open(f\"{outdir}/trimmed_{code}.fasta\", \"w\") as fh_out:\n",
    "            Bio.SeqIO.write(seq_record, fh_out, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a201d722-fa88-45e0-975b-65094ae0d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_cutting_positions(fasta_pdb_original:str, fasta_pdb_trimmed:str, pdb_name:str):\n",
    "\n",
    "    #this function will take in both .fasta files and then make an alignment through clustalO.\n",
    "    # we then read in the alignment in another subroutine and cut the gap positions.\n",
    "    \n",
    "    outdir = os.path.basename(fasta_pdb_original)\n",
    "    \n",
    "    bash_curl_cmd = f\"clustalo -i {fasta_pdb_original} -v\"\n",
    "    \n",
    "    bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "\n",
    "    print(bash_curl_cmd)\n",
    "    print(\"we start now\")\n",
    "    \n",
    "    with open(f\"{outdir}/{pdb_name}_original_vs_trimm.fasta\", \"w\") as fasta_gap_out:\n",
    "        #run first cmd which setups query database based on our input fasta file\n",
    "        result_setup_query_db = run(bash_curl_cmd_rdy, stdout=fasta_gap_out, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f88bd10d-137b-4b44-80a5-405935f0c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_gaps_trimmed(original_vs_trim_aln:str)->dict:\n",
    "\n",
    "    #this function will retrieve the gaps from the alignment between trimmed and original.\n",
    "    gap_dict = defaultdict()\n",
    "    \n",
    "    alignment = AlignIO.read(original_vs_trim_aln, \"fasta\")\n",
    "\n",
    "    for record in alignment:\n",
    "        sequence = record.seq\n",
    "        #print(f\"Sequence ID: {record.id}\")\n",
    "        #print(f\"Full Sequence: {sequence}\")\n",
    "\n",
    "        gap_dict[record.id] = sequence\n",
    "\n",
    "    print(gap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2e229f3-f28e-4e03-96da-7fe4ce5d0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting_novel_trimmed_strucs(path_to_pdb:str, gap_dict:dict):\n",
    "\n",
    "    #make sure the 2nd entry in the dict is the trimmed_structure_key and its value is the trimmed seq.\n",
    "    pdb_name = list(gap_dict.keys())[1]  \n",
    "    \n",
    "    class NonGapSelect(Select):\n",
    "        def accept_residue(self, residue):\n",
    "            residue_number = residue.id[1]\n",
    "            pdb_id = residue.get_parent().get_parent().id\n",
    "            gaps = gap_dict.get(pdb_id, \"\")  # Get gaps\n",
    "\n",
    "            # Determine whether the residue should be rejected based on gaps in the sequence\n",
    "            return 0 if len(gaps) >= residue_number and gaps[residue_number - 1] == \"-\" else 1\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    #crucial pdb name here because we filter based on that!\n",
    "    structure = parser.get_structure(pdb_name, path_to_pdb)\n",
    "\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(path_to_pdb, select=NonGapSelect())\n",
    "    \n",
    "    #now we grab the gap_dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3468,
   "id": "4b751a61-887e-4d96-a7f5-9552c26a8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hinge_pred('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_1su4_A.pdb', \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b9081-927c-4d09-bc12-2296c4f42b38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RUN MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3469,
   "id": "46a75496-938d-45a9-939b-b044a6bb6d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n"
     ]
    }
   ],
   "source": [
    "#First: download all pdb.. /home/cond... do the tm score and rmsc comp...then select based on that...\n",
    "#then we will repair gaps for those retrieved structures.\n",
    "\n",
    "path = \"/home/micnag/bioinformatics/uniprot/mmseq_protein_hits_raw.json\" #testcase\n",
    "\n",
    "hits = open(path, \"r\")\n",
    "testcase = json.load(hits)\n",
    "key_lst = []\n",
    "val_lst = []\n",
    "i = 0\n",
    "for keys, vals in testcase.items():\n",
    "    i +=1\n",
    "    if keys == \"P00533\":\n",
    "        print(i)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3470,
   "id": "3e7864a7-4f5b-47a7-88c7-59b5c82d2a8a",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14913\n",
      "A0A024RBG1\n",
      "['3i7v.pdb', '2q9p.pdb', '3mcf.pdb', '3h95.pdb', '2duk.pdb', '6woa.pdb', '6woi.pdb', '4hfq.pdb', '6wod.pdb', '6pck.pdb', '6wob.pdb', '6wof.pdb', '6wo9.pdb', '5ltu.pdb', '7nnj.pdb', '6wo8.pdb', '6wo7.pdb', '3i7u.pdb', '2fvv.pdb', '6woc.pdb', '6pcl.pdb', '6wog.pdb', '7tn4.pdb', '6woh.pdb', '6woe.pdb']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2q9p.pdb\n",
      "<Model (1 chain, 16 ligands)>\n",
      "this is sorted lens: [('A', 194)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [194]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2q9p_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7v.pdb\n",
      "<Model (1 chain, 5 ligands)>\n",
      "this is sorted lens: [('A', 134)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [134]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7v_A_0.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2q9p_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7v_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2duk.pdb\n",
      "<Model (1 chain, 0 ligands)>\n",
      "this is sorted lens: [('A', 138)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [138]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2duk_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95.pdb\n",
      "<Model (2 chains, 6 ligands)>\n",
      "this is sorted lens: [('A', 199), ('A', 199)]\n",
      "this is accepted chains: ['A', 'A'], and this is accepted ranges: [199, 199]\n",
      "this is chain_label: A\n",
      "A ['A', 'A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3mcf.pdb\n",
      "<Model (1 chain, 1 ligand)>\n",
      "this is sorted lens: [('A', 136)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [136]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3mcf_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6pck.pdb\n",
      "<Model (1 chain, 4 ligands)>\n",
      "this is sorted lens: [('A', 148)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [148]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6pck_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woa.pdb\n",
      "<Model (1 chain, 7 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woa_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wod.pdb\n",
      "<Model (1 chain, 4 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wod_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woi.pdb\n",
      "<Model (1 chain, 7 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woi_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wob.pdb\n",
      "<Model (1 chain, 7 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wob_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wof.pdb\n",
      "<Model (1 chain, 6 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wof_A_0.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woa_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2duk_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wod_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wof_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3mcf_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6pck_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wob_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woi_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo9.pdb\n",
      "<Model (1 chain, 6 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo9_A_0.pdb\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/4hfq.pdb\n",
      "<Model (2 chains, 30 ligands)>\n",
      "this is sorted lens: [('A', 203), ('B', 203)]\n",
      "this is accepted chains: ['A', 'B'], and this is accepted ranges: [203, 203]\n",
      "this is chain_label: A\n",
      "A ['A', 'B']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/4hfq_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/7nnj.pdb\n",
      "<Model (1 chain, 5 ligands)>\n",
      "this is sorted lens: [('A', 140)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [140]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/7nnj_A_0.pdb\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo8.pdb\n",
      "<Model (1 chain, 7 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo8_A_0.pdb\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/4hfq_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo9_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo8_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/5ltu.pdb\n",
      "<Model (2 chains, 8 ligands)>\n",
      "this is sorted lens: [('A', 180), ('B', 180)]\n",
      "this is accepted chains: ['A', 'B'], and this is accepted ranges: [180, 180]\n",
      "this is chain_label: A\n",
      "A ['A', 'B']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/5ltu_A_0.pdb\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/7nnj_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/5ltu_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "this is result: {'3i7v.pdb': 1}\n",
      "this is result: {'2q9p.pdb': 1}\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo7.pdb\n",
      "<Model (1 chain, 7 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo7_A_0.pdb\n",
      "this is chain_label: A\n",
      "A ['A', 'A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_A_1.pdb\n",
      "this is result: {'3mcf.pdb': 1}\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6pcl.pdb\n",
      "<Model (1 chain, 6 ligands)>\n",
      "this is sorted lens: [('A', 148)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [148]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6pcl_A_0.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wo7_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_0.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_1.pdb']\n",
      "['A']\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6pcl_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woc.pdb\n",
      "<Model (1 chain, 6 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woc_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/7tn4.pdb\n",
      "<Model (1 chain, 7 ligands)>\n",
      "this is sorted lens: [('A', 172)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [172]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/7tn4_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2fvv.pdb\n",
      "<Model (1 chain, 3 ligands)>\n",
      "this is sorted lens: [('A', 194)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [194]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2fvv_A_0.pdb\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wog.pdb\n",
      "<Model (1 chain, 2 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wog_A_0.pdb\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "this is chain_label: B\n",
      "B ['A', 'B']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/4hfq_B_1.pdb\n",
      "this is chain_label: B\n",
      "B ['A', 'B']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/5ltu_B_1.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6wog_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woh.pdb\n",
      "<Model (1 chain, 3 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woh_A_0.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2fvv_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woe.pdb\n",
      "<Model (1 chain, 4 ligands)>\n",
      "this is sorted lens: [('A', 169)]\n",
      "this is accepted chains: ['A'], and this is accepted ranges: [169]\n",
      "this is chain_label: A\n",
      "A ['A']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woe_A_0.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woc_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woh_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/5ltu_0.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/5ltu_1.pdb']\n",
      "['A', 'B']\n",
      "we start monomeric rechain inside\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/7tn4_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u.pdb\n",
      "<Model (4 chains, 30 ligands)>\n",
      "this is sorted lens: [('A', 134), ('B', 134), ('D', 134), ('C', 134)]\n",
      "this is accepted chains: ['A', 'B', 'D', 'C'], and this is accepted ranges: [134, 134, 134, 134]\n",
      "this is chain_label: A\n",
      "A ['A', 'B', 'D', 'C']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_A_0.pdb\n",
      "Original chain ID: A\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/4hfq_0.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/4hfq_1.pdb']\n",
      "['A', 'B']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/6woe_0.pdb']\n",
      "['A']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_0.pdb\n",
      "Original chain ID: B\n",
      "New chain ID: B\n",
      "we save now:\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_1.pdb\n",
      "Original chain ID: B\n",
      "New chain ID: B\n",
      "we save now:\n",
      "[Errno 2] No such file or directory: ''\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_0.pdb\n",
      "this is chain_label: B\n",
      "B ['A', 'B', 'D', 'C']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_B_1.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_0.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_1.pdb']\n",
      "['A', 'B']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_1.pdb\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_0.pdb\n",
      "[Errno 2] No such file or directory: ''\n",
      "Original chain ID: B\n",
      "New chain ID: B\n",
      "we save now:\n",
      "this is result: {'3h95.pdb': 2}\n",
      "this is result: {'2duk.pdb': 1}\n",
      "this is result: {'6woa.pdb': 1}\n",
      "this is result: {'6woi.pdb': 1}\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_1.pdb\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_0.pdb\n",
      "[Errno 2] No such file or directory: ''\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_1.pdb\n",
      "[Errno 2] No such file or directory: ''\n",
      "this is result: {'4hfq.pdb': 2}\n",
      "this is result: {'6wod.pdb': 1}\n",
      "this is result: {'6pck.pdb': 1}\n",
      "this is result: {'6wob.pdb': 1}\n",
      "this is result: {'6wof.pdb': 1}\n",
      "this is result: {'6wo9.pdb': 1}\n",
      "this is result: {'5ltu.pdb': 2}\n",
      "this is result: {'7nnj.pdb': 1}\n",
      "this is result: {'6wo8.pdb': 1}\n",
      "this is result: {'6wo7.pdb': 1}\n",
      "this is chain_label: D\n",
      "D ['A', 'B', 'D', 'C']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_D_2.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_0.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_1.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_2.pdb']\n",
      "['A', 'D', 'B']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: B\n",
      "New chain ID: B\n",
      "we save now:\n",
      "Original chain ID: D\n",
      "New chain ID: C\n",
      "we save now:\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_0.pdb\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_1.pdb\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_2.pdb\n",
      "[Errno 2] No such file or directory: ''\n",
      "this is chain_label: C\n",
      "C ['A', 'B', 'D', 'C']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_C_3.pdb\n",
      "we append now path\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_0.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_1.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_2.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3i7u_3.pdb']\n",
      "['A', 'D', 'C', 'B']\n",
      "we start monomeric rechain inside\n",
      "Original chain ID: A\n",
      "New chain ID: A\n",
      "we save now:\n",
      "Original chain ID: B\n",
      "New chain ID: B\n",
      "we save now:\n",
      "Original chain ID: C\n",
      "New chain ID: C\n",
      "we save now:\n",
      "Original chain ID: C\n",
      "New chain ID: D\n",
      "we save now:\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_0.pdb\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_1.pdb\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_2.pdb\n",
      "this is save location: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_3.pdb\n",
      "[Errno 2] No such file or directory: ''\n",
      "this is result: {'3i7u.pdb': 4}\n",
      "this is result: {'2fvv.pdb': 1}\n",
      "this is result: {'6woc.pdb': 1}\n",
      "this is result: {'6pcl.pdb': 1}\n",
      "this is result: {'6wog.pdb': 1}\n",
      "this is result: {'7tn4.pdb': 1}\n",
      "this is result: {'6woh.pdb': 1}\n",
      "this is result: {'6woe.pdb': 1}\n",
      "This is oligostates:\n",
      "defaultdict(<class 'str'>, {'3i7v.pdb': 1, '2q9p.pdb': 1, '3mcf.pdb': 1, '3h95.pdb': 2, '2duk.pdb': 1, '6woa.pdb': 1, '6woi.pdb': 1, '4hfq.pdb': 2, '6wod.pdb': 1, '6pck.pdb': 1, '6wob.pdb': 1, '6wof.pdb': 1, '6wo9.pdb': 1, '5ltu.pdb': 2, '7nnj.pdb': 1, '6wo8.pdb': 1, '6wo7.pdb': 1, '3i7u.pdb': 4, '2fvv.pdb': 1, '6woc.pdb': 1, '6pcl.pdb': 1, '6wog.pdb': 1, '7tn4.pdb': 1, '6woh.pdb': 1, '6woe.pdb': 1})\n",
      "defaultdict(<class 'str'>, {'3i7v.pdb': 1, '2q9p.pdb': 1, '3mcf.pdb': 1, '3h95.pdb': 2, '2duk.pdb': 1, '6woa.pdb': 1, '6woi.pdb': 1, '4hfq.pdb': 2, '6wod.pdb': 1, '6pck.pdb': 1, '6wob.pdb': 1, '6wof.pdb': 1, '6wo9.pdb': 1, '5ltu.pdb': 2, '7nnj.pdb': 1, '6wo8.pdb': 1, '6wo7.pdb': 1, '3i7u.pdb': 4, '2fvv.pdb': 1, '6woc.pdb': 1, '6pcl.pdb': 1, '6wog.pdb': 1, '7tn4.pdb': 1, '6woh.pdb': 1, '6woe.pdb': 1})\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7v_A.pdb', 'A') (1, 134)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_C.pdb', 'C') (1, 134)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woh_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wog_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_AB.pdb', 'AB') (1, 203)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_A.pdb', 'A') (1, 203)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wob_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_AB.pdb', 'AB') (143, 312)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_AB.pdb', 'AB') (8, 147)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_B.pdb', 'B') (143, 312)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woe_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pcl_A.pdb', 'A') (8, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2q9p_A.pdb', 'A') (10, 141)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wod_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo8_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woa_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_A.pdb', 'A') (1, 133)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo9_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2duk_A.pdb', 'A') (8, 145)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wof_A.pdb', 'A') (10, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_A.pdb', 'A') (9, 146)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_ABCD.pdb', 'ABCD') (1, 134)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pck_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_B.pdb', 'B') (1, 203)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2fvv_A.pdb', 'A') (8, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo7_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_A.pdb', 'A') (143, 312)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7nnj_A.pdb', 'A') (9, 146)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7tn4_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woi_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_B.pdb', 'B') (8, 147)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woc_A.pdb', 'A') (9, 142)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_B.pdb', 'B') (1, 133)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3mcf_A.pdb', 'A') (16, 151)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_D.pdb', 'D') (1, 134)\n",
      "length of dictionary before recursive_reduction: 12\n",
      "{(8, 147): 29, (1, 203): 3, (143, 312): 3}\n",
      "(8, 147)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_ABCD.pdb', 'ABCD')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pck_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pcl_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woe_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3mcf_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_B.pdb', 'B')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2duk_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wog_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7tn4_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_B.pdb', 'B')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_C.pdb', 'C')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_D.pdb', 'D')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woi_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7nnj_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2q9p_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo9_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wob_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2fvv_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woc_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wod_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woa_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7v_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wof_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woh_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo7_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo8_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_AB.pdb', 'AB')\n",
      "(1, 203)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_AB.pdb', 'AB')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_B.pdb', 'B')\n",
      "(143, 312)\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_A.pdb', 'A')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_AB.pdb', 'AB')\n",
      "('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_B.pdb', 'B')\n",
      "this is pdb_ranges defaultdict(None, {('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7v_A.pdb', 'A'): (1, 134), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_C.pdb', 'C'): (1, 134), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woh_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wog_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_AB.pdb', 'AB'): (1, 203), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_A.pdb', 'A'): (1, 203), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wob_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_AB.pdb', 'AB'): (143, 312), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_AB.pdb', 'AB'): (8, 147), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_B.pdb', 'B'): (143, 312), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woe_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pcl_A.pdb', 'A'): (8, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2q9p_A.pdb', 'A'): (10, 141), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wod_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo8_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woa_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_A.pdb', 'A'): (1, 133), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo9_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2duk_A.pdb', 'A'): (8, 145), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wof_A.pdb', 'A'): (10, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_A.pdb', 'A'): (9, 146), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_ABCD.pdb', 'ABCD'): (1, 134), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pck_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_B.pdb', 'B'): (1, 203), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2fvv_A.pdb', 'A'): (8, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo7_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_A.pdb', 'A'): (143, 312), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7nnj_A.pdb', 'A'): (9, 146), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7tn4_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woi_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_B.pdb', 'B'): (8, 147), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woc_A.pdb', 'A'): (9, 142), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_B.pdb', 'B'): (1, 133), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3mcf_A.pdb', 'A'): (16, 151), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_D.pdb', 'D'): (1, 134)})\n",
      "this is domain_dict: {(8, 147): {('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_ABCD.pdb', 'ABCD'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pck_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6pcl_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woe_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3mcf_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_B.pdb', 'B'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2duk_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wog_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7tn4_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_B.pdb', 'B'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_C.pdb', 'C'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7u_D.pdb', 'D'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woi_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/7nnj_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2q9p_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo9_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wob_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/2fvv_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woc_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wod_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woa_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3i7v_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wof_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6woh_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo7_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/6wo8_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/5ltu_AB.pdb', 'AB')}, (1, 203): {('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/4hfq_B.pdb', 'B')}, (143, 312): {('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_A.pdb', 'A'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_AB.pdb', 'AB'), ('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/3h95_B.pdb', 'B')}}\n",
      "this is oligostates: ['monomer', 'dimer', 'tetramer']\n",
      "Directory: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147, Best Hit: /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2duk_A.pdb, Template Score: 105.0\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2duk_A.pdb\n",
      "['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7v_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_C.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woh_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wog_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wob_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woe_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6pcl_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2q9p_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wod_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo8_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woa_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo9_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2duk_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wof_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/5ltu_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6pck_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2fvv_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo7_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/7nnj_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/7tn4_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woi_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/5ltu_B.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woc_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_B.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3mcf_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_D.pdb']\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_A.pdb 0.7177 2.56\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6pcl_A.pdb 0.9495 0.69\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2q9p_A.pdb 0.9379 0.63\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wob_A.pdb 0.9503 0.67\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo8_A.pdb 0.9503 0.67\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7v_A.pdb 0.7503 2.26\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woa_A.pdb 0.9474 0.72\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wod_A.pdb 0.9514 0.65\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_C.pdb 0.7295 2.61\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wog_A.pdb 0.9480 0.71\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woe_A.pdb 0.9519 0.64\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woh_A.pdb 0.9479 0.71\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wof_A.pdb 0.9447 0.64\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2fvv_A.pdb 0.9461 0.74\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/5ltu_A.pdb 0.8833 0.94\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/7nnj_A.pdb 0.9054 1.13\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo7_A.pdb 0.9496 0.69\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo9_A.pdb 0.9496 0.68\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6pck_A.pdb 0.9493 0.68\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woc_A.pdb 0.9505 0.67\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/7tn4_A.pdb 0.9503 0.67\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woi_A.pdb 0.9507 0.66\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3mcf_A.pdb 0.8794 0.79\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/5ltu_B.pdb 0.8919 0.96\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_B.pdb 0.7561 2.28\n",
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_D.pdb 0.7582 2.27\n",
      "this is oligostate: monomer\n",
      "this is pdbs left: ['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7v_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_C.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woh_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wog_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wob_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woe_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6pcl_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2q9p_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wod_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo8_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woa_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo9_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2duk_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wof_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/5ltu_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6pck_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/2fvv_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6wo7_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/7nnj_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/7tn4_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woi_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/5ltu_B.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/6woc_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_B.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3mcf_A.pdb', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/merged_cleaned_files/monomer/pos_8_147/3i7u_D.pdb']\n",
      "134\n",
      "133\n",
      "134\n",
      "142142142142142142\n",
      "142\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "142141\n",
      "\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "\n",
      "\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6wob_Ax.B99990001.pdb          906.54852\n",
      "\n",
      "6wob_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6wog_Ax.B99990001.pdb          909.62256\n",
      "\n",
      "6wog_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6wo8_Ax.B99990001.pdb          743.85492\n",
      "\n",
      "6wo8_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "3i7v_Ax.B99990001.pdb          699.62842\n",
      "\n",
      "3i7v_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6woe_Ax.B99990001.pdb          879.12329\n",
      "\n",
      "6woe_A\n",
      "142\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      ">> Summary of successfully produced models:\n",
      "\n",
      "Filename                          molpdf6woa_Ax.B99990001.pdb          784.45215\n",
      "\n",
      "6woa_A\n",
      "\n",
      "----------------------------------------\n",
      "3i7u_Cx.B99990001.pdb          783.01666\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "\n",
      "3i7u_CFilename                          molpdf\n",
      "\n",
      "----------------------------------------\n",
      "6wod_Ax.B99990001.pdb          912.86316\n",
      "\n",
      "6wod_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      ">> Summary of successfully produced models:\n",
      "\n",
      "Filename                          molpdfFilename                          molpdf\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "2q9p_Ax.B99990001.pdb          843.46625\n",
      "\n",
      "2q9p_A\n",
      "6woh_Ax.B99990001.pdb          897.74829\n",
      "\n",
      "6woh_A\n",
      "142\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms142\n",
      "\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "142----------------------------------------\n",
      "\n",
      "6pcl_Ax.B99990001.pdb          822.82581\n",
      "\n",
      "6pcl_A\n",
      "142\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies142\n",
      "\n",
      "142\n",
      "142\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "142\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "142\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies142\n",
      "\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "3i7u_Ax.B99990001.pdb          856.24030\n",
      "\n",
      "3i7u_A\n",
      "142\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6wo9_Ax.B99990001.pdb          785.75964\n",
      "\n",
      "6wo9_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "133\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "7tn4_Ax.B99990001.pdb          770.84760\n",
      "\n",
      "7tn4_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "2duk_Ax.B99990001.pdb          694.39545\n",
      "\n",
      "2duk_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "\n",
      "----------------------------------------0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      "\n",
      "6pck_Ax.B99990001.pdb          872.30182\n",
      "6pck_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6wof_Ax.B99990001.pdb          888.52380\n",
      "\n",
      "6wof_A\n",
      "142\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6woi_Ax.B99990001.pdb          886.21637\n",
      "\n",
      "6woi_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "\n",
      ">> Summary of successfully produced models:Filename                          molpdf\n",
      "\n",
      "Filename                          molpdf\n",
      ">> Summary of successfully produced models:----------------------------------------\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "5ltu_Ax.B99990001.pdb          855.856992fvv_Ax.B99990001.pdb          992.28876\n",
      "6wo7_Ax.B99990001.pdb          789.06049\n",
      "\n",
      "5ltu_A\n",
      "\n",
      "2fvv_A\n",
      "\n",
      "\n",
      "6wo7_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf134\n",
      "\n",
      "----------------------------------------\n",
      "5ltu_Bx.B99990001.pdb         1038.70740\n",
      "\n",
      "5ltu_B\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "0 atoms in HETATM/BLK residues constrained\n",
      "to protein atoms within 2.30 angstroms\n",
      "and protein CA atoms within 10.00 angstroms\n",
      "0 atoms in residues without defined topology\n",
      "constrained to be rigid bodies\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "7nnj_Ax.B99990001.pdb         1077.01172\n",
      "\n",
      "7nnj_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "6woc_Ax.B99990001.pdb          795.30042\n",
      "\n",
      "6woc_A\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "3i7u_Bx.B99990001.pdb          731.75568\n",
      "\n",
      "3i7u_B\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "3i7u_Dx.B99990001.pdb          894.45099\n",
      "\n",
      "3i7u_D\n",
      "\n",
      ">> Summary of successfully produced models:\n",
      "Filename                          molpdf\n",
      "----------------------------------------\n",
      "3mcf_Ax.B99990001.pdb          760.69720\n",
      "\n",
      "3mcf_A\n"
     ]
    }
   ],
   "source": [
    "## First: download all pdb.. /home/cond... do the tm score and rmsc comp...then select based on that...\n",
    "#then we will repair gaps for those retrieved structures.\n",
    "\n",
    "path = \"/home/micnag/bioinformatics/uniprot/mmseq_protein_hits_raw.json\" #testcase\n",
    "\n",
    "hits = open(path, \"r\")\n",
    "testcase = json.load(hits)\n",
    "key_lst = []\n",
    "val_lst = []\n",
    "i = 0\n",
    "for keys, vals in testcase.items():\n",
    "    i +=1\n",
    "    if keys == \"P11716\":\n",
    "        print(i)\n",
    "        pass\n",
    "    key_lst.append(keys)\n",
    "    val_lst.append(vals)\n",
    "\n",
    "\n",
    "#we parse from here through all.\n",
    "hit_list = list(zip(key_lst,val_lst))\n",
    "\n",
    "#2024:2025 egfr\n",
    "\n",
    "#try 6:7 good case.\n",
    "#so is 202:203\n",
    "#24-25 is faulty. so is 34-35 and 37:38\n",
    "\n",
    "#555-556 seems good case to illustrate.\n",
    "#5803 = ACSM2A gene\n",
    "\n",
    "\n",
    "#O14983 SERCA 744:745\n",
    "#O75311 GLRA 1401:1402\n",
    "#P48167 GLRB 4400:4401\n",
    "#P00326 ADHG 2004:2005\n",
    "#P06756 Integrin checknumbers 2356:2357  THIS case is totally catastrophic. #final stuff to fix for mixed heteromers.\n",
    "i = 0\n",
    "#Q8N142 is Adenylosuccinate synthetase isozyme 1 9096:9097\n",
    "#2569:2570 = LOX5\n",
    "#calmodulin 1 2689:2690  !rerun again when NMR is properly working.\n",
    "\n",
    "#P02788 Lactotransferin 2175:2176\n",
    "#P02911 LAO-binding protein try as well. \n",
    "#Q8TEX9 Importin-4 10086:10087\n",
    "#Q14974 Real importin subunit beta with strucs: 6418:6419\n",
    "\n",
    "#Q16774 guanylate kinase 6766:6767   ! rerun again when NMR is properly working! seems interesting.\n",
    "#raf1 + complexes\n",
    "print(len(hit_list))\n",
    "#744:745 serca\n",
    "#2203:2204 kraf\n",
    "for keys, entries in hit_list[0:1]:\n",
    "    #print(keys, entries)\n",
    "    query, templates, seq_sim, query_start, query_end, temp_start, temp_end = keys, entries[0],\\\n",
    "    entries[1],entries[2],entries[3],entries[4],entries[5] \n",
    "    \n",
    "    print(query)\n",
    "    #print(sorted(templates))\n",
    "    os.chdir(\"/home/micnag/bioinformatics\") #just to start properly.\n",
    "    \n",
    "    #2203:2204 == RAF1\n",
    "    fetch_struc_1(query, templates, seq_sim, query_start, query_end, temp_start, temp_end, run_NMA=True, run_PCA=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813bb3b-19b0-4caf-a6ff-b1ff380b38a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TO DO LOG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9ad97-852d-4f1b-80d3-181d67cd1650",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### repair and check for repairability.\n",
    "\n",
    "+ repair modeller pipeline should work : done\n",
    "+ rechaining / renumbering after modelling: done\n",
    "+ funnelling repaired chains into full biological ensembles: done\n",
    "\n",
    "\n",
    "### NOW WE NEED TO RENUMBER AFTER REPAIR!\n",
    "\n",
    "REPAIR IS DONE AND WORKS.\n",
    "\n",
    "\n",
    "## NEXT STEP:\n",
    "\n",
    "remove duplicate TEMPLATE from MSA USALIGN.\n",
    "example whats wrong:\n",
    "\n",
    ">2q9p.pdb:A\tL=132\td0=4.26\tseqID=0.874\tTM-score=0.93282\n",
    "---RTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIF-ENQERKHRTYVYVLIVTEVLEDWEDSV-NIGRKREWFKIEDAIKVLQYHKPVQASYFE----------\n",
    ">2q9p.pdb:A\tL=132\td0=4.26\tseqID=0.874\tTM-score=0.93282\t*\n",
    "---RTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIF-ENQERKHRTYVYVLIVTEVLEDWEDSV-NIGRKREWFKIEDAIKVLQYHKPVQASYFE----------\n",
    ">3mcf.pdb:A\tL=136\td0=4.33\tseqID=0.801\tTM-score=0.85092\n",
    "FK----------KRAACLCFRSEREDEVLLVSSSRYPDRWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGVFEQNQDPKHRTYVYVLTVTELLEDWEDSV-SIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLKLGGSPTN\n",
    "...\n",
    "\n",
    "2q9p is twice there\n",
    "\n",
    "\n",
    "+ **ABOVE BUG IS ALREADY FIXED** - 12/06/2023\n",
    "\n",
    "\n",
    "### Implement according structural cutting based on 1:1 correspondence residues (i.e remove all that dont have 1:1 correspondence)\n",
    "\n",
    "+ do the structural alignment and cut all residues that dont have 1:1 correspondence: **done**\n",
    "+ prepare structures to only have those residues i.e all structures will end up with the same amount of residues : **done**\n",
    "+ run domenicos PCA pipeline and integrate it into workflow: **done**\n",
    "+ plot pca results (PCA1 PCA2): **done**\n",
    "\n",
    "\n",
    "## Assemble FULL pipeline and run preliminary test runs.\n",
    "\n",
    "\n",
    "pipeline assembled:\n",
    "\n",
    "bugfixes:\n",
    "\n",
    "+ fixed fetch fasta during repair:\n",
    "\n",
    "previous fetch fasta would always grab full length protein fasta which would lead to full length protein repair reconstruction. This is not desired. We want only to restore the structure present (irrespective if the structure is only a fragment of the full length protein)\n",
    "\n",
    "TBD:\n",
    "\n",
    "+ multichain alignment in msa\n",
    "Currently we only align chain A. Multichain alignment are not possible but we can run for all chains against all respective chains e.g chain A vs A / B vs B etc? Done\n",
    "\n",
    "\n",
    "\n",
    "establish 1 to 1 full length alignments with USAlign and then cut according to the results of n - 1 seq alignments against the reference structure.\n",
    "\n",
    "+ for multichain alignment, output file looks different. \n",
    "come up with a correct way to parse output file and make sure that chains are properly aligned!\n",
    "\n",
    "\n",
    "+ set up proper PCA plots for each oligomer against its respective oligomers.\n",
    "#\n",
    "\n",
    "\n",
    "+ for NMR modells, consider that high seq similarity is equivalently good and means same protein but Calmodulin 1 e.g 6y95 and 6y94 are seen as different with low tm score ! mistake !they are same sequence and protein.\n",
    "\n",
    "\n",
    "+ NOW NMR modells are taken as separate PDBS into account and will also be part of the ensemble!\n",
    "fixed on 19.7.2023\n",
    "\n",
    "\n",
    "# <b> TODO: </b>\n",
    "NEED FOR OLIGOMERS TO DO SEQUENCE BASED GROUPING FOR EACH CHAIN AGAINST EACH CHAIN... A VS A... B VS B\n",
    "\n",
    "check EGFR case for repair mutated residues. there is an issue with a CY0 modified residue that will result in a weird alanine.\n",
    "_mutate function needs to be checked.\n",
    "\n",
    "2j5e.pdb shows what is wrong...\n",
    "\n",
    "\n",
    "+ check the alignment if it cuts properly the residues that it SHOULD CUT. There might be an issue with the residue selection that are KEPT FOR PCA.\n",
    "\n",
    "\n",
    "+ pipeline mistakes small ligand bounds in other chains as oligomers... should be monomeric still.. \n",
    "THIS ERROR IS FIXED ALREADY.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NEXT THURSDAY:\n",
    "\n",
    "Continue with HAIL and incorporation of GNOMAD DATABASE INFO.\n",
    "Mutational mapping new is construction site now.\n",
    "https://hail.is/tutorial.html\n",
    "Check NUMBERING on the pdb structures. SEEMS TO BE OFF (maybe for repaired structures)????\n",
    "\n",
    "DECOUPLE NMA RUNS FROM PCA... PCA IS A WHOLE OWN STORY...\n",
    "WE RUN NMA ANALYSIS ON THE FRESH POPULATION OF STRUCTURES, irrespective of gaps. TAKE STRUCS FROM THE POS X-Y DIRECTORY. DONT REPAIR (AT FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc6e52-616a-4658-b80a-a45b52bd0074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
