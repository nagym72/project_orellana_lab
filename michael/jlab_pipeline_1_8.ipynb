{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6266200-2843-4bff-ae79-088353476d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modules required for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2c053508-7da1-4170-97f2-195f37728b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pdb-tools\n",
    "#!pip3 install atomium\n",
    "\n",
    "#!pip install sympy\n",
    "#!pip install UpSetPlot\n",
    "#!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba5f6a2-3f77-4a7e-a1f1-458b2c9a21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdb\n",
    "import requests\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil\n",
    "import hail as hl\n",
    "import glob\n",
    "# Import from installed package\n",
    "#from pypdb.clients.pdb.pdb_client import *\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import Bio\n",
    "import pandas as pd\n",
    "from Bio.PDB import PDBParser, PDBIO, Select, MMCIFParser\n",
    "from Bio.SeqIO import PirIO\n",
    "#from Bio import pairwise2\n",
    "from Bio import Align, PDB\n",
    "from io import StringIO\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "from modeller.parallel import job, local_slave\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import logging\n",
    "import subprocess\n",
    "import shlex\n",
    "from subprocess import PIPE, run\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup  #required later to download SIFT files.\n",
    "import atomium\n",
    "from Bio.PDB import PDBParser, PDBIO, Structure\n",
    "from sympy import Union, Interval\n",
    "from itertools import compress\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.gridspec as gridspe\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from bravado.client import SwaggerClient\n",
    "from Bio import AlignIO\n",
    "from pycanal import Canal\n",
    "#import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path\n",
    "from Bio.PDB import Superimposer, PDBParser\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from packman import molecule\n",
    "from packman.apps import predict_hinge\n",
    "#lot import generate_counts\n",
    "#from upsetplot import UpSet\n",
    "#from upsetplot import from_memberships\n",
    "#from upsetplot import from_contents\n",
    "#from upsetplot import from_indicators\n",
    "from scipy import stats\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "#logging.getLogger(\"requests\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a8716",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basehelper function:\n",
    "\n",
    "+ get_url\n",
    "+ get_gene_name_uniprot / get_gene_name redundant function.\n",
    "+ get_uniprot_id\n",
    "+ get_fasta_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9114a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)  \n",
    "        if not response.ok:\n",
    "            print(response.text)\n",
    "    except:\n",
    "        response.raise_for_status()\n",
    "        #sys.exit()\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab87c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_name_uniprot(uniprot_id:str):\n",
    "    \n",
    "    fields = \"gene_primary\"\n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.text\n",
    "    #print(resp)\n",
    "    resp = resp.split(\"\\n\")\n",
    "    \n",
    "    #result_lst = [x.split(\"\\t\") for x in resp]\n",
    "    #result_lst = result_lst[0:-1]\n",
    "    #result_sort = sorted(result_lst, key= lambda x : x[2])\n",
    "    #resp = result_sort[0]\n",
    "    #print(resp)\n",
    "    return resp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fb88c0-673b-45da-812e-84f7e8ee12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_name = get_gene_name_uniprot(\"1wio\")\n",
    "#print(gene_name)\n",
    "#get_prot_name = get_uniprot_id(gene_name)\n",
    "#print(get_prot_name)\n",
    "#print(get_gene_fasta(get_prot_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce4f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniprot_id(uniprot_id:str):\n",
    "    fields = \"accession\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines()\n",
    "    for lines in resp:\n",
    "        lines = lines.decode()\n",
    "        if lines == \"Entry\":\n",
    "            continue\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ec2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_fasta(uniprot_id:str):\n",
    "\n",
    "    #print(\"we are in get gene fasta\")\n",
    "    \"this is already overworked. should work.\"\n",
    "    #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "    \n",
    "    fields = \"sequence\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    seq = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    for lines in resp:\n",
    "        if i > 0:\n",
    "            seq += lines\n",
    "            #print(lines)\n",
    "        i += 1\n",
    "    \n",
    "    #print(seq)\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c648c6-a378-4332-bddc-c9a841a882f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb_id_target= \"6lle\"\n",
    "#get_rcsb_fasta(pdb_id_target=pdb_id_target, chain=\"A\")\n",
    "#\n",
    "#query = \"P16615\"\n",
    "#main_prot_seq = get_gene_fasta(query)\n",
    "#print(main_prot_seq)\n",
    "#print(len(main_prot_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbbb9d7-312d-42be-b604-13a8cb11fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_fasta_from_pdb_id(uniprot_id:str, chain:str):\n",
    "    \n",
    "    #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "    \n",
    "    fields = \"sequence\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    res = \"\"\n",
    "    i = 0\n",
    "    seq = 0\n",
    "    aln_code = []\n",
    "        \n",
    "    for lines in resp:\n",
    "        # if we encounter a new seq (e.g multi chain):\n",
    "        if lines[0] == \">\":\n",
    "            #we check if its the first seq. We make out custom header\n",
    "            continue\n",
    "        else:\n",
    "            res += lines\n",
    "            \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b17a7e9-4406-40ad-bf77-1f12519ccd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta_and_write(uniprot_id:str, path:str):\n",
    "    \n",
    "    \n",
    "    URL = f\"https://www.rcsb.org/fasta/entry/{uniprot_id}/display\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    #print(resp)\n",
    "    \n",
    "    res = \"\"\n",
    "    i = 0\n",
    "    seq = 0\n",
    "    chain = string.ascii_uppercase \n",
    "    \n",
    "    aln_code = []\n",
    "    \n",
    "    for lines in resp:\n",
    "        # if we encounter a new seq (e.g multi chain):\n",
    "        if lines[0] == \">\":\n",
    "            #we check if its the first seq. We make out custom header\n",
    "            origin_header = lines.split(\"|\")\n",
    "            if seq == 0:\n",
    "                res += f\">{uniprot_id}_{chain[i]}\\n\"\n",
    "                aln_code.append(f\"{uniprot_id}_{chain[i]}\")\n",
    "                i += 1\n",
    "                seq += 1\n",
    "                continue\n",
    "            #if its the nth seq, we add a \\n beforehand.\n",
    "            else:\n",
    "                \n",
    "                res += f\"\\n>{uniprot_id}_{chain[i]}\\n\"\n",
    "                aln_code.append(f\"{uniprot_id}_{chain[i]}\")\n",
    "                i += 1\n",
    "                continue\n",
    "        res += lines\n",
    "    \n",
    "    print(origin_header)\n",
    "    print(res)\n",
    "    #write to file and leave\n",
    "    #with open(f\"{path}x.fasta\", \"w\") as fasta_out:\n",
    "    #    fasta_out.write(res)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #return aln_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf42015-0f83-4005-b8e6-42d9b46c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hugo_name(uniprot_id:str):\n",
    "    \"\"\"\n",
    "    Retrieve the HUGO gene symbol from a Uniprot identifier.\n",
    "\n",
    "    Parameters:\n",
    "    - uniprot_id (str): The Uniprot identifier for the protein.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: The HUGO gene symbol corresponding to the Uniprot identifier.\n",
    "                  Returns None if no gene symbol is found. \"\"\"\n",
    "                  \n",
    "    \n",
    "    fields = \"gene_names\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    \n",
    "    resp = get_url(URL)\n",
    "    \n",
    "    resp = resp.text\n",
    "    \n",
    "    resp = resp.replace(\"Gene Names\", \"\")\n",
    "    \n",
    "    resp = resp.replace(\"\\n\", \"\")\n",
    "    \n",
    "    genes = resp.split(\" \")\n",
    "\n",
    "    #return None if we find nothing.\n",
    "    return genes[0] if len(genes) != 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb0fffa-bbd3-4a47-9a3d-1181bed44081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbioportal_info(gene_name:str, study_id=\"msk_impact_2017\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve the mutation info from the cbioportal associated with our Uniprot ID.\n",
    "\n",
    "    Parameters:\n",
    "    - gene_name (str): The gene name identifier for the protein (HUGO format) Call first get_hugo_name(uniprot_id:str) to retrieve it.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    - pandas DataFrame or None: The HUGO gene symbol corresponding to the Uniprot identifier is used to search the cbioPortal API.\n",
    "                  Returns None if no gene symbol is found. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cbioportal = SwaggerClient.from_url('https://www.cbioportal.org/api/v2/api-docs',\n",
    "                                    config={\"validate_requests\":False,\"validate_responses\":False,\"validate_swagger_spec\": False})\n",
    "\n",
    "    for a in dir(cbioportal):\n",
    "        cbioportal.__setattr__(a.replace(' ', '_').lower(), cbioportal.__getattr__(a))\n",
    "    \n",
    "    muts = cbioportal.mutations.getMutationsInMolecularProfileBySampleListIdUsingGET(\n",
    "    molecularProfileId=f\"{study_id}_mutations\", # {study_id}_mutations gives default mutations profile for study \n",
    "    sampleListId=f\"{study_id}_all\", # {study_id}_all includes all samples\n",
    "    projection=\"DETAILED\").result()\n",
    "    \n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    mutation_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    gene_symbol_lst = []\n",
    "    mutation_type_lst = []\n",
    "    protein_change_lst = []\n",
    "    sample_id_lst = []\n",
    "    tumor_alt_count_lst = []\n",
    "    tumor_ref_count_lst = []\n",
    "    mutationStatus_lst = []\n",
    "    norm_alt_cnt_lst = []\n",
    "    norm_ref_cnt_lst = []\n",
    "    alleleSpecificCopyNumber_lst = []\n",
    "    driver_filt_lst = []\n",
    "    driver_filt_annot_lst = []\n",
    "    driver_tier_filt_lst = []\n",
    "    driver_tier_filt_annot_lst = []\n",
    "\n",
    "    \n",
    "    # Populate the DataFrame with mutation information\n",
    "    for mutation in muts:\n",
    "        #print(mutation)\n",
    "        gene_symbol = mutation.gene.hugoGeneSymbol\n",
    "        mutation_type = mutation.mutationType\n",
    "        protein_change = mutation.proteinChange\n",
    "        sample_id = mutation.sampleId\n",
    "        tumor_alt_count = mutation.tumorAltCount\n",
    "        tumor_ref_count = mutation.tumorRefCount\n",
    "        mutation_status = mutation.mutationStatus\n",
    "        norm_alt_cnt = mutation.normalAltCount\n",
    "        norm_ref_cnt = mutation.normalRefCount\n",
    "        alleleSpecificCopyNumber = mutation.alleleSpecificCopyNumber\n",
    "        driver_filt = mutation.driverFilter\n",
    "        driver_filt_annot = mutation.driverFilterAnnotation\n",
    "        driver_tier_filt = mutation.driverTiersFilter\n",
    "        driver_tier_filt_annot = mutation.driverTiersFilterAnnotation\n",
    "        \n",
    "# 'driverFilterAnnotation', 'driverTiersFilter', 'driverTiersFilterAnnotation',\n",
    "\n",
    "\n",
    "        \n",
    "        gene_symbol_lst.append(gene_symbol)\n",
    "        mutation_type_lst.append(mutation_type)\n",
    "        protein_change_lst.append(protein_change)\n",
    "        sample_id_lst.append(sample_id)\n",
    "        tumor_alt_count_lst.append(tumor_alt_count) \n",
    "        tumor_ref_count_lst.append(tumor_ref_count)\n",
    "        mutationStatus_lst.append(mutation_status)\n",
    "        norm_alt_cnt_lst.append(norm_alt_cnt)\n",
    "        norm_ref_cnt_lst.append(norm_ref_cnt)\n",
    "        alleleSpecificCopyNumber_lst.append(alleleSpecificCopyNumber)\n",
    "        driver_filt_lst.append(driver_filt)\n",
    "        driver_filt_annot_lst.append(driver_filt_annot)\n",
    "        driver_tier_filt_lst.append(driver_tier_filt)\n",
    "        driver_tier_filt_annot_lst.append(driver_filt_annot)\n",
    "        \n",
    "        \n",
    "    mutation_df[\"gene_symbol\"] = gene_symbol_lst\n",
    "    mutation_df[\"mutation_type\"] = mutation_type_lst\n",
    "    mutation_df[\"prot_change\"] = protein_change_lst\n",
    "    mutation_df[\"sample_id\"] = sample_id_lst\n",
    "    mutation_df[\"tumor_alt_count\"] = tumor_alt_count_lst\n",
    "    mutation_df[\"tumor_ref_count\"] = tumor_ref_count_lst\n",
    "    mutation_df[\"norm_alt_count\"] = tumor_alt_count_lst\n",
    "    mutation_df[\"norm_ref_count\"] = tumor_ref_count_lst\n",
    "    mutation_df[\"mutation_status\"] = mutationStatus_lst\n",
    "    mutation_df[\"alleleSpecificCopyNumber\"] = alleleSpecificCopyNumber_lst\n",
    "    mutation_df[\"driver_filt\"] = driver_filt_lst\n",
    "    mutation_df[\"driver_filt_annot\"] = driver_filt_annot_lst\n",
    "    mutation_df[\"driver_tier_filt\"] = driver_tier_filt_lst\n",
    "    mutation_df[\"driver_tier_filt_annot\"] = driver_filt_annot_lst\n",
    "\n",
    "    \n",
    "    \n",
    "    mutation_sub_df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        for mutation in mutation_df[\"gene_symbol\"]:\n",
    "            if mutation == gene_name:\n",
    "                mutation_sub_df = mutation_df[mutation_df[\"gene_symbol\"] == gene_name]\n",
    "                mutation_sub_df = mutation_sub_df[mutation_sub_df[\"mutation_type\"] == \"Missense_Mutation\"]\n",
    "    except:\n",
    "        print(f\"gene name: {gene_name} was not found\")\n",
    "    \n",
    "    return mutation_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec2a3c8-f7f7-4f90-a794-14dd028dc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#integrate get_rcsb_fasta into pipeline and substitute with get_gene_fasta!\n",
    "#ToBeDone\n",
    "\n",
    "def get_rcsb_fasta(pdb_id_target:str, chain:str):\n",
    "    \n",
    "    '''This function will take the uniprot id and the requiered chain and fetch ONLY the sequence of the\n",
    "    corresponding chain. This will be used for modeller repair as template in order to ONLY build\n",
    "    the required part of the protein (i.e part of the protein the structure covers) and not the fasta for the full length protein\n",
    "    which would lead to a full reconstruction of e.g EGFR 1-1xxx instead of maybe only repairing 300-460'''\n",
    "    \n",
    "    URL = f\"https://www.rcsb.org/fasta/entry/{pdb_id_target}/display\"\n",
    "    \n",
    "    resp = get_url(URL)\n",
    "    resp = resp.iter_lines(decode_unicode=True)\n",
    "    \n",
    "    \n",
    "    #here we store each chain and their fasta respectively. If multiple chains are homo oligomer their fasta\n",
    "    #are stored together.\n",
    "    \n",
    "    fasta_per_chains = defaultdict()\n",
    "        \n",
    "    for lines in resp:\n",
    "        \n",
    "        if lines[0] == \">\":\n",
    "            #we store all headers given they contain the chain info.\n",
    "            #print(lines)\n",
    "            origin_header = lines.split(\"|\") #split by |\n",
    "            #print(origin_header)\n",
    "            chain_info = origin_header[1] #2nd entry is chain info\n",
    "            chain_info = chain_info[6:] # we are only interested in A, B , C , D not the keyword \"chains\" \n",
    "            #print(chain_info)\n",
    "            \n",
    "            #lets try to capture author annotated chains as well.\n",
    "            try:\n",
    "                \n",
    "                author_chain = chain_info[-2] #this is the authors chain info.\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                author_chain = \"Placeholder\"\n",
    "                \n",
    "            chain_info = re.sub('auth.[A-Z]', '', chain_info)  #if the authors have other chain labels we dont care\n",
    "            chain_info = chain_info.replace(\"[]\",\"\")  #this is removal from from the authors chain labels.\n",
    "            \n",
    "            chain_info = chain_info.replace(\",\",\"\") # remove , and merge them\n",
    "            chain_info = chain_info.replace(\" \",\"\") #make str out of those as key for a dict. \"ABCD\"\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            fasta_per_chains[chain_info] = lines\n",
    "            fasta_per_chains[author_chain] = lines\n",
    "            \n",
    "    \n",
    "    print(fasta_per_chains)\n",
    "    \n",
    "    #now lets look the prior names of the chains before renumbering.\n",
    "    \n",
    "    \n",
    "    #with open(f\"{path}/reports/chain_relabeling_protocol.csv\", \"r\") as chain_names_csv:\n",
    "    #    for lines in chain_names_csv:\n",
    "    #        splitted_csv = lines.split()\n",
    "    #        if splitted_csv[0] == pdb_id_target:\n",
    "    #            old = splitted_csv[1].split()\n",
    "    #            new = splitted_csv[2].split()\n",
    "                \n",
    "    \n",
    "    \n",
    "    for fasta_chains, seq in fasta_per_chains.items():\n",
    "        avail_chain = [x for x in fasta_chains]\n",
    "        #print(avail_chain)\n",
    "        if chain in avail_chain:\n",
    "            return seq\n",
    "            \n",
    "    \n",
    "    #try author chain annotation as well if not working.\n",
    "    \n",
    "    \n",
    "    \n",
    "    #return None if we dont find anything.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8496ac2f-ef6f-4d88-a603-c84665a823ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb_id_target= \"6lle\"\n",
    "#get_rcsb_fasta(pdb_id_target=pdb_id_target, chain=\"A\")\n",
    "#\n",
    "#query = \"\"\n",
    "#main_prot_seq = get_gene_fasta(query)\n",
    "#print(len(main_prot_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6dedd3-f126-4c95-83ed-cac2b08fcece",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Helper functions need overview to check which is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c949e0-d64c-43bf-a6cf-9dd76a43f23f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Following functions:\n",
    "\n",
    "+ Setup directories and prep input files\n",
    "+ Prepare templates\n",
    "+ _write_report_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02744083-db9e-4250-8656-6580d195c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories_prepare_input_files_1(gene_name:str, prot_fasta:str, domain_dict:dict):\n",
    "    \n",
    "    #check if the directory is not already there.\n",
    "    #setup all required directories now.\n",
    "    \n",
    "    #hardcorded anchor location.\n",
    "    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "    \n",
    "    # Create directories if not already present\n",
    "    dirs_to_create = [\"reports\", \"shifts\", \"mutational_mapping\"]\n",
    "\n",
    "    for directory in dirs_to_create:\n",
    "        \n",
    "        dir_path = os.path.join(path, directory)\n",
    "        \n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "\n",
    "    # Create or update pdbfile.txt for batch download\n",
    "    \n",
    "    pdbfile_path = os.path.join(path, \"pdbfile.txt\")\n",
    "    \n",
    "    rcsb_unique_ids = list(set(rcsbs[0:4] for vals in domain_dict.values() for rcsbs in vals))\n",
    "    \n",
    "    with open(pdbfile_path, \"w\") as pdb_tar:\n",
    "        pdb_tar.write(\",\".join(rcsb_unique_ids))\n",
    "\n",
    "    \n",
    "    # Store the main fasta of the canonical isoform for later mapping\n",
    "    main_isoform_path = os.path.join(path, \"reports/main_isoform_fasta.txt\")\n",
    "    \n",
    "    with open(main_isoform_path, \"w\") as main:\n",
    "        main.write(prot_fasta)\n",
    "\n",
    "    \n",
    "    # Create chain_relabeling_protocol.csv\n",
    "    chain_relabeling_path = os.path.join(path, \"reports/chain_relabeling_protocol.csv\")\n",
    "    \n",
    "    with open(chain_relabeling_path, \"w\") as fh_chain:\n",
    "        fh_chain.write(\"pdb_file,previous_chains,new_chains\\n\")\n",
    "    \n",
    "    # Create download_dict\n",
    "    download_dict = {rcsb_id: \"\".join(rcsb[-1] for rcsb in rcsbs) for rcsb_id, rcsbs in domain_dict.items()}\n",
    "    \n",
    "    return download_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb4d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories_prepare_input_files(gene_name:str, prot_fasta:str, domain_dict:dict):\n",
    "    \n",
    "    #check if the directory is not already there.\n",
    "    #setup all required directories now.\n",
    "    \n",
    "    #hardcorded anchor location.\n",
    "    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "    \n",
    "    \n",
    "    #dont download twice structures that have 2 chains.\n",
    "    \"\"\" We want to avoid downloading since its a bottleneck and speed up the pipeline.\n",
    "        For that reason we will make a dict where each key = rcsb id and the values are the chains\n",
    "        we need from that pdb.\"\"\"\n",
    "        \n",
    "        \n",
    "    download_dict = defaultdict(str)\n",
    "    \n",
    "    rcsb_ids2 = []\n",
    "    \n",
    "    for keys, vals in domain_dict.items():\n",
    "        for rcsbs in vals:\n",
    "            rcsb_ids2.append(rcsbs[0:4])\n",
    "            download_dict[rcsbs[0:4]] += rcsbs[-1]\n",
    "            \n",
    "    #get rid of duplicates and store only relevant ids\n",
    "    rcsb_unique_ids = list(set(rcsb_ids2))\n",
    "    \n",
    "    try:\n",
    "        #make it and switch to it\n",
    "        os.mkdir(path)\n",
    "        #os.mkdir(f\"{path}/fastas\")\n",
    "        os.mkdir(f\"{path}/reports\")\n",
    "        os.mkdir(f\"{path}/shifts\")\n",
    "        os.mkdir(f\"{path}/mutational_mapping\")\n",
    "        #os.mkdir(f\"{path}/no_gaps\")\n",
    "        #os.mkdir(f\"{path}/missing_repairable_gaps\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Dir already exists\")\n",
    "        #if we already have it, we dont need to go through it again\n",
    "        \n",
    "    #this part will contain the info for curl to get the pdb files.\n",
    "    \n",
    "    with open(f\"{path}/pdbfile.txt\", \"w\") as pdb_tar:\n",
    "        #the input file for batch download is in format:\n",
    "        #entries=pdb_code_chain, sep= \",\"   e.g 4rch_A,5juu_B...\n",
    "        for rcsb_id in rcsb_unique_ids:\n",
    "            \n",
    "            pdb_tar.write(rcsb_id)\n",
    "            \n",
    "            pdb_tar.write(\",\")\n",
    "    \n",
    "    #we also store the main fasta of the canonical isoform for later mapping.\n",
    "    with open(f\"{path}/reports/main_isoform_fasta.txt\", \"w\") as main:\n",
    "        main.write(prot_fasta)\n",
    "        \n",
    "    \n",
    "    with open(f\"{path}/reports/chain_relabeling_protocol.csv\", \"w\") as fh_chain:\n",
    "        fh_chain.write(\"pdb_file,previous_chains,new_chains\")\n",
    "        fh_chain.write(\"\\n\")\n",
    "    \n",
    "    return download_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8743bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_templates(gene_name_target:str,\n",
    "                      templates:list,\n",
    "                      seq_sim:list,\n",
    "                      query_start:list,\n",
    "                      query_end:list,\n",
    "                      temp_start:list,\n",
    "                      temp_end:list,\n",
    "                      path:str, oligodict:list):\n",
    "    \n",
    "    \"\"\"Make function out of making templates and reference structures.\"\"\"\n",
    "    #setup\n",
    "    \n",
    "    \n",
    "    #this retrieves the main len of the protein we are interested in\n",
    "    main_target_seq_len = get_seq_len(path=f\"{path}\", rcsb_id=\"None\", template=True)\n",
    "    #181 for NUD4B\n",
    "    \n",
    "    #gives back the length of each template.\n",
    "    temp_lengths = [f[0] - f[1] for f in zip(temp_end, temp_start)]\n",
    "    \n",
    "    homology_list = sorted(list(zip(templates,seq_sim, temp_lengths)), key= lambda x: x[1], reverse=True)\n",
    "    #print(homology_list)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        #gather target templates that will be used to compute tm scores and align scores\n",
    "        ref_list = []\n",
    "        #here we store human hits.\n",
    "        priority_list = []\n",
    "        #if we retrieve not as many hits, we take what we get\n",
    "        if len(homology_list) < 100:\n",
    "            \n",
    "            max_end = len(homology_list)\n",
    "            \n",
    "        #else we take max 100 potential templates\n",
    "        else:\n",
    "            \n",
    "            max_end = 100\n",
    "        #this will be used later to set a ref struc template.\n",
    "        \n",
    "        \n",
    "        for entry , seq, lengths in homology_list[0:max_end]:\n",
    "            \n",
    "            #this is the rcsb_id\n",
    "            prot_check = entry[0:4]\n",
    "            \n",
    "            #this retrieves gene_name\n",
    "            prot_name = get_gene_name(prot_check)\n",
    "            #this corresponds to the species the gene belongs to\n",
    "            if prot_name[-5:] == \"HUMAN\":\n",
    "                \n",
    "                #if its human, we take it as template and break.\n",
    "                reference_structure = entry+\".pdb\"\n",
    "                #temp score will help to find ideal target\n",
    "                temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                           seq_len=lengths,\n",
    "                           main_seq_len=main_target_seq_len)\n",
    "                #we store both seq similarity and potential .pdb file\n",
    "                priority_list.append((reference_structure, prot_name, temp_score_val, lengths, float(seq)))\n",
    "                continue\n",
    "                \n",
    "            #else we take as many hits we can and select the highest seq similarity out of those.\n",
    "            reference = entry+\".pdb\"\n",
    "            \n",
    "            #temp score will help to find ideal target\n",
    "            temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                       seq_len=lengths,\n",
    "                       main_seq_len=main_target_seq_len)\n",
    "            \n",
    "            ref_list.append((reference, prot_name, temp_score_val, lengths, float(seq))) \n",
    "            \n",
    "    except:\n",
    "        print(f\"We did not find structures.\\n selected template was:{prot_name}\")\n",
    "        print(f\"reference list contained: {ref_list}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(priority_list) != 0:\n",
    "        #if we found a human template we set it as reference structure.\n",
    "        priority_list_sorted = sorted(priority_list, key=lambda x : x[2], reverse=False)\n",
    "        print(f\"we found as reference structure(s): {priority_list[0][1]} -> {priority_list_sorted[0][0]}\")\n",
    "        \n",
    "        #little helper function to write out which template was choosen.\n",
    "        _write_report_template(path=path, \n",
    "                               template_gene_name=priority_list_sorted[0][1],\n",
    "                               template_rcsb=priority_list_sorted[0][0],\n",
    "                               template_length=priority_list_sorted[0][3],\n",
    "                               seq_id =priority_list_sorted[0][4],\n",
    "                               query_length=main_target_seq_len)\n",
    "        \n",
    "        return priority_list_sorted\n",
    "    \n",
    "    else:\n",
    "        #else we take the next best thing that has highest seq similarity hoping this is mouse or something.\n",
    "        ref_list_sorted = sorted(ref_list, key=lambda x : x[2], reverse=False)\n",
    "        print(f\"we found no human reference structure(s) but instead: {ref_list[0][1]} -> {ref_list_sorted[0][0]}\")\n",
    "        \n",
    "        #little helper function to write out which template was choosen.\n",
    "        _write_report_template(path=path, \n",
    "                               template_gene_name=ref_list_sorted[0][1],\n",
    "                               template_rcsb=ref_list_sorted[0][0],\n",
    "                               template_length=ref_list_sorted[0][3],\n",
    "                               seq_id =priority_list_sorted[0][4],\n",
    "                               query_length=main_target_seq_len,\n",
    "                               position_dir = position_dir,\n",
    "                               oligostate_dir=oligostate_dir)\n",
    "        \n",
    "        return ref_list_sorted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec5c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_report_template(path:str, template_gene_name:str, \n",
    "                           template_rcsb:str, template_length:str, \n",
    "                           query_length:str, seq_id:str,\n",
    "                           position_dir:str,\n",
    "                           oligostate_dir:str):\n",
    "    \n",
    "    \n",
    "    #the last dir corresponds to the gene_name of query.\n",
    "    #this gives the split dirs that make up path: e.g /home/micnag/b would give [\"home\", \"micnag\", \"b\"]\n",
    "    path_split = path.split(os.sep)\n",
    "    \n",
    "    \n",
    "    with open(f\"{path}/reports/selected_template_{oligostate_dir}_{position_dir}.tsv\", \"w\") as fh_report:\n",
    "        fh_report.write(\"Query_gene_name\\tTemplate_gene_name\\tTemplate_rcsb\\tTemplate_length\\tQuery_length\\tSequence_identity\")\n",
    "        fh_report.write(\"\\n\")\n",
    "        fh_report.write(f\"{path_split[-1]}\\t{template_gene_name}\\t{template_rcsb}\\t{template_length}\\t{query_length}\\t{seq_id}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fc41e57-2ec2-432d-834e-327f08537ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcsb_info(path:str):\n",
    "    \n",
    "    '''This function will take the uniprot id and the requiered chain and fetch ONLY the sequence of the\n",
    "    corresponding chain. This will be used for modeller repair as template in order to ONLY build\n",
    "    the required part of the protein (i.e part of the protein the structure covers) and not the fasta for the full length protein\n",
    "    which would lead to a full reconstruction of e.g EGFR 1-1xxx instead of maybe only repairing 300-460'''\n",
    "    \n",
    "    pdb_list = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    pdb_list = [f[0:4] for f in pdb_list if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    try:\n",
    "            \n",
    "        os.mkdir(f\"{path}/PCA_structures_info\")\n",
    "            \n",
    "    except Exception as error:\n",
    "            \n",
    "        print(error)\n",
    "    \n",
    "    \n",
    "    infodict = defaultdict()\n",
    "    \n",
    "    for pdb_id_target in pdb_list:\n",
    "        \n",
    "        URL = f\"https://www.rcsb.org/fasta/entry/{pdb_id_target}/display\"\n",
    "        \n",
    "        resp = get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "\n",
    "        #here we store each chain and their fasta respectively. If multiple chains are homo oligomer their fasta\n",
    "        #are stored together.\n",
    "            \n",
    "        for lines in resp:\n",
    "\n",
    "            if lines[0] == \">\":\n",
    "                #we store all headers given they contain the chain info.\n",
    "                lines_grp = lines.split(\"|\")\n",
    "                pdb_id = lines_grp[0].replace(\">\",\"\")\n",
    "                pdb_id = pdb_id[0:4].lower()\n",
    "                infodict[pdb_id] = lines_grp[1:]\n",
    "\n",
    "    with open(f\"{path}/PCA_structures_info/PCA_rcsb_headers.tsv\", \"w\") as fh_out:\n",
    "        for keys, vals in infodict.items():\n",
    "            fh_out.write(keys)\n",
    "            fh_out.write(\"\\t\")\n",
    "            for entries in vals:\n",
    "                fh_out.write(entries)\n",
    "                fh_out.write(\"\\t\")\n",
    "            fh_out.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "777cc5de-6935-4352-adec-b67afba67022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ADH1G_HUMAN/dimer/pos_1_375\"\n",
    "\n",
    "#get_rcsb_info(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f11170-a27c-46ef-8254-9b5a671e0424",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DOWNLOAD PDBS function\n",
    "\n",
    "requires:\n",
    "\n",
    "+ batch_download_modified.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57df4a41-23ae-4d43-82ce-524be6f660cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(file_list, download_type, path):\n",
    "    \n",
    "    results = []\n",
    "    for file in file_list:\n",
    "        \n",
    "        bash_curl_cmd = f\"./batch_download_modified.sh -f {file} -o {path} -{download_type}\"\n",
    "        \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "        \n",
    "        result = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        results.append(result.stdout.split(\"\\n\")[:-1])  # Skip the last empty element\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5b08116-235f-4312-8c06-5c43758462a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_download_pdbs(gene_name):\n",
    "    \n",
    "    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "    \n",
    "    os.chdir(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs\")\n",
    "    \n",
    "   ## Setup directories\n",
    "   #check_path = f\"{path}/monomeric\"\n",
    "   #\n",
    "   #if not os.path.exists(check_path):\n",
    "   #    \n",
    "   #    os.makedirs(check_path)\n",
    "\n",
    "    # List of PDB and MMCIF files to download\n",
    "    pdbfile_list = [f\"{path}/pdbfile.txt\"]  # Adjust the number based on your requirements\n",
    "    \n",
    "    mmciffile_list = [f\"{path}/mmciffile.txt\"]\n",
    "\n",
    "    # Download PDBs in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures_pdb = [executor.submit(download_files, pdbfile_list, 'p', path)]\n",
    "        wait(futures_pdb)\n",
    "\n",
    "    # Print PDB download result\n",
    "    for future_pdb in futures_pdb:\n",
    "        result_pdb = future_pdb.result()[0]\n",
    "        if len(result_pdb) > 0:\n",
    "            print(result_pdb)\n",
    "\n",
    "    # Download MMCIFs\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures_mmcif = [executor.submit(download_files, mmciffile_list, 'c', path)]\n",
    "        wait(futures_mmcif)\n",
    "\n",
    "    # Print MMCIF download result\n",
    "    for future_mmcif in futures_mmcif:\n",
    "        result_mmcif = future_mmcif.result()[0]\n",
    "        if len(result_mmcif) > 0:\n",
    "            print(result_mmcif)\n",
    "\n",
    "    #print(f\"Dir has already some pdbs -> {len(os.listdir(check_path))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4881a01-7ee7-4b99-90dc-8cd8641d662f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f6b7d-2a04-490d-967c-186ea0b94ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa9143-1369-4ce8-9460-78ade5860e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d457ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def batch_download_pdbs(gene_name:str):\n",
    "#    \n",
    "#    \"\"\"Function to download pdbs and if not possible.. download mmcif.\"\"\"\n",
    "#    \n",
    "#    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "#    \n",
    "#    #ugly but works.\n",
    "#    os.chdir(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/\")\n",
    "#    #this bash cmd will fetch all possible pdbs.    \n",
    "#    bash_curl_cmd = f\"./batch_download_modified.sh -f {path}/pdbfile.txt -o {path} -p\"\n",
    "#    \n",
    "#    bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "#    \n",
    "#    #in the .sh file I piped to stdout if we did not manage to get a pdb.\n",
    "#    #stdout -> rcsbid_chain that did not work\n",
    "#    \n",
    "#    #if we already downloaded, dont download again!.\n",
    "#    \n",
    "#    check_path = f\"{path}/monomeric\"\n",
    "#    \n",
    "#    query_files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f[-4:] == \".pdb\"]\n",
    "#    \n",
    "#    #if we have already pdbs downloaded, dont do it again. IF monomeric dir already exists, we dont need to run it again.\n",
    "#    if len(query_files) == 0 and os.path.isdir(check_path) == False:\n",
    "#        result_pdbs = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "#                             universal_newlines=True)\n",
    "#        \n",
    "#        \"\"\"THIS PART IS CONCERNING WITH MMCIF FILES AND NEEDS TO BE UPDATED LATER.\"\"\"\n",
    "#        result_list = result_pdbs.stdout.split(\"\\n\")\n",
    "#\n",
    "#        #now lets check if there are mmcif files to gather.\n",
    "#        if len(result_list) != 0:\n",
    "#            #first setup input file that is similar to pdbfile.txt and then rerun bash for mmcif\n",
    "#            with open(f\"{path}/mmciffile.txt\", \"w\") as mmcif_tar:\n",
    "#            \n",
    "#                for mmcifs in result_list[:-1]: #last one is always empty\n",
    "#                \n",
    "#                    mmcif_tar.write(mmcifs)\n",
    "#                \n",
    "#                    mmcif_tar.write(\",\")\n",
    "#                \n",
    "#            bash_curl_cmd = f\"./batch_download_modified.sh -f {path}/mmciffile.txt -o {path} -c\"\n",
    "#        \n",
    "#            bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "#        \n",
    "#            result_pdbs = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "#                             universal_newlines=True)\n",
    "#    else:\n",
    "#        print(f\"Dir has already some pdbs -> {len(query_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce79db-d570-4649-b8c5-81910ac692e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper function / Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1483724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cleanup(path:str):\n",
    "        \n",
    "    #we dont need tmp pdb files given that all what we want is in dirs and no longer in this main dir.\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    for file in onlyfiles:\n",
    "        os.remove(f\"{path}/{file}\")\n",
    "    \n",
    "    #also remove the shifts tmp folder to save disc space.\n",
    "    try:\n",
    "        shutil.rmtree(f'{path}/shifts')\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698df52-b405-44b8-9275-3524665928dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reduce redundancy and split proteins into domain chunks / group similar structure ranges together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da7404-7a17-47eb-a79d-91ca46f99888",
   "metadata": {
    "tags": []
   },
   "source": [
    "## _Split domains + make_groups\n",
    "\n",
    "make_groups uses:\n",
    "\n",
    "+ <span style=\"color:blue\">_recursive_union_reduction</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90bffdcb-831a-4d74-9126-8ca538c2f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_domains_1(templates:list, temp_start:list, temp_end:list, query_start:list, query_end:list):\n",
    "  \n",
    "    abs_seq_min = min(temp_start)\n",
    "    abs_seq_max = max(temp_end)\n",
    "    \n",
    "    abs_range = abs_seq_max - abs_seq_min\n",
    "    tolerance = 0.2 * abs_range\n",
    "\n",
    "    seq_ranges = list(zip(query_start, query_end, templates))\n",
    "    print(seq_ranges)\n",
    "    \n",
    "    # Find the majority vote\n",
    "    majority_range = _majority_vote(seq_ranges)\n",
    "\n",
    "    # Extend the winner's range by tolerance\n",
    "    winner_start, winner_end = majority_range\n",
    "    extended_start = max(abs_seq_min, winner_start - tolerance)\n",
    "    extended_end = min(abs_seq_max, winner_end + tolerance)\n",
    "    extended_range = (extended_start, extended_end)\n",
    "\n",
    "    # Filter sequences within the extended range\n",
    "    selected_ranges = [(start_q, end_q, template) for start_q, end_q, template in seq_ranges\n",
    "                       if extended_start <= start_q <= extended_end and extended_start <= end_q <= extended_end]\n",
    "\n",
    "    # Merge overlapping ranges\n",
    "    merged_ranges = merge_overlapping_ranges(selected_ranges)\n",
    "\n",
    "    print(f\"Winner's range: {extended_range}\")\n",
    "    print(f\"These are the merged ranges: {list(merged_ranges.keys())}\")\n",
    "\n",
    "    return merged_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6194670c-3d81-47e5-ad02-f5f66586163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_groups(datalst:list, val_list:list):\n",
    "    \n",
    "    storage_dict = defaultdict(list)\n",
    "    \n",
    "    for ranges, vals in zip(datalst, val_list):\n",
    "        storage_dict[Interval(ranges[0],ranges[1])] = vals\n",
    "    \n",
    "    \n",
    "    #works here still\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    intervals = [Interval(begin, end) for (begin, end) in datalst]\n",
    "    \n",
    "    \n",
    "    #print(\"this are the intervals we deal with:\")\n",
    "    #print(intervals)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #good so far. all 34 structures are in here for case NUDT4\n",
    "    \n",
    "    interval_dict = defaultdict()\n",
    "    \n",
    "    for interv in intervals:\n",
    "        interval_dict[interv] = interv\n",
    "        \n",
    "\n",
    "    print(f\"length of dictionary before recursive_reduction: {len(interval_dict)}\")\n",
    "    if len(interval_dict) != 1:\n",
    "        result = _recursive_union_reduction(interval_dict, 0)\n",
    "        print(f\"length of dictionary after recursive_reduction: {len(result)}\\n\")\n",
    "    else:\n",
    "        result = interval_dict\n",
    "        #for keys, vals in result.items():\n",
    "        #    print(len(vals))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    interval_collection_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in result.items():\n",
    "        #for each key we need to find all associated intervals within the key range e.g all subsets.\n",
    "        \n",
    "        for interval_member in intervals:\n",
    "            condition = interval_member.is_subset(keys)\n",
    "            condition_2 = interval_member.measure > 0.7*keys.measure\n",
    "            if condition and condition_2:\n",
    "                interval_collection_dict[keys].append(interval_member)\n",
    "    \n",
    "    #print(len(intervals))\n",
    "    \n",
    "    rcsb_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in interval_collection_dict.items():\n",
    "        \n",
    "        if type(vals) == list:\n",
    "            rcsb_tmp = []\n",
    "            for val in vals:\n",
    "                tmp = storage_dict[val]\n",
    "                #print(f\"this is current key: {keys}\")\n",
    "                #print(f\"this is current structures: {storage_dict[val]}\")\n",
    "                #print(tmp)\n",
    "                for rcsbs in tmp:\n",
    "                    rcsb_tmp.append(rcsbs)\n",
    "            \n",
    "        else:\n",
    "            rcsb_tmp = []\n",
    "            tmp = storage_dict[val]\n",
    "            for rcsbs in tmp:\n",
    "                rcsb_tmp.append(rcsbs)\n",
    "        \n",
    "            rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        \n",
    "        \n",
    "    #for keys, vals in rcsb_dict.items():\n",
    "        #print(keys)\n",
    "        #print(vals)\n",
    "        #print(\"\\n\")\n",
    "    \n",
    "    return rcsb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6c23e-f1ae-4dc5-8c4b-0bba4147b868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1838055-4ba6-4e13-afd3-bdc405a414ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_domains(templates:list, temp_start:list, temp_end:list, query_start:list, query_end:list):\n",
    "  \n",
    "    print(len(templates))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #this will give the maximal possible range we will look into.\n",
    "    abs_seq_min = min(temp_start)\n",
    "    abs_seq_max = max(temp_end)\n",
    "    \n",
    "    abs_range = abs_seq_max - abs_seq_min\n",
    "    tolerance = 0.2 *abs_range\n",
    "    \n",
    "    seq_lengths = []\n",
    "    \n",
    "    for start, end in zip(query_start, query_end):\n",
    "        seq_lengths.append(end - start)\n",
    "\n",
    "    \n",
    "    seq_ranges = []\n",
    "    \n",
    "    testdict = defaultdict(list)\n",
    "    \n",
    "    for temp, start_q, end_q, length in zip(templates, query_start, query_end, seq_lengths):\n",
    "        testdict[(start_q, end_q)].append(temp)\n",
    "    \n",
    "    start_stop_list = []\n",
    "    val_list = []\n",
    "    \n",
    "    \n",
    "    for keys, values in testdict.items():\n",
    "        start_stop_list.append(keys)\n",
    "        val_list.append(values)\n",
    "    \n",
    "    print(f\"These are the ranges that we deal with: {start_stop_list} \\n\")\n",
    "    \n",
    "    if len(start_stop_list) != 1:\n",
    "        split_dictionary = _make_groups(start_stop_list, val_list)    \n",
    "    else:\n",
    "        split_dictionary = defaultdict(list)\n",
    "    \n",
    "    print(f\"These are the merged ranges: {[x for x in split_dictionary.keys()]}\")\n",
    "    \n",
    "    \n",
    "    single_interval_dict = defaultdict(list)\n",
    "    \n",
    "    #i = 0\n",
    "    #for vals in val_list:\n",
    "        #print(vals)\n",
    "        #print(Interval(start_stop_list[0][0], start_stop_list[0][1]))\n",
    "    #    single_interval_dict[Interval(start_stop_list[i][0], start_stop_list[i][1])] = vals\n",
    "    #    i += 1\n",
    "    \n",
    "    #this contains the domains we need to split and each set of structures associated per domain.\n",
    "    \n",
    "    split_return_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in split_dictionary.items():\n",
    "        #only care about more than 1 structure.\n",
    "        if len(vals) > 1: \n",
    "            split_return_dict[keys] = vals\n",
    "    \n",
    "    cnt = 0\n",
    "    for keys, vals in split_return_dict.items():\n",
    "        cnt += len(vals)\n",
    "        \n",
    "    return split_return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16ac58f3-3242-4d87-970a-d87348817e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_union_reduction(dictionary_to_parse:dict, num_of_iterations:int):\n",
    "    \n",
    "    #print(\"this is dictionary to parse:\")\n",
    "    #print(dictionary_to_parse)\n",
    "    \n",
    "    if num_of_iterations < 20:\n",
    "        \n",
    "        #if everything merged together we dont need to continue\n",
    "        if len(dictionary_to_parse) == 1:\n",
    "            return dictionary_to_parse\n",
    "        \n",
    "        num_of_iterations += 1\n",
    "        val_1 = []\n",
    "        val_2 = []\n",
    "        \n",
    "        for keys, vals in dictionary_to_parse.items():\n",
    "            val_1.append(keys)\n",
    "            val_2.append(keys)\n",
    "        \n",
    "        union_dict = defaultdict(list)        \n",
    "        \n",
    "        for val in val_1:\n",
    "            union_new = False\n",
    "            for val_x in val_2:\n",
    "                if val != val_x:\n",
    "                    m_1 = val.measure\n",
    "                    m_2 = val_x.measure\n",
    "                    m_3 = val_x.intersect(val).measure\n",
    "                    \n",
    "                    #print(val, val_x)\n",
    "                    #print(m_1, m_2, m_3)\n",
    "                    condition_1 = m_3 <= m_1 and m_3 >= 0.8* m_1\n",
    "                    condition_2 = m_3 <= m_2 and m_3 >= 0.8* m_2\n",
    "                    #print(condition_1, condition_2)\n",
    "                    if condition_1 and condition_2:\n",
    "                        union_new = Union(val, val_x)\n",
    "                        val_tmp_1 = val\n",
    "                        val_tmp_2 = val_x\n",
    "            \n",
    "            if union_new:\n",
    "                union_dict[union_new].append(val_tmp_1)\n",
    "                union_dict[union_new].append(val_tmp_2)\n",
    "                \n",
    "            else:\n",
    "        \n",
    "                union_dict[val].append(val)\n",
    "\n",
    "        dictionary_to_parse = union_dict\n",
    "        \n",
    "        dictionary_to_parse = _recursive_union_reduction(dictionary_to_parse, num_of_iterations)\n",
    "    \n",
    "    \n",
    "    return dictionary_to_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17009c-b2f3-48f4-bb1a-16dfa92b6fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800ff84-3812-413e-a4d0-07b64f925b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d810a640-8ea8-4346-8c32-15ce51cb54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _majority_vote(seq_ranges):\n",
    "    # Count occurrences of each range\n",
    "    count_dict = defaultdict(int)\n",
    "    for start_q, end_q, template in seq_ranges:\n",
    "        count_dict[(start_q, end_q)] += 1\n",
    "\n",
    "    # Find the range with the maximum count\n",
    "    majority_range = max(count_dict, key=count_dict.get)\n",
    "    \n",
    "    return majority_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "794f2de6-6990-4fd0-8c3b-9e551439589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_ranges(ranges):\n",
    "    sorted_ranges = sorted(ranges, key=lambda x: x[0])\n",
    "\n",
    "    merged_ranges = []\n",
    "    current_start, current_end, current_template = sorted_ranges[0]\n",
    "\n",
    "    for start, end, template in sorted_ranges[1:]:\n",
    "        if start <= current_end:\n",
    "            # Merge overlapping ranges\n",
    "            current_end = max(current_end, end)\n",
    "        else:\n",
    "            # Add the current merged range to the result\n",
    "            merged_ranges.append((current_start, current_end, current_template))\n",
    "            # Start a new merged range\n",
    "            current_start, current_end, current_template = start, end, template\n",
    "\n",
    "    # Add the last merged range\n",
    "    merged_ranges.append((current_start, current_end, current_template))\n",
    "\n",
    "    return dict(((start, end), templates) for start, end, templates in merged_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b865832f-6194-4bc9-8771-3fe38389e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_values_for_overlapping_ranges(merged_ranges, original_dict):\n",
    "    merged_dict = defaultdict(list)\n",
    "\n",
    "    for merged_range in merged_ranges:\n",
    "        for key, value in original_dict.items():\n",
    "            if key[0] <= merged_range[0] and key[1] >= merged_range[1]:\n",
    "                merged_dict[tuple(merged_range)].extend(value)\n",
    "\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf8038a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_domains(templates:list, temp_start:list, temp_end:list, query_start:list, query_end:list):\n",
    "  \n",
    "    print(len(templates))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #this will give the maximal possible range we will look into.\n",
    "    abs_seq_min = min(temp_start)\n",
    "    abs_seq_max = max(temp_end)\n",
    "    \n",
    "    abs_range = abs_seq_max - abs_seq_min\n",
    "    tolerance = 0.2 *abs_range\n",
    "    \n",
    "    seq_lengths = []\n",
    "    \n",
    "    for start, end in zip(query_start, query_end):\n",
    "        seq_lengths.append(end - start)\n",
    "\n",
    "    \n",
    "    seq_ranges = []\n",
    "    \n",
    "    testdict = defaultdict(list)\n",
    "    \n",
    "    for temp, start_q, end_q, length in zip(templates, query_start, query_end, seq_lengths):\n",
    "        testdict[(start_q, end_q)].append(temp)\n",
    "    \n",
    "    start_stop_list = []\n",
    "    val_list = []\n",
    "    \n",
    "    \n",
    "    for keys, values in testdict.items():\n",
    "        start_stop_list.append(keys)\n",
    "        val_list.append(values)\n",
    "    \n",
    "    print(f\"These are the ranges that we deal with: {start_stop_list} \\n\")\n",
    "    \n",
    "    if len(start_stop_list) != 1:\n",
    "        split_dictionary = _make_groups(start_stop_list, val_list)    \n",
    "    else:\n",
    "        split_dictionary = defaultdict(list)\n",
    "    \n",
    "    print(f\"These are the merged ranges: {[x for x in split_dictionary.keys()]}\")\n",
    "    \n",
    "    \n",
    "    single_interval_dict = defaultdict(list)\n",
    "    \n",
    "    #i = 0\n",
    "    #for vals in val_list:\n",
    "        #print(vals)\n",
    "        #print(Interval(start_stop_list[0][0], start_stop_list[0][1]))\n",
    "    #    single_interval_dict[Interval(start_stop_list[i][0], start_stop_list[i][1])] = vals\n",
    "    #    i += 1\n",
    "    \n",
    "    #this contains the domains we need to split and each set of structures associated per domain.\n",
    "    \n",
    "    split_return_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in split_dictionary.items():\n",
    "        #only care about more than 1 structure.\n",
    "        if len(vals) > 1: \n",
    "            split_return_dict[keys] = vals\n",
    "    \n",
    "    cnt = 0\n",
    "    for keys, vals in split_return_dict.items():\n",
    "        cnt += len(vals)\n",
    "        \n",
    "    return split_return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "920e20a6-ba73-4637-8737-ced3e70ea1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_groups(datalst:list, val_list:list):\n",
    "    \n",
    "    storage_dict = defaultdict(list)\n",
    "    \n",
    "    for ranges, vals in zip(datalst, val_list):\n",
    "        storage_dict[Interval(ranges[0],ranges[1])] = vals\n",
    "    \n",
    "    \n",
    "    #works here still\n",
    "    result_dict = defaultdict(list)\n",
    "    \n",
    "    intervals = [Interval(begin, end) for (begin, end) in datalst]\n",
    "    \n",
    "    \n",
    "    #print(\"this are the intervals we deal with:\")\n",
    "    #print(intervals)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #good so far. all 34 structures are in here for case NUDT4\n",
    "    \n",
    "    interval_dict = defaultdict()\n",
    "    \n",
    "    for interv in intervals:\n",
    "        interval_dict[interv] = interv\n",
    "        \n",
    "\n",
    "    print(f\"length of dictionary before recursive_reduction: {len(interval_dict)}\")\n",
    "    if len(interval_dict) != 1:\n",
    "        result = _recursive_union_reduction(interval_dict, 0)\n",
    "        print(f\"length of dictionary after recursive_reduction: {len(result)}\\n\")\n",
    "    else:\n",
    "        result = interval_dict\n",
    "        #for keys, vals in result.items():\n",
    "        #    print(len(vals))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    interval_collection_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in result.items():\n",
    "        #for each key we need to find all associated intervals within the key range e.g all subsets.\n",
    "        \n",
    "        for interval_member in intervals:\n",
    "            condition = interval_member.is_subset(keys)\n",
    "            condition_2 = interval_member.measure > 0.7*keys.measure\n",
    "            if condition and condition_2:\n",
    "                interval_collection_dict[keys].append(interval_member)\n",
    "    \n",
    "    #print(len(intervals))\n",
    "    \n",
    "    rcsb_dict = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in interval_collection_dict.items():\n",
    "        \n",
    "        if type(vals) == list:\n",
    "            rcsb_tmp = []\n",
    "            for val in vals:\n",
    "                tmp = storage_dict[val]\n",
    "                #print(f\"this is current key: {keys}\")\n",
    "                #print(f\"this is current structures: {storage_dict[val]}\")\n",
    "                #print(tmp)\n",
    "                for rcsbs in tmp:\n",
    "                    rcsb_tmp.append(rcsbs)\n",
    "            \n",
    "        else:\n",
    "            rcsb_tmp = []\n",
    "            tmp = storage_dict[val]\n",
    "            for rcsbs in tmp:\n",
    "                rcsb_tmp.append(rcsbs)\n",
    "        \n",
    "            rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        rcsb_dict[keys] = rcsb_tmp\n",
    "        \n",
    "        \n",
    "        \n",
    "    #for keys, vals in rcsb_dict.items():\n",
    "        #print(keys)\n",
    "        #print(vals)\n",
    "        #print(\"\\n\")\n",
    "    \n",
    "    return rcsb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b977a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_union_reduction(dictionary_to_parse:dict, num_of_iterations:int):\n",
    "    \n",
    "    #print(\"this is dictionary to parse:\")\n",
    "    #print(dictionary_to_parse)\n",
    "    \n",
    "    if num_of_iterations < 20:\n",
    "        \n",
    "        #if everything merged together we dont need to continue\n",
    "        if len(dictionary_to_parse) == 1:\n",
    "            return dictionary_to_parse\n",
    "        \n",
    "        num_of_iterations += 1\n",
    "        val_1 = []\n",
    "        val_2 = []\n",
    "        \n",
    "        for keys, vals in dictionary_to_parse.items():\n",
    "            val_1.append(keys)\n",
    "            val_2.append(keys)\n",
    "        \n",
    "        union_dict = defaultdict(list)        \n",
    "        \n",
    "        for val in val_1:\n",
    "            union_new = False\n",
    "            for val_x in val_2:\n",
    "                if val != val_x:\n",
    "                    m_1 = val.measure\n",
    "                    m_2 = val_x.measure\n",
    "                    m_3 = val_x.intersect(val).measure\n",
    "                    \n",
    "                    #print(val, val_x)\n",
    "                    #print(m_1, m_2, m_3)\n",
    "                    condition_1 = m_3 <= m_1 and m_3 >= 0.8* m_1\n",
    "                    condition_2 = m_3 <= m_2 and m_3 >= 0.8* m_2\n",
    "                    #print(condition_1, condition_2)\n",
    "                    if condition_1 and condition_2:\n",
    "                        union_new = Union(val, val_x)\n",
    "                        val_tmp_1 = val\n",
    "                        val_tmp_2 = val_x\n",
    "            \n",
    "            if union_new:\n",
    "                union_dict[union_new].append(val_tmp_1)\n",
    "                union_dict[union_new].append(val_tmp_2)\n",
    "                \n",
    "            else:\n",
    "        \n",
    "                union_dict[val].append(val)\n",
    "\n",
    "        dictionary_to_parse = union_dict\n",
    "        \n",
    "        dictionary_to_parse = _recursive_union_reduction(dictionary_to_parse, num_of_iterations)\n",
    "    \n",
    "    \n",
    "    return dictionary_to_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936621c-5061-47cc-8bac-17eb6c7309bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split based on oligomers.\n",
    "\n",
    "Input:\n",
    "\n",
    "1. <span style=\"color:green\">interval_dirs:dict</span>\n",
    "2. <span style=\"color:green\">oligostates:dict</span> \n",
    "3. <span style=\"color:green\">path:str</span> \n",
    "4. <span style=\"color:green\">rangedir:dict</span>\n",
    "\n",
    "Output:\n",
    "makes and sets up the required directories for the different oligomeric states.\n",
    "\n",
    "Helper functions:\n",
    "\n",
    "+ <span style=\"color:blue\">get_seq_len</span>\n",
    "+ <span style=\"color:blue\">get_gene_name</span>\n",
    "+ <span style=\"color:blue\">temp_score</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33e65007-e6f4-4510-9554-d979d0dcc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oligo_dirs(interval_dirs, oligostates, path, rangedir):\n",
    "    oligodirdict = {\n",
    "        \"1\": 'monomer',\n",
    "        \"2\": 'dimer',\n",
    "        \"3\": 'trimer',\n",
    "        \"4\": 'tetramer',\n",
    "        \"5\": 'pentamer',\n",
    "        \"6\": 'hexamer',\n",
    "        \"7\": 'heptamer',\n",
    "        \"8\": 'oktamer',\n",
    "        \"9\": 'nonamer',\n",
    "        \"10\": 'decamer',\n",
    "        \"11\": 'undecamer',\n",
    "        \"12\": 'dodecamer',\n",
    "        \"13\": 'tridecamer',\n",
    "        \"14\": 'tetradecamer',\n",
    "        \"15\": 'pentadecamer',\n",
    "        \"16\": 'hexadecamer',\n",
    "        \"17\": 'heptadecamer',\n",
    "        \"18\": 'oktadecamer',\n",
    "        \"19\": 'nonadecamer',\n",
    "        \"20\": 'eicosamer'\n",
    "    }\n",
    "\n",
    "    intervaldirs = list(interval_dirs.keys())\n",
    "\n",
    "    dirs_to_make = set(oligostates.values())\n",
    "\n",
    "    for intervals in intervaldirs:\n",
    "        \n",
    "        start, end = intervals.start, intervals.end\n",
    "        \n",
    "        pos_name = f\"pos_{start}_{end}\"\n",
    "\n",
    "        for dirs in dirs_to_make:\n",
    "            \n",
    "            try:\n",
    "                name = oligodirdict[str(dirs)]\n",
    "                os.makedirs(f\"{path}/{name}/{pos_name}\")\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "\n",
    "    for keys, values in oligostates.items():\n",
    "        try:\n",
    "            previous_path = f\"{path}/merged_cleaned_files/{keys}.pdb\"\n",
    "\n",
    "            for ranges, vals in rangedir.items():\n",
    "                for val in vals:\n",
    "                    if val.startswith(keys[0:4]):\n",
    "                        start, end = ranges.start, ranges.end\n",
    "                        dir_name_to_use = f\"pos_{start}_{end}\"\n",
    "                        print(dir_name_to_use)\n",
    "                        savepath = f\"{path}/{oligodirdict[str(values)]}/{dir_name_to_use}\"\n",
    "\n",
    "                        shutil.copy(previous_path, savepath)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9887ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def make_oligo_dirs(interval_dirs:dict, oligostates:dict, path:str, rangedir:dict):\n",
    "#    \n",
    "#    #build those out of the  /monomeric subdirectory!!!\n",
    "#    \n",
    "#    #intervaldir = f\"{path}/monomeric\"\n",
    "#    \n",
    "#    #print(\"this is interval dirs\")\n",
    "#    #print(interval_dirs)\n",
    "#    \n",
    "#    intervaldirs = [ f for f in interval_dirs.keys()]\n",
    "#    \n",
    "#    #print(intervaldirs)\n",
    "#    \n",
    "#    oligodirdict = dict([(\"1\",'monomer'),\n",
    "#                 (\"2\",'dimer'),\n",
    "#                 (\"3\",'trimer'),\n",
    "#                 (\"4\",'tetramer'),\n",
    "#                 (\"5\",'pentamer'),\n",
    "#                 (\"6\",'hexamer'),\n",
    "#                 (\"7\",'heptamer'),\n",
    "#                 (\"8\",'oktamer'),\n",
    "#                 (\"9\",'nonamer'),\n",
    "#                 (\"10\",'decamer'),\n",
    "#                 (\"11\",'undecamer'),\n",
    "#                 (\"12\",'dodecamer'),\n",
    "#                 (\"13\",'tridecamer'),\n",
    "#                 (\"14\",'tetradecamer'),\n",
    "#                 (\"15\",'pentadecamer'),\n",
    "#                 (\"16\",'hexadecamer'),\n",
    "#                 (\"17\",'heptadecamer'),\n",
    "#                 (\"18\",'oktadecamer'),\n",
    "#                 (\"19\",'nonadecamer'),\n",
    "#                 (\"20\",'eicosamer')])\n",
    "#    \n",
    "#\n",
    "#    print(\"here start dict coll\")\n",
    "#    print(intervaldirs)\n",
    "#    print(oligostates)\n",
    "#    print(rangedir)\n",
    "#    \n",
    "#    #grabs info about which directory to make.\n",
    "#    dirs_to_make = list(set([x for x in oligostates.values()]))\n",
    "#    \n",
    "#    for intervals in intervaldirs:\n",
    "#        start = intervals[0]\n",
    "#        end = intervals[1]\n",
    "#        \n",
    "#        pos_name = f\"pos_{start}_{end}\"\n",
    "#        for dirs in dirs_to_make:\n",
    "#            try:\n",
    "#                \n",
    "#                name = oligodirdict[str(dirs)]\n",
    "#                #print(f\"{path}/{name}/{pos_name}\")\n",
    "#                \n",
    "#                os.makedirs(f\"{path}/{name}/{pos_name}\")\n",
    "#                \n",
    "#            except Exception as error:\n",
    "#                print(error)\n",
    "#        \n",
    "#    \n",
    "#    #here might be an issue\n",
    "#    #print(\"this is oligostates dict\")\n",
    "#    #print(oligostates)\n",
    "#    for keys, values in oligostates.items():\n",
    "#        try:\n",
    "#            #HERE SEEMS TO BE AN ISSUE WITH KEYS. CHECK.\n",
    "#            previous_path = f\"{path}/merged_cleaned_files/{keys}.pdb\"\n",
    "#            \n",
    "#            \n",
    "#            #range is 19-180 in first iteration.\n",
    "#            for ranges, vals in rangedir.items():\n",
    "#                for val in vals:\n",
    "#                    #current val: 6ckv_A, current key: 5wdd                    \n",
    "#                    #defaultdict(<class 'str'>, {'5wdd': 1, '6ckv_1': 1, '6ckv_2': 1, '6ckv_3': 1, '6ckv_4': 1, '6ckv_5': 1, '6ckv_6': 1, '6ckv_7': 1, '6ckv_8': 1, '6ckv_9': 1, '6ckv_10': 1, '6ckv_11': 1, '6ckv_12': 1, '6ckv_13': 1, '6ckv_14': 1, '6ckv_15': 1, '6ckv_16': 1, '6ckv_17': 1, '6ckv_18': 1, '6ckv_19': 1, '6ckv_20': 1, '6v4m': 1, '6yld': 2})\n",
    "#                    #defaultdict(None, {Interval(19, 180): ['6ckv_A', '5wdd_A', '5wdd_B', '6v4m_A', '6yld_A', '6yld_C']})\n",
    "#                    if val[0:4] == keys[0:4]:\n",
    "#                        start = ranges[0]\n",
    "#                        end = ranges[1]\n",
    "#                        \n",
    "#                        dir_name_to_use = f\"pos_{start}_{end}\"\n",
    "#            \n",
    "#                        #print(f\"{path}/{oligodirdict[str(values)]}/{range_to_save}\")\n",
    "#            \n",
    "#                        savepath = f\"{path}/{oligodirdict[str(values)]}/{dir_name_to_use}\"\n",
    "#\n",
    "#                        #print(f\"previous path: {previous_path}, savepath: {savepath}\")\n",
    "#                        shutil.copy(previous_path, savepath)\n",
    "#            \n",
    "#        except Exception as error:\n",
    "#            print(error)\n",
    "#    \n",
    "#    try:\n",
    "#        #shutil.rmtree(f\"{path}/merged_cleaned_files\")\n",
    "#        pass\n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903eee79-3433-468c-b783-bbb32bc9d60a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Atomium part / Biological assemblies\n",
    "\n",
    "### Major Subfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e80b6-1abd-44ce-a442-3b88911ea61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85be6d95-0cd0-4c48-b5b2-901f944f9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biological_assemblies_atomium_1(path, gene_name, main_iso_seq, main_protein_seq):\n",
    "    \n",
    "    relevant_files = [f for f in os.listdir(path) if f.endswith((\".pdb\", \".cif\")) and len(f) == 8]\n",
    "\n",
    "    oligostates = defaultdict(str)\n",
    "\n",
    "    if not os.path.exists(os.path.join(path, \"merged_cleaned_files\")):\n",
    "        \n",
    "        os.mkdir(os.path.join(path, \"merged_cleaned_files\"))\n",
    "\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        process_func = partial(_process_file, path, gene_name, main_protein_seq)\n",
    "        \n",
    "        results = executor.map(process_func, relevant_files)\n",
    "\n",
    "        for result in results:\n",
    "            \n",
    "            oligostates.update(result)\n",
    "\n",
    "    print(\"This is oligostates\")\n",
    "    \n",
    "    print(oligostates)\n",
    "    \n",
    "    return oligostates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e2d2370-64b9-42ee-8eb3-26299fbd4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_file(path, gene_name, main_protein_seq, files):\n",
    "    #helper function to split between nmr and xray / cryoem\n",
    "    try:\n",
    "        pdb1 = atomium.open(os.path.join(path, files))\n",
    "        \n",
    "        model_len = pdb1.models\n",
    "        \n",
    "        if len(model_len) > 5:\n",
    "\n",
    "            print(\"we go into _NMR_ensemble\")\n",
    "            return _NMR_ensemble(path=path, files=files, gene_name=gene_name)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            return {files[0:4]: _non_NMR_structures(path=path, files=files, gene_name=gene_name, main_protein_seq=main_protein_seq)}\n",
    "    \n",
    "    except Exception as error:\n",
    "        print(\"process file did not work\")\n",
    "        print(error)\n",
    "        \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66156b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biological_assemblies_atomium(path:str, gene_name:str, main_iso_seq:str,\n",
    "                                     main_protein_seq:str):\n",
    "\n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    relevant_files = [f for f in onlyfiles if f[-3:] == \"pdb\" or f[-3:] == \"cif\"]\n",
    "    relevant_files = [f for f in relevant_files if len(f) == 8]\n",
    "    \n",
    "    oligodict = dict([(\"1\",'monomer'),\n",
    "                 (\"2\",'dimer'),\n",
    "                 (\"3\",'trimer'),\n",
    "                 (\"4\",'tetramer'),\n",
    "                 (\"5\",'pentamer'),\n",
    "                 (\"6\",'hexamer'),\n",
    "                 (\"7\",'heptamer'),\n",
    "                 (\"8\",'oktamer'),\n",
    "                 (\"9\",'nonamer'),\n",
    "                 (\"10\",'decamer'),\n",
    "                 (\"11\",'undecamer'),\n",
    "                 (\"12\",'dodecamer'),\n",
    "                 (\"13\",'tridecamer'),\n",
    "                 (\"14\",'tetradecamer'),\n",
    "                 (\"15\",'pentadecamer'),\n",
    "                 (\"16\",'hexadecamer'),\n",
    "                 (\"17\",'heptadecamer'),\n",
    "                 (\"18\",'oktadecamer'),\n",
    "                 (\"19\",'nonadecamer'),\n",
    "                 (\"20\",'eicosamer')])\n",
    "             \n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\",\n",
    "              \"G\", \"H\", \"I\", \"J\", \"K\", \"L\",\n",
    "              \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n",
    "              \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "    \n",
    "    oligostates = defaultdict(str)  # initialize a dict that keeps track of oligostates.\n",
    "    \n",
    "    if os.path.exists(f\"{path}/merged_cleaned_files\") == False:\n",
    "        os.mkdir(f\"{path}/merged_cleaned_files\")\n",
    "    \n",
    "    for files in relevant_files:\n",
    "        \n",
    "        #lets try for each file (could be that some are empty and we require skip without breaking the loop)\n",
    "        try:\n",
    "\n",
    "            pdb1 = atomium.open(f'{path}/{files}')\n",
    "            \n",
    "            #if there are more models in there we have an NMR structure.\n",
    "            model_len = pdb1.models\n",
    "            \n",
    "            #this means we have an NMR ensemble.\n",
    "            if len(model_len) > 5:   #usually they are 20\n",
    "\n",
    "                oligo_nmr = _NMR_ensemble(path = path,\n",
    "                             files = files, gene_name=gene_name)\n",
    "                #merge them into the normal oligostates dict.\n",
    "                for keys, vals in oligo_nmr.items():\n",
    "                    #keys = pdb_code _ idx e.g 2ro8_10    2ro8_11 etc.\n",
    "                    oligostates[keys] = vals\n",
    "                \n",
    "                #continue needs to still be here.\n",
    "                continue\n",
    "                \n",
    "            #else means we have NO NMR structure.\n",
    "            else:\n",
    "                \n",
    "                oligostate = _non_NMR_structures(path=path,\n",
    "                                   files=files,\n",
    "                                   gene_name=gene_name,\n",
    "                                   main_protein_seq=main_protein_seq)\n",
    "                \n",
    "                oligostates[files[0:4]] = oligostate\n",
    "                \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "            # we still continue with the next file if we cant open e.g because its empty file.\n",
    "            continue\n",
    "    \n",
    "    #we return oligostates dict.\n",
    "    print(\"this is oligostates\")\n",
    "    print(oligostates)\n",
    "    return oligostates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a63c892-563e-4d09-9b6b-114c0c779106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for XRAY and CRYO-EM ensembles.\n",
    "def _non_NMR_structures(path:str,\n",
    "                        files:str,\n",
    "                        gene_name:str,\n",
    "                        main_protein_seq:str):\n",
    "    \n",
    "    \"\"\"This function takes in the the pdb file that is xray or cryoem and rechains each chain. \n",
    "    Additionally, we merge the new labelled chains into a merged_pdb file for further use.\"\"\"\n",
    "    \n",
    "    #open the pdb file\n",
    "    pdb1 = atomium.open(f'{path}/{files}')\n",
    "    \n",
    "    assemblies = [pdb1.generate_assembly(n + 1) for n in range(len(pdb1.assemblies))]\n",
    "    \n",
    "    #we take the first one(this is the biological unit built from the asymmetric unit)\n",
    "    assembly = assemblies[0]\n",
    "    \n",
    "    print(f'{path}/{files}')\n",
    "    print(assembly)\n",
    "    #this works for our purpose.\n",
    "    \n",
    "    seq_chains = []\n",
    "    \n",
    "    accepted_chains = []\n",
    "    \n",
    "    for x in assembly.chains():\n",
    "        \n",
    "        seq = x.sequence #the sequence of each chain\n",
    "        \n",
    "        seq_len = len(seq) #the len of each chain aka its number of residues\n",
    "        \n",
    "        chain_label = x.id #the chain identifier\n",
    "        \n",
    "        seq_chains.append((chain_label,seq_len)) \n",
    "    \n",
    "    \n",
    "    sorted_lens = sorted(seq_chains, key= lambda x: x[1], reverse=True) #reverse = true :largest first.\n",
    "\n",
    "    acc_range = []\n",
    "        \n",
    "    for chains, lens in sorted_lens:\n",
    "        #first is largest and always accepted\n",
    "        if len(accepted_chains) == 0 or lens > 0.8 * min(acc_range):\n",
    "            #then we continue to append only if its similar size to prevent small peptides from interfering.\n",
    "            accepted_chains.append(chains)\n",
    "            acc_range.append(lens)\n",
    "        \n",
    "    \n",
    "    print(f\"this is accepted chains: {accepted_chains}, and this is accepted ranges: {acc_range}\")\n",
    "\n",
    "    \"\"\"\n",
    "    if the chain contains more than 70% of the number of residues that the main fasta seq has we continue.\n",
    "    this part might become tricky if we have only partial structures of full length stuff available. might be\n",
    "    adjusted later.\n",
    "    \"\"\"\n",
    "   \n",
    "    oligostate = len(accepted_chains)  #this excludes small peptides ect from being mistaken as oligomers.\n",
    "            \n",
    "            \n",
    "    path_list = []\n",
    "            \n",
    "    seen_chains = []\n",
    "    \n",
    "    for idx, chains in enumerate(assembly.chains()):\n",
    "                \n",
    "        chain_label = chains.id\n",
    "        \n",
    "        if chain_label in accepted_chains:  #if its accepted we save it.\n",
    "            \n",
    "            print(chain_label, accepted_chains)\n",
    "            \n",
    "            print(f\"{path}/{files[0:4]}_{chain_label}_{idx}.pdb\")\n",
    "\n",
    "            chains.save(f\"{path}/{files[0:4]}_{chain_label}_{idx}.pdb\")\n",
    "        \n",
    "            path_to_pdb = f\"{path}/{files[0:4]}_{chain_label}_{idx}.pdb\"\n",
    "        \n",
    "            #gap_dict = check_gaps_single_struc(path_to_pdb)\n",
    "            \n",
    "            #repairability = repair_viability(gap_dict)\n",
    "            \n",
    "            #if repairability:\n",
    "            #    try:\n",
    "            #        #modeller block here.\n",
    "            #        repairing_residues(path=path, \n",
    "            #                       pdb_id_target=f\"{files[0:4]}_{chain_label}_{idx}\")\n",
    "            #        \n",
    "            #    except Exception as error:\n",
    "            #        print(\"the error occured in for idx, chains in enumerate(assembly.chains()):\")\n",
    "            #        print(error)\n",
    "            \n",
    "            print(\"we append now path\")\n",
    "            path_list.append(f\"{path}/{files[0:4]}_{chain_label}_{idx}.pdb\")\n",
    "            seen_chains.append(chain_label)\n",
    "            \n",
    "            path_list = sorted(path_list, key=lambda x: int(x[-5]))\n",
    "            #print(path_list)\n",
    "            unique_chains = list(set(seen_chains))\n",
    "\n",
    "            #for paths in path_list:\n",
    "\n",
    "            _merge_pdb_chains(path_list, pdb_name=files[0:4],\n",
    "                      gene_name=gene_name,\n",
    "                      seen_chains=seen_chains, \n",
    "                      unique_chains=unique_chains)\n",
    "    \n",
    "            \n",
    "        else: #we continue.\n",
    "            \"\"\"This means the part that is actually attached is not part of the protein but is instead a bound ligand!\n",
    "            if this ligand analysis becomes relevant in the future in the context of mutational investigations\n",
    "            we might adjust this part here.\"\"\"\n",
    "            continue\n",
    "    \n",
    "    #we return the oligostate of this file and merge it into dict as return value.\n",
    "    return oligostate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40fbc11d-1fc3-4e05-8e4f-5919229de5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN\"\n",
    "#files = \"4h1w.pdb\"\n",
    "#gene_name = \"serca\"\n",
    "\n",
    "\n",
    "#main_prot_seq = get_gene_fasta(\"O14983\")\n",
    "#_non_NMR_structures(path=path,\n",
    "#                        files=files,\n",
    "#                        gene_name=gene_name,\n",
    "#                        main_protein_seq=main_prot_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d27d57b-2d6f-422e-892b-62deeed4760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for NMR ensembles.\n",
    "def _NMR_ensemble(path:str, files:str, gene_name:str):\n",
    "\n",
    "    \n",
    "    \"\"\"This function takes in the NMR ensemble and splits each state into a respective PDB file.\"\"\"\n",
    "    \n",
    "    \n",
    "    #open the pdb file\n",
    "    pdb1 = atomium.open(f'{path}/{files}')\n",
    "    \n",
    "    assembly = pdb1.generate_assembly(1)\n",
    "    \n",
    "    \n",
    "    oligostates = defaultdict()\n",
    "    \n",
    "    for idx, model in enumerate(pdb1.models):\n",
    "                        \n",
    "        path_list = []\n",
    "        seen_chains = []\n",
    "        \n",
    "        #here we save the structure.\n",
    "        model.save(f\"{path}/{files[0:4]}_{idx+1}.pdb\")\n",
    "        \n",
    "        #we append path\n",
    "        path_list.append(f\"{path}/{files[0:4]}_{idx+1}.pdb\")\n",
    "        #we append seen chains.\n",
    "        for chains in assembly.chains():\n",
    "            \n",
    "            #here we grab the chain ID\n",
    "            chain_label = chains.id\n",
    "            fasta_seq = chains.sequence\n",
    "            \n",
    "            oligostates[f\"{files[0:4]}_{idx+1}\"] = len(assembly.chains())\n",
    "            \n",
    "            seen_chains.append(chain_label)\n",
    "        \n",
    "            unique_chains = list(set(seen_chains))\n",
    "            \n",
    "        #for paths in path_list:\n",
    "        # we treat it like its a normal monomer (since NMR is single chain most of the time)\n",
    "        _merge_pdb_chains(path_list, pdb_name=f\"{files[0:4]}_{idx+1}\", \n",
    "                          gene_name=gene_name,\n",
    "                          seen_chains=seen_chains,\n",
    "                          unique_chains=unique_chains)\n",
    "        \n",
    "        \n",
    "    return oligostates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd753b2c-3e2d-4753-b1d5-7e961e3cc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/test/laura_test\"\n",
    "#files = \"2ro8.pdb\"\n",
    "#gene_name = \"NAC3_HUMAN\"\n",
    "\n",
    "#oligostate = _NMR_ensemble(path=path, \n",
    "#             files=files,\n",
    "#             gene_name=gene_name)\n",
    "#\n",
    "#print(oligostate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89138c41-459d-448b-ba8c-3d10ccd099ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testblock for atomium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af6f232e-b6ba-4f5a-a9f9-58c0c5623ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for get_biological_assemblies_atomium\n",
    "\n",
    "#gene_name = \"CALM1_HUMAN\"\n",
    "#path = \"/home/micnag/bioinformatics/test/atomium_CALM1_testset\"\n",
    "\n",
    "#main_iso_seq = None\n",
    "\n",
    "#print(get_biological_assemblies_atomium(gene_name=gene_name,\n",
    "#                                  path=path,\n",
    "#                                 main_iso_seq=main_iso_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a92c9994-9068-4c9c-a0cd-1634c97899b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hetero_atoms_1(pdb_file:str, path:str):\n",
    "    \"\"\"This function will grab the correct chains and overwrite the pdbs with only correct chains.\"\"\"\n",
    "    \n",
    "    #used to get rid of hetero atoms and wrong chain.\n",
    "    #inherit from Select, pass additional arg to __init__: correct_chain id\n",
    "    class NonHetAndCorrectChainSelect(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "            \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[0] == \" \" else 0\n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    prot_name = pdb_file[0:4]\n",
    "    full_path = f\"{path}/{pdb_file}\"\n",
    "    \n",
    "    structure = parser.get_structure(prot_name, full_path)\n",
    "    \n",
    "    non_canonical_aas = defaultdict()\n",
    "\n",
    "    canonical_aas = {'VAL', 'ILE', 'LEU', 'GLU', 'GLN',\n",
    "                     'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR',\n",
    "                     'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA',\n",
    "                     'GLY', 'PRO', 'CYS'}\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                curr_res = residue.get_resname()\n",
    "                curr_pos = residue.get_id()[1]\n",
    "\n",
    "                if curr_res not in canonical_aas:\n",
    "                    print(curr_res)\n",
    "                    non_canonical_aas[curr_pos] = (curr_res, chain.get_id())\n",
    "    \n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    \n",
    "    save_path = full_path\n",
    "    \n",
    "    io.save(save_path, NonHetAndCorrectChainSelect())\n",
    "    \n",
    "    return non_canonical_aas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c78b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hetero_atoms(pdb_file:str, path:str):\n",
    "    \"\"\"This function will grab the correct chains and overwrite the pdbs with only correct chains.\"\"\"\n",
    "    \n",
    "    #used to get rid of hetero atoms and wrong chain.\n",
    "    #inherit from Select, pass additional arg to __init__: correct_chain id\n",
    "    class NonHet_and_correct_chain_Select(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "            \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[0] == \" \" else 0\n",
    "        \n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"{pdb_file[0:4]}\"\n",
    "    \n",
    "    fullpath = f\"{path}/{pdb_file}\"\n",
    "    \n",
    "    \n",
    "    print(\"we are here\")\n",
    "    print(fullpath)\n",
    "    structure = parser.get_structure(prot_name, fullpath)\n",
    "    \n",
    "    \n",
    "    non_canoncial_aas = defaultdict()\n",
    "    \n",
    "    \n",
    "    canonical_aas = ['VAL', 'ILE', 'LEU', 'GLU', 'GLN' ,\n",
    "                    'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR', \n",
    "                    'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA', \n",
    "                    'GLY', 'PRO', 'CYS']\n",
    "    \n",
    "    \n",
    "    for models in structure:\n",
    "        for chains in models:\n",
    "            \n",
    "            chain = chains.get_id()\n",
    "            \n",
    "            for residues in chains:\n",
    "                curr_res = residues.get_resname()\n",
    "                curr_pos = residues.get_id()[1] #the position\n",
    "                \n",
    "                \n",
    "                if curr_res not in canonical_aas:\n",
    "                    print(curr_res)\n",
    "                    non_canoncial_aas[curr_pos] = (curr_res, chain)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #io object to save structure object to file\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    \n",
    "    savepath = fullpath\n",
    "    \n",
    "    #we save all structures to the monomeric category.\n",
    "    io.save(savepath, NonHet_and_correct_chain_Select())\n",
    "    \n",
    "    return non_canoncial_aas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98605370-720a-4135-8f03-7b25d7d66285",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Function**:\n",
    "<b><span style=\"color:orange\">_merge_pdb_chains:</span></b>\n",
    "\n",
    "*Input:*\n",
    "1. <span style=\"color:green\">path_list:str</span> \n",
    "2. <span style=\"color:green\">pdb_name:str</span>\n",
    "3. <span style=\"color:green\">gene_name:str</span>\n",
    "4. <span style=\"color:green\">seen_chains:str</span>\n",
    "5. <span style=\"color:green\">unique_chains:str</span>\n",
    "\n",
    "<span style=\"color:green\"></span>\n",
    "\n",
    "calls internally following helper functions:\n",
    "\n",
    "+ <span style=\"color:blue\">_homomer_check</span>\n",
    "+ <span style=\"color:blue\">_pure_oligomer_rechaining</span>\n",
    "+ <span style=\"color:blue\">_mixed_oligomer_rechaining</span>\n",
    "+ <span style=\"color:blue\">_monomeric_rechaining</span>\n",
    "\n",
    "\n",
    "requires additionally:\n",
    "\n",
    "+ <span style=\"color:red\">pdb_merge.py</span> \n",
    "+ <span style=\"color:red\">pdb_tidy.py</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a131aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_pdb_chains(path_list:str, pdb_name:str, gene_name:str,\n",
    "                     seen_chains:str, unique_chains:str):\n",
    "    \n",
    "    #needs to be adjusted how to handle hetero-mers.\n",
    "    outfile = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{gene_name}\"\n",
    "    #for testing purpose we put this here : return\n",
    "    \n",
    "    letterdict = {1 : \"A\", 2 : \"B\", 3 : \"C\", 4 : \"D\",\n",
    "                 5 : \"E\", 6 : \"F\",7 : \"G\", 8 : \"H\",\n",
    "                 9 : \"I\", 10 : \"J\",11 : \"K\", 12 : \"L\",\n",
    "                 13 : \"M\", 14 : \"N\",15 : \"O\", 16 : \"P\",\n",
    "                 17 : \"Q\", 18 : \"R\",19 : \"S\", 20 : \"T\",\n",
    "                 21 : \"U\", 22 : \"V\",23 : \"W\", 24 : \"X\",\n",
    "                 25 : \"Y\", 26 : \"Z\" }\n",
    "    \n",
    "    #that means we have duplicates.\n",
    "    \n",
    "    #oligomeric heterodimer case first!\n",
    "    \n",
    "    #default setting\n",
    "    \n",
    "    oligohomomer = False\n",
    "    print(f\"this is pathlist : {path_list}, and this is pdb name: {pdb_name}\")\n",
    "    \n",
    "    if len(seen_chains) != len(unique_chains):\n",
    "        #means its an oligomer.\n",
    "        oligomer = True\n",
    "        #now check if homo or hetero oligomer.\n",
    "        oligohomomer = _homomer_check(seen_chains, unique_chains)\n",
    "    else:\n",
    "        oligomer = False\n",
    "        \n",
    "    #means its an oligomer and a homo oligomer.\n",
    "    if oligohomomer:\n",
    "        \n",
    "        new_chains = _pure_oligomer_rechaining(seen_chains=seen_chains,\n",
    "                                   unique_chains=unique_chains,\n",
    "                                   path_list=path_list,\n",
    "                                   letterdict=letterdict)\n",
    "    \n",
    "    #means its an oligomer but a hetero oligomer.\n",
    "    if oligohomomer == False and oligomer:\n",
    "        \n",
    "        new_chains = _mixed_oligomer_rechaining(seen_chains=seen_chains,\n",
    "                                   unique_chains=unique_chains,\n",
    "                                   path_list=path_list,\n",
    "                                   letterdict=letterdict)\n",
    "    \n",
    "    #means its just either a single chain of the protein or some co crystallized binding complex protein as other chains.\n",
    "    if oligomer == False:\n",
    "\n",
    "        new_chains = _monomeric_rechaining(seen_chains=seen_chains,\n",
    "                                   unique_chains=unique_chains,\n",
    "                                   path_list=path_list,\n",
    "                                   letterdict=letterdict)\n",
    "     \n",
    "    #to be continued\n",
    "    \n",
    "    \n",
    "    #print(\"this is path list and chains\")\n",
    "    #print(path_list)\n",
    "    #print(seen_chains)\n",
    "    #print(new_chains)\n",
    "    \n",
    "    pdb_paths = \"\"\n",
    "    \n",
    "    for entries in path_list:\n",
    "        pdb_paths += f\"{entries} \"\n",
    "    \n",
    "    #print(\"this is pdb_paths\")\n",
    "    \n",
    "    #print(pdb_paths)\n",
    "    \n",
    "    print(f\"we start with bash_merge and save it at: {outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\")\n",
    "    \n",
    "    bash_merge = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_merge.py {pdb_paths}\"\n",
    "        \n",
    "    bash_curl_cmd_rdy = bash_merge.split()\n",
    "    \n",
    "    bash_tidy_cmd = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_tidy.py {outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\"\n",
    "    \n",
    "    bash_tidy_cmd_rdy = bash_tidy_cmd.split()\n",
    "    \n",
    "    #print(\"this is the merge command\")\n",
    "    \n",
    "    #print(bash_curl_cmd_rdy)\n",
    "    \n",
    "    with open(f\"{outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\", \"w\") as fh_out:\n",
    "        result_pdbs = run(bash_curl_cmd_rdy, stdout=fh_out, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "        \n",
    "    \n",
    "    #print(\"this is the tidy command thats breaking the merge.\")\n",
    "    #print(bash_tidy_cmd_rdy)\n",
    "    \n",
    "    with open(f\"{outfile}/merged_cleaned_files/{pdb_name}.pdb\", \"w\") as fh_out2:\n",
    "        results_tidy = run(bash_tidy_cmd_rdy, stdout=fh_out2, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "    \n",
    "    #we remove tmp intermediate files.\n",
    "    #os.remove(f\"{outfile}/merged_cleaned_files/{pdb_name}_merged.pdb\")\n",
    "    \n",
    "    with open(f\"{outfile}/reports/chain_relabeling_protocol.csv\", \"a\") as fh_chain:\n",
    "        outtext = f'{pdb_name},{\" \".join(sorted(seen_chains, reverse=False))},{\" \".join(new_chains)}'\n",
    "        fh_chain.write(outtext)\n",
    "        fh_chain.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39e91712-7dd5-4b19-bf1e-966fdd6a2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _homomer_check(seen_chains:list, unique_chains:list):\n",
    "    \n",
    "    # ABCDEF    set: ABCDEF  vs unique:ABCDEF\n",
    "    \n",
    "    # AA   set : A vs unique A\n",
    "    \n",
    "    #if they both are the same set but the seen chain len is larger e.g [A A] vs [A] but as set\n",
    "    #they will both be [A] and [A] -> Means its a homodimer.\n",
    "    \n",
    "    \n",
    "    \n",
    "    #this means they differ and now it could be either a mixed or a homo oligomer.\n",
    "    #if len(unique chains == 1) this means the assymetric unit consists of 1 chain. and the\n",
    "    #biological unit is simply generated through means of symmetry operations.\n",
    "    \n",
    "    if len(seen_chains) != len(unique_chains) and len(unique_chains) == 1:\n",
    "        return True\n",
    "    \n",
    "    #this case covers the mixed oligomers. nevertheless they are oligomers!\n",
    "    if len(seen_chains) != len(unique_chains) and len(unique_chains) != 1:\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c285ea6f-9ddf-4ed6-82ee-dbbca143297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pure_oligomer_rechaining(seen_chains:list,\n",
    "                                 unique_chains:list,\n",
    "                                 path_list:list,\n",
    "                                 letterdict:dict):    \n",
    "    \n",
    "    \n",
    "    seen_chains = sorted(seen_chains, reverse=False)\n",
    "\n",
    "    merged_chain_paths = zip(seen_chains, path_list)\n",
    "    \n",
    "    for idx, (chains, path_to_pdb) in enumerate(merged_chain_paths):        \n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "                \n",
    "        prot_name = f\"default\"\n",
    "                \n",
    "        #open the correct pdb and rechain it.\n",
    "        structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "        \n",
    "        new_chain = letterdict[idx+1]\n",
    "        \n",
    "        new_chain_seq = []\n",
    "        \n",
    "        for models in structure_template:\n",
    "            for chains in models:\n",
    "                \n",
    "                chains.id = \"_\"\n",
    "                \n",
    "                chains.id = new_chain\n",
    "                \n",
    "                new_chain_seq.append(new_chain)\n",
    "                \n",
    "                io = PDBIO()\n",
    "                \n",
    "                io.set_structure(structure_template)\n",
    "                \n",
    "                io.save(path_to_pdb)\n",
    "                \n",
    "    return new_chain_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a46a1ea-e77b-4ad9-a09a-a5ff94a0eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mixed_oligomer_rechaining(seen_chains:list,\n",
    "                               unique_chains:list,\n",
    "                               path_list:list,\n",
    "                               letterdict:dict):\n",
    "    \n",
    "    seen_chains = sorted(seen_chains, reverse=False)\n",
    "\n",
    "    merged_chain_paths = zip(seen_chains, path_list)\n",
    "    \n",
    "    chain_seq_len = len(seen_chains) #e.g 6\n",
    "    \n",
    "    shift = len(unique_chains) # e.g 3\n",
    "    \n",
    "    blocksize = int(chain_seq_len / shift) # e.g 2\n",
    "    \n",
    "    block_count = int(chain_seq_len/blocksize)\n",
    "    \n",
    "    # A A B B C C becomes A D B E C F\n",
    "    \n",
    "    # B B C C becomes A C B D \n",
    "    \n",
    "    #i = 1\n",
    "    \n",
    "    # A D \n",
    "    \n",
    "    # block 1 2 3 for A A B B C C \n",
    "    \n",
    "    # 0 2 1 3\n",
    "    \n",
    "    j = 0\n",
    "    new_chain_seq = []\n",
    "    for blocks in range(1, block_count+1):\n",
    "        # each block has 2 members:\n",
    "        #first iteration: 1 \n",
    "        # second iteration: 2\n",
    "        # third iteration: 3\n",
    "        for i in range(0, blocksize):\n",
    "            # first iteration A D\n",
    "            # second iteration B E\n",
    "            # third iteration C F\n",
    "            new_chain = letterdict[blocks+i*shift]\n",
    "            new_chain_seq.append(new_chain)\n",
    "            path_to_pdb = path_list[j]\n",
    "            #continue in path\n",
    "            j += 1\n",
    "            \n",
    "            #lets renumber the structure now.\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            \n",
    "            prot_name = f\"default\"\n",
    "            \n",
    "            #open the correct pdb and rechain it.\n",
    "            structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "            \n",
    "            for models in structure_template:\n",
    "                for chains in models:\n",
    "                    chains.id = \"_\"\n",
    "                    chains.id = new_chain\n",
    "            \n",
    "            io = PDBIO()\n",
    "            \n",
    "            io.set_structure(structure_template)\n",
    "            io.save(path_to_pdb)\n",
    "\n",
    "    return new_chain_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b1705f5-58e9-4c49-b227-02b316829b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _monomeric_rechaining(seen_chains:list,\n",
    "                          unique_chains:list,\n",
    "                          path_list:list,\n",
    "                          letterdict:dict):\n",
    "\n",
    "    print(\"we start monomeric rechain inside\")\n",
    "    new_chain_seq = []\n",
    "\n",
    "    for idx, (original_chains, path_to_pdb) in enumerate(zip(seen_chains, path_list)):\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        \n",
    "        prot_name = \"default\"\n",
    "\n",
    "        # Open the correct PDB and rechain it.\n",
    "        structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "\n",
    "        # Get the new chain ID\n",
    "        new_chain = letterdict[idx + 1]\n",
    "\n",
    "        for model in structure_template:\n",
    "            for original_chain in model:\n",
    "                print(\"Original chain ID:\", original_chain.id)\n",
    "                original_chain.id = \"_\"\n",
    "                original_chain.id = new_chain\n",
    "                print(\"New chain ID:\", original_chain.id)\n",
    "\n",
    "        # Save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure_template)\n",
    "\n",
    "        print(\"we save now:\")\n",
    "        io.save(path_to_pdb)\n",
    "\n",
    "        new_chain_seq.append(new_chain)\n",
    "\n",
    "    return new_chain_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8246c-5000-49f1-bff0-c2a266cd02da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# prepares references used for USAlign later downstream.\n",
    "\n",
    "### Major Subfunction.\n",
    "\n",
    "*Input:*\n",
    "\n",
    "1. <span style=\"color:green\">main_gene_name:str</span>\n",
    "2. <span style=\"color:green\">templates:list</span>\n",
    "3. <span style=\"color:green\">seq_sim:list</span>     \n",
    "4. <span style=\"color:green\">query_start:list</span>                      \n",
    "5. <span style=\"color:green\">query_end:list</span>\n",
    "6. <span style=\"color:green\">temp_start:list</span>                      \n",
    "7. <span style=\"color:green\">temp_end:list</span>                    \n",
    "8. <span style=\"color:green\">path:str</span>\n",
    "9. <span style=\"color:green\">oligodict:list</span>\n",
    "\n",
    "*Output*:\n",
    "\n",
    "<b><span style=\"color:brown\">template_dict:dir</span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3373d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_references(main_gene_name:str,\n",
    "                      templates:list,\n",
    "                      seq_sim:list,\n",
    "                      query_start:list,\n",
    "                      query_end:list,\n",
    "                      temp_start:list,\n",
    "                      temp_end:list,\n",
    "                      path:str, oligodict:list,\n",
    "                      custom_template=False):\n",
    "    \n",
    "    \"\"\"Make function out of making templates and reference structures.\"\"\"\n",
    "    #setup\n",
    "    \n",
    "    print(\"we start prepare templates!!\")\n",
    "    \n",
    "    #this retrieves the main len of the protein we are interested in\n",
    "    main_target_seq_len = get_seq_len(path=f\"{path}\", rcsb_id=\"None\", template=True)\n",
    "    #181 for NUD4B\n",
    "    \n",
    "    #gives back the length of each template.\n",
    "    temp_lengths = [f[0] - f[1] for f in zip(temp_end, temp_start)]\n",
    "    \n",
    "    homology_list = sorted(list(zip(templates,seq_sim, temp_lengths)), key= lambda x: x[1], reverse=True)\n",
    "    \n",
    "    homology_dict = defaultdict(tuple)\n",
    "    \n",
    "    #homology dict will be used later to fetch corresponding templates for each oligomer and each domain.\n",
    "    for entries in homology_list:\n",
    "        homology_dict[entries[0][0:4]] = (entries[1], entries[2])\n",
    "    \n",
    "    \n",
    "    acceptable_dirnames = [\"monomer\",\n",
    "                           \"dimer\",\n",
    "                           \"trimer\",\n",
    "                           \"tetramer\",\n",
    "                           \"pentamer\",\n",
    "                           \"hexamer\",\n",
    "                           \"heptamer\",\n",
    "                           \"oktamer\",\n",
    "                           \"nonamer\",\n",
    "                           \"decamer\",\n",
    "                           \"undecamer\",\n",
    "                           \"dodecamer\",\n",
    "                           \"tridecamer\",\n",
    "                           \"tetradecamer\",\n",
    "                           \"pentadecamer\",\n",
    "                           \"hexadecamer\",\n",
    "                           \"heptadecamer\",\n",
    "                           \"oktadecamer\",\n",
    "                           \"nonadecamer\",\n",
    "                           \"eicosamer\"\n",
    "    ]\n",
    "    \n",
    "    intervaldirs = [ f for f in os.scandir(path) if f.is_dir() and f.name in acceptable_dirnames]\n",
    "    \n",
    "    intervalpathdict = defaultdict(list)\n",
    "    \n",
    "    for subentities in intervaldirs:\n",
    "        intervals = [ f for f in os.scandir(subentities.path) if f.is_dir()]\n",
    "        for interv in intervals:\n",
    "            onlyfiles = [f for f in os.listdir(interv.path) if os.path.isfile(os.path.join(interv.path, f))]\n",
    "            if len(onlyfiles) != 0:\n",
    "                intervalpathdict[interv.path] = onlyfiles\n",
    "                \n",
    "    template_dict = defaultdict(str)\n",
    "    \n",
    "    #print(\"this is intervalpathdict\")\n",
    "    #print(intervalpathdict)\n",
    "    \n",
    "    \n",
    "    for keys, vals in intervalpathdict.items():\n",
    "        \n",
    "        \n",
    "        fin_dir_list = keys.split(\"/\")\n",
    "        \n",
    "        #save position dir\n",
    "        position_dir = fin_dir_list[-1]\n",
    "        #and oligostate in order to write report later on.\n",
    "        oligostate_dir = fin_dir_list[-2]\n",
    "        \n",
    "        ref_list = []\n",
    "        #here we store human hits.\n",
    "        priority_list = []\n",
    "        #if we retrieve not as many hits, we take what we get\n",
    "        \n",
    "        if len(vals) < 100:\n",
    "                \n",
    "            max_end = len(vals)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            max_end = 100\n",
    "        \n",
    "        \n",
    "        print(f\"pos dir: {position_dir}, oligostate: {oligostate_dir}, val: {vals}\")\n",
    "        \n",
    "        exact_hits = []\n",
    "    \n",
    "        #group 2: other species same gene.  e.g KRS1_DROSOPHILA instead of KRS1_HUMAN\n",
    "        other_species_same_gene = []\n",
    "    \n",
    "        #group 3: other HUMAN but not exact gene. KRS2_HUMAN instead of KRS1\n",
    "        other_human_similar_gene = []\n",
    "    \n",
    "        #we dont find anything from the above.\n",
    "        suboptimal_templates = []\n",
    "        \n",
    "        for val in vals:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                prot_check = val[0:4] #pdb code without chain id\n",
    "            \n",
    "                prot_name = get_gene_name(prot_check)\n",
    "            \n",
    "                seq, lengths = homology_dict[prot_check][0], homology_dict[prot_check][1] \n",
    "            \n",
    "                #print(seq, lengths)\n",
    "                \n",
    "                #this means its human and our main prot is always human. but this should be extended.\n",
    "                #sometimes its better to have e.g MOUSE protein instead another less related human protein!\n",
    "                \n",
    "                \n",
    "                \n",
    "                #case 1\n",
    "                if prot_name == main_gene_name:\n",
    "                    temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                                                \n",
    "                    exact_hits.append((val, prot_name, temp_score_val))\n",
    "                    continue\n",
    "                \n",
    "                #case 2\n",
    "                if prot_name[:-6] == main_gene_name[:-6]:\n",
    "                    temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                                                \n",
    "                    other_species_same_gene.append((val, prot_name, temp_score_val))\n",
    "                    continue\n",
    "                \n",
    "                #case 3\n",
    "                if prot_name[-5:] == \"HUMAN\":\n",
    "                    temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                                                \n",
    "                    other_human_similar_gene.append((val, prot_name, temp_score_val))\n",
    "                    continue\n",
    "                \n",
    "                #if its none of the above.\n",
    "                temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                                seq_len=lengths,\n",
    "                                                main_seq_len=main_target_seq_len)\n",
    "                \n",
    "                suboptimal_templates.append((val, prot_name, temp_score_val))\n",
    "                \n",
    "            except Exception as error: \n",
    "                print(error)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #case 0 : custom_template = True which means we want a search based on a specific pdb template.\n",
    "        \n",
    "        pdb_shorts = [x[0:4] for x in vals]  #just interested in 4 letter pdb codes.\n",
    "        \n",
    "        if custom_template in pdb_shorts:\n",
    "            #we check if submitted struc is available at the current oligomer level.\n",
    "            try:\n",
    "                \n",
    "                prot_check = custom_template[0:4] #only pdb code without chain id\n",
    "                prot_name = get_gene_name(custom_template)\n",
    "                \n",
    "                #if we have it in the dict its fine. (it should be there if we did a proper seq search beforehand)\n",
    "                seq, lengths = homology_dict[prot_check][0], homology_dict[prot_check][1] \n",
    "                \n",
    "                #we calculate temp score \n",
    "                temp_score_val = temp_score(seq_sim=float(seq),\n",
    "                                        seq_len=lengths,\n",
    "                                        main_seq_len=main_target_seq_len)\n",
    "                \n",
    "                \n",
    "                #we count it as a perfect hit.\n",
    "                exact_hits.append((f\"{custom_template[0:4]}.pdb\", prot_name, 0))\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(exact_hits) != 0:\n",
    "            \n",
    "            exact_sorted = sorted(exact_hits, key= lambda x: x[2])\n",
    "            print(\"this is exact sorted\")\n",
    "            print(exact_sorted)\n",
    "            \n",
    "            with open(f\"{path}/mutational_mapping/{oligostate_dir}_{position_dir}.csv\", \"w\") as fh_out:\n",
    "                #we take it from here\n",
    "                #we only store the first potential structure for each range and oligomeric state!\n",
    "                previous_path = f\"{path}/{oligostate_dir}/{position_dir}/{exact_hits[0][0]}\"\n",
    "                #and store it here.\n",
    "                savepath = f\"{path}/mutational_mapping/{oligostate_dir}_{position_dir}_{exact_hits[0][0]}\"\n",
    "                shutil.copy(previous_path, savepath)\n",
    "                fh_out.write(exact_hits[0][0])\n",
    "                fh_out.write(\",\")\n",
    "                fh_out.write(exact_hits[0][1])\n",
    "                fh_out.write(\"\\n\")\n",
    "        \n",
    "        #print(exact_hits)\n",
    "        #print(other_species_same_gene)\n",
    "        #print(other_human_similar_gene)\n",
    "        #print(suboptimal_templates)\n",
    "        \n",
    "        \n",
    "        pos_oligomer = f'{oligostate_dir}/{position_dir}'\n",
    "        template_dict[pos_oligomer] = [exact_hits, other_species_same_gene, other_human_similar_gene,\n",
    "                                      suboptimal_templates]\n",
    "\n",
    "    #for keys, vals in template_dict.items():\n",
    "    #    print(keys)\n",
    "    #    print(vals)\n",
    "    #    print(\"\\n\")\n",
    "    \n",
    "    print(\"template dict is\")\n",
    "    print(template_dict)\n",
    "    \n",
    "    return template_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88b1b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_len(path:str, rcsb_id:str, template=False):\n",
    "    \n",
    "    seq_len = 0\n",
    "\n",
    "    target = f\"{path}/reports/{rcsb_id}.fasta\"\n",
    "    \n",
    "    if template:\n",
    "        target = f\"{path}/reports/main_isoform_fasta\"\n",
    "    \n",
    "    try:\n",
    "        with open(target, \"r\") as fasta_file:\n",
    "            for lines in fasta_file:\n",
    "                if lines[0] != \">\":\n",
    "                    lines = lines.replace(\"\\n\",\"\") #get rid of newline char\n",
    "                    seq_len += len(lines) #add length of line to seq    \n",
    "    except:\n",
    "        #it did not work.\n",
    "        return 0 \n",
    "    \n",
    "    return seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37e6dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_name(uniprot_id:str):\n",
    "    \n",
    "    fields = \"id\"\n",
    "    \n",
    "    URL = f\"https://rest.uniprot.org/uniprotkb/search?format=tsv&fields={fields}&query={uniprot_id}\"\n",
    "    resp = get_url(URL)\n",
    "    resp = resp.text\n",
    "    resp = resp.split(\"\\n\")\n",
    "    return resp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52f4011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_score(seq_sim:float, seq_len:float, main_seq_len:float):\n",
    "    \n",
    "    \n",
    "    #seq_similarity * sequence len\n",
    "    tmp = (seq_sim*seq_len)\n",
    "    \n",
    "    #to prevent sqrt 0 for ideal matches.\n",
    "    eps = 1e-10\n",
    "    \n",
    "    #minimum squared diff between tmp and main_seq len is best\n",
    "    temp_score = math.sqrt((tmp - main_seq_len+eps)**2)\n",
    "    \n",
    "    return temp_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486406ae-0c7d-4a2e-84e3-e982276e5b03",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SEQ shift compared to UNIPROT + renumber whole structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd4008-c178-4edf-8a5e-1b8de6da3b89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### function *get_shifts*:\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">get_shift.sh</span>\n",
    "\n",
    "Output:\n",
    "\n",
    "<span style=\"color:brown\">shift_dict</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7939b47-38ac-44bd-b241-93005f7b3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shift(pdb, link_path):\n",
    "    shift_dict = defaultdict()\n",
    "\n",
    "    searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "    resp = get_url(searchp)\n",
    "    resp = resp.json()\n",
    "\n",
    "    for pdb_id, pdb_info in resp.items():\n",
    "        for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "            for mapping in uniprot_info['mappings']:\n",
    "                chain_id = mapping['chain_id']\n",
    "                unp_start = mapping['unp_start']\n",
    "                unp_end = mapping['unp_end']\n",
    "                \n",
    "                author_start = mapping['start']['author_residue_number']\n",
    "                author_end = mapping['end']['author_residue_number']\n",
    "\n",
    "                if author_start is None:\n",
    "                    author_start = unp_start\n",
    "                if author_end is None:\n",
    "                    author_end = unp_end\n",
    "\n",
    "                shift_start = unp_start - author_start\n",
    "                shift_end = unp_end - author_end\n",
    "\n",
    "                shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "\n",
    "    return shift_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f99b2994-5aa2-4481-ad03-1aedcb066e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_shift_calculation(pdbfolder):\n",
    "    onlyfiles = [f for f in os.listdir(pdbfolder) if os.path.isfile(os.path.join(pdbfolder, f))]\n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "\n",
    "    link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "    shift_dict = defaultdict()\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Using partial to create a function with fixed parameters (link_path)\n",
    "        calculate_shift_partial = partial(calculate_shift, link_path=link_path)\n",
    "        # Map the calculation function to each pdb in parallel\n",
    "        results = executor.map(calculate_shift_partial, pdbs)\n",
    "\n",
    "        # Combine the results\n",
    "        for result in results:\n",
    "            for keys, vals in result.items():\n",
    "                shift_dict[keys] = vals\n",
    "\n",
    "    # Print or write the results\n",
    "    for keys, vals in shift_dict.items():\n",
    "        print(keys, vals)\n",
    "\n",
    "    with open(f\"{pdbfolder}/shifts/shiftdict.txt\", \"w\") as shift_out:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            shift_out.write(keys)\n",
    "            shift_out.write(str(vals))\n",
    "            shift_out.write(\"\\n\")\n",
    "\n",
    "    return shift_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c08286-dd8b-4a5e-a849-7cb71d21eef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8435b465-5ba3-4124-a516-642a0e1745eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_shifts(pdbfolder:str):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(pdbfolder) if os.path.isfile(os.path.join(pdbfolder, f))]\n",
    "    \n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "\n",
    "\n",
    "    link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "    #we will fetch the results through request.\n",
    "    shift_dict = defaultdict()\n",
    "    \n",
    "    for pdb in pdbs:\n",
    "        searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "        #print(searchp)\n",
    "        resp = get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        \n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                for mapping in uniprot_info['mappings']:\n",
    "                    chain_id = mapping['chain_id']\n",
    "                    unp_start = mapping['unp_start']\n",
    "                    unp_end = mapping['unp_end']\n",
    "                    #print(chain_id)\n",
    "                    #print(unp_start)\n",
    "                    #print(unp_end)\n",
    "                    author_start = mapping['start']['author_residue_number']\n",
    "                    author_end = mapping['end']['author_residue_number']\n",
    "\n",
    "                    #print(author_start)\n",
    "                    #print(author_end)\n",
    "\n",
    "                    if author_start == None:\n",
    "                        author_start = unp_start\n",
    "                    if author_end == None:\n",
    "                        author_end = unp_end\n",
    "\n",
    "                    shift_start = unp_start - author_start\n",
    "                    shift_end = unp_end - author_end\n",
    "\n",
    "                    \n",
    "                    shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "\n",
    "    for keys, vals in shift_dict.items():\n",
    "        print(keys, vals)\n",
    "\n",
    "    with open(f\"{pdbfolder}/shifts/shiftdict.txt\", \"w\") as shift_out:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            shift_out.write(keys)\n",
    "            shift_out.write(str(vals))\n",
    "            shift_out.write(\"\\n\")\n",
    "    \n",
    "    return shift_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37ddf263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shifts(pdbfolder:str):\n",
    "    \n",
    "    savefolder = f\"{pdbfolder}/shifts\"\n",
    "    basepathlst = pdbfolder.split(\"/\")\n",
    "    #print(basepathlst)\n",
    "    \n",
    "    basepath = \"/\".join(basepathlst[1:5])\n",
    "    #print(basepath)\n",
    "    \n",
    "    os.chdir(f\"/{basepath}\")\n",
    "    \n",
    "    bash_correct_shift = f\"./get_shift.sh {pdbfolder} {savefolder}\"\n",
    "    \n",
    "    #print(bash_correct_shift)\n",
    "            \n",
    "    bash_command = bash_correct_shift.split()\n",
    "    \n",
    "    result = run(bash_command, stdout=PIPE, stderr=PIPE,\n",
    "                universal_newlines=True)\n",
    "\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "    #print(result.stdout)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        shift_dict = defaultdict()\n",
    "        \n",
    "        with open(f\"{pdbfolder}/shift_dict.txt\", \"r\") as shiftdic_fh:\n",
    "            for lines in shiftdic_fh:\n",
    "                key, val = lines.replace(\"\\n\",\"\").split(\":\")\n",
    "                shift_dict[key] = val\n",
    "                \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #write a report file as well\n",
    "    with open(f\"{pdbfolder}/reports/shift_reports.txt\", \"w\") as shiftout:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            shiftout.write(keys)\n",
    "            shiftout.write(\"\\t\")\n",
    "            shiftout.write(vals)\n",
    "            shiftout.write(\"\\n\")\n",
    "        \n",
    "    return shift_dict   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7650a-376e-4141-8724-3f96ad14ca38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### function *renumber_whole_structures*:\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">pdb_shiftres_by_chain.py</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc9fff5f-23a8-4313-8715-7df862b267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_structure(files, shift_dict, path):\n",
    "    \n",
    "    for keys, vals in shift_dict.items():\n",
    "        \n",
    "        if files == keys[0:4] and vals != str(0):\n",
    "            \n",
    "            chain = keys[-1]\n",
    "            shift = int(vals)\n",
    "\n",
    "            filepath = f\"{path}/{files}.pdb\"\n",
    "\n",
    "            # Should we really shift by shift + 1??? or just shift?\n",
    "            bash_cmd = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "\n",
    "            bash_cmd_rdy = bash_cmd.split()\n",
    "            \n",
    "            with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "            # Now replace the original one with the temp file.\n",
    "            os.replace(f\"{filepath}_tmp\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b56ce4e4-cbfe-4c58-a18a-785248e70340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_renumbering(shift_dict, path):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    relevant_files = [f[0:4] for f in onlyfiles if f[-3:] == \"pdb\" or f[-3:] == \"cif\"]\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Using partial to create a function with fixed parameters (shift_dict, path)\n",
    "        renumber_structure_partial = partial(renumber_structure, shift_dict=shift_dict, path=path)\n",
    "        # Map the renumbering function to each relevant file in parallel\n",
    "        executor.map(renumber_structure_partial, relevant_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc1d4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_whole_structures(shift_dict:dict, path:str):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    relevant_files = [f[0:4] for f in onlyfiles if f[-3:] == \"pdb\" or f[-3:] == \"cif\"]\n",
    "    \n",
    "    #print(shift_dict)\n",
    "    for files in relevant_files:\n",
    "        for keys, vals in shift_dict.items():\n",
    "            #print(files, keys[0:4])\n",
    "            if files == keys[0:4] and vals != str(0):\n",
    "                \n",
    "                chain = keys[-1]\n",
    "                shift = int(vals)\n",
    "                \n",
    "                filepath = f\"{path}/{files}.pdb\"\n",
    "                \n",
    "                #should we really shift by shift + 1??? or just shift?\n",
    "                \n",
    "                bash_cmd = f\"python /home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "                \n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "                \n",
    "                with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                    result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                         universal_newlines=True)\n",
    "                \n",
    "                #now replace the original one with the temp file.\n",
    "                os.replace(f\"{filepath}_tmp\", filepath)\n",
    "                            \n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93807ef0-63f0-497d-9789-be855dc652e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/\"\n",
    "\n",
    "#shift_dict = get_shifts(pdbfolder=path)\n",
    "\n",
    "#renumber_repaired_single_chain(shift_dict=shift_dict, path = path, pdb_name=\"7nnj_B_0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c803330-9a48-43f1-a9c5-ced836ff7a07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# USALIGN + REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6aa3a5-a83b-414d-8790-6a0be4de0e11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### USAlign \n",
    "\n",
    "Input:\n",
    "\n",
    "1. <span style=\"color:green\">templates:dict</span>\n",
    "2. <span style=\"color:green\">gene_name:str</span>\n",
    "3. <span style=\"color:green\">path:str</span>\n",
    "4. <span style=\"color:green\"> **optional** report=True</span>\n",
    "\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">./USalign (.exe, C++)</span>\n",
    "\n",
    "\n",
    "helper_functions:\n",
    "\n",
    "+ <span style=\"color:blue\">_get_tm_scores_and_rmsd</span>\n",
    "+ <span style=\"color:blue\">us_report</span>\n",
    "+ <span style=\"color:blue\">multiple_seq_alignment_pre_pca_processing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05532757-89f4-4557-b627-75d3f957ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_usalign(template, path_struc_1):\n",
    "    \n",
    "    bash_tm_and_rmsd_calc = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_struc_1} {template} -outfmt 2\"\n",
    "    bash_command = bash_tm_and_rmsd_calc.split()\n",
    "    \n",
    "    try:\n",
    "        result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        tm_2, rmsd = _get_tm_scores_and_rmsd(result.stdout)\n",
    "        \n",
    "        if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "            return path_struc_1, tm_2, rmsd\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3069431e-c48d-4ac8-9544-c5b70d9ccfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def USAlign(templates, gene_name, path, report=True):\n",
    "    \n",
    "    report_path = path\n",
    "    result_list = []\n",
    "    overall_result_list = []\n",
    "    template_dict = defaultdict()\n",
    "    min_seq_len_dict = defaultdict()\n",
    "\n",
    "    for keys, vals in templates.items():\n",
    "        found = False\n",
    "        for lists in vals:\n",
    "            for x in lists:\n",
    "                template_dict[keys] = x\n",
    "                found = True\n",
    "                break\n",
    "            if found:\n",
    "                break\n",
    "\n",
    "    for keys, vals in template_dict.items():\n",
    "        pdb_id = vals[0]\n",
    "        template_score = vals[2]\n",
    "        basepath = keys.split(\"/\")\n",
    "        dir_path_for_usalign = f\"{path}/{basepath[0]}/{basepath[1]}\"\n",
    "\n",
    "        template = f\"{dir_path_for_usalign}/{pdb_id}\"\n",
    "        pdb_files = [f for f in os.listdir(dir_path_for_usalign) if os.path.isfile(os.path.join(dir_path_for_usalign, f))]\n",
    "\n",
    "        if len(pdb_files) < 2:\n",
    "            continue\n",
    "\n",
    "        # Using ThreadPoolExecutor for parallelization\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Use executor.map to parallelize USAlign calls\n",
    "            futures = [\n",
    "                executor.submit(run_usalign, template, f\"{dir_path_for_usalign}/{rcsb_file}\")\n",
    "                for rcsb_file in pdb_files\n",
    "                if rcsb_file != pdb_id\n",
    "            ]\n",
    "\n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    path_struc_1, tm_2, rmsd = result\n",
    "                    us_report(path_struc_1, pdb_id, tm_2, rmsd, path, template_score=template_score, oligomer=basepath[0], pos=basepath[1])\n",
    "\n",
    "        min_seq_len = multiple_seq_alignment_pre_pca_processing_version_2(path_to_pdbs=dir_path_for_usalign, template=pdb_id)\n",
    "        min_seq_len_dict[keys] = min_seq_len\n",
    "\n",
    "    print(\"this is min seq len dict\")\n",
    "    print(min_seq_len_dict)\n",
    "    return min_seq_len_dict, template_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ee19f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def USAlign(templates:dict, gene_name:str, path:str,\n",
    "#            report=True)-> list:\n",
    "#    \n",
    "#    \"\"\" This function will call subprocess bash script USalign to compute Tms and RMSD values\n",
    "#        for our template and each other protein available on path.\n",
    "#        \n",
    "#        params:\n",
    "#                template: contains the potential template structure.\n",
    "#                \n",
    "#        RETURN: \n",
    "#                list of tuple: (TEMPLATE, QUERY, TM2, RMSD)\"\"\"\n",
    "#\n",
    "#    #here we save the report\n",
    "#    report_path = f\"{path}\"\n",
    "#    \n",
    "#    #this path is used to handle the structures for each ensemble at each oligomeric level of information\n",
    "#    result_list = []\n",
    "#    \n",
    "#    #example ('5ltu_A.pdb', 'NUDT4_HUMAN', 9.159999999899997, 179, 0.96)\n",
    "#    #index to try for selection of template (if highest ranked template number 1 does not work because file is unreadable, emprelevant_dirs    #try select template ranked 2)\n",
    "#    \n",
    "#    #we go through all potential templates\n",
    "#    \n",
    "#    overall_result_list = []\n",
    "#    \n",
    "#    \n",
    "#    #for each category (position and oligomer) we need to pick a template. \n",
    "#    #naturally, ideal exact hits are first order priority.\n",
    "#    \n",
    "#    #vals == list of lists\n",
    "#    \n",
    "#    \n",
    "#    template_dict = defaultdict()\n",
    "#    \n",
    "#    min_seq_len_dict = defaultdict()\n",
    "#    \n",
    "#    print(\"this is templates\")\n",
    "#    print(templates)\n",
    "#    \n",
    "#    for keys, vals in templates.items():\n",
    "#        #lists in vals: single list\n",
    "#        #if this condition is true we break the search and continue.\n",
    "#        found = False\n",
    "#        for lists in vals:\n",
    "#            #entries of single list.\n",
    "#            if found == True:\n",
    "#                break\n",
    "#            for x in lists:\n",
    "#                #lets append the first hit.\n",
    "#                template_dict[keys] = x\n",
    "#                found = True\n",
    "#                break\n",
    "#    \n",
    "#    for keys, vals in template_dict.items():\n",
    "#        pdb_id = vals[0]\n",
    "#        pdb_gene_name = vals[1]\n",
    "#        template_score = vals[2]\n",
    "#        \n",
    "#        #I used \">\" as a seperator before for oligomer>position\n",
    "#        basepath = keys.split(\"/\")\n",
    "#        \n",
    "#        dir_path_for_usalign = f\"{path}/{basepath[0]}/{basepath[1]}\" \n",
    "#        \n",
    "#        print(\"this is dir path for usalign\")\n",
    "#        print(dir_path_for_usalign)\n",
    "#        \n",
    "#        #this corresponds to our selected template.\n",
    "#        template = f\"{dir_path_for_usalign}/{pdb_id}\"\n",
    "#        \n",
    "#        #now lets compare it against all other structures in this directory\n",
    "#        pdb_files = [f for f in os.listdir(dir_path_for_usalign) if os.path.isfile(os.path.join(dir_path_for_usalign, f))]\n",
    "#        \n",
    "#        #if we have less than 2 files we dont need to any of the below.\n",
    "#        \n",
    "#        if len(pdb_files) < 2:\n",
    "#            \n",
    "#            continue\n",
    "#        \n",
    "#        for rcsb_file in pdb_files:\n",
    "#            #dont run USALIGN against itself..\n",
    "#            if rcsb_file != pdb_id:\n",
    "#                \n",
    "#                path_struc_1 = f\"{dir_path_for_usalign}/{rcsb_file}\"\n",
    "#                \n",
    "#                bash_tm_and_rmsd_calc = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_struc_1} {template} -outfmt 2\"\n",
    "#            \n",
    "#            \n",
    "#            \n",
    "#                bash_command = bash_tm_and_rmsd_calc.split()\n",
    "#                \n",
    "#                try:\n",
    "#\n",
    "#                    result = run(bash_command, stdout=PIPE, stderr=PIPE, \n",
    "#                         universal_newlines=True)\n",
    "#    \n",
    "#                    \n",
    "#                    tm_2, rmsd = _get_tm_scores_and_rmsd(result.stdout) #both results are str.\n",
    "#                \n",
    "#                    \n",
    "#                    print(rcsb_file, pdb_id)\n",
    "#                    print(tm_2, rmsd)\n",
    "#                    if float(tm_2) > 0.5: #and float(rmsd) > 0.5:  #lets try less strict!\n",
    "#                        \n",
    "#                        us_report(rcsb_file, pdb_id, tm_2, rmsd, path,\n",
    "#                              template_score=template_score,\n",
    "#                              oligomer=basepath[0], pos=basepath[1])\n",
    "#                    #changed from f\"{path_struc_1}\" to path_struc_1\n",
    "#                    else:\n",
    "#                        os.remove(path_struc_1)\n",
    "#                    \n",
    "#                except Exception as error:\n",
    "#                    print(error)\n",
    "#    \n",
    "#    \n",
    "#    \n",
    "#    \n",
    "#        #now run the all against all\n",
    "#        #lets try version 1 for the full length all chains alignments.\n",
    "#        min_seq_len = multiple_seq_alignment_pre_pca_processing_version_2(path_to_pdbs=dir_path_for_usalign,\n",
    "#                                                  template=pdb_id)\n",
    "#        \n",
    "#        #set global minima to min seq len\n",
    "#        min_seq_len_dict[keys] = min_seq_len\n",
    "#        \n",
    "#    \n",
    "#    \n",
    "#    print(\"this is min seq len dict\")\n",
    "#    print(min_seq_len_dict)\n",
    "#    return min_seq_len_dict, template_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d5294-f14f-4bfa-bb6d-8848f4d05802",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Us_report:\n",
    "\n",
    "simple helper function that writes out a report:\n",
    "\n",
    "sep=\"\\t\"\n",
    "\n",
    "Scheme:\n",
    "\n",
    "Query, Template, TM_2, RMSD, Template_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b145f8e3-dbd3-4d06-8848-d675fe589ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_report(rcsb_file:str, pdb_id:str, tm_2:str, rmsd:str, path:str,\n",
    "             oligomer:str, pos:str, template_score:str):\n",
    "    \n",
    "    \n",
    "    path_to_assembly = f\"{path}/reports/{oligomer}_{pos}.csv\"\n",
    "    \n",
    "    if os.path.isfile(path_to_assembly) == False:\n",
    "        with open(path_to_assembly, \"w\") as fh_start:\n",
    "            fh_start.write(\"Query\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"Template\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"TM_2\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"RMSD\")\n",
    "            fh_start.write(\",\")\n",
    "            fh_start.write(\"Template_score\")\n",
    "            fh_start.write(\"\\n\")\n",
    "    \n",
    "    with open(path_to_assembly, \"a\") as fh_out:\n",
    "        fh_out.write(str(rcsb_file))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(pdb_id))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(tm_2))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(rmsd))\n",
    "        fh_out.write(\",\")\n",
    "        fh_out.write(str(template_score))\n",
    "        fh_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2cf9c-4626-4078-8fe7-325b4e9e2066",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _get_tm_scores_and_rmsd:\n",
    "\n",
    "simple helper function that filters USAlign output and returns TM + RMSD vals as tuple:\n",
    "\n",
    "(TM_2, RMSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b69486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tm_scores_and_rmsd(results:str):\n",
    "    \"\"\" helper function to retrieve tm scores and rmsd\n",
    "        We are only interested in TM_2 and RMSD.\n",
    "        IF RMSD is HIGH and TM_2 HIGH that means we have a conformer.\n",
    "        IF RMSD is LOW and TM_2 HIGH that means we have the same structure in the same conformer\n",
    "        IF RMSD is HIGH and TM_2 LOW that means the structures are not related.\"\"\"\n",
    "    \n",
    "    \"\"\"['#PDBchain1', 'PDBchain2', 'TM1', 'TM2', 'RMSD', 'ID1', 'ID2', 'IDali', 'L1', 'L2', 'Lali\\n/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/2duk_A.pdb:A', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/5ltu_A.pdb:A', \n",
    "    '0.8833', '0.9565', '0.94', '0.920', '1.000', '1.000', '138', '127', '127\\n']\"\"\"\n",
    "    \n",
    "    res_list = results.split(\"\\t\")\n",
    "    \n",
    "    #this one is from the mobile protein\n",
    "    tm_1 = res_list[12]\n",
    "    #this one belongs to the target protein (the one we superimpose the mobile protein onto)\n",
    "    tm_2 = res_list[13]\n",
    "    \n",
    "    #rmsd used to judge cutoff for trashing structures.\n",
    "    rmsd = res_list[14]\n",
    "    \n",
    "    #we return tm_2 and rmsd\n",
    "    return ((tm_2, rmsd))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6008e-3f99-4948-a4eb-c06d7b635939",
   "metadata": {
    "tags": []
   },
   "source": [
    "### multiple_seq_alignment_pre_pca_processing\n",
    "\n",
    "requires:\n",
    "\n",
    "+ <span style=\"color:red\">./USalign (.exe, C++)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaa682-a3d9-4c9b-ad29-bf75480ffe55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ad396e5-2c7c-4ccf-ad8e-08422d15d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testpath = \"/home/micnag/bioinformatics/test/usaligntester\"\n",
    "\n",
    "#gap_dict = read_msa_file_version_1(path=testpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ba19ff8-eb32-4535-9955-facc6276b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/FAK1_HUMAN/monomer/pos_410_689\"\n",
    "#template_struc = \"2v7a.pdb\"\n",
    "#print(_remove_deviating_length_strucs(path_to_pdbs=path, template_struc=template_struc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271c63b-9871-4d54-9132-34ff9383bfc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA PROCESSING PIPELINE 2 (full length all chains 1 vs 1 alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "815dee0c-57a5-40ad-94b0-feabb44bb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_seq_alignment_pre_pca_processing_version_1(path_to_pdbs:str,\n",
    "                                                  template:str):\n",
    "    \n",
    "    \"\"\"-mm 1 -ter 0  for oligomers and all chains should be aligned.\"\"\"\n",
    "    \n",
    "    \n",
    "    min_seq_len = _remove_deviating_length_strucs(path_to_pdbs=path_to_pdbs, template_struc=template)\n",
    "    \n",
    "    \n",
    "    only_pdbs = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    \n",
    "    with open(f\"{path_to_pdbs}/chain_list.txt\", \"w\") as chain_out:\n",
    "        for pdbs in only_pdbs:\n",
    "            #only for pdb files.\n",
    "            if pdbs[-4:] == \".pdb\":\n",
    "                chain_out.write(pdbs)\n",
    "                chain_out.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_pdbs}/MSA_dir\")\n",
    "    \n",
    "    except:\n",
    "        print(\"already there\")\n",
    "    \n",
    "    \n",
    "    USalign_loc = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/\"\n",
    "    for pdbs in only_pdbs:\n",
    "        if pdbs != template:\n",
    "            \n",
    "            bash_cmd = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_to_pdbs}/{pdbs} {path_to_pdbs}/{template} -mm 1 -ter 0\"\n",
    "            \n",
    "            try:\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "                with open(f\"{path_to_pdbs}/MSA_dir/{template[0:4]}_{pdbs[0:4]}.txt\", \"w\") as msa_out:\n",
    "                    result = run(bash_cmd_rdy, stdout=msa_out, stderr=PIPE, \n",
    "                        universal_newlines=True)\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "    return min_seq_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "215051c3-c736-40ad-9ea8-da90d42db9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_seq_alignment_pre_pca_processing_version_2(path_to_pdbs:str,\n",
    "                                                  template:str):\n",
    "    \n",
    "    \"\"\"seq based alignment because we already did a structure based alignment.\"\"\"\n",
    "    \n",
    "    #NOW we need to do a rigorous structure alignment different subroutine!\n",
    "    \n",
    "    \n",
    "    \n",
    "    #to get min seq len\n",
    "    min_seq_len = _remove_deviating_length_strucs(path_to_pdbs=path_to_pdbs, template_struc=template)\n",
    "    \n",
    "    \n",
    "    only_pdbs = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    only_pdbs = [f for f in only_pdbs if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    with open(f\"{path_to_pdbs}/chain_list.txt\", \"w\") as chain_out:\n",
    "        for pdbs in only_pdbs:\n",
    "            #only for pdb files.\n",
    "            if pdbs[-4:] == \".pdb\":\n",
    "                chain_out.write(pdbs)\n",
    "                chain_out.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_pdbs}/MSA_dir\")\n",
    "     \n",
    "    except:\n",
    "        print(\"already there\")\n",
    "    \n",
    "    \n",
    "    USalign_loc = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/\"\n",
    "    for pdbs in only_pdbs:\n",
    "        if pdbs != template:\n",
    "            \n",
    "            print(pdbs, template)\n",
    "            bash_cmd = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/USalign {path_to_pdbs}/{pdbs} {path_to_pdbs}/{template} -byresi 5\"\n",
    "            \n",
    "            try:\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "                with open(f\"{path_to_pdbs}/MSA_dir/{template[0:4]}_{pdbs[0:4]}.txt\", \"w\") as msa_out:\n",
    "                    result = run(bash_cmd_rdy, stdout=msa_out, stderr=PIPE, \n",
    "                        universal_newlines=True)\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "    \n",
    "    \n",
    "    return min_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215fa7e-6267-4902-9a77-fd92d6c3f698",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Structure based alignment and cutting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6c505624-399e-44f7-9d1d-38cd3a728277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_alignment(pdb, main_prot_seq, pdb_info_dict, solid_ensemble, to_be_checked, \n",
    "            solid_ensemble_lock, to_be_checked_lock):\n",
    "\n",
    "    seq_to_check = pdb_info_dict[pdb][2]\n",
    "\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    alignments = aligner.align(main_prot_seq, seq_to_check)\n",
    "    \n",
    "    best_alignment = max(aligned, key=lambda x: x.score)\n",
    "    \n",
    "    score = best_alignment.score\n",
    "    print(pdb, len(seq_to_check), score)\n",
    "    \n",
    "    # this means we have more than 80% identity on the sequence level, aka it should be the same protein.\n",
    "    if score > 0.8 * len(seq_to_check):  \n",
    "        with solid_ensemble_lock:\n",
    "            solid_ensemble.append(pdb)\n",
    "    else:\n",
    "        with to_be_checked_lock:\n",
    "            to_be_checked.append(pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe29a273-e04f-4b2c-ba69-410aa3c691a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_based_repair_1_wrapper(args):\n",
    "\n",
    "    #function for parallelization of modeller.\n",
    "    path_to_pdb, cutoff, pdb, min_len, max_len, structural_length, main_prot_seq, use_main = args\n",
    "    structure_based_repair_1(\n",
    "        path_to_pdb=path_to_pdb,\n",
    "        cutoff=cutoff,\n",
    "        pdb_id=pdb,\n",
    "        min_len=min_len,\n",
    "        max_len=max_len,\n",
    "        structural_length=structural_length,\n",
    "        main_prot_seq=main_prot_seq,\n",
    "        use_main=use_main\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea3c6a99-8758-430a-aa7c-5ca616200d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_based_cutting_1(path_to_pdbs:str, template:str, main_prot_seq:str):\n",
    "\n",
    "\n",
    "    print(\"Performing structure-based alignment\")\n",
    "    \n",
    "    \"\"\"This function will perform the structure based alignment based on the SUPERIMPOSER PDB.BIO class\n",
    "        \"\"\"\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    pdbs = [f for f in onlyfiles if f.endswith(\".pdb\")]\n",
    "\n",
    "    try:\n",
    "        new_directory_path = Path(path_to_pdbs) / \"PCA\" / \"clean_ensemble\"\n",
    "        \n",
    "        new_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    except Exception as error:\n",
    "        \n",
    "        print(f\"We could not make the directory: {new_directory_path}. Error: {error}\")\n",
    "\n",
    "    template_return = None  # Initialize template_return before the loop\n",
    "\n",
    "    pdb_info_dict = defaultdict()\n",
    "    \n",
    "    solid_ensemble = []\n",
    "    \n",
    "    for pdb in pdbs:\n",
    "        \n",
    "        pdb_path = os.path.join(path_to_pdbs, pdb)\n",
    "        \n",
    "        pdb_parser = Bio.PDB.PDBParser(QUIET=True)\n",
    "        \n",
    "        sample_structure = pdb_parser.get_structure(\"sample\", pdb_path)\n",
    "        \n",
    "        ref_chain = [x.get_id() for x in sample_structure.get_chains()]\n",
    "        \n",
    "        io = Bio.PDB.PDBIO()\n",
    "        \n",
    "        io.set_structure(sample_structure)\n",
    "        \n",
    "        io.save(os.path.join(path_to_pdbs, \"PCA\", f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"))\n",
    "\n",
    "        if pdb == template:\n",
    "            template_return = f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"\n",
    "\n",
    "        struc_start, struc_stop, sequence = select_c_alpha(os.path.join(path_to_pdbs, \"PCA\", f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"))\n",
    "        pdb_info_dict[f\"original_{pdb[0:4]}_{''.join(ref_chain)}.pdb\"] = (struc_start, struc_stop, sequence)\n",
    "\n",
    "    print(pdb_info_dict)\n",
    "    print(\"still works after select_c_alpha\")\n",
    "    #len_ref = select_c_alpha(f\"{template}\")  #template path is full.\n",
    " \n",
    "    #pdb_lengths.append(len_ref) #we add it in the mix.\n",
    "    \n",
    "    \n",
    "    cnts_start = Counter(val[0] for val in pdb_info_dict.values())\n",
    "    cnts_stop = Counter(val[1] for val in pdb_info_dict.values())\n",
    "    \n",
    "    \n",
    "    highest_occ_start = cnts_start.most_common()  #grabs the highest frequency start of structures\n",
    "    highest_occ_stops = cnts_stop.most_common()  #grabs the highest frequency stop of structures\n",
    "    \n",
    "    print(highest_occ_start)\n",
    "    print(highest_occ_stops)\n",
    "    \n",
    "    min_len = highest_occ_start[0][0] # this corresponds to the majority vote and its associated starts.\n",
    "    max_len = highest_occ_stops[0][0] # this corresponds to the majority vote and its associated stops.\n",
    "    \n",
    "    print(min_len)\n",
    "    print(max_len)\n",
    "    \n",
    "    #so here we did majority voting (1) boundaries are established.\n",
    "    \n",
    "    #for keys, vals in pdb_info_dict.items():\n",
    "    #    print(keys, vals)    #val is: (start, stop, sequence(in full length!))\n",
    "    \n",
    "    solid_ensemble = [] #here we store good hits.\n",
    "    \n",
    "    to_be_checked = []\n",
    "    #we need MAIN FASTA SEQ HERE\n",
    "    # Gather all pdb files\n",
    "    pdb_files = [f for f in os.listdir(os.path.join(path_to_pdbs, \"PCA\")) if f.endswith(\".pdb\")]\n",
    "    #print(len(main_prot_seq))\n",
    "    \n",
    "    # Define locks for thread safety\n",
    "    solid_ensemble_lock = threading.Lock()\n",
    "    to_be_checked_lock = threading.Lock()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        print(pdb_info_dict)\n",
    "        # Submit tasks to the thread pool\n",
    "        futures = [executor.submit(\n",
    "            perform_alignment, pdb, main_prot_seq, pdb_info_dict, solid_ensemble, to_be_checked, \n",
    "            solid_ensemble_lock, to_be_checked_lock\n",
    "        ) for pdb in pdb_files]\n",
    "\n",
    "        # Wait for all tasks to complete and collect results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()  # This call ensures exceptions are propagated\n",
    "\n",
    "    print(\"this is solid ensemble\")\n",
    "    print(solid_ensemble)\n",
    "    #\n",
    "    print(\"this is to be checked\")\n",
    "    print(to_be_checked)\n",
    "    \n",
    "    \n",
    "    # Parallelized repair process.\n",
    "\n",
    "    # Create a list of arguments for the function\n",
    "    args_list = [(f\"{path_to_pdbs}/PCA\", 10, pdb, min_len, max_len,\n",
    "                  len(pdb_info_dict[pdb][2]),main_prot_seq, True) for pdb in solid_ensemble]\n",
    "\n",
    "    # Parallelize the calls using ProcessPoolExecutor\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        executor.map(structure_based_repair_1_wrapper, args_list)\n",
    "\n",
    "    solid_ensemble = [os.path.join(f\"{path_to_pdbs}/PCA/clean_ensemble\", pdb) for pdb in os.listdir(f\"{path_to_pdbs}/PCA/clean_ensemble\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/PCA/clean_ensemble\", pdb))]\n",
    "    \n",
    "   \n",
    "    #solidify ensemble by removing strong outliers.\n",
    "    \n",
    "    lst_ground_truth = [os.path.join(f\"{path_to_pdbs}/PCA/clean_ensemble\", pdb) for pdb in solid_ensemble]\n",
    "    \n",
    "    for pdb in solid_ensemble:\n",
    "        lst_ground_truth.append(f\"{pdb}\")\n",
    "    \n",
    "    #lets check the RMSD WITHIN the best hits to remove strong outliers.\n",
    "    \n",
    "    #lets return the average rmsd and use this as criterion! NEEDS TO BE DONE ATFER LUNCH\n",
    "    off_strucs, tot_mean, tot_sd = _calc_mean_tresholds_lst_1(lst_ground_truth)  #_1 is updated speed version\n",
    "    print(off_strucs)\n",
    "    \n",
    "    #remove off strucs. now compute mean rmsd against the ground truth assumed ensemble.\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_pdbs}/PCA/repaired_ensemble\")\n",
    "    \n",
    "    except:\n",
    "        print(f\"we could not make the dir : {path_to_pdbs}/PCA/repaired_ensemble\")\n",
    "\n",
    "    # Create a list of arguments for the function\n",
    "    args_list = [(f\"{path_to_pdbs}/PCA\", 10, pdb, min_len, max_len,\n",
    "                  len(pdb_info_dict[pdb][2]),main_prot_seq, False) for pdb in to_be_checked]\n",
    "\n",
    "    # Parallelize the calls using ProcessPoolExecutor\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        executor.map(structure_based_repair_1_wrapper, args_list)\n",
    "\n",
    "        \n",
    "    #after repair we check rmsd.\n",
    "    \n",
    "    lst_to_check = [f for f in os.listdir(f\"{path_to_pdbs}/PCA/repaired_ensemble\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/PCA/repaired_ensemble\", f))]\n",
    "    \n",
    "    print(\"we check now rmsd for repaired alternative strucs\")\n",
    "    \n",
    "    for pdb in lst_to_check:\n",
    "\n",
    "        try:\n",
    "            rmsd_avg = _calc_rmsd_dev(lst_to_pdbs=lst_ground_truth,\n",
    "                                  pdb_to_check=f\"{path_to_pdbs}/PCA/repaired_ensemble/{pdb}\")\n",
    "        \n",
    "            #if rmsd is low we add it to the ensemble.\n",
    "            if rmsd_avg < (tot_mean * 2 * tot_sd):\n",
    "                print(f\"we append {pdb} with rmsd avg: {rmsd_avg}, against {tot_mean * 2 * tot_sd} as threshold.\")\n",
    "                solid_ensemble.append(pdb)\n",
    "                #we move it to another dir.\n",
    "                shutil.copy(pdb, f\"{path_to_pdbs}/PCA/clean_ensemble/{os.path.basename(pdb)}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"we reject {pdb} with rmsd avg: {rmsd_avg}, against {tot_mean * 2 * tot_sd} as threshold.\")\n",
    "                os.remove(pdb)\n",
    "        \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "    \n",
    "    return template_return\n",
    "    #return pdbs to be fetched from the clean ensemble to another function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a4fd9bec-74a4-4917-a9f9-fce5dc277ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_ensemble_list(work_dir, pdb_files):\n",
    "    #this function acts as an helper function in pca_laura_pipeline_1\n",
    "    with open(f\"{work_dir}/ensemble.txt\", \"w\") as esmbl:\n",
    "        for entry in pdb_files:\n",
    "            esmbl.write(entry[9:-4] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "91a85431-e9f7-4274-8850-064d3d6006af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_multi_pdb(work_dir, pdb_files):\n",
    "    #this function acts as an helper function in pca_laura_pipeline_1\n",
    "    ms = Structure.Structure(\"master\")\n",
    "    i = 0\n",
    "    for file in pdb_files:\n",
    "        location = os.path.join(work_dir, \"clean_ensemble\", file)\n",
    "        prot_name = file[:-4]\n",
    "        structure = PDBParser(QUIET=True).get_structure(prot_name, location)\n",
    "        for model in structure:\n",
    "            new_model = model.copy()\n",
    "            new_model.id = i\n",
    "            new_model.serial_num = i + 1\n",
    "\n",
    "            # Iterate through atoms and set B-factor to 0 because this causes issues with pcatoolS\n",
    "            for chain in new_model:\n",
    "                for residue in chain:\n",
    "                    for atom in residue:\n",
    "                        atom.set_bfactor(0)\n",
    "\n",
    "            i += 1\n",
    "            ms.add(new_model)\n",
    "\n",
    "    pdb_io = PDBIO()\n",
    "    pdb_io.set_structure(ms)\n",
    "    pdb_io.save(f\"{work_dir}/multi_ensemble.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5d67b39-734b-499d-8945-0d6042c8654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _move_executables(basepath, work_dir):\n",
    "    #this helper function is required for domenico and laura stuff to work.\n",
    "    try:\n",
    "        files_to_move = [f for f in os.listdir(basepath) if os.path.isfile(os.path.join(basepath, f))]\n",
    "        for file in files_to_move:\n",
    "            shutil.copy(os.path.join(basepath, file), work_dir)\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45d30d05-954b-459d-836f-4b46ebed233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pdb_files(folder_path):\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Iterate over the files and remove those with the .pdb extension\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdb\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15a65a40-10ca-418a-b632-29422f38a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_pca_pipeline(work_dir, template, protein, num_structures):\n",
    "\n",
    "    #here we run the pca pipeline.\n",
    "    os.chdir(work_dir)\n",
    "    _move_executables(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/PCA_pipeline\", work_dir)\n",
    "\n",
    "    \n",
    "    # Part 1: PCA\n",
    "    bash_cmd1 = f\"./getpca.sh multi_ensemble.pdb {template} refpdb_ref\"\n",
    "    _run_command(bash_cmd1, \"Part 1\")\n",
    "    \n",
    "    # Part 2: Convert evec files\n",
    "    bash_cmd2 = f\"./convert.sh refpdb_ref_pca.evec\"\n",
    "    _run_command(bash_cmd2, \"Part 2\")\n",
    "\n",
    "    # Part 3: Projections\n",
    "    bash_cmd3 = f\"./getproj.sh {template} refpdb_ref_pca.evec {protein} multi_ensemble.pdb\"\n",
    "    _run_command(bash_cmd3, \"Part 3\")\n",
    "\n",
    "    return work_dir, protein, num_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6339fdc-6523-4c38-ac37-5f5fa4d37124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_command(bash_cmd, part_name):\n",
    "    try:\n",
    "        result = run(bash_cmd.split(), stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "        print(f\"{part_name} output:\\n{result.stdout}\")\n",
    "        print(f\"{part_name} errors:\\n{result.stderr}\")\n",
    "    except Exception:\n",
    "        print(f\"Script did not work. Parameters: {bash_cmd.split()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57e7e1a2-d274-4faa-90a3-d0e1bdf5ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_template_1(work_dir, pdb_files:str):\n",
    "\n",
    "    \n",
    "    clean_ensemble_dir = os.path.join(work_dir, \"clean_ensemble\")\n",
    "\n",
    "     # Define the paths for the source and destination files\n",
    "    source_path = os.path.join(clean_ensemble_dir, pdb_files[0])\n",
    "    destination_path = os.path.join(work_dir, pdb_files[0])\n",
    "\n",
    "    # Copy the first structure file to the working directory with the original filename\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    #we copied it into work dir.\n",
    "    return pdb_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a67a273-84ca-4b8b-b9ca-e030a79cb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#work_dir, protein, num_struc = pca_laura_pipeline_1(path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA\",\n",
    "#                    template=None, protein=\"SERCA\")\n",
    "#_plot_PCA_new(input_dir=work_dir, protein=protein, num_struc=num_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b3f23f2-c010-497e-b439-5df2d24d1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_laura_pipeline_1(path, template, protein):\n",
    "    \n",
    "    work_dir = path\n",
    "\n",
    "    clean_dir = f\"{path}/clean_ensemble\"\n",
    "    \n",
    "    os.chdir(work_dir)\n",
    "\n",
    "    pdb_files = [f for f in os.listdir(f\"{path}/clean_ensemble\") if os.path.isfile(os.path.join(f\"{path}/clean_ensemble\", f))]\n",
    "    \n",
    "    #we store the required pdb file names here in save_ensemble_list\n",
    "    _save_ensemble_list(work_dir, pdb_files)\n",
    "\n",
    "\n",
    "    #first remove all previous pdbs\n",
    "    remove_pdb_files(path)\n",
    "    \n",
    "    #we need a multi_pdb file to work with laura code.\n",
    "    _create_multi_pdb(work_dir, pdb_files)\n",
    "\n",
    "\n",
    "    #we should choose a template from clean ensemble !!!\n",
    "\n",
    "    #TBD TOMORROW\n",
    "\n",
    "    #template is FULL PATH TO CLEAN ENSEMBLE.\n",
    "    template = select_template_1(work_dir, pdb_files)\n",
    "    print(f\"we select as template for pca: {template}\")\n",
    "    #workdir , template, protein, and len of pdb files aka how many are in pca.\n",
    "    return _run_pca_pipeline(work_dir, template, protein, len(pdb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2cc6016a-38cf-4a40-9df8-5b2f4fed218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_PCA_new(input_dir:str, protein:str, num_struc:int):\n",
    "    \n",
    "    proj_dict = defaultdict()\n",
    "    \n",
    "    labels = []\n",
    "    with open(f\"{input_dir}/ensemble.txt\", \"r\") as label_in:\n",
    "        for lines in label_in:\n",
    "            lines = lines.replace(\"\\n\",\"\")\n",
    "            labels.append(lines[-6:])\n",
    "    \n",
    "    \n",
    "    var_list = []\n",
    "    \n",
    "    #they are already sorted.\n",
    "    with open(f\"{input_dir}/{protein}_evec3.dat\", \"r\") as var_readin:\n",
    "        for lines in var_readin:\n",
    "            line = lines.split()\n",
    "            if len(line) == 2:\n",
    "                var_list.append(float(line[1]))\n",
    "\n",
    "    expl_var = var_list / np.sum(var_list)\n",
    "    \n",
    "    with open(f\"{input_dir}/{protein}.mode_12.proj\", \"r\") as res_proj:\n",
    "        for lines in res_proj:\n",
    "            lines = lines.replace(\"\\n\",\"\")\n",
    "            #print(lines.split(\"      \"))  #6 spaces ??? \n",
    "            lines = lines.split(\"      \")\n",
    "            num = lines[1].replace(\" \",\"\")\n",
    "            \n",
    "            PC1 = float(lines[2].replace(\" \",\"\"))\n",
    "            PC2 = float(lines[3].replace(\" \",\"\"))\n",
    "            \n",
    "            proj_dict[str(num)] = (PC1, PC2)\n",
    "            \n",
    "    \n",
    "            \n",
    "    PC_1 = []\n",
    "    PC_2 = []\n",
    "    for keys, vals in proj_dict.items():\n",
    "        #print(vals)\n",
    "        PC_1.append(vals[0])\n",
    "        PC_2.append(vals[1])\n",
    "    \n",
    "    \n",
    "    plot_df = pd.DataFrame(columns=[\"PC1\", \"PC2\"])\n",
    "    plot_df[\"PC1\"] = PC_1\n",
    "    plot_df[\"PC1\"] = plot_df[\"PC1\"] * (-1)\n",
    "    plot_df[\"PC2\"] = PC_2\n",
    "    plot_df[\"labels\"] = labels\n",
    "    #michael9_test_df[\"PC2\"] = michael9_test_df[\"PC2\"] * (-1)\n",
    "    #michael9_test_df[\"labels\"] = labels\n",
    "    \n",
    "    fig = px.scatter(plot_df, x= \"PC1\",y= \"PC2\",\n",
    "                hover_data=[\"labels\"],labels={\"labels\": \"labels\"},\n",
    "                custom_data = [\"labels\"])\n",
    "\n",
    "    fig.update_traces(marker_size=10,\n",
    "                    hovertemplate=\"<b>RCSB: %{customdata[0]}</b>\",\n",
    "                    hoverlabel=dict(\n",
    "                    font_size=40),  # Set the hover label font size\n",
    "                        \n",
    "                        \n",
    "                    mode=\"markers+text\", selector=dict(type='scatter')    \n",
    "                        \n",
    "        )\n",
    "    \n",
    "    fig.update_layout(plot_bgcolor='lavender',\n",
    "                      paper_bgcolor='white', \n",
    "                     width = 800,\n",
    "                     height = 600,\n",
    "                     xaxis_title=f'PC 1 {expl_var[0]*100:.2f}%',  # Set the x-axis label\n",
    "                     yaxis_title=f'PC 2 {expl_var[1]*100:.2f}%',   # Set the y-axis label\n",
    "                     legend_title_text='Clusters',  # Set the legend title\n",
    "                     showlegend=True,  # Show the legend\n",
    "                     title={\n",
    "                     'text': f'PCA of {protein}: {num_struc} structures',  # Set the title text\n",
    "                     'x': 0.5,  # Center the title horizontally (0 to 1)\n",
    "                     'font': {'size': 30}  # Customize title font size and weight\n",
    "                     }\n",
    "                      \n",
    "                     )\n",
    "    \n",
    "    \n",
    "    pio.write_image(fig, f\"{input_dir}/{protein}_PC_plot.png\", format=\"png\")\n",
    "    \n",
    "    #return fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c431596-0922-44b9-a9d1-97357ebe954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file = \"/home/micnag/result_test_struc_align/PCA/SERCA.mode_12.proj\"\n",
    "#labels_file = \"/home/micnag/result_test_struc_align/PCA/ensemble.txt\"\n",
    "#var_file = \"/home/micnag/result_test_struc_align/PCA/SERCA_evec3.dat\"\n",
    "#protein = \"SERCA\"\n",
    "#num_struc = 86\n",
    "\n",
    "#_plot_PCA(input_file=input_file, \n",
    "#         labels_file=labels_file,\n",
    "#          explained_var_file=var_file,\n",
    "#         protein=protein,\n",
    "#         num_struc=num_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "351579e3-8cec-4e6d-a3c8-3f8fd3f4c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/result_test_struc_align/clean_ensemble\"\n",
    "#template = \"/home/micnag/result_test_struc_align/clean_ensemble/original_2c9m_A.pdb\"\n",
    "\n",
    "#pca_laura_pipeline(path=path, template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9caf1ae1-fa40-4dd5-94de-908f03d0a5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### os.chdir(\"/home/micnag/\") #just because there is an error is we stuck within that makes problems\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#path = \"/home/micnag/result_test_struc_align\"\n",
    "#query = \"O14983\"\n",
    "#main_prot_seq = get_gene_fasta(query)\n",
    "#template = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/2c9m.pdb\"\n",
    "#structure_based_cutting(path_to_pdbs=path, template=template, main_prot_seq=main_prot_seq, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3a43b6b-7571-4059-91f9-f0973d42d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_proper_length_1(path:str,start:int, stop:int, use_main:bool):\n",
    "    \n",
    "    #sel only c_alpha\n",
    "    class RangeOnly(Select):\n",
    "        def __init__(self, start, stop):\n",
    "            super().__init__()\n",
    "            self.start = start\n",
    "            self.stop = stop\n",
    "            \n",
    "        def accept_atom(self, atom):\n",
    "            return 1 if atom.id == \"CA\" else 0\n",
    "            \n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if self.start <= residue.id[1] <= self.stop else 0 \n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    prot_name = f\"default\"\n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    structure_len = [x.get_id()[1] for x in structure.get_residues()]\n",
    "    \n",
    "    if not use_main:\n",
    "        \n",
    "        start += structure_len[0] - 1  # Correct for overhang\n",
    "        \n",
    "    if len(structure_len) < (stop - start) + 1:\n",
    "        \n",
    "        print(f\"Removing {path}\")\n",
    "        \n",
    "        os.remove(path)\n",
    "        return False\n",
    "\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "\n",
    "    range_selector = RangeOnly(start, stop)\n",
    "    io.save(path, range_selector)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ce997c8-683f-4c40-bda3-c3a687cddfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_proper_length(path:str,start:int, stop:int, use_main:bool):\n",
    "    \n",
    "     #sel only c_alpha\n",
    "    class range_only(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "        \n",
    "        def accept_atom(self, atom):\n",
    "            return 1 if atom.id == \"CA\" else 0\n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_residue(self, residue):\n",
    "            return 1 if residue.id[1] >= start and residue.id[1] <= stop else 0\n",
    "        \n",
    "    \n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    \n",
    "    #now we load it again\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"default\"\n",
    "    \n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    structure_len = structure.get_residues()\n",
    "    \n",
    "    struc_len = [x.get_id()[1] for x in structure_len]\n",
    "    \n",
    "    #this is start\n",
    "    #1\n",
    "    #this is stop\n",
    "    #994\n",
    "    #/home/micnag/result_test_struc_align/original_4kyt_A.pdb\n",
    "    #992 994\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_main == False:    \n",
    "        \n",
    "        start = struc_len[0]\n",
    "        stop = stop + start -1 # this corresponds to correcting for the overhang!\n",
    "        \n",
    "    print(f\"we set as start: {start} and as stop: {stop}\")\n",
    "    \n",
    "    if len(struc_len) < (stop-start)+1:\n",
    "        \n",
    "        #print(path)\n",
    "        #print(struc_len,stop-start+1)\n",
    "        #print(struc_len)\n",
    "        #we remove it then.\n",
    "        #print(f\"we remove {path}\")\n",
    "\n",
    "        #os.remove(path)\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        io = PDBIO()\n",
    "            \n",
    "        io.set_structure(structure)\n",
    "    \n",
    "        #savepath needs to be changed.\n",
    "        io.save(path, range_only())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcc43c4f-9c19-4b85-8dc9-81e4469b3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/result_test_struc_align/original_5zmv_A.pdb\"\n",
    "#_select_proper_length(path=path, stop=994)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7176052f-9bb6-4d25-acf5-2b45ab507a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_based_repair_1(path_to_pdb:str, pdb_id:str, min_len:int,\n",
    "                          max_len:int, \n",
    "                          structural_length:int, \n",
    "                          use_main:bool,\n",
    "                          main_prot_seq:str,\n",
    "                          cutoff=10):\n",
    "    \n",
    "    print(\"we check gaps now\")\n",
    "    pdb_path = Path(path_to_pdb) / pdb_id\n",
    "    print(f\"We check gaps now: {pdb_path}\")\n",
    "\n",
    "    #first we check the gaps\n",
    "    gaps = _gap_localization_1(f\"{path_to_pdb}/{pdb_id}\")\n",
    "\n",
    "    #[(1, 78), (85, 501), (508, 990), (1015, 1042)]\n",
    "    print(gaps)\n",
    "\n",
    "\n",
    "    #we need to change to the homology modeller dir.\n",
    "\n",
    "    try:\n",
    "        os.chdir(path_to_pdb)\n",
    "        print(os.getcwd())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Repair all that have less than 10 gaps.\n",
    "        gap_ranges = [y - x for x, y in gaps if (y < max_len and x >= min_len)]\n",
    "        gap_sorted = sorted(gap_ranges, reverse=True)\n",
    "\n",
    "        print(gap_sorted)\n",
    "        #if there are no gaps but they dont match the full length e.g miss a bit in N or C termini OR have gaps < 10:\n",
    "        if not gap_sorted or gap_sorted[0] <= 10:\n",
    "            print(f\"PDB is: {pdb_id}, use_main: {use_main}\")\n",
    "            mini_repair_residues_2(os.path.join(path_to_pdb, pdb_id), max_len, main_prot_seq, use_main)\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during repair: {e}\")\n",
    "        # Remove the file if repair fails\n",
    "        os.remove(os.path.join(path_to_pdb, pdb_id))\n",
    "        return\n",
    "    \n",
    "    if structural_length < max_len:\n",
    "        try:\n",
    "            mini_repair_residues_2(os.path.join(path_to_pdb, pdb_id), max_len, main_prot_seq, use_main)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during repair: {e}\")\n",
    "            os.remove(os.path.join(path_to_pdb, pdb_id))\n",
    "            return\n",
    "\n",
    "    \n",
    "    # If we deleted it, we don't need to continue\n",
    "    keep_struc = _select_proper_length_1(os.path.join(path_to_pdb, pdb_id), min_len, max_len, use_main)\n",
    "    if not keep_struc:\n",
    "        print(\"Removed the structure, returning!\")\n",
    "        try:\n",
    "            os.remove(os.path.join(path_to_pdb, pdb_id))\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "    non_canonical = remove_hetero_atoms_1(path=path_to_pdb, pdb_file=pdb_id)\n",
    "\n",
    "    if non_canonical:\n",
    "        for keys, vals in non_canonical.items():\n",
    "            _mutate_non_standard_aa_1(\n",
    "                os.path.join(path_to_pdb, pdb_id),\n",
    "                non_standard_residue=vals[0],\n",
    "                residue=keys,\n",
    "                chain=vals[1]\n",
    "            )\n",
    "\n",
    "    print(\"Shuffling repaired structures\")\n",
    "\n",
    "    dest_folder = \"clean_ensemble\" if use_main else \"repaired_ensemble\"\n",
    "    dest_path = os.path.join(path_to_pdb, dest_folder, pdb_id)\n",
    "    \n",
    "    print(f\"Copying from {os.path.join(path_to_pdb, pdb_id)} to : {dest_path}\")\n",
    "    shutil.copy(os.path.join(path_to_pdb, pdb_id), dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2a2daee9-6e76-4ba6-a082-440e6aa0d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/home/micnag/bioinformatics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3874cd5f-b08c-4ba5-a263-a082400573dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsd_1(pdb_path:str, template:str):\n",
    "    \n",
    "    aligner = Superimposer()\n",
    "    \n",
    "    pdb_parser = PDBParser(QUIET = True)\n",
    "        \n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", pdb_path)\n",
    "    template_structure = pdb_parser.get_structure(\"template\", template)\n",
    "    \n",
    "            \n",
    "    sample_model = list(sample_structure.get_atoms())\n",
    "    template_model = list(template_structure.get_atoms())\n",
    "            \n",
    "    aligner.set_atoms(template_model, sample_model)\n",
    "    aligner.apply(sample_model)\n",
    "    \n",
    "    return aligner.rms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "183dd3a9-d7b7-4c0d-80da-c7bb396a48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_rmsd_dev(lst_to_pdbs:list, pdb_to_check:str):\n",
    "    \n",
    "    avg_rms = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks to the thread pool\n",
    "        futures = [executor.submit(calculate_rmsd_parallel, pdb, pdb_to_check) for pdb in lst_to_pdbs]\n",
    "\n",
    "        # Collect results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                res = future.result()\n",
    "                avg_rms.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating RMSD: {e}\")\n",
    "\n",
    "    print(pdb_to_check, avg_rms)\n",
    "    # Return the mean RMSD\n",
    "    return np.mean(avg_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92e422d6-d7ef-400d-a1cb-9ef2ceb3c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmsd_parallel(pdb, temp):\n",
    "    return get_rmsd_1(pdb_path=pdb, template=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b2dc23d-13ea-4fa2-85ce-1e9a62c6275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_mean_tresholds_lst_1(lst_to_pdbs):\n",
    "    avg_rms = defaultdict(list)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Iterate over unique pairs of structures\n",
    "        futures = []\n",
    "        for i, temp in enumerate(lst_to_pdbs[:-1]):\n",
    "            for pdb in lst_to_pdbs[i + 1:]:\n",
    "                future = executor.submit(calculate_rmsd_parallel, pdb, temp)\n",
    "                futures.append((pdb, future))\n",
    "\n",
    "        # Collect results\n",
    "        for pdb, future in futures:\n",
    "            try:\n",
    "                res = future.result()\n",
    "                avg_rms[pdb].append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calculating RMSD for {pdb}: {e}\")\n",
    "\n",
    "    total_means = [np.mean(vals) for vals in avg_rms.values()]\n",
    "    tot_mean = np.mean(total_means)\n",
    "    tot_sd = np.std(total_means)\n",
    "\n",
    "    off_strucs = [(keys, vals) for keys, vals in avg_rms.items() if np.mean(vals) > tot_mean + 2 * tot_sd]\n",
    "\n",
    "    print(off_strucs)\n",
    "    return off_strucs, tot_mean, tot_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2f80c6a-93e5-4dd7-8c36-affb700e67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_mean_tresholds_lst(lst_to_pdbs:list):\n",
    "    \n",
    "    avg_rms = defaultdict(list)\n",
    "    \n",
    "    #each against each\n",
    "    for temp in lst_to_pdbs:\n",
    "        #we compute the average rmsd for each struc against all others.\n",
    "        for pdb in lst_to_pdbs:\n",
    "            res = get_rmsd(pdb_path=pdb, \n",
    "                  template=temp)\n",
    "        \n",
    "            avg_rms[pdb].append(res)\n",
    "        \n",
    "\n",
    "    total_means = []\n",
    "\n",
    "    tot_mean_dict = defaultdict()\n",
    "    for keys, vals in avg_rms.items():\n",
    "        print(keys)\n",
    "    \n",
    "        tot = 0\n",
    "        for v in vals:\n",
    "            tot += v\n",
    "    \n",
    "        total_means.append(tot/len(vals))\n",
    "        print(tot/len(vals))\n",
    "        \n",
    "        tot_mean_dict[keys] = tot/len(vals) \n",
    "        \n",
    "    tot_mean = np.mean(total_means)\n",
    "    tot_sd = np.std(total_means)\n",
    "    \n",
    "    off_strucs = [(keys, vals) for keys, vals in tot_mean_dict.items() if vals > tot_mean + 2*tot_sd]\n",
    "    \n",
    "    print(off_strucs)\n",
    "    return (off_strucs, tot_mean, tot_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9bba6ba1-611d-4ec1-a284-3cd752447c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pdb_test = \"/home/micnag/result_test_struc_align\"\n",
    "\n",
    "def _calc_mean_tresholds(path_to_pdbs:str):\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "\n",
    "    avg_rms = defaultdict(list)\n",
    "    \n",
    "    #each against each\n",
    "    for temp in onlyfiles:\n",
    "        #we compute the average rmsd for each struc against all others.\n",
    "        for pdb in onlyfiles:\n",
    "            res = get_rmsd(pdb_path=f\"{path_to_pdbs}/{pdb}\", \n",
    "                  template=f\"{path_to_pdbs}/{temp}\")\n",
    "        \n",
    "            avg_rms[pdb].append(res)\n",
    "        \n",
    "\n",
    "    total_means = []\n",
    "\n",
    "    tot_mean_dict = defaultdict()\n",
    "    for keys, vals in avg_rms.items():\n",
    "        print(keys)\n",
    "    \n",
    "        tot = 0\n",
    "        for v in vals:\n",
    "            tot += v\n",
    "    \n",
    "        total_means.append(tot/len(vals))\n",
    "        print(tot/len(vals))\n",
    "        \n",
    "        tot_mean_dict[keys] = tot/len(vals) \n",
    "        \n",
    "    tot_mean = np.mean(total_means)\n",
    "    tot_sd = np.std(total_means)\n",
    "    \n",
    "    off_strucs = [(keys, vals) for keys, vals in tot_mean_dict.items() if vals > tot_mean + 2*tot_sd]\n",
    "    print(off_strucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "255fed41-2506-4d3d-a812-3d78c042273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_calc_mean_tresholds(path_to_pdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dab714af-0211-4c38-98cd-b6f230dbce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gap_localization_1(pdb_path: str):\n",
    "    \"\"\"Helper function to compute the start and stops of gaps for later potential reconstruction.\"\"\"\n",
    "    \n",
    "    gap_ranges = []\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "    \n",
    "    test_res = test_structure.get_residues()\n",
    "    start = end = None\n",
    "\n",
    "    for res in test_res:\n",
    "        res_id = int(res.get_id()[1])\n",
    "        if end is None:\n",
    "            start = end = res_id\n",
    "        elif res_id == end + 1:\n",
    "            end = res_id\n",
    "        else:\n",
    "            if start != end:\n",
    "                gap_ranges.append((start, end))\n",
    "            start = end = res_id\n",
    "\n",
    "    if start is not None and start != end:\n",
    "        gap_ranges.append((start, end))\n",
    "\n",
    "    # Convert the list of gap ranges to a list of gap tuples\n",
    "    gap_tuples = [(start, end-1) for start, end in gap_ranges]\n",
    "    merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "    \n",
    "    return merged_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "462e4b47-61e2-4573-b330-0523ec08e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _structures_to_select_as_reference(structure_pdbs:str, gaps:list):\n",
    "    \n",
    "    np.random.seed(7) #lets make it random. we dont want to parse the whole ensemble.\n",
    "    \n",
    "    #either 10% of ensemble as template or min 5:\n",
    "    \n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(structure_pdbs) if os.path.isfile(os.path.join(structure_pdbs, f))]\n",
    "    \n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    struc_templates = []\n",
    "    \n",
    "    rng_num = np.unique(np.random.randint(0, len(structure_pdbs)-1, 3*len(onlyfiles)))  #we draw 60 samples and keep only uniques\n",
    "    \n",
    "    i = 0 #we start with first member of rng_num\n",
    "    \n",
    "    while len(struc_templates) < 4:\n",
    "        \n",
    "        #ifonlyfilesexhausted all potential templates we shall return None\n",
    "        if i == len(onlyfiles):\n",
    "            return None\n",
    "        \n",
    "        pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "            \n",
    "        struc_chosen = rng_num[i]\n",
    "        \n",
    "        test_structure = pdb_parser.get_structure(\"test\", f\"{structure_pdbs}/{pdbs[struc_chosen]}\")  #this choses the structure that has idx i from rng_num\n",
    "        \n",
    "        test_res = test_structure.get_residues()\n",
    "                                    \n",
    "        max_len = [x for x in test_res] #just for max len purpose.\n",
    "        \n",
    "        #need to reset generator!!!\n",
    "        test_res = test_structure.get_residues()\n",
    "        \n",
    "        \n",
    "        ext_gap_upper = gaps[1] + 5 if gaps[1] + 5 < len(max_len) else len(max_len) # if it goes beyong possible we stick with end range.\n",
    "        ext_gap_lower = gaps[0] - 5 if gaps[0] - 5 > 0 else gaps[0]  #min is pos 1. we cant go negative\n",
    "        \n",
    "        res_id_lst = [x.get_id()[1] for x in test_res if x.get_id()[1] >= ext_gap_lower and x.get_id()[1] <= ext_gap_upper] # +5 as buffer\n",
    "        \n",
    "        diff_required = ext_gap_upper - ext_gap_lower + 1\n",
    "        \n",
    "        \n",
    "        # this is borders ext:\n",
    "        #512 500\n",
    "        #this is required\n",
    "        #13\n",
    "\n",
    "        #print(\"this is borders ext:\")\n",
    "        #print(ext_gap_upper, ext_gap_lower)\n",
    "        #print(\"this is required\")\n",
    "        #print(diff_required)\n",
    "        \n",
    "        #print(gaps[1]+5 - gaps[0]- 5 + 1)\n",
    "        #implement that we grab CA_IDX GAPS[0] to GAPS[1] instead of idx of atomlist (which can have gaps and then is shifted)\n",
    "        \n",
    "        if len(res_id_lst) == (diff_required):\n",
    "            struc_templates.append(f\"{structure_pdbs}/{pdbs[struc_chosen]}\")\n",
    "            \n",
    "        #print(len(struc_templates))\n",
    "        i += 1\n",
    "        #reset if we get all templates for struc 1\n",
    "        if len(struc_templates) == 3:\n",
    "            i = 0\n",
    "        \n",
    "        \n",
    "    #these we use as templates.    \n",
    "    return struc_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a4ec20b-3651-4829-ba5d-c37f7411d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gap_alignment(structure_pdbs:str, gaps:list):\n",
    "    \n",
    "    \"\"\"This function will make an alignment locally around the gaps and compute RMSD.\n",
    "    IF LOW RMSD: we seal the gap.\n",
    "    IF HIGH RMSD: we cut the gap from all structures\"\"\"\n",
    "    \n",
    "    #structures are all aligned already. now we should do a local small alignment and then compute rmsd.\n",
    "    \n",
    "    gap_dict = defaultdict()\n",
    "    \n",
    "    for gap in gaps:\n",
    "    \n",
    "        ref_strucs = _structures_to_select_as_reference(structure_pdbs, gap)\n",
    "    \n",
    "        onlyfiles = [f for f in os.listdir(structure_pdbs) if os.path.isfile(os.path.join(structure_pdbs, f))]\n",
    "    \n",
    "        pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "        \n",
    "        for j, template in enumerate(ref_strucs):\n",
    "            \n",
    "            #here we store for each template the rmsd\n",
    "            template_rmsd = 0\n",
    "            \n",
    "            #print(template)\n",
    "            \n",
    "            pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "            \n",
    "        \n",
    "            template_structure = pdb_parser.get_structure(\"template\", f\"{template}\")\n",
    "            \n",
    "            template_model = template_structure.get_atoms()\n",
    "            \n",
    "            ext_gap_upper = gap[1] + 5\n",
    "            ext_gap_lower = gap[0] - 5\n",
    "            \n",
    "            \n",
    "            template_atoms = [x for x in template_model if x.get_full_id()[3][1] >= ext_gap_lower \n",
    "                              and x.get_full_id()[3][1] <= ext_gap_upper]\n",
    "            \n",
    "            template_ids = [x.get_full_id()[3][1] for x in template_atoms]\n",
    "            \n",
    "            \n",
    "            #print(template_atoms)\n",
    "            #print(template_ids)\n",
    "            \n",
    "            \n",
    "            template_model_list = [x for x in template_model]  #select atoms from start to finish but they have + 5 on both ends.\n",
    "            \n",
    "            #nested loop.\n",
    "            \n",
    "            #first gap all samples\n",
    "            for i, samples in enumerate(pdbs):\n",
    "                \n",
    "                pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "            \n",
    "                aligner = Bio.PDB.Superimposer()\n",
    "        \n",
    "                sample_structure = pdb_parser.get_structure(\"sample\", f\"{structure_pdbs}/{samples}\")\n",
    "            \n",
    "                sample_model = sample_structure.get_atoms()\n",
    "                \n",
    "                sample_atoms = [x for x in sample_model if x.get_full_id()[3][1] >= ext_gap_lower \n",
    "                        and x.get_full_id()[3][1] <= ext_gap_upper]\n",
    "                \n",
    "                if len(sample_atoms) == len(template_atoms):\n",
    "                    \n",
    "                    #if they are the same we proceed.\n",
    "                    \n",
    "                    #we start procedure of alignment if this is done.\n",
    "                    \n",
    "                    aligner.set_atoms(fixed=template_atoms, moving=sample_atoms)\n",
    "                    \n",
    "                    aligner.apply(sample_atoms)\n",
    "                    #print(template, samples)\n",
    "                    #print(aligner.rms)\n",
    "            \n",
    "                    template_rmsd += aligner.rms\n",
    "        #print(((template_rmsd/i))/j)\n",
    "                    \n",
    "                    \n",
    "        #outer most we compute the average rmsd between all template and all structs.\n",
    "        #j means we average over all templates.\n",
    "        #ref strucs 0 is just the first suitable template.\n",
    "        gap_dict[gap] = (template_rmsd/i)/j\n",
    "        \n",
    "        \n",
    "    #print(gap_dict)\n",
    "    return gap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d00023c0-2b69-4662-848a-8929d9806b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_prot_seq = \"MKFKPNQTRTYDREGFKKRAACLCFRSEQEDEVLLVSSSRYPDQWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGIFENQDRKHRTYVYVLTVTEILEDWEDSVNIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLKLGCSPTNGNSSVPSLPDNNALFVTAAPPSGVPSSIR\"\n",
    "#use_main = True\n",
    "#stop_pos = 176\n",
    "#path_to_pdb = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_181/PCA/original_2duk_A.pdb\"\n",
    "\n",
    "#mini_repair_residues_2(path_to_pdb=path_to_pdb, stop_pos=stop_pos, main_prot_seq=main_prot_seq, use_main=use_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba5030a6-ec58-43da-b42e-723673c104d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repair_artefacts(pdb_basep, pdb_id_target):\n",
    "    \n",
    "    pattern = f\"original_{pdb_id_target}\"\n",
    "    print(pattern)\n",
    "    \n",
    "    # Remove original_****_*.ali file\n",
    "    ali_files = glob.glob(f\"{pattern}.ali\")\n",
    "    for file in ali_files:\n",
    "        os.remove(file)\n",
    "\n",
    "\n",
    "    # Remove original_****_A.pdb\n",
    "    old_pdb_file = f\"{pattern}.pdb\"\n",
    "    if os.path.exists(old_pdb_file):\n",
    "        os.remove(old_pdb_file)\n",
    "        \n",
    "    # Rename original_****_Ax.B99990001.pdb to original_****_A.pdb\n",
    "    repaired_pdb_file = f\"{pattern}x.B99990001.pdb\"\n",
    "    if os.path.exists(repaired_pdb_file):\n",
    "        new_pdb_file = repaired_pdb_file.replace('x.B99990001', '')\n",
    "        shutil.move(repaired_pdb_file, new_pdb_file)\n",
    "\n",
    "    # Remove original_****_*x.D00000001\n",
    "    d_files = glob.glob(f\"{pattern}x.D00000001\")\n",
    "    for file in d_files:\n",
    "        os.remove(file)\n",
    "\n",
    "    # Remove other files\n",
    "    extensions_to_remove = ['.fasta', '.ini', '.rsr', '.sch', '.V99990001']\n",
    "    for ext in extensions_to_remove:\n",
    "        files = glob.glob(f\"{pattern}x{ext}\")\n",
    "        for file in files:\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cba33cf7-9c74-46eb-a5b6-57488f9e76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_repair_residues_2(path_to_pdb:str, stop_pos:int, main_prot_seq:str, use_main:bool):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function repairs structures with gaps less than 7 residues per gap.\n",
    "\n",
    "    Args:\n",
    "    - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "    - stop_pos (int): Stop position.\n",
    "    - main_prot_seq (str): Main protein sequence.\n",
    "    - use_main (bool): Whether to use the main protein.\n",
    "\n",
    "    Output:\n",
    "    Repaired structures.\n",
    "    \"\"\"\n",
    "    log.none()  # no stdout spam\n",
    "    env = Environ()  # setup env for modelling\n",
    "    aln = Alignment(env)  # setup the alignment\n",
    "    mdl = Model(env)  # setup the model\n",
    "    \n",
    "    # current working directory\n",
    "    pdb_id_target = path_to_pdb.split(\"/\")[-1][9:-4]\n",
    "    \n",
    "    pdb_basep = \"/\".join(path_to_pdb.split(\"/\")[:-1])\n",
    "    print(pdb_basep)\n",
    "\n",
    "    # this step needs to be done before calling this function! os.chdir(pdb_basep)\n",
    "    pdb_id_chain = pdb_id_target[5]\n",
    "\n",
    "    print(f\"Using main: {use_main}, PDB target: {pdb_id_target}\")\n",
    "\n",
    "    if use_main:\n",
    "        \n",
    "        fasta_seq = main_prot_seq\n",
    "        \n",
    "    else:\n",
    "\n",
    "        gene_name = get_gene_name_uniprot(f\"{pdb_id_target[0:4]}\")\n",
    "        \n",
    "        get_prot_name = get_uniprot_id(gene_name)\n",
    "        \n",
    "        fasta_seq = get_gene_fasta(get_prot_name)\n",
    "\n",
    "    #pdb_basep}/original_{pdb_id_target}.pdb  current dir .\n",
    "    \n",
    "    # start stop grab:\n",
    "    pdb_parser = PDBParser(QUIET=True)\n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", f\"{pdb_basep}/original_{pdb_id_target}.pdb\")\n",
    "    sample_res = sample_structure.get_residues()\n",
    "    sample_list = [x.get_id()[1] for x in sample_res]\n",
    "    start = sample_list[0]\n",
    "    stop = len(fasta_seq)\n",
    "\n",
    "    print(start, stop)\n",
    "\n",
    "    #works here\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "    \n",
    "    code = f\"original_{pdb_id_target}\"\n",
    "    \n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{pdb_id_chain}\", f\"{stop}:{pdb_id_chain}\"))\n",
    "    \n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "\n",
    "    with open(f\"./original_{pdb_id_target}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">original_{pdb_id_target}x\\n\")\n",
    "        fastaout.write(fasta_seq)\n",
    "\n",
    "    aln_code = f\"original_{pdb_id_target}x\"\n",
    "    \n",
    "    #works\n",
    "    #print(\"it is still working here\")\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    \n",
    "    aln.append(file=f\"./original_{pdb_id_target}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "\n",
    "    # Additional debugging: Print alignment content\n",
    "    #print(\"Alignment content before salign:\")\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "        \n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50), alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    #print(\"Alignment content after salign:\")\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "    \n",
    "    aln.write(file=f\"original_{pdb_id_target}.ali\")\n",
    "\n",
    "    # Debugging: Print the contents of the alignment file\n",
    "    with open(f\"original_{pdb_id_target}.ali\", \"r\") as ali_file:\n",
    "        print(\"Alignment file contents:\")\n",
    "        print(ali_file.read())\n",
    "    \n",
    "    # Debugging: Print ALIGN_CODES(1) and the sequence identifiers in the alignment file\n",
    "    #print(\"here we go\")\n",
    "\n",
    "    #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "\n",
    "    #print(\"Sequence identifiers in the alignment file:\")\n",
    "\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    #print(f\"{pdb_id_target}.ali\")\n",
    "    #print(f\"{pdb_id_target}\")\n",
    "    #print(aln_code)\n",
    "\n",
    "    a = AutoModel(env, alnfile=f\"original_{pdb_id_target}.ali\", knowns=f\"original_{pdb_id_target}\", sequence=aln_code)\n",
    "    \n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "\n",
    "    try:\n",
    "        # Build the model(s)\n",
    "        a.make()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during modeling: {e}\")\n",
    "\n",
    "\n",
    "    #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    remove_repair_artefacts(pdb_basep=pdb_basep, pdb_id_target=pdb_id_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dde0fbdb-9a76-4040-923c-81692a876e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_repair_residues_1(path_to_pdb:str, stop_pos:int, main_prot_seq:str, use_main:bool):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function repairs structures with gaps less than 7 residues per gap.\n",
    "\n",
    "    Args:\n",
    "    - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "    - stop_pos (int): Stop position.\n",
    "    - main_prot_seq (str): Main protein sequence.\n",
    "    - use_main (bool): Whether to use the main protein.\n",
    "\n",
    "    Output:\n",
    "    Repaired structures.\n",
    "    \"\"\"\n",
    "    \n",
    "    log.none()  # no stdout spam\n",
    "    env = Environ()  # setup env for modelling\n",
    "    aln = Alignment(env)  # setup the alignment\n",
    "    mdl = Model(env)  # setup the model\n",
    "    \n",
    "    # current working directory\n",
    "    current_pth = os.getcwd()\n",
    "\n",
    "    pdb_id_target = path_to_pdb.split(\"/\")[-1][9:-4]\n",
    "    pdb_basep = \"/\".join(path_to_pdb.split(\"/\")[:-1])\n",
    "\n",
    "    pdb_id_chain = pdb_id_target[5]\n",
    "\n",
    "    print(f\"Using main: {use_main}, PDB target: {pdb_id_target}\")\n",
    "\n",
    "    if use_main:\n",
    "        \n",
    "        fasta_seq = main_prot_seq\n",
    "        \n",
    "    else:\n",
    "\n",
    "        gene_name = get_gene_name_uniprot(f\"{pdb_id_target[0:4]}\")\n",
    "        \n",
    "        get_prot_name = get_uniprot_id(gene_name)\n",
    "        \n",
    "        fasta_seq = get_gene_fasta(get_prot_name)\n",
    "\n",
    "    #works here\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(f\"{pdb_basep}/{pdb_id_target}\")\n",
    "        \n",
    "    except FileExistsError:\n",
    "        \n",
    "        print(\"Directory already exists\")\n",
    "\n",
    "    os.chdir(f\"{pdb_basep}/{pdb_id_target}\")\n",
    "    \n",
    "    shutil.copy(f\"{pdb_basep}/original_{pdb_id_target}.pdb\", f\"{pdb_basep}/{pdb_id_target}/{pdb_id_target}.pdb\")\n",
    "    \n",
    "    # start stop grab:\n",
    "    pdb_parser = PDBParser(QUIET=True)\n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", f\"{pdb_basep}/{pdb_id_target}/{pdb_id_target}.pdb\")\n",
    "    sample_res = sample_structure.get_residues()\n",
    "    sample_list = [x.get_id()[1] for x in sample_res]\n",
    "    start = sample_list[0]\n",
    "    stop = len(fasta_seq)\n",
    "\n",
    "    print(start, stop)\n",
    "\n",
    "    #works here\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "    \n",
    "    code = f\"{pdb_id_target}\"\n",
    "    \n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{pdb_id_chain}\", f\"{stop}:{pdb_id_chain}\"))\n",
    "    \n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "\n",
    "    with open(f\"./{pdb_id_target}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">{pdb_id_target}x\\n\")\n",
    "        fastaout.write(fasta_seq)\n",
    "\n",
    "    aln_code = f\"{pdb_id_target}x\"\n",
    "    \n",
    "    #works\n",
    "    #print(\"it is still working here\")\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    \n",
    "    aln.append(file=f\"./{pdb_id_target}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "\n",
    "    # Additional debugging: Print alignment content\n",
    "    #print(\"Alignment content before salign:\")\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "        \n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50), alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    #print(\"Alignment content after salign:\")\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "    \n",
    "    aln.write(file=f\"{pdb_id_target}.ali\")\n",
    "\n",
    "    # Debugging: Print the contents of the alignment file\n",
    "    with open(f\"{pdb_id_target}.ali\", \"r\") as ali_file:\n",
    "        print(\"Alignment file contents:\")\n",
    "        print(ali_file.read())\n",
    "    \n",
    "    # Debugging: Print ALIGN_CODES(1) and the sequence identifiers in the alignment file\n",
    "    #print(\"here we go\")\n",
    "\n",
    "    #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "\n",
    "    #print(\"Sequence identifiers in the alignment file:\")\n",
    "\n",
    "    for record in aln:\n",
    "        print(record.code)\n",
    "\n",
    "    #print(os.getcwd())\n",
    "    #print(f\"{pdb_id_target}.ali\")\n",
    "    #print(f\"{pdb_id_target}\")\n",
    "    #print(aln_code)\n",
    "\n",
    "    a = AutoModel(env, alnfile=f\"{pdb_id_target}.ali\", knowns=f\"{pdb_id_target}\", sequence=aln_code)\n",
    "    j = job(host='localhost')\n",
    "    for i in range(12):\n",
    "        j.append(local_slave())\n",
    "    a.use_parallel_job(j)\n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "\n",
    "    try:\n",
    "        # Build the model(s)\n",
    "        a.make()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during modeling: {e}\")\n",
    "\n",
    "    \n",
    "    os.chdir(current_pth)\n",
    "\n",
    "    pdbs_sorted = sorted([f for f in os.listdir(f\"{pdb_basep}/{pdb_id_target}\") if f.endswith(\".pdb\")], key=len)\n",
    "    \n",
    "    print(\"this is pdb sorted:\")\n",
    "    print(pdbs_sorted)\n",
    "    \n",
    "    print(f\"we copy now from {pdb_basep}/{pdb_id_target}/{pdbs_sorted[1]} to: {pdb_basep}/{pdb_id_target}/original_{pdbs_sorted[1]}\")\n",
    "    shutil.copy(f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[1]}\", f\"{pdb_basep}/{pdb_id_target}/original_{pdbs_sorted[1]}\")\n",
    "    \n",
    "    os.rename(f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[1]}\", f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[0]}\")\n",
    "\n",
    "    shutil.copy(f\"{pdb_basep}/{pdb_id_target}/{pdbs_sorted[0]}\", f\"{pdb_basep}/original_{pdbs_sorted[0]}\")\n",
    "    os.chdir(current_pth)\n",
    "    print(f\"we remove : {pdb_basep}/{pdb_id_target}\")\n",
    "    shutil.rmtree(f\"{pdb_basep}/{pdb_id_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d7a31025-3d2a-454a-b260-831b4d78489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _new_cut_nums(start:int, stop:int, gap_dict:dict):\n",
    "    \n",
    "    for keys, vals in gap_dict.items():\n",
    "        start_n = keys[0]\n",
    "        stop_n = keys[1]\n",
    "        \n",
    "        if start_n < start and not start_n < 0:\n",
    "            start = start_n\n",
    "        \n",
    "        if stop_n > stop:\n",
    "            stop = stop_n\n",
    "        \n",
    "        #in case its negative. we make it 1.\n",
    "        if start < 0:\n",
    "            start = 1\n",
    "    \n",
    "    print(start, stop)\n",
    "    return start, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dff54110-4ab9-4267-ab19-27ada84d5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_pdb_test = \"/home/micnag/result_test_struc_align\"\n",
    "\n",
    "\n",
    "#gaps = _gap_localization(\"/home/micnag/result_test_struc_align/2oa0.pdb_aligned.pdb\")\n",
    "#_gap_alignment(path_to_pdb_test, gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9c35760-ed43-48f5-b129-273b1e35a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_pdbs = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "#template = f\"{path_to_pdbs}/3w5b_A_CA.pdb\"\n",
    "\n",
    "#path_to_pdbs = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/\"\n",
    "#template = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/2c88.pdb\"\n",
    "\n",
    "#structure_based_cutting(path_to_pdbs=path_to_pdbs, \n",
    "#                       template=template)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c392d7-c74f-4025-909c-7ee412905661",
   "metadata": {
    "tags": []
   },
   "source": [
    "# msa_file_readin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e60c9425-c4d3-4531-887e-1f6272593ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_msa_file_version_1(path:str):\n",
    "    \n",
    "    \"\"\"Here we need to fetch information from N oligomer alignment files each containing info about 1 structure\n",
    "    being aligned towards the template structure.\"\"\"\n",
    "    \n",
    "    msa_dir = f\"{path}/MSA_dir\"\n",
    "    \n",
    "    only_msa = [f for f in os.listdir(msa_dir) if os.path.isfile(os.path.join(msa_dir, f))]\n",
    "    \n",
    "    #with this version, we need to analyze n - 1 alignment files for n structures with a designated template structure.\n",
    "    \n",
    "    seq_dict = defaultdict(list)\n",
    "    \n",
    "    for files in only_msa:\n",
    "        \n",
    "        headers = []\n",
    "        seqs = []\n",
    "        \n",
    "        #each files needs to be read out.\n",
    "        #each file contains fluffer + 2 sequences. ALL chains against all chains. First seq is always template.\n",
    "        \n",
    "        seq = False\n",
    "        \n",
    "        with open(f\"{msa_dir}/{files}\", \"r\") as msa: \n",
    "            i = 0\n",
    "            for lines in msa:\n",
    "                if lines[0:4] == \"Name\":\n",
    "            \n",
    "                    header_line = lines.split(\"/\")  #split at whitespace \n",
    "                    pdb_code = header_line[-1][0:4]\n",
    "                    \n",
    "                    #this looks good\n",
    "                    \n",
    "                    headers.append(pdb_code)\n",
    "                    \n",
    "                if lines[0:4] == '(\":\"':\n",
    "                    seq = True\n",
    "                    continue\n",
    "                #print(\"this is apparent correct line\")\n",
    "                #print(correct_lines)\n",
    "                \n",
    "                if seq:\n",
    "                    \n",
    "                    lines = lines.replace(\"*\",\"\")\n",
    "                    sequence = lines.replace(\"\\n\",\"\")\n",
    "                    \n",
    "                    tot_seq = len(sequence)\n",
    "                    removed_gap = sequence.replace(\"-\",\"\")\n",
    "                    remov_seq = len(removed_gap)\n",
    "                    \n",
    "                    seqs.append(sequence)\n",
    "                    \n",
    "               \n",
    "            #print(headers)\n",
    "            #print(seqs)\n",
    "            #ugly solution but only 0 and 2 are seq rest is filler. 2 will be the template 0 will be the structure thats superimposed onto\n",
    "            #template\n",
    "            seqs = [seqs[0], seqs[2]]\n",
    "            \n",
    "            #print(headers)\n",
    "            #print(seqs)\n",
    "            \n",
    "            for pdb_codes, seqs in zip(headers, seqs):\n",
    "                #print(pdb_codes, seqs)\n",
    "                seq_dict[pdb_codes].append(seqs)\n",
    "                \n",
    "            \n",
    "            \n",
    "    #print(seq_dict.keys())\n",
    "    \n",
    "\n",
    "    #seq dict will contain n times template seq and n-1 other sequences.\n",
    "    #we will use this dict to cut out proper positions.\n",
    "    \n",
    "    \n",
    "    '''check all gap positions.'''\n",
    "    \n",
    "    gaps_redundant = []\n",
    "    deletion_list = []\n",
    "\n",
    "    \n",
    "    for pdbs, seqs in seq_dict.items():\n",
    "        i = 0   #we count through the whole seq but only add gaps to the gaps_redundant list.\n",
    "        #print(pdbs, seqs)\n",
    "        print(len(seqs))\n",
    "        if len(seqs) > 1:  #thats our template. because all others will have 1 and seqs will have n seqs (corresponding to n comparisons\n",
    "            #between template and pdb)\n",
    "            #for seqlst in seqs:\n",
    "            #    seqlst_without_gaps = seqlst.replace(\"-\",\"\")\n",
    "            #    \n",
    "            #    gaplen = len(seqlst)-len(seqlst_without_gaps)\n",
    "            #    if gaplen > 0.01 * len(seqlst):\n",
    "            #        \n",
    "            #        #we remove the structure:\n",
    "            #        #print(pdbs)\n",
    "            #        #os.remove()\n",
    "            #        continue\n",
    "            #    i = 1\n",
    "            #    for seq in seqlst:\n",
    "            #        if seq == \"-\":\n",
    "            #            if i not in gaps_redundant:\n",
    "            #                gaps_redundant.append(i)\n",
    "            #            \n",
    "            #            i += 1\n",
    "            #            continue\n",
    "            #        else:\n",
    "            #            i += 1\n",
    "            continue\n",
    "                    \n",
    "        else:\n",
    "            #print(seqs)\n",
    "            for seqlst in seqs:\n",
    "                \n",
    "                gapless = seqlst.replace(\"-\",\"\")\n",
    "                withgap = len(seqlst)\n",
    "                withoutgap = len(gapless)\n",
    "                print(withgap, withoutgap)\n",
    "                if (withgap - withoutgap) > 0.10 * withoutgap: #here we can experiment how strict we want to be.\n",
    "                    print(\"this is too much gap\")\n",
    "                    print(pdbs)\n",
    "                    print(withgap, withoutgap)\n",
    "                    deletion_list.append(pdbs)\n",
    "                    continue\n",
    "                for aa in seqlst:\n",
    "                    if aa == \"-\":  #which means there is a gap.\n",
    "                        #print(aa)\n",
    "                        if i not in gaps_redundant:\n",
    "                            gaps_redundant.append(i)\n",
    "                        \n",
    "                        i += 1\n",
    "                        continue\n",
    "                    i += 1\n",
    "    \n",
    "    \n",
    "    gaps_to_remove = sorted(list(set(gaps_redundant)))  #set to make sure there are no duplicates.\n",
    "        \n",
    "    #print(\"apparent gaps to remove\")\n",
    "    #print(gaps_to_remove)\n",
    "    \n",
    "    return gaps_to_remove, deletion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c4e47ca2-6ab0-46a4-8d33-06dd6aac8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#listlen = read_msa_file_version_1(path=path)\n",
    "#print(len(listlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e48fdeb6-9e0f-4433-89f2-a821a3bbcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_deviating_length_strucs(path_to_pdbs:str, template_struc:str):\n",
    "    \n",
    "    \"\"\"This function will take in the pdbs in the directory of interest and remove those that are simply too large.\n",
    "    If they are too small this will also be removed.\n",
    "    All length criteria are based on the selected reference structure.\"\"\"\n",
    "    \n",
    "    \n",
    "    #first the template_struc to set length standard.\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    \n",
    "    prot_path = f\"{path_to_pdbs}/{template_struc}\"\n",
    "    \n",
    "    print(prot_path)\n",
    "    prot_name = \"default\"\n",
    "    \n",
    "    seq_length_ref = 0\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        structure = parser.get_structure(prot_name, prot_path)\n",
    "        \n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                for residue in chain:\n",
    "                    seq_length_ref += 1\n",
    "            \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #now we parse through all of the rest and remove those that are less/more than 20% of the ref struc length.\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(path_to_pdbs) if os.path.isfile(os.path.join(path_to_pdbs, f))]\n",
    "    \n",
    "    pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #we keep track of which is the shortest ref seq because we want to cut down later based on that size\n",
    "    #to end up with uniform lengths.\n",
    "    \n",
    "    shortes_seq_len = seq_length_ref\n",
    "    #now we remove all pdbs that dont fullfill the above criteria.\n",
    "    for pdb in pdbs:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            seq_len = 0\n",
    "            \n",
    "            prot_path = f\"{path_to_pdbs}/{pdb}\"\n",
    "            \n",
    "            prot_name = \"default\"\n",
    "            \n",
    "            structure = parser.get_structure(prot_name, prot_path)\n",
    "        \n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    for residue in chain:\n",
    "                        seq_len += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(pdb, seq_len)\n",
    "            #if this is the case, we remove it.\n",
    "            if seq_len < 0.9 * seq_length_ref or seq_len > 1.1 * seq_length_ref:  #here we can experiment how strict we want to be.\n",
    "                \n",
    "                print(f\"we remove {pdb} because its seq_len is: {seq_len} and the refsef is : {seq_length_ref}\")  \n",
    "                os.remove(f\"{path_to_pdbs}/{pdb}\")\n",
    "                \n",
    "            else:\n",
    "                #check if its shorter than the current shortest seq . This block only counts those that are not rejected\n",
    "                #i.e nothing thats 80% or less than refseq.\n",
    "                if seq_len < shortes_seq_len:\n",
    "                    shortes_seq_len = seq_len\n",
    "                \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "    return shortes_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "52c8e085-7c93-4443-ab54-4313f384357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mutate_non_standard_aa_1(path_to_pdb:str,\n",
    "                           non_standard_residue:str,\n",
    "                           residue:int,\n",
    "                           chain:str):\n",
    "    \n",
    "    \"\"\"Mutates a non-standard amino acid in a PDB file.\"\"\"\n",
    "    \n",
    "    path_to_script = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_mutate.py\"\n",
    "    path_to_error_script = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_delresname.py\"\n",
    "    path_to_tidy = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_tidy.py\"\n",
    "\n",
    "    input_file = f\"{path_to_pdb}_new\"\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, \"w\") as pdb_out:\n",
    "            bash_code = f\"python {path_to_script} {path_to_pdb} {chain} {residue} {non_standard_residue} ALA\"\n",
    "            run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        resulting_success = True\n",
    "\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(\"Attempting to delete non-standard residue...\")\n",
    "        \n",
    "        with open(input_file, \"w\") as pdb_out:\n",
    "            bash_code = f\"python {path_to_error_script} -{non_standard_residue} {path_to_pdb}\"\n",
    "            run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "    #now we tidy the file to adhere to the most common pdb standard\n",
    "    try:\n",
    "        with open(path_to_pdb, \"w\") as pdb_out:\n",
    "            bash_code = f\"python {path_to_tidy} {input_file}\"\n",
    "            run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        print(\"Cleaned the file. Outfile is at\", path_to_pdb)\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "    #we dont need the intermediate file that was only created to prevent read/write from same file.\n",
    "    try:\n",
    "        os.remove(input_file)  # Remove intermediate file\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    return resulting_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a56d3-df7b-44e6-ab8e-aa7b919c6f75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PRE PCA PROCESSING / FETCH PROPER RESIDUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "026065dd-3434-4f73-a810-2dae424226f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_structures_pca(remove_pos:list, path_to_strucs:str,\n",
    "                       max_seq_len:str, delete_pdbs:str):\n",
    "    \n",
    "    '''The kept positions are based on str position (not residue pos) so we need to\n",
    "    change each structure to 1 based residue counting \n",
    "    and then select ONLY those residues that are NOT in the remove positions range'''\n",
    "    \n",
    "    #renumber first all structures.\n",
    "    \n",
    "    pdbs = []\n",
    "    \n",
    "    with open(f\"{path_to_strucs}/chain_list.txt\", \"r\") as chain_file:\n",
    "        for lines in chain_file:\n",
    "            if lines != \"\\n\":\n",
    "                #this will contain files as \"4r3d.pdb\"\n",
    "                pdb = lines.replace(\"\\n\",\"\")\n",
    "                \n",
    "                if pdb[0:4] in delete_pdbs:\n",
    "                    print(f\"{path_to_strucs}/{pdb}\")\n",
    "                    os.remove(f\"{path_to_strucs}/{pdb}\")  # we remove it if its deemed bad.\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    pdbs.append(pdb)\n",
    "    \n",
    "    \n",
    "    #now we rechain copies.\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{path_to_strucs}/PCA\")\n",
    "    except Exception as error:\n",
    "        print(f\"{path_to_strucs}/PCA already exists\")\n",
    "    \n",
    "    \n",
    "    #loop over pdb list\n",
    "    \n",
    "    #seems to work.\n",
    "    \n",
    "    #required for the script to work.\n",
    "    \n",
    "    '''This block below needs to be tested.'''\n",
    "    \n",
    " \n",
    "    #first renumbering! because gap pos are 1 counted and start in all structures with 1 (e.g even if structure atom is normally positon 32 it will start with 1.\n",
    "    \n",
    "    pdb_reres_path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_reres.py\"\n",
    "    \n",
    "    for pdb_entries in pdbs:\n",
    "        \n",
    "        bash_cmd = f\"python {pdb_reres_path} -1 {path_to_strucs}/{pdb_entries}\"\n",
    "                \n",
    "        bash_cmd_rdy = bash_cmd.split()\n",
    "        \n",
    "        try:\n",
    "            with open(f\"{path_to_strucs}/PCA/{pdb_entries}\", \"w\") as fh_tmp:\n",
    "                result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                         universal_newlines=True)\n",
    "            \n",
    "            print(f\"we renumbered {pdb_entries}\")\n",
    "        \n",
    "        except Exception as error:\n",
    "            print(f\"we did not renumber {pdb_entries}\")\n",
    "        \n",
    "            print(error)\n",
    "    \n",
    "    #first remove the HET Atoms.\n",
    "    for pdb_entries in pdbs:\n",
    "        \n",
    "        non_canonical = remove_hetero_atoms_1(pdb_file=f\"{pdb_entries}\", path=f\"{path_to_strucs}/PCA\")\n",
    "        \n",
    "        #this means we have something special that needs to be taken care of.\n",
    "        \n",
    "        if non_canonical:\n",
    "            \n",
    "            #loop over dict that contains all positions that deviate from standard.\n",
    "            \n",
    "            #structure of dict is : key: position value: tuple: (aa_type, chain)\n",
    "            print(\"This is non canonical:\")\n",
    "            print(non_canonical)\n",
    "            \n",
    "            \n",
    "            remove_targets = []\n",
    "            fulllst = []\n",
    "            for keys, vals in non_canonical.items():\n",
    "                print(keys)\n",
    "                if keys in remove_targets:\n",
    "                    continue\n",
    "                else:\n",
    "                    remove_targets.append(keys)\n",
    "                    fulllst.append((keys, vals[0], vals[1]))\n",
    "\n",
    "            for (pos, hits, chain) in fulllst:\n",
    "                \n",
    "                print(pos, hits, chain)\n",
    "                #call function iteratively to replace step by step.\n",
    "                result = _mutate_non_standard_aa_1(path_to_pdb=f\"{path_to_strucs}/PCA/{pdb_entries}\",\n",
    "                       non_standard_residue=hits,residue=pos,chain = chain)\n",
    "                \n",
    "                if result == False:\n",
    "                    print(f\"we did not manage to delete and so we need to introduce a gap at {pos}\")\n",
    "                    #if we could not repair and were forced to delete it we append this and take it as a gap. otherwise its fine if it got \"repaired\" to ALA\n",
    "                    remove_pos.append(pos)\n",
    "                    \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(remove_pos)\n",
    "    \n",
    "\n",
    "    #here ends removal of het atoms.\n",
    "    \n",
    "    #pdb_reres_path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_reres.py\"\n",
    "    #\n",
    "    #for pdb_entries in pdbs:\n",
    "    #    \n",
    "    #    bash_cmd = f\"python {pdb_reres_path} -1 {path_to_strucs}/{pdb_entries}\"\n",
    "    #            \n",
    "    #    bash_cmd_rdy = bash_cmd.split()\n",
    "    #    \n",
    "    #    with open(f\"{path_to_strucs}/PCA/{pdb_entries}\", \"w\") as fh_tmp:\n",
    "    #        result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "    #                     universal_newlines=True)\n",
    "    #        \n",
    "            \n",
    "        \n",
    "    #now only select those residues that we need.\n",
    "    \n",
    "    \n",
    "    print(\"max seq len is:\")\n",
    "    print(max_seq_len)\n",
    "    \n",
    "    #we use this class and overwrite it for our purpose.\n",
    "    class ResidueSelect(Bio.PDB.Select):\n",
    "        def accept_residue(self, res):\n",
    "            #we accept all residues that are in the kept-position list\n",
    "            if res.id[1] in remove_pos or res.id[1] > max_seq_len:\n",
    "                return False\n",
    "            #but we reject those that are not shared between all structures.\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    \n",
    "    #now we select only those that we need and overwrite existing files.\n",
    "    for pdb_entries in pdbs:\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "                \n",
    "        prot_name = f\"default\"\n",
    "                \n",
    "        #open the correct pdb and rechain it.\n",
    "        structure_template = parser.get_structure(prot_name, \n",
    "                            f\"{path_to_strucs}/PCA/{pdb_entries}\")\n",
    "        \n",
    "        io = PDBIO()\n",
    "            \n",
    "        io.set_structure(structure_template)\n",
    "            \n",
    "        io.save(f\"{path_to_strucs}/PCA/{pdb_entries}\", ResidueSelect())\n",
    "        \n",
    "    #this return will contain the number of CAs required for the next step.\n",
    "    #second part of tuple contains the number of pdb structures in the ensemble.\n",
    "    \n",
    "    #pdbs 0 = ref struc.\n",
    "    \n",
    "    print(\"this is pdbs:\")\n",
    "    print(pdbs)\n",
    "    \n",
    "    #if its 0 we dont care any further.\n",
    "    if len(pdbs) == 0:\n",
    "        return None\n",
    "    \n",
    "    return pdbs[0] #else we return the template.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "121afd42-dbd4-4640-aa97-2beb94f0b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#max_seq_len = 10000\n",
    "#prep_structures_pca(remove_pos=[], path_to_strucs=path,\n",
    "#                       max_seq_len= max_seq_len, delete_pdbs=\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e40a9941-604f-4a78-8924-ec01f675096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "#rem_pos = [1,2,3]\n",
    "#path_to_struc = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AACS_HUMAN/trimer/pos_16_671\"\n",
    "\n",
    "#max_seq_len = 1000\n",
    "\n",
    "\n",
    "#prep_structures_pca(path_to_strucs=path_to_struc,max_seq_len=max_seq_len, remove_pos=rem_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5e57a63b-7a91-4c48-81ee-37a8282e18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_gaps = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 27, 28, 32, 33, 35, 36, 37, 38, 47, 49, 50, 51, 52, 53, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 97, 98, 99, 100, 101, 122, 123, 124, 125, 148, 149, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368]\n",
    "\n",
    "path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/FAK1_HUMAN/monomer/pos_410_689\"\n",
    "\n",
    "chains = \"A\"\n",
    "\n",
    "ref_struc_pdb = \"2v7a\"\n",
    "\n",
    "min_seq_len = 241\n",
    "                \n",
    "#then we will prepare those structures, cut gaps and will be left with uniform length\n",
    "#pdbs that have 1:1 correspondence between each residue against all pdbs.\n",
    "#ref_struc_pdb_id = prep_structures_pca(path_to_strucs=path, \n",
    "#                        remove_pos=remove_gaps,\n",
    "#                        max_seq_len = min_seq_len) #min_seq_len is length of shortes structure in the ensemble.\n",
    "                \n",
    "                \n",
    "#chains_of_ref = _get_chain_labels(path, ref_struc_pdb_id)\n",
    "                \n",
    "#dir_path_pca = f\"{path}/PCA/\"\n",
    "                \n",
    "                \n",
    "#PCA_domenico_new(path_to_pdbs=dir_path_pca,\n",
    "#                ref_struc_pdb_id=ref_struc_pdb_id[0:4],\n",
    "#                chains_of_reference_struc=chains_of_ref)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9ac14-2d1d-4ddc-abe6-921345a1b65a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA PLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "43b04234-095f-4c87-a344-fd6c80a394c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_domenico_new(path_to_pdbs:str, ref_struc_pdb_id:str,\n",
    "                    chains_of_reference_struc:str):\n",
    "    \n",
    "    \n",
    "    \"\"\"This function will take in the input directory with the pdb files that\n",
    "    are going to be used for PCA.\n",
    "    Domenicos pipeline consisting of 3 scripts needs to be MOVED around into the working dir\n",
    "    and then removed back after the job is done.\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    path_to_pdbs: the directory that contains the pdbs.\n",
    "    ref_struc_pdb_id: the 4 digit code OF THE REFERENCE STRUCTURE.\n",
    "    \n",
    "    exe are:\n",
    "    \n",
    "    + add_chain_label_ali\n",
    "    + run_pca_domenico\n",
    "    + write_CA\n",
    "    \n",
    "    the shellscript that combined them is:\n",
    "    + PCA_pipeline.sh\n",
    "    \n",
    "    Additional files required there are:\n",
    "    \n",
    "    + pdbs\n",
    "    + ensemble.txt\n",
    "    \n",
    "    All work needs to be done in the same directory (i.e the working dir) and we need to os.chdir() to this\n",
    "    dir and afterwords os.chdir() back.\n",
    "    \n",
    "    Execution:\n",
    "\n",
    "    bash PCA_pipeline.sh INPUT1 INPUT2\n",
    "\n",
    "    INPUT1 : pdb id (4 digits) of reference structure\n",
    "    INPUT2 : chains id (max 100 characters) of reference structure\n",
    "\n",
    "    INPUT1 and INPUT2 need to be the same as in the first row of the \"ensemble.txt\" file\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "          \n",
    "    print(\"we are inside PCA_domenico_new\")\n",
    "          \n",
    "        \n",
    "    \n",
    "    #check first which structures are present:\n",
    "    #get_rcsb_info(path=path_to_pdbs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"chains:{chains_of_reference_struc}\")\n",
    "    \n",
    "    print(f\"ref_struc_pdb:{ref_struc_pdb_id}\")\n",
    "          \n",
    "          \n",
    "    #first we need to make the working dir.\n",
    "    \n",
    "    basepath = \"/home/micnag/bioinformatics/domenico_pca\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        #move executables to this new location.\n",
    "        \n",
    "        os.chdir(f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/add_chain_label_ali\",f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/write_CA\",f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/run_pca_domenico\",f\"{path_to_pdbs}\")\n",
    "        shutil.copy(f\"{basepath}/PCA_pipeline.sh\",f\"{path_to_pdbs}\")\n",
    "    \n",
    "        #change into new working dir.\n",
    "        os.chdir(f\"{path_to_pdbs}\")\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        os.chdir(f\"{path_to_pdbs}\")\n",
    "        \n",
    "    scriptcall = f\"./PCA_pipeline.sh\"\n",
    "    \n",
    "    \n",
    "    #first we remove all deviating CA files (in case there are any\n",
    "    _remove_deviating_ca(path_to_pdbs=path_to_pdbs,\n",
    "                        ref_struc_pdb_id=ref_struc_pdb_id)\n",
    "    \n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(f\"{path_to_pdbs}/\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/\", f))]\n",
    "    only_pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    \n",
    "    #we need to create ensemble.txt which is required by the pipeline.\n",
    "    #format : each line : <4 digit pdb> <chain(s)>\n",
    "    with open(f\"{path_to_pdbs}/ensemble.txt\", \"w\") as ensemble_txt:\n",
    "        for pdb_ids in only_pdbs:\n",
    "            pdb_id = pdb_ids[:-4]  #we just need the 4 digit ids. \n",
    "            chain = chains_of_reference_struc #this has to be the same as our reference structure.\n",
    "            \n",
    "            append_str = pdb_id + \" \" + chain + \"\\n\"\n",
    "            print('append string is:', append_str)\n",
    "            ensemble_txt.write(append_str)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(os.getcwd())\n",
    "    \n",
    "    print(\"these files are present in the directory:\")\n",
    "    \n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(f\"{path_to_pdbs}/\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/\", f))]\n",
    "    \n",
    "    print(onlyfiles)\n",
    "    \n",
    "    bash_cmd = f\"{scriptcall} {ref_struc_pdb_id} {chains_of_reference_struc}\"\n",
    "    \n",
    "    print(bash_cmd)\n",
    "    \n",
    "    bash_cmd_rdy = bash_cmd.split()\n",
    "    \n",
    "    try:\n",
    "        result = run(bash_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                 universal_newlines=True)\n",
    "            \n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "            \n",
    "    except:\n",
    "        print(f\"Script did not work. Here are the supplied parameters \\n{bash_cmd_rdy}\")\n",
    "        \n",
    "    #if it worked we restore the previous status quo:\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        #remove them from working dir back to normal storage dir.\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/add_chain_label_ali\")\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/write_CA\")\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/run_pca_domenico\")\n",
    "        os.remove(f\"{path_to_pdbs}/PCA/PCA_pipeline.sh\")\n",
    "        \n",
    "        #change back into previous work dir.\n",
    "        os.chdir(f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs\")\n",
    "        \n",
    "    except Exception as error:\n",
    "        \n",
    "        os.chdir(f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs\")\n",
    "        print(\"Something did not work out at reshuffling .exe back into their former location.\")\n",
    "        print(error)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82756aa8-4f9b-4e71-8f38-e382e15cccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_deviating_ca(path_to_pdbs:str, ref_struc_pdb_id:str):\n",
    "    \"\"\"helper function to remove deviating C-Alpha number files.\"\"\"\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(f\"{path_to_pdbs}/\") if os.path.isfile(os.path.join(f\"{path_to_pdbs}/\", f))]\n",
    "    only_pdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    \n",
    "    print(f\"this is template path: {path_to_pdbs}/{ref_struc_pdb_id}.pdb\")\n",
    "    template_path = f\"{path_to_pdbs}/{ref_struc_pdb_id}.pdb\"\n",
    "        \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = \"noname\"\n",
    "    structure = parser.get_structure(prot_name, template_path)\n",
    "    \n",
    "    i = 0\n",
    "    for res in structure.get_atoms():\n",
    "        #we only count CA. if structures deviate from that.. remove them.\n",
    "        if res.get_id() == \"CA\":\n",
    "            i += 1\n",
    "    \n",
    "    \n",
    "    for pdbs in only_pdbs:\n",
    "        \n",
    "        try:\n",
    "            parser = PDBParser(QUIET=True)\n",
    "    \n",
    "            prot_name = \"noname\"\n",
    "            structure = parser.get_structure(prot_name, f\"{path_to_pdbs}/{pdbs}\")\n",
    "        \n",
    "            j = 0\n",
    "            for res in structure.get_atoms():\n",
    "            #we only count CA. if structures deviate from that.. remove them.\n",
    "                if res.get_id() == \"CA\":\n",
    "                    j += 1\n",
    "        \n",
    "            #this means it has different CA\n",
    "            if j != i:\n",
    "                print(pdbs)\n",
    "                print(j)\n",
    "                os.remove(f\"{path_to_pdbs}/{pdbs}\")\n",
    "                \n",
    "                \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "            #if we cant open it.. its faulty. remove it.\n",
    "            os.remove(f\"{path_to_pdbs}/{pdbs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "21139ed7-df47-4415-92a4-5bacd1426656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalize this plot function.\n",
    "\n",
    "def _plot_PCA(path_to_PCA:str):\n",
    "    \n",
    "    \"\"\"Quick function to plot the first 2 PC and their corresponding explained variances.\"\"\"\n",
    "    \n",
    "    expl_var = f\"{path_to_PCA}/pc_variances.txt\"\n",
    "    pca = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    structure_path = f\"{path_to_PCA}/ensemble.txt\"\n",
    "    \n",
    "    structures = []\n",
    "    \n",
    "    title_path = path_to_PCA.split(\"/\")\n",
    "    gene = title_path[-4]\n",
    "    oligomer = title_path[-3]\n",
    "    position = title_path[-2]\n",
    "    \n",
    "    \n",
    "    expl_var_vals = []\n",
    "    with open(expl_var, \"r\") as var:\n",
    "        for entries in var:\n",
    "            entries = entries.replace(\"\\n\",\"\")\n",
    "            expl_var_vals.append(float(entries))\n",
    "    \n",
    "    \n",
    "    with open(structure_path, \"r\") as struc_in:\n",
    "        for lines in struc_in:\n",
    "            pdb = lines.replace(\"\\n\",\" \")\n",
    "            pdb = pdb[0:4]\n",
    "            structures.append(pdb)\n",
    "            \n",
    "    #we compute the first 10.\n",
    "    PCA = defaultdict(list)\n",
    "    \n",
    "    with open(pca, \"r\") as pc_read:\n",
    "        for lines in pc_read:\n",
    "            #lets collect each column as a separate list\n",
    "            \n",
    "            #each member in the list will correspond to a structure and its associated PC value.\n",
    "            for i in range(0,10):\n",
    "                PCA[i+1].append(float(lines.split()[i]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot(PCA[1], PCA[2], \"o\")\n",
    "    \n",
    "    zipped_PCA = list(zip(PCA[1],PCA[2]))\n",
    "    \n",
    "    \n",
    "    accept_lst = [\"3w5b\", \"2oa0\", \"6hxb\"]\n",
    "\n",
    "    for i, (x,y) in enumerate(zipped_PCA):\n",
    "        \n",
    "        label = \"{}\".format(structures[i])\n",
    "        \n",
    "        if label in accept_lst:\n",
    "            print(label)\n",
    "            plt.annotate(label, # this is the text\n",
    "                 (x, y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(-10,-20), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "            print(x,y)\n",
    "            plt.plot(x,y , \"o\",color=\"red\")\n",
    "            \n",
    "    plt.title(f\"{gene}\\n{oligomer} {position}\")\n",
    "    plt.xlabel(f\"PCA 1 (variance {expl_var_vals[0]}%)\")\n",
    "    plt.ylabel(f\"PCA 2 (variance {expl_var_vals[1]}%)\")\n",
    "    \n",
    "    \n",
    "    plt.savefig(f\"{path_to_PCA}/PC_plot2.png\")\n",
    "    plt.clf()\n",
    "\n",
    "#path_to_PCA = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"   \n",
    "    \n",
    "#_plot_PCA(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "58eb5226-42c5-4ec8-b738-702299b13e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_PCA = \"/home/micnag/bioinformatics/test/test_pca\"   \n",
    "    \n",
    "#_plot_PCA(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "376712de-5844-463a-8b4b-b6ba2adf2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porcupine_plot(path_to_PCA:str):\n",
    "    \"\"\"Function to create porcupine plots\"\"\"\n",
    "    \n",
    "    \n",
    "    path_to_pca_comp = f\"{path_to_PCA}/PCs.txt\"\n",
    "    \n",
    "    path_to_pca_proj = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    pca_components = np.loadtxt(path_to_pca_comp, usecols=[0,1])\n",
    "    pca_scores = np.loadtxt(path_to_pca_proj, usecols=[0,1])\n",
    "    \n",
    "    normalized_pca_components = pca_components / np.linalg.norm(pca_components, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    #print(normalized_pca_components[0:10])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_scores[:, 0], pca_scores[:, 1], color='blue', label='Data Points')\n",
    "\n",
    "    for i in range(len(pca_scores)):\n",
    "        x0, y0 = pca_scores[i]\n",
    "        dx, dy = normalized_pca_components[i]\n",
    "        plt.arrow(x0, y0, dx, dy, color='red', width=0.02, head_width=0.1, length_includes_head=True)\n",
    "    \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Porcupine Plot - PCA Data')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ab61ac4d-aa0d-4d44-be4a-a2a8d5beba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "\n",
    "#porcupine_plot(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df1de9-a981-4092-8400-57b425971544",
   "metadata": {
    "tags": []
   },
   "source": [
    "# REPAIR STATION MODELLER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0827c39c-34ce-4eb8-bb49-51151a9e7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursive helper function to measure gaps\n",
    "def _catchup(idx:int, resseq:int, start:int):\n",
    "    \n",
    "    '''This helper function will start iteratively catching up to the gap by incrementing\n",
    "    each iteration by 1 until the idx matches the current resseq\n",
    "    The amount of iterations needed to get back to the resseq is returned together\n",
    "    with the start and end of the gap.'''\n",
    "    \n",
    "    #if we have a gap.\n",
    "    if idx != resseq:\n",
    "        #close gapsize by 1 and recall function recursively.\n",
    "        idx += 1\n",
    "        return _catchup(idx,resseq, start) #needs function call otherwise return:None\n",
    "    \n",
    "    #if we sealed the gap we return the report.\n",
    "    return(idx, resseq, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fa8add00-b42d-4a9a-82dc-a354dd8e90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gaps_single_struc(path:str, report=True)-> dict:\n",
    "    ''' This function should take all pdbs (already removed from het atoms)\n",
    "        and sort them according to a fixed max_gap parameter.\n",
    "        If this gap length is exceeded, we trash the structure.\n",
    "        Otherwise, we separate in two folders:\n",
    "        structures that are completely intact and those that need repair\n",
    "        but have gaps smaller than max_gap.\n",
    "        If report = True we will also write out a report documenting the gaps.'''\n",
    "    \n",
    "    #setup\n",
    "    #set path to folderpath\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_path = path.split(sep=\"/\")\n",
    "    \n",
    "    #last entry of path is 4r23_A_0.pdb. so we need 4r23_A_0 \n",
    "    prot_name = prot_path[-1][:-4]\n",
    "    \n",
    "    '''main part'''\n",
    "    #print(onlyfiles)\n",
    "    \n",
    "    gap_dict = defaultdict(str)\n",
    "    \n",
    "\n",
    "    fullpath = f\"{path}\"   # ../P0633/1c0f.pdb\n",
    "    \n",
    "    try:\n",
    "        structure = parser.get_structure(prot_name, fullpath)\n",
    "        \n",
    "        #we will return the gaps list at the end of function (explained there at return)\n",
    "        gaps = []\n",
    "        start_res = []\n",
    "        '''Just used to grab the first residue in each chain!'''\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                chains = chain.get_residues()\n",
    "                for residue in chains:\n",
    "                    tmp = residue.get_full_id()[3][1]\n",
    "                    start_res.append(tmp)\n",
    "                    break\n",
    "       \n",
    "        i = 0 #we iterate through the list of start_res given that we can have many chains!\n",
    "        # e.g each chain can start with a different residue... e.g pos 5 pos 10 ect.\n",
    "        for model in structure:\n",
    "            \n",
    "            for chain in model:\n",
    "                \n",
    "                #we store the chain id for later purpose of repairing\n",
    "                chain_id = chain.get_full_id()[2]\n",
    "                \n",
    "                idx = start_res[i] #get first residue position\n",
    "                \n",
    "                i += 1 #shift pointer by 1 to get next start residue pos in next chain iteration.\n",
    "                \n",
    "                \"\"\"THIS PART NEEDS MAJOR OVERHAUL!\"\"\" #done mostly\n",
    "                for res in chain.get_residues():\n",
    "                \n",
    "                    resseq = res.get_full_id()[3][1]\n",
    "                    #print(idx, resseq)\n",
    "                    \n",
    "                    if idx != resseq: #this means we have a gap!\n",
    "\n",
    "                        if idx > resseq:\n",
    "                            continue\n",
    "                        #store start of gap for later return\n",
    "                        start = idx\n",
    "                    \n",
    "                        #we try to catch it then\n",
    "                        res = _catchup(idx,resseq, start) #recursive call\n",
    "                    \n",
    "                        diffval = res[0]-res[2] #diff between end of gap and start of gap = gaplength\n",
    "                        \n",
    "                        gaps.append((chain_id, res[2]-1, res[0], diffval)) # end - start\n",
    "                        #print(gaps)\n",
    "                        idx = res[0]+1 #update to bring idx back to current resseq\n",
    "                        continue\n",
    "                    \n",
    "                    idx += 1\n",
    "                    if idx < resseq:\n",
    "                        print(f\"idx is {idx}, resseq is {resseq}\")\n",
    "                        break\n",
    "                        \n",
    "            gap_dict[prot_name] = gaps\n",
    "    except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "        \n",
    "    return gap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ce323029-996e-42c4-80f9-54c57550b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_viability(gap_dict:dict, max_gap=7):\n",
    "    \n",
    "    \"\"\"Quick check if the pdb can be salvaged or if it should be left as it is.\"\"\"\n",
    "    \n",
    "    gap_lengths = []\n",
    "    \n",
    "    for pdb, gaps in gap_dict.items():\n",
    "        \n",
    "        if len(gaps) == 0:\n",
    "            return False\n",
    "        for gap in gaps:\n",
    "            single_gap = gap[3]\n",
    "            gap_lengths.append(single_gap)\n",
    "            \n",
    "    gap_lengths_sort = sorted(gap_lengths, reverse=True)\n",
    "    \n",
    "    #if we find that the max gap is too large we return False else True\n",
    "    return False if gap_lengths_sort[0] > max_gap else True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d033d-9543-4ac3-abdf-bda1e48b7a50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## REPAIR\n",
    "\n",
    "+ build model from fasta files\n",
    "+ rechain new model \n",
    "+ cut according to residues that we need (i.e remove N and C-terminal overhangs, which is only spaghetti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b112a4b6-b1f3-4852-8d49-e28cfc357ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _start_stop_fasta(pdb_id_target:str, path:str):\n",
    "    \n",
    "    \"\"\"This function reads in a structure and returns start / end of it to limit the seq which \n",
    "    will be used to rebuild gapped structures\"\"\"\n",
    "    \n",
    "\n",
    "    fullpath = f\"{path}/{pdb_id_target}.pdb\"\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    protname = pdb_id_target\n",
    "    chain_corr = pdb_id_target[5:6]\n",
    "    \n",
    "    #chain_corr = \"A\"  #THIS IS ALWAYS A BECAUSE MODELLER MAKES ALL RESULT FILES BE CHAIN A. DONT CHANGE IT TO WHATEVER CHAIN IT REALLY IS.\n",
    "   \n",
    "    structure = parser.get_structure(protname, fullpath)\n",
    "    \n",
    "    positions = []\n",
    "    seq = []\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            if chain.get_id() == chain_corr:\n",
    "                for residues in chain:\n",
    "                    res_num = residues.get_id()[1]\n",
    "                    if res_num > 0:\n",
    "                        positions.append(res_num) #we discard negative entries\n",
    "                        seq.append(residues.get_resname())\n",
    "    \n",
    "    start, stop = positions[0], positions[-1]\n",
    "    #gives back start and stop.\n",
    "    return start, stop, chain_corr\n",
    "\n",
    "#_start_stop_fasta(pdb_id_target=\"2duk_A_0.pdb\", path=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "45d83b5c-eba9-476f-acc5-e825bf1235a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _renumber_struc(path_to_struc:str,\n",
    "                    start_num:str, \n",
    "                    chain:str):\n",
    "    \n",
    "    #renumbering fresh struc:\n",
    "    \n",
    "    shiftres_location = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/pdb_shiftres_by_chain.py\"\n",
    "    \n",
    "    #rechain again.\n",
    "    parser = PDBParser(QUIET=True)\n",
    "                \n",
    "    prot_name = f\"default\"\n",
    "                \n",
    "    #open the correct pdb and rechain it.\n",
    "    structure_template = parser.get_structure(prot_name, path_to_struc)\n",
    "        \n",
    "    new_chain = chain \n",
    "        \n",
    "    for models in structure_template:\n",
    "        for chains in models:\n",
    "                \n",
    "            chains.id = \"_\"\n",
    "            \n",
    "            chains.id = new_chain\n",
    "            \n",
    "            io = PDBIO()\n",
    "            \n",
    "            io.set_structure(structure_template)\n",
    "            \n",
    "            io.save(path_to_struc)\n",
    "    \n",
    "    #we continue\n",
    "    \n",
    "    print(\"This is path, start num and chain\")\n",
    "    print(f\"{path_to_struc}\")\n",
    "    \n",
    "    print(f\"{start_num}\")\n",
    "    print(f\"{chain}\")\n",
    "    \n",
    "    bash_cmd = f\"python {shiftres_location} {path_to_struc} {start_num} {chain}\"\n",
    "    \n",
    "    bash_cmd_rdy = bash_cmd.split()\n",
    "    \n",
    "    with open(f\"{path_to_struc}_tmp\", \"w\") as fh_tmp:\n",
    "        result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "             universal_newlines=True)\n",
    "    \n",
    "    #now replace the original one with the temp file.\n",
    "    os.replace(f\"{path_to_struc}_tmp\", f\"{path_to_struc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "562c1588-2bc4-46e9-8368-fd9715671362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut_proper_models(path_to_struc:str,\n",
    "                   start_num:str, end_num:str,\n",
    "                       chain:str):\n",
    "    \n",
    "    \n",
    "    class ResidueSelect(Bio.PDB.Select):\n",
    "        def accept_residue(self, res):\n",
    "            #we accept all residues that are within start and stop \n",
    "            if res.id[1] >= int(start_num) and res.id[1] <= int(end_num) and res.parent.id == new_chain:  #i changed chain to new_chain\n",
    "                return True\n",
    "            #but we reject those that are before (N-terminal overhang) or after (C-terminal overhang)\n",
    "            #these are anyhow just spaghetti model.\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    #rechain again.\n",
    "    parser = PDBParser(QUIET=True)\n",
    "                \n",
    "    prot_name = f\"default\"\n",
    "                \n",
    "    #open the correct pdb and rechain it.\n",
    "    structure_template = parser.get_structure(prot_name, path_to_struc)\n",
    "        \n",
    "    new_chain = chain \n",
    "        \n",
    "    for models in structure_template:\n",
    "        for chains in models:\n",
    "                \n",
    "            chains.id = \"_\"\n",
    "            \n",
    "            chains.id = new_chain\n",
    "            \n",
    "            io = PDBIO()\n",
    "            \n",
    "            io.set_structure(structure_template)\n",
    "            \n",
    "            io.save(path_to_struc, ResidueSelect())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea38450-c87e-4dd4-93dc-807798d450f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### WE NEED TO UPDATE THE FASTA PROCESSING IN ORDER TO INCLUDE ALL CHAINS PROPERLY.\n",
    "\n",
    "Needs to take chain A and B separately in order to not mess up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0fb59-c874-479d-a54c-827604d7b5fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ALL OLIGOMER PIPELINE \n",
    "(PCA prep + PCA + PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "663fc75a-6000-4851-ad4d-548b93c8b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_oligomer_runs(path:str, min_seq_len_dict:dict):\n",
    "    \n",
    "    \"\"\"This function basically will call a bunch of other functions \n",
    "    for each potential oligomeric state and each position within them.\"\"\"\n",
    "    \n",
    "    oligodirectories = [\"monomer\",\n",
    "                        \"dimer\",\n",
    "                        \"trimer\",\n",
    "                        \"tetramer\",\n",
    "                        \"pentamer\",\n",
    "                        \"hexamer\",\n",
    "                        \"heptamer\",\n",
    "                        \"oktamer\",\n",
    "                        \"nonamer\",\n",
    "                        \"decamer\",\n",
    "                        \"undecamer\",\n",
    "                        \"dodecamer\",\n",
    "                        \"tridecamer\",\n",
    "                        \"tetradecamer\",\n",
    "                        \"pentadecamer\",\n",
    "                        \"hexadecamer\",\n",
    "                        \"heptadecamer\",\n",
    "                        \"oktadecamer\",\n",
    "                        \"nonadecamer\",\n",
    "                        \"eicosamer\"\n",
    "    ]\n",
    "    \n",
    "    relevant_dirs = []\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        if file in oligodirectories:\n",
    "            d = os.path.join(path, file)\n",
    "            relevant_dirs.append(d)\n",
    "            \n",
    "    \n",
    "    print(\"this is relevant dirs\")\n",
    "    #now we go through each of those oligomer dirs and for each position:\n",
    "    \n",
    "    \n",
    "    #lets see here why we dont go into some higher oligomers pca.\n",
    "    print(relevant_dirs)\n",
    "    \n",
    "    dir_dictionary = defaultdict(list)\n",
    "    \n",
    "    for dirs in relevant_dirs:\n",
    "\n",
    "        dir_dictionary[dirs] = os.listdir(dirs)\n",
    "    \n",
    "    \n",
    "    print(\"this is dir dictionary\")\n",
    "    print(dir_dictionary)\n",
    "    \n",
    "    path_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in dir_dictionary.items():\n",
    "        subdir_list = []\n",
    "        for subdir in vals:\n",
    "            new_path = keys + \"/\" + subdir\n",
    "            subdir_list.append(new_path)\n",
    "\n",
    "        path_dict[keys] = subdir_list \n",
    "\n",
    "    \n",
    "    for keys, vals in path_dict.items():\n",
    "        print(\"this is path_dict.items\")\n",
    "        print(keys, vals)\n",
    "        for dir_paths in vals:\n",
    "            \n",
    "            #now we need to parse through ALL of these directories:\n",
    "            \n",
    "            onlyfiles = [f for f in os.listdir(dir_paths) if os.path.isfile(os.path.join(dir_paths, f))]\n",
    "            \n",
    "            onlypdbs = [f for f in onlyfiles if f[-4:] == \".pdb\"]\n",
    "            \n",
    "            print(\"This is onlypdbs from within the run_all_oligomers\")\n",
    "            print(onlypdbs)\n",
    "            if len(onlypdbs) < 5:\n",
    "                #if thats the case that we dont have more than 2 structures \n",
    "                #we go to the next dir.\n",
    "                continue\n",
    "            \n",
    "            # '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/monomer/pos_31_97'\n",
    "            path_split = dir_paths.split(\"/\")\n",
    "            oligostate = path_split[-2] #this corresponds to monomer e.g\n",
    "            position_ = path_split[-1] #thi sis pos_31_97 e.g\n",
    "            \n",
    "            search_oligo_pos = f\"{oligostate}>{position_}\"\n",
    "            \n",
    "            print(\"this is min seq len dict at search oligo pos\")\n",
    "            print(min_seq_len_dict[search_oligo_pos])\n",
    "            min_seq_len = min_seq_len_dict[search_oligo_pos]\n",
    "            print(min_seq_len)\n",
    "            \n",
    "            \n",
    "            #first we grab which positions are gap and whats the max seq length will be.\n",
    "            remove_pos, delete_pdbs = read_msa_file_version_1(dir_paths)\n",
    "            \n",
    "            \n",
    "            print(\"this is remove pos\")\n",
    "            print(remove_pos)\n",
    "            print(\"this is delete_pdbs\")\n",
    "            print(delete_pdbs)\n",
    "            \n",
    "            #then we will prepare those structures, cut gaps and will be left with uniform length\n",
    "            #pdbs that have 1:1 correspondence between each residue against all pdbs.\n",
    "            \n",
    "            ref_struc_pdb_id = prep_structures_pca(path_to_strucs=dir_paths, \n",
    "                                remove_pos=remove_pos,delete_pdbs=delete_pdbs,\n",
    "                                max_seq_len = min_seq_len) #min_seq_len is length of shortes structure in the ensemble.\n",
    "            \n",
    "            if ref_struc_pdb_id == None:\n",
    "                continue #means we are done here.\n",
    "            \n",
    "            print(\"we enter chains_of ref\")\n",
    "            print(\"this is dir path pca and ref_struc_pdb_id\")\n",
    "            print(dir_paths, ref_struc_pdb_id)\n",
    "            chains_of_ref = _get_chain_labels(dir_paths, ref_struc_pdb_id)\n",
    "            \n",
    "            \n",
    "            print(\"we enter now domenico_new\")\n",
    "            \n",
    "            dir_path_pca = f\"{dir_paths}/PCA\"\n",
    "            \n",
    "            \n",
    "            print(dir_path_pca, ref_struc_pdb_id[:-4], chains_of_ref)\n",
    "            \n",
    "            #ref_struc [0:4] = 4 digit code\n",
    "            \n",
    "            #grep -a because temp output file in domenicos code after gromacs alignment contained\n",
    "            #binary stuff.\n",
    "            \n",
    "            #try:\n",
    "            #    PCA_domenico_new(path_to_pdbs=dir_path_pca,\n",
    "            #                 ref_struc_pdb_id=ref_struc_pdb_id[:-4],\n",
    "            #                 chains_of_reference_struc=chains_of_ref)\n",
    "\n",
    "            #\n",
    "    \n",
    "            #    #plot results as well:\n",
    "            #\n",
    "            #    #here we need dir_path_pca but above in PCA_domenico_new\n",
    "            #    #we only need dir paths!!!!!\n",
    "            #    #_plot_PCA(path_to_PCA=dir_path_pca)\n",
    "            #    \n",
    "            #except Exception as error:\n",
    "            #    print(error)\n",
    "            #    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2e87af28-40a4-46e7-b937-69fadb2efd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_chain_labels(dir_paths:str, ref_struc_pdb_id:str):\n",
    "    \n",
    "    \"\"\"Helper function to extract chain label info\"\"\"\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "                \n",
    "    prot_name = f\"default\"\n",
    "               \n",
    "    path_to_pdb = f\"{dir_paths}/{ref_struc_pdb_id}\"\n",
    "    \n",
    "    #open the correct pdb and rechain it.\n",
    "    structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "    \n",
    "    \n",
    "    chains_str = \"\"\n",
    "    for models in structure_template:\n",
    "        for chains in models:\n",
    "            chains_str += chains.id\n",
    "    \n",
    "    \n",
    "    return chains_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bb7b20cd-16d2-4fce-a921-b2cee8ef33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/\"\n",
    "#all_oligomer_runs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5cc9bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report_gaps(report_file_loc:str, result_gaps:dict):\n",
    "    \"\"\"needs documentation.\"\"\"\n",
    "    with open(report_file_loc, \"w\") as fh_report:\n",
    "        for keys, vals in result_gaps.items():\n",
    "            fh_report.write(str(keys[0:6]))\n",
    "            fh_report.write(\",\")\n",
    "            #split list into string sep = ;\n",
    "            if len(vals) == 0: #means we have no gaps\n",
    "                fh_report.write(\"No_gap\")\n",
    "                fh_report.write(\"\\n\")\n",
    "                continue\n",
    "            #start and endgap are the positions that are still PRESENT in the structure. so first missing res is start+1\n",
    "            for entries in vals: #is a tuple containing (CHAIN, STARTGAP, ENDGAP, LEN GAP)\n",
    "                for k in entries:\n",
    "                    fh_report.write(str(k)+\" \")  #k = entry of tuple\n",
    "                fh_report.write(\";\")    \n",
    "            fh_report.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135d292-0dfc-447f-bb70-b2ba313f85d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mutational analysis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "41f90bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mutations_for_mapping(directory_to_search:str, prot_name:str):\n",
    "    \n",
    "    \"\"\"NEEDS TO BE WORKED ON / IS JUST A TEMPLATE NOW.\"\"\"\n",
    "    \n",
    "    path=\"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "    \n",
    "    #fetches the primary gene name that is used to store mutations in csv folder.\n",
    "    gene_name = get_gene_name_uniprot(uniprot_id=prot_name)\n",
    "    \n",
    "    #print(gene_name)\n",
    "    # Read all files from a directory, and read your input argument\n",
    "    files = os.listdir(directory_to_search)\n",
    "    \n",
    "    # Sort file names by name\n",
    "    files = sorted(files) \n",
    "\n",
    "    all_iso_forms = []\n",
    "    \n",
    "    #seems to work\n",
    "    for file_name in files:\n",
    "\n",
    "        if file_name.startswith(gene_name):\n",
    "            all_iso_forms.append(file_name)\n",
    "    \n",
    "    \n",
    "    #print(all_iso_forms)\n",
    "\n",
    "    mutation_pos_count_dict = defaultdict(int)\n",
    "    \n",
    "    for isoform in all_iso_forms:\n",
    "        mutation_iso_dict = defaultdict(int)\n",
    "        with open(f\"{path}/{isoform}\", \"r\") as fh_test:\n",
    "            for entries in fh_test:\n",
    "                mut_list = entries.split(\",\")\n",
    "            \n",
    "    \n",
    "            #this should do the trick. sorts based on position in increasing order.\n",
    "            mut_sorted = sorted(mut_list, key=lambda x: int(x[1:-1]), reverse=False)\n",
    "    \n",
    "            #for entries in mut_sorted:\n",
    "            #    print(entries)\n",
    "    \n",
    "            #count mutations per position\n",
    "            for entries in mut_sorted:\n",
    "                pos = int(entries[1:-1])\n",
    "                mutation_iso_dict[pos] += 1\n",
    "                \n",
    "        mutation_pos_count_dict[isoform] = mutation_iso_dict\n",
    "    #print(mutation_pos_count_dict)\n",
    "\n",
    "    #for keys, vals in mutation_pos_count_dict.items():\n",
    "        #print(keys, vals)\n",
    "        #for isoforms, muts in keys.items():\n",
    "        #    print(isoforms)\n",
    "        #    print(muts)\n",
    "            \n",
    "    ##test purpose\n",
    "    #with open(\"/home/micnag/bioinformatics/test/RNASET2_muts.csv\", \"w\") as fh_out:\n",
    "    #    for keys, vals in mutation_pos_count_dict.items():\n",
    "    #        fh_out.write(str(keys))\n",
    "    #        fh_out.write(\",\")\n",
    "    #        fh_out.write(str(vals))\n",
    "    #        fh_out.write(\"\\n\")\n",
    "    \n",
    "    #print(mutation_pos_count_dict)\n",
    "    return mutation_pos_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a8bcc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surr_mutations(pdb_file:str, mutation_dict:dict,  \n",
    "                   outpath:str, cutoff=8, protname=\"default\"):\n",
    "    \n",
    "    \n",
    "    #print(mutation_dict)\n",
    "    \n",
    "    mutated_neighbours = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for keys, vals in mutation_dict.items():\n",
    "        tmp = []\n",
    "        for position, freq in vals.items():\n",
    "            tmp.append(position)\n",
    "        \n",
    "        mutated_neighbours[keys] = tmp\n",
    "        \n",
    "    with open(pdb_file, \"r\") as pdbfile1:\n",
    "\n",
    "        # first we need to make extract all atoms from our pdb file.\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        \n",
    "        structure = parser.get_structure(protname, pdbfile1)\n",
    "\n",
    "        # Selection.unfold_entities(<structure object>, <level of information that you want>)\n",
    "        # other levels are \"C\" for chain, \"R\" for residue, \"A\" for atom and so on.\n",
    "        \n",
    "\n",
    "        atom_list = Bio.PDB.Selection.unfold_entities(structure, \"A\")\n",
    "        \n",
    "        # lets get the coordinates of all atoms now\n",
    "        \n",
    "        #for each atom we store his parent residue\n",
    "        atom_coords = [(atom.get_coord(), atom.get_parent()) for atom in atom_list]\n",
    "        \n",
    "        for atoms in atom_list:\n",
    "            print(atoms.get_id())\n",
    "        # we provide as argument here the Selection.unfold.entities object which has all atoms.\n",
    "        ns = Bio.PDB.NeighborSearch(atom_list)  # this class object has the .search() method defined in its __init__\n",
    "        \n",
    "        \n",
    "        \n",
    "        #here we store all neighbours\n",
    "        general_neighbours = defaultdict(list)\n",
    "        \n",
    "        for atoms in atom_coords:\n",
    "            \n",
    "            f'''For each atom we will make a search for all surrounding atoms that are within {cutoff} A radius.'''\n",
    "            parent_res = atoms[1].get_id()[1]  #returns the residue number.\n",
    "            \n",
    "            proximal_atoms = ns.search(atoms[0], 8, \"R\")\n",
    "            \n",
    "            # I SET HERE search for atoms[0] because atoms is a tuple containing of coordinates\n",
    "            # and parent residue name see line 75 + 76 #print(atom_coords[0])\n",
    "\n",
    "            f\"\"\"Synthax: ns.search(<target object>, <Cutoff to be searched for>, \n",
    "            <type of information level that should be returned>\n",
    "            R means we dont want the single atoms that are within {cutoff}A \n",
    "            found but instead their corresponding residues. For all atoms we would set <A> instead of <R>\"\"\"\n",
    "\n",
    "            # this function searches through a target (in our case each atom as we loop through all available atoms)\n",
    "            # and returns a list with all atoms within specified atoms .\n",
    "            \n",
    "                \n",
    "            for residues in proximal_atoms:  # we go through all residues that were found within cutoff A\n",
    "            \n",
    "                id_x = residues.get_id()[1]\n",
    "                # get_id gives us a tuple with shape (\"\", \"residue number\", \"optinal flag\").\n",
    "                # Out of this tuple we want the residue id which is [1]\n",
    "                # we only want residues that we dont have already in the list.\n",
    "                # Makes no sense to add stuff that is already in there\n",
    "                general_neighbours[parent_res].append(id_x)\n",
    "                \n",
    "        \n",
    "        # if we have all we append the whole list to the dictionary. we take the atoms parent residue name as a key.\n",
    "        non_redundant_neighbours = defaultdict(int)\n",
    "        \n",
    "        \n",
    "        for keys, vals in general_neighbours.items():\n",
    "            unique_hits = list(set([x for x in vals if x != keys]))\n",
    "            \n",
    "            non_redundant_neighbours[keys] = unique_hits\n",
    "        \n",
    "        \n",
    "        \n",
    "        #now lets check how many mutations are within them.\n",
    "        \n",
    "        for isoform, mutations in mutated_neighbours.items():\n",
    "            print(isoform)\n",
    "            print(mutations)\n",
    "            with open(f\"{outpath}/{isoform[:-4]}_mutated_surroundings.tsv\", \"w\") as fh_out:\n",
    "                for position, neighbours in non_redundant_neighbours.items():\n",
    "                \n",
    "                    #make sets of both (just to use intersection)\n",
    "                    neighbours = set(neighbours)\n",
    "                    mutations = set(mutations)\n",
    "                    \n",
    "                    shared_mut_pos = neighbours.intersection(mutations)\n",
    "                    shared_mut_lst = list(sorted(list(shared_mut_pos)))\n",
    "                    print(position)\n",
    "                    print(shared_mut_lst)\n",
    "                    \n",
    "                    fh_out.write(str(position))\n",
    "                    fh_out.write(\"\\t\")\n",
    "                    for hits in shared_mut_lst:\n",
    "                        \n",
    "                        fh_out.write(str(hits))\n",
    "                        fh_out.write(\" \")\n",
    "                    \n",
    "                    fh_out.write(\"\\t\")\n",
    "                    fh_out.write(str(len(shared_mut_lst)))\n",
    "                    fh_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "98a5562b-a102-4e0e-83fb-3764b24937e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surr_mutations_calpha_1(pdb_file:str, mutation_dict:dict,  \n",
    "                   outpath:str, NMA_info:dict, cutoff=8, protname=\"default\"):\n",
    "    \n",
    "    #print(mutation_dict)\n",
    "    mutated_neighbours = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in mutation_dict.items():\n",
    "        tmp = []\n",
    "        for position, freq in vals.items():\n",
    "            tmp.append(position)\n",
    "        \n",
    "        mutated_neighbours[keys] = tmp\n",
    "        \n",
    "    \n",
    "    #print(mutated_neighbours)\n",
    "    with open(pdb_file, \"r\") as pdbfile1:\n",
    "\n",
    "        # first we need to make extract all atoms from our pdb file.\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        \n",
    "        structure = parser.get_structure(protname, pdbfile1)\n",
    "\n",
    "        # Selection.unfold_entities(<structure object>, <level of information that you want>)\n",
    "        # other levels are \"C\" for chain, \"R\" for residue, \"A\" for atom and so on\n",
    "        atom_list = Bio.PDB.Selection.unfold_entities(structure, \"A\")\n",
    "        \n",
    "        # lets get the coordinates of all atoms now\n",
    "        #only CALPHA\n",
    "        atom_coords = [atom for atom in atom_list if atom.get_id() == \"CA\"]\n",
    "    \n",
    "        # we provide as argument here the Selection.unfold.entities object which has all atoms.\n",
    "        ns = Bio.PDB.NeighborSearch(atom_coords)  # this class object has the .search() method defined in its __init__\n",
    "        \n",
    "        #here we store all neighbours\n",
    "        general_neighbours = defaultdict(list)\n",
    "        \n",
    "        for atoms in atom_coords:\n",
    "            \n",
    "            f'''For each atom we will make a search for all surrounding atoms that are within {cutoff} A radius.'''\n",
    "            parent_res = atoms.get_parent().get_id()[1]  #returns the residue number.\n",
    "            \n",
    "            proximal_atoms = ns.search(atoms.get_coord(), 8, \"A\")\n",
    "            \n",
    "            # I SET HERE search for atoms[0] because atoms is a tuple containing of coordinates\n",
    "            # and parent residue name see line 75 + 76 #print(atom_coords[0])\n",
    "\n",
    "            f\"\"\"Synthax: ns.search(<target object>, <Cutoff to be searched for>, \n",
    "            <type of information level that should be returned>\n",
    "            R means we dont want the single atoms that are within {cutoff}A \n",
    "            found but instead their corresponding residues. For all atoms we would set <A> instead of <R>\"\"\"\n",
    "\n",
    "            # this function searches through a target (in our case each atom as we loop through all available atoms)\n",
    "            # and returns a list with all atoms within specified atoms .\n",
    "            \n",
    "            for atms in proximal_atoms:  # we go through all residues that were found within cutoff A\n",
    "                \n",
    "                id_x = atms.get_id()\n",
    "                # get_id gives us a tuple with shape (\"\", \"residue number\", \"optinal flag\").\n",
    "                # Out of this tuple we want the residue id which is [1]\n",
    "                # we only want residues that we dont have already in the list.\n",
    "                # Makes no sense to add stuff that is already in there\n",
    "                if id_x == \"CA\":\n",
    "                    general_neighbours[parent_res].append(atms.get_parent().get_id()[1])\n",
    "                \n",
    "        \n",
    "        # if we have all we append the whole list to the dictionary. we take the atoms parent residue name as a key.\n",
    "        non_redundant_neighbours = defaultdict(int)\n",
    "        \n",
    "        for keys, vals in general_neighbours.items():\n",
    "            unique_hits = list(set([x for x in vals if x != keys]))\n",
    "            \n",
    "            non_redundant_neighbours[keys] = unique_hits\n",
    "        \n",
    "        \n",
    "        #now lets check how many mutations are within them.\n",
    "        \n",
    "        for isoform, mutations in mutated_neighbours.items():\n",
    "            #print(mutations)\n",
    "            \n",
    "            if NMA_info:\n",
    "                \n",
    "                NMA_dict = defaultdict()\n",
    "                NMA_scores = [x[1] for x in NMA_info] # we grab the scores knowing that it contains for each residue from start to finish a score.\n",
    "                #these scores are RENUMBERED FROM POS 1 even if POS 1 in the protein has resnum 134 e.g\n",
    "            else:\n",
    "                NMA_scores = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            df_mutation = pd.DataFrame()\n",
    "            \n",
    "                \n",
    "            #will contain all the neighbouring CA of each position as a list of lists.\n",
    "            neighbour_list = []\n",
    "            #these are the res nums of the CAs\n",
    "            prot_idx = []\n",
    "            \n",
    "            #surrounding mutated res list\n",
    "            mutated_res_neighbours = []\n",
    "            \n",
    "            #number of mutated neighbours\n",
    "            num_mutated_neighbours = []\n",
    "            \n",
    "            #NMA score list\n",
    "            NMA_score_attached_vals = []\n",
    "            \n",
    "            \n",
    "            i = 0 \n",
    "            for position, neighbours in non_redundant_neighbours.items():\n",
    "                \n",
    "                #make sets of both (just to use intersection)\n",
    "                \n",
    "                neighbours = set(neighbours)\n",
    "                \n",
    "                neighbour_list.append(list(neighbours))\n",
    "                prot_idx.append(position)\n",
    "            \n",
    "                mutations = set(mutations)\n",
    "                \n",
    "                shared_mut_pos = neighbours.intersection(mutations)\n",
    "                shared_mut_lst = list(sorted(list(shared_mut_pos)))\n",
    "                \n",
    "                mutated_res_neighbours.append(shared_mut_lst)\n",
    "                \n",
    "                num_mutated_neighbours.append(len(shared_mut_lst))\n",
    "                \n",
    "                if len(NMA_scores) != 0:\n",
    "                    \n",
    "                    NMA_score_attached_vals.append(NMA_scores[i])\n",
    "                    \n",
    "                    i += 1\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    NMA_score_attached_vals.append(np.NaN)\n",
    "                    i += 1\n",
    "            \n",
    "            #print(neighbour_list)\n",
    "            \n",
    "            df_mutation[\"neighbour_CAs\"] = [x for x in list(neighbour_list)]\n",
    "            df_mutation[\"mutated_neighbours_CAs\"] = [x for x in list(mutated_res_neighbours)] if len(mutated_res_neighbours) > 0 else np.NaN\n",
    "            df_mutation[\"num_mutated_neighbours\"] = [int(x) for x in list(num_mutated_neighbours)]\n",
    "            df_mutation[\"NMA_scores\"] = NMA_score_attached_vals\n",
    "            df_mutation.index = prot_idx\n",
    "            #print(pdb_file)\n",
    "            df_mutation.to_csv(outpath, sep=\"\\t\")\n",
    "            #print(df_mutation.head())\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2a66ee7b-f6a7-49db-8afd-d3547e9426fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mutational_mapping_new(dir_to_proteins:str, main_prot_name:str, include_NMA=True):\n",
    "    \n",
    "     #good candidate to put all of them into 1 function. Next time wrap it up.\n",
    "    \"\"\"mutational mapping part\"\"\"\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"{dir_to_proteins}/Mutational_mapping\")\n",
    "    \n",
    "    except Exception as error:\n",
    "        #print(error)\n",
    "        pass\n",
    "    \n",
    "    #this dir path needs to be hardcoded because it is constant in every case.4\n",
    "    dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "    \n",
    "    #path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{main_prot_name}\"\n",
    "\n",
    "    if include_NMA:\n",
    "        \n",
    "        NMA_dir = f\"{dir_to_proteins}/NMA\"\n",
    "        try:\n",
    "            dirs_to_check = [d for d in os.listdir(NMA_dir) if os.path.isdir(os.path.join(NMA_dir, d))]\n",
    "            \n",
    "            #print(dirs_to_check)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        \n",
    "        NMA_dict = defaultdict()\n",
    "        \n",
    "        for dirs in dirs_to_check:\n",
    "            path_to_check = f\"{dirs[:-4]}.ca.pdb_bfactor_res.txt\"\n",
    "            try:\n",
    "                #we append a dic as val to key from NMA_dict\n",
    "                NMA_scores_per_struc = []\n",
    "                \n",
    "                with open(f\"{NMA_dir}/{dirs}/{path_to_check}\", \"r\") as fh_nma_in:\n",
    "                    for lines in fh_nma_in:\n",
    "                        lines = lines.split()\n",
    "                        pos, score = lines[0], lines[1]\n",
    "                        NMA_scores_per_struc.append((int(pos), float(score)))\n",
    "                    \n",
    "                    #we append the new dict to a global dict.\n",
    "                    NMA_dict[dirs] = NMA_scores_per_struc\n",
    "                        \n",
    "            except Exception as error:\n",
    "                #print(f\"we could not open {NMA_dir}/{dirs}/{path_to_check}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "    #for strucs, combo_scores in NMA_dict.items():\n",
    "    #    print(strucs)\n",
    "    #    print(combo_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_file_input = [f for f in os.listdir(dir_to_proteins) if os.path.isfile(os.path.join(dir_to_proteins, f))]\n",
    "    #grab only pdbs\n",
    "    pdb_mapping_inputs = [f for f in all_file_input if f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #grab corresponding mutational dictionary for each gene\n",
    "    mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=main_prot_name)\n",
    "    \n",
    "    #now lets map on each structure for each oligomer.\n",
    "    #check if there are pdb structures to map\n",
    "    if len(pdb_mapping_inputs) != 0:\n",
    "        #if yes map for each position and oligomer.\n",
    "        for pdb_files in pdb_mapping_inputs:\n",
    "            \n",
    "            \n",
    "            if include_NMA:\n",
    "                try:\n",
    "                    NMA_info = NMA_dict[pdb_files]\n",
    "                except:\n",
    "                    NMA_info = False\n",
    "            else:\n",
    "                NMA_info = False\n",
    "            \n",
    "            \n",
    "            #only CA-based. 8 A cutoff.\n",
    "            \n",
    "            pdb_file_path = f\"{dir_to_proteins}/{pdb_files}\"\n",
    "            \n",
    "            surr_mutations_calpha_1(pdb_file=pdb_file_path, mutation_dict=mut_dictionary, protname=main_prot_name,  \n",
    "                      outpath=f\"{dir_to_proteins}/Mutational_mapping/{pdb_files[:-4]}.tsv\", cutoff=8,\n",
    "                                 NMA_info=NMA_info)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c2ff15f2-316b-4005-8d91-d25c4f7b46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test new block mutational mapping\n",
    "\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/monomer/pos_1_181\"\n",
    "#run_mutational_mapping_new(dir_to_proteins=path, main_prot_name=\"NUD4B_HUMAN\", include_NMA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6fd82b42-dbde-4fba-9380-6def64fca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing mutational mapping block.\n",
    "\n",
    "#dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "\n",
    "#pdb_file = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/mutational_mapping/3w5b.pdb\"\n",
    "\n",
    "#outpath = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/mutational_mapping\"\n",
    "\n",
    "#mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=\"AT2A1_HUMAN\")\n",
    "#surr_mutations_calpha(pdb_file=pdb_file, mutation_dict=mut_dictionary, protname=\"AT2A1_HUMAN\",  \n",
    "#                   outpath=outpath, cutoff=8)\n",
    "#mutational_freq_hist(mut_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811c359-4cd7-4b28-8ced-3742180c461c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7522d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutational_freq_hist(mutational_dict:dict):\n",
    "    \n",
    "    #print(mutational_dict)\n",
    "    \n",
    "    tmp_dict = defaultdict()\n",
    "    \n",
    "    for keys, vals in mutational_dict.items():\n",
    "        tmp_list = []\n",
    "        for key, val in vals.items():\n",
    "            tmp_list.append((key,val))\n",
    "        tmp_sort = sorted(tmp_list, key=lambda x: x[1], reverse=True)\n",
    "        tmp_dict[keys] = tmp_sort\n",
    "    \n",
    "\n",
    "    all_isoform_dicts = defaultdict(list)\n",
    "    only_muts = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in tmp_dict.items():\n",
    "        min_pos = 1\n",
    "        max_pos = max([x[0] for x in vals])\n",
    "        \n",
    "        for (pos, mut) in vals:\n",
    "            all_isoform_dicts[keys].append((pos, mut))\n",
    "            only_muts[keys].append((pos, mut))\n",
    "            \n",
    "        for i in range(min_pos, max_pos):\n",
    "            if i not in [x[0] for x in vals]:\n",
    "                all_isoform_dicts[keys].append((i, 0))\n",
    "    \n",
    "    \n",
    "    overlap_mutations = defaultdict(int)\n",
    "    \n",
    "    for keys, vals in only_muts.items():\n",
    "        \n",
    "        for muts in vals:\n",
    "            \n",
    "            overlap_mutations[muts[0]] += muts[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    shared_list = []\n",
    "    \n",
    "    #for keys, vals in all_isoform_dicts.items():\n",
    "    #    #setup min max to find the boundaries for our mutations on the protein.\n",
    "    #    min_pos = min([int(x[1]) for x in vals])\n",
    "    #    max_pos = max([int(x[1]) for x in vals])\n",
    "    #    #width = (max_pos-min_pos)\n",
    "    #\n",
    "    #    mutations_list = [x[1] for x in vals]\n",
    "    #    avg_mut = np.average(mutations_list)\n",
    "    #    #print(min_pos, max_pos, avg_mut)\n",
    "    #    shared_list.append([x for x in vals])\n",
    "        \n",
    "    #print(shared_list)\n",
    "        \n",
    "    #result = set(shared_list[0]).intersection(*shared_list)\n",
    "    #print(result)\n",
    "\n",
    "\n",
    "    \"\"\"MIGHT BE NEEDED LATER\"\"\"\n",
    "    fig = plt.figure(figsize=(100, 50))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    #threshold_avg = avg_mut\n",
    "    #threshold_1_sd = avg_mut + 1*np.std(mutations_list)\n",
    "    #threshold_2_sd = avg_mut + 2*np.std(mutations_list)\n",
    "    #threshold_3_sd = avg_mut + 3*np.std(mutations_list)\n",
    "    \n",
    "    \n",
    "    #below_avg = []\n",
    "    #sd_1 = []\n",
    "    #sd_2 = []\n",
    "    #sd_3 = []\n",
    "    \n",
    "    #for keys, vals in mutational_dict.items():\n",
    "    #    if vals > threshold_3_sd:\n",
    "    #        sd_3.append((keys, vals))\n",
    "    #        continue\n",
    "    #    if vals > threshold_2_sd:\n",
    "    #        sd_2.append((keys, vals))\n",
    "    #        continue\n",
    "    #    if vals > threshold_1_sd:\n",
    "    #        sd_1.append((keys, vals))\n",
    "    #        continue\n",
    "    #    if vals < threshold_avg:\n",
    "    #        below_avg.append((keys, vals))\n",
    "    #        continue\n",
    "    #\n",
    "    #x_below_avg = [x[0] for x in below_avg]\n",
    "    #y_below_avg = [x[1] for x in below_avg]\n",
    "    #\n",
    "    #x_sd_1 = [x[0] for x in sd_1]\n",
    "    #y_sd_1 = [x[1] for x in sd_1]\n",
    "    #\n",
    "    #x_sd_2 = [x[0] for x in sd_2]\n",
    "    #y_sd_2 = [x[1] for x in sd_2]\n",
    "    #\n",
    "    #x_sd_3 = [x[0] for x in sd_3]\n",
    "    #y_sd_3 = [x[1] for x in sd_3]\n",
    "    \n",
    "    #ax.bar(x_below_avg,y_below_avg, color = 'grey', width=1,label='Below avg')\n",
    "    #ax.bar(x_sd_1,y_sd_1, color = 'yellow', width=1, label='Above 1 SD') \n",
    "    #ax.bar(x_sd_2,y_sd_2, color = 'orange', width=1, label='Above 2 SD')\n",
    "    #ax.bar(x_sd_3,y_sd_3, color = 'red', label='Above 3 SD')\n",
    "    \n",
    "    ax.bar(overlap_mutations.keys(), overlap_mutations.values())\n",
    "    \n",
    "    #ax.bar(mutational_dict.keys(), mutational_dict.values(), color=[\"red\", \"blue\"])\n",
    "    #ax.set_title(f\"Number of mutations in 8A vicinity\", {'fontsize': 120})\n",
    "    #ax.set_xlabel('Position', fontsize=80)\n",
    "    #ax.set_ylabel('Mutations in Neighbourhood [Mutations / 8A surroundings]', fontsize=80)\n",
    "    #this does not take into account if the protein is actually longer.  needs improvement if this is important.\n",
    "    #ax.hlines(avg_mut, xmin=0, xmax=max_pos, color=\"grey\", linestyles=\"dotted\",linewidth=3)\n",
    "    #ax.hlines(avg_mut+2*np.std(mutations_list), xmin=0, xmax=max_pos, color=\"r\", linestyles=\"dashed\",linewidth=4)\n",
    "    #ax.hlines(avg_mut+3*np.std(mutations_list), xmin=0, xmax=max_pos, color=\"r\", linestyles=\"dashed\",linewidth=5)\n",
    "    #ax.tick_params(labelsize=60)\n",
    "    #ax.xaxis.set_ticks(np.arange(0, max_pos, 10))\n",
    "    #ax.legend(fontsize=80, borderpad=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "46cd7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "\n",
    "#mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=\"RNT2_HUMAN\")\n",
    "#mutational_freq_hist(mut_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "89999e9c-bb86-4669-a8f0-32f29f3bf760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to cluster PCA automatically\n",
    "\n",
    "\n",
    "def PCA_clustering(path_to_PCA:str):\n",
    "    \n",
    "    \"\"\"Function will take in the PCA data and perform affinity propagation clustering.\"\"\"\n",
    "    \n",
    "    \n",
    "    expl_var = f\"{path_to_PCA}/pc_variances.txt\"\n",
    "    pca = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    \n",
    "    #centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "    #X, labels_true = make_blobs(\n",
    "    #        n_samples=300, centers=centers, cluster_std=0.5, random_state=0)\n",
    "    \n",
    "    \n",
    "    #read in the first 2 PC from PCA file.\n",
    "    pca_array = np.loadtxt(pca, dtype=\"float\",\n",
    "                           usecols=[0,1])\n",
    "    \n",
    "    pca_mean = np.mean(pca_array)\n",
    "    pca_std = np.std(pca_array)\n",
    "    print(pca_mean)\n",
    "    print(pca_std)\n",
    "    \n",
    "    pca_normalized = (pca_array-pca_mean)/pca_std\n",
    "    \n",
    "    #print(pca_normalized)\n",
    "    #normalize array\n",
    "    #pca_array_normalized = stats.zscore(pca_array, axis=None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    af = AffinityPropagation(random_state=0,max_iter=1000,damping=0.7).fit(pca_normalized)\n",
    "    \n",
    "    cluster_centers_indices = af.cluster_centers_indices_\n",
    "    labels = af.labels_\n",
    "    \n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    \n",
    "    \n",
    "    print(cluster_centers_indices)\n",
    "    print(labels)\n",
    "    \n",
    "    cnt_dict = defaultdict()\n",
    "    \n",
    "    hits = set(labels)\n",
    "    \n",
    "    for x in hits:\n",
    "        cnt_dict[x] = 0\n",
    "        \n",
    "    for x in labels:\n",
    "        cnt_dict[x] += 1\n",
    "        \n",
    "    \n",
    "    highest_groups = []\n",
    "    for keys, vals in cnt_dict.items():\n",
    "        highest_groups.append((keys, vals))\n",
    "        \n",
    "    \n",
    "    highest_sort = sorted(highest_groups, key=lambda x : x[1], reverse=True)\n",
    "    \n",
    "    print(highest_sort)\n",
    "    #print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    #print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "    #print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "    #print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "    #print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\n",
    "    #print(\n",
    "    #    \"Adjusted Mutual Information: %0.3f\"\n",
    "    #    % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
    "    #)\n",
    "    #print(\n",
    "    #    \"Silhouette Coefficient: %0.3f\"\n",
    "    #    % metrics.silhouette_score(X, labels, metric=\"sqeuclidean\")\n",
    "    #)\n",
    "\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    \n",
    "    colors = plt.cycler(\"color\", plt.cm.viridis(np.linspace(0, 1, 4)))\n",
    "    \n",
    "    for k, col in zip(range(n_clusters_), colors):\n",
    "        class_members = labels == k\n",
    "        cluster_center = pca_array[cluster_centers_indices[k]]\n",
    "        plt.scatter(\n",
    "            pca_array[class_members, 0], pca_array[class_members, 1], color=col[\"color\"], marker=\".\"\n",
    "        )\n",
    "        plt.scatter(\n",
    "            cluster_center[0], cluster_center[1], s=14, color=col[\"color\"], marker=\"o\"\n",
    "        )\n",
    "        for x in pca_array[class_members]:\n",
    "            plt.plot(\n",
    "                [cluster_center[0], x[0]], [cluster_center[1], x[1]], color=col[\"color\"]\n",
    "            )\n",
    "    \n",
    "    plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "302d876f-5dbd-42c5-8db5-889b8091453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_PCA = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "#PCA_clustering(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "824a24f1-9887-4438-908c-66e8fc2968eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_clustering_2(path_to_PCA:str):\n",
    "    \n",
    "    \"\"\"Function will take in the PCA data and perform affinity propagation clustering.\"\"\"\n",
    "    \n",
    "    \n",
    "    expl_var = f\"{path_to_PCA}/pc_variances.txt\"\n",
    "    pca = f\"{path_to_PCA}/exp_ensemble_proj_PC.txt\"\n",
    "    \n",
    "    \n",
    "    np.random.seed(0)\n",
    "    #centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "    #X, labels_true = make_blobs(\n",
    "    #        n_samples=300, centers=centers, cluster_std=0.5, random_state=0)\n",
    "    \n",
    "    \n",
    "    #read in the first 2 PC from PCA file.\n",
    "    pca_array = np.loadtxt(pca, dtype=\"float\",\n",
    "                           usecols=[0,1])\n",
    "    \n",
    "    pca_mean = np.mean(pca_array)\n",
    "    pca_std = np.std(pca_array)\n",
    "    \n",
    "    #normalize\n",
    "    pca_normalized = (pca_array-pca_mean)/pca_std\n",
    "    \n",
    "    \n",
    "    clust = OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.05)\n",
    "    \n",
    "    # Run the fit\n",
    "    clust.fit(pca_normalized)\n",
    "    labels_050 = cluster_optics_dbscan(\n",
    "    reachability=clust.reachability_,\n",
    "    core_distances=clust.core_distances_,\n",
    "    ordering=clust.ordering_,\n",
    "    eps=0.5)\n",
    "    \n",
    "    labels_200 = cluster_optics_dbscan(\n",
    "    reachability=clust.reachability_,\n",
    "    core_distances=clust.core_distances_,\n",
    "    ordering=clust.ordering_,\n",
    "    eps=2)\n",
    "\n",
    "    space = np.arange(len(pca_array))\n",
    "    reachability = clust.reachability_[clust.ordering_]\n",
    "    labels = clust.labels_[clust.ordering_]\n",
    "    \n",
    "    print(\"these are the number of conformers we found\")\n",
    "    print(len(set([x for x in labels if x >= 0])))\n",
    "    print(set(labels))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    G = gridspec.GridSpec(2, 3)\n",
    "    ax1 = plt.subplot(G[0, :])\n",
    "    ax2 = plt.subplot(G[1, 0])\n",
    "    ax3 = plt.subplot(G[1, 1])\n",
    "    ax4 = plt.subplot(G[1, 2])\n",
    "    \n",
    "    # Reachability plot\n",
    "    colors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\n",
    "    for klass, color in zip(range(0, 5), colors):\n",
    "        Xk = space[labels == klass]\n",
    "        Rk = reachability[labels == klass]\n",
    "        ax1.plot(Xk, Rk, color, alpha=0.3)\n",
    "    ax1.plot(space[labels == -1], reachability[labels == -1], \"k.\", alpha=0.3)\n",
    "    ax1.plot(space, np.full_like(space, 2.0, dtype=float), \"k-\", alpha=0.5)\n",
    "    ax1.plot(space, np.full_like(space, 0.5, dtype=float), \"k-.\", alpha=0.5)\n",
    "    ax1.set_ylabel(\"Reachability (epsilon distance)\")\n",
    "    ax1.set_title(\"Reachability Plot\")\n",
    "    \n",
    "    # OPTICS\n",
    "    colors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\n",
    "    for klass, color in zip(range(0, 5), colors):\n",
    "        Xk = pca_array[clust.labels_ == klass]\n",
    "        ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\n",
    "    ax2.plot(pca_array[clust.labels_ == -1, 0], pca_array[clust.labels_ == -1, 1], \"k+\", alpha=0.1)\n",
    "    ax2.set_title(\"Automatic Clustering\\nOPTICS\")\n",
    "    \n",
    "    # DBSCAN at 0.5\n",
    "    colors = [\"g.\", \"r.\", \"b.\", \"c.\", \"y.\", \"m.\"]\n",
    "    for klass, color in zip(range(0, 4), colors):\n",
    "        Xk = pca_array[labels_050 == klass]\n",
    "        ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\n",
    "    ax3.plot(pca_array[labels_050 == -1, 0], pca_array[labels_050 == -1, 1], \"k+\", alpha=0.1)\n",
    "    ax3.set_title(\"Clustering at 0.5 epsilon cut\\nDBSCAN\")\n",
    "    \n",
    "    # DBSCAN at 2.\n",
    "    colors = [\"g.\", \"m.\", \"y.\", \"c.\"]\n",
    "    for klass, color in zip(range(0, 4), colors):\n",
    "        Xk = pca_array[labels_200 == klass]\n",
    "        ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\n",
    "    ax4.plot(pca_array[labels_200 == -1, 0], pca_array[labels_200 == -1, 1], \"k+\", alpha=0.1)\n",
    "    ax4.set_title(\"Clustering at 2.0 epsilon cut\\nDBSCAN\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "98ca3558-74c9-494c-89e3-609803bd2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_PCA = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994/PCA\"\n",
    "#PCA_clustering_2(path_to_PCA=path_to_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "13e78d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutational_upsetplot(mutational_dict:dict, save_file=True):\n",
    "    \n",
    "    #print(mutational_dict)\n",
    "    \n",
    "    tmp_dict = defaultdict()\n",
    "    \n",
    "    isoforms_names = []\n",
    "    for keys, vals in mutational_dict.items():\n",
    "        tmp_list = []\n",
    "        #store names of isoforms.\n",
    "        if not isoforms_names:\n",
    "            isoforms_names.append(keys)\n",
    "        for key, val in vals.items():\n",
    "            tmp_list.append((keys,key,val))\n",
    "        tmp_sort = sorted(tmp_list, key=lambda x: x[1], reverse=True)\n",
    "        tmp_dict[keys] = tmp_sort\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_isoform_dicts = defaultdict(list)\n",
    "    only_muts = defaultdict(list)\n",
    "    \n",
    "    for keys, vals in tmp_dict.items():\n",
    "        min_pos = 1\n",
    "        max_pos = max([x[1] for x in vals])\n",
    "        \n",
    "        for (isoform, pos, mut) in vals:\n",
    "            all_isoform_dicts[keys].append((isoform[:-4],pos, mut))\n",
    "            only_muts[keys].append((pos, mut))\n",
    "            \n",
    "        for i in range(min_pos, max_pos):\n",
    "            if i not in [x[0] for x in vals]:\n",
    "                all_isoform_dicts[keys].append((isoform[:-4], i, 0))\n",
    "    \n",
    "\n",
    "    tmp_ = []\n",
    "    for keys, vals in all_isoform_dicts.items():\n",
    "        for x in vals:\n",
    "            tmp_.append(x)\n",
    "    \n",
    "    \n",
    "    sorted_full_isoforms = sorted(tmp_, key=lambda x: x[1], reverse=False)\n",
    "    \n",
    "    max_val = sorted_full_isoforms[-1][1]\n",
    "      \n",
    "    muts = defaultdict(int)\n",
    "    isoforms = defaultdict(list)\n",
    "    \n",
    "    for entries in sorted_full_isoforms:\n",
    "        if entries[2] == 0:\n",
    "            continue\n",
    "        muts[(entries[1])] += entries[2]\n",
    "        isoforms[str(entries[1])].append(entries[0])\n",
    "        \n",
    "    datalst = []\n",
    "    members = []\n",
    "    \n",
    "    for i in range(1, max_pos):\n",
    "        datalst.append(muts[i])\n",
    "        members.append(isoforms[str(i)])\n",
    "\n",
    "    #mut_freq_overlap = from_memberships(members[0:5], data=data[0:5])\n",
    "    \n",
    "    example = from_memberships(\n",
    "    members, data=datalst)\n",
    "    \n",
    "    \n",
    "    \n",
    "    UpSet.plot(example, subset_size=\"sum\", orientation=\"horizontal\",\n",
    "        sort_by=\"degree\", sort_categories_by=\"cardinality\",\n",
    "        facecolor=\"navy\",shading_color=\"lightgray\",\n",
    "        show_counts=True,other_dots_color=.2)\n",
    "    \n",
    "    title = min(isoforms_names)[:-4]\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    #if save_file:\n",
    "        #plt.savefig(\"/home/micnag/bioinformatics/test/myplot.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "42b8cd13-cdb7-4cdc-937d-6e455d2e0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_c_alpha(path:str):\n",
    "    \n",
    "    \n",
    "    #sel only c_alpha\n",
    "    class C_alpha_only(Select):\n",
    "        def __init__(self, *args):\n",
    "            super().__init__(*args)\n",
    "            \n",
    "        #overload accept_residue inherited from Select with this conditional return\n",
    "        def accept_atom(self, atom):\n",
    "            return 1 if atom.id == \"CA\" else 0\n",
    "        \n",
    "        #overloaded to only accept positive residue numbering.\n",
    "        def accept_residue(self, residue):      \n",
    "            return 1 if residue.id[1] > 0 else 0    \n",
    "        \n",
    "    lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                    ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                    ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                    ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "    \n",
    "    canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "    #filelst    path\n",
    "    #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"default\"\n",
    "    \n",
    "    #print(\"we are here\")\n",
    "    #print(fullpath)\n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    # Select C-alpha atoms and save the modified structure\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(path, C_alpha_only())\n",
    "    \n",
    "    # Reload the modified structure for further processing\n",
    "    structure = parser.get_structure(prot_name, path)\n",
    "    \n",
    "    # Extract sequence and range information\n",
    "    struc_full = [canonical_aas[x.get_resname()] for x in structure.get_residues()]\n",
    "    struc_full = \"\".join(struc_full)\n",
    "    \n",
    "    struc_range = [x.get_id()[1] for x in structure.get_residues()]\n",
    "    struc_start, struc_stop = struc_range[0], struc_range[-1]\n",
    "    \n",
    "    return struc_start, struc_stop, struc_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fcfbbdc6-5f71-4fa8-876a-4005866741e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select_c_alpha(\"/home/micnag/pdb_neg_test.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3f8cfde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dir_to_read = \"/home/micnag/bioinformatics/mutational_collection_cosmic/mutations/overall_mutations\"\n",
    "\n",
    "#mut_dictionary = read_mutations_for_mapping(directory_to_search=dir_to_read, prot_name=\"RNT2_HUMAN\")\n",
    "#mutational_upsetplot(mut_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e1297-ba0f-4ea6-a655-201631e8b352",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normal mode analysis block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d99d3134-9de1-43f6-9e7f-453865a8b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMA_pipeline(path:str):\n",
    "    \n",
    "    baselocation = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/NMA_pipeline\"\n",
    "        \n",
    "    #we work in a clean work dir.\n",
    "    try:\n",
    "        os.mkdir(f\"{path}/NMA\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #fetch all file names that we need to shuffle.\n",
    "    files_to_shuffle = [f for f in os.listdir(baselocation) if os.path.isfile(os.path.join(baselocation, f))]\n",
    "    \n",
    "    #we move all required files to the work dir.\n",
    "    for files in files_to_shuffle:\n",
    "        #copy to target folder\n",
    "        shutil.copy(f\"{baselocation}/{files}\", f\"{path}/NMA\")\n",
    "        \n",
    "    #lets start NMA for all structure\n",
    "    os.chdir(f\"{path}/NMA\")\n",
    "    \n",
    "    result_files = [\"evec.dat\", \"constants.dat\", \"hessian.dat\"]\n",
    "\n",
    "    complete_struc_path = f\"{path}/PCA/clean_ensemble\"\n",
    "    \n",
    "    onlyfiles = [f for f in os.listdir(complete_struc_path) if os.path.isfile(os.path.join(complete_struc_path, f)) and f[-4:] == \".pdb\"]\n",
    "\n",
    "    #set b factor to 0 otherwise this interfers with memory allocation if the columns are not separated (occ. and b fac)\n",
    "    \n",
    "    \n",
    "    for pdbs in onlyfiles:\n",
    "        print(\"we run now nma with\")\n",
    "        print(pdbs)\n",
    "\n",
    "        #first we set b fac to 0\n",
    "        _set_b_factors_0(path=f\"{complete_struc_path}/{pdbs}\")\n",
    "        \n",
    "        run_nma = f\"./get_nma.sh {complete_struc_path}/{pdbs}\"\n",
    "        \n",
    "        bash_cmd = run_nma.split()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            #lets keep it here to be save\n",
    "            os.chdir(f\"{path}/NMA\")\n",
    "            result = run(bash_cmd, stdout=PIPE, stderr=PIPE,\n",
    "                universal_newlines=True)\n",
    "        \n",
    "            print(result.stdout)\n",
    "            print(result.stderr)\n",
    "            \n",
    "            os.mkdir(f\"{path}/NMA/{pdbs}\")\n",
    "            \n",
    "            #we move the CA-only structure also for representation.\n",
    "            \n",
    "            shutil.move(f\"{complete_struc_path}/{pdbs[0:4]}.ca.pdb\", f\"{path}/NMA/{pdbs}\")\n",
    "            \n",
    "            #we move the result files to another dir where we work later on for visualization.\n",
    "            for resultf in result_files:\n",
    "                shutil.move(f\"{complete_struc_path}/{resultf}\", f\"{path}/NMA/{pdbs}\")\n",
    "            \n",
    "            #now we call prep nma scores to compute the b factor associated values.\n",
    "            prep_nma_scores(dir_with_normal_modes=f\"{path}/NMA/{pdbs}\", nm=10)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1c12f74e-cdf2-4ccc-bcb5-a110666dfca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_nma_scores(dir_with_normal_modes:str, nm=10):\n",
    "    \n",
    "    \n",
    "    #./ptraj_evec_bfactor_cum.pl nma 1kju.ca.pdb evec.dat 10 testbfactorpdb\n",
    "    pdb_list= [f for f in os.listdir(dir_with_normal_modes) if os.path.isfile(os.path.join(dir_with_normal_modes, f)) and f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #from here we grab our executables and scripts\n",
    "    baselocation = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/NMA_pipeline\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        shutil.copy(f\"{baselocation}/ptraj_evec_bfactor_cum.pl\", f\"{dir_with_normal_modes}\")\n",
    "        \n",
    "        #so we execute the script from here.\n",
    "        os.chdir(dir_with_normal_modes)\n",
    "        \n",
    "        run_perl = f\"./ptraj_evec_bfactor_cum.pl nma {pdb_list[0]} evec.dat {nm} {pdb_list[0][:-4]}\"\n",
    "    \n",
    "        bash_cmd = run_perl.split()\n",
    "        \n",
    "        with open(f\"{dir_with_normal_modes}/{pdb_list[0]}_bfactor_res.txt\", \"w\") as bfout:\n",
    "        \n",
    "            result = run(bash_cmd, stdout=bfout, stderr=PIPE,\n",
    "                universal_newlines=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "170b1c2b-bd54-4aec-bc36-f1c7ec159358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/nma_test/NMA/1kju.pdb\"\n",
    "#prep_nma_scores(dir_with_normal_modes=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "101d4459-5b6a-4f44-b8e3-e5dba53a02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/micnag/bioinformatics/nma_test\"\n",
    "\n",
    "#NMA_pipeline(path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb1b6c-478d-4cde-b851-9b88d2df6e74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main function body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "76caadbe-1fcd-47cc-9092-455ccf712fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_struc_1(query:str, templates:str, seq_sim:str, query_start:str, query_end:str,\n",
    "                temp_start:str, temp_end:str,\n",
    "                run_NMA:bool, run_PCA:bool)-> None:\n",
    "    \n",
    "    ''' Input: dictionary containing:\n",
    "        \n",
    "        Key: Gene_name / Uniprot ID\n",
    "        \n",
    "        Values: [0]: All rcsb ids of homologs and their respective chains e.g 8DGD_A for chain A of 8DGD\n",
    "                [1]: Seq similarity [0,1]\n",
    "                \n",
    "        We parse through all mmseq2 hit rcsb ids and retrieve associated structures.\n",
    "        First retrieved structure will be used as REFERENCE structure.\n",
    "        This makes sense because mmseqs2 hits are sorted by seq similarity and highest seq\n",
    "        similarity is listed first (assuming this seq similarity hit corresponds to the best \n",
    "        structural similarity hit).\n",
    "        Then we select the correct chain and compute RMSD / TM score for each respective pair against\n",
    "        the Reference structue. We use the first entry corresponding to the highest seq similarity as template to align other structures against.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #go through all steps and check logic and redundancy.\n",
    "    \n",
    "    uniprot_id = query\n",
    "    #Used to store protein name for matching with cosmic_results later downstream \n",
    "    main_prot_name = get_gene_name_uniprot(query) \n",
    "    \n",
    "    #domain dict will be used to create sub directories later.\n",
    "    domain_dict = _split_domains(templates, temp_start, temp_end, query_start, query_end)\n",
    "\n",
    "    \n",
    "    print(\"this is the output of split domains\")\n",
    "    print(domain_dict)\n",
    "    \n",
    "    \n",
    "    print(main_prot_name)\n",
    "    \n",
    "    path = f\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/{main_prot_name}\"\n",
    "    \n",
    "    #here we store the protein original fasta\n",
    "    main_prot_seq = get_gene_fasta(query)\n",
    "    \n",
    "    #print(main_prot_seq)\n",
    "    \n",
    "    #setup directories and makes batch download pdb input file\n",
    "    chain_dict = setup_directories_prepare_input_files_1(gene_name = main_prot_name,\n",
    "                                          prot_fasta = main_prot_seq,\n",
    "                                          domain_dict = domain_dict)\n",
    "    \n",
    "    \n",
    "\n",
    "    #print(f\"we work currently in the directory: {main_prot_name}\")\n",
    "    print(\"we start download\")\n",
    "    #we download both pdbs as well as mmcif files here.\n",
    "    batch_download_pdbs(gene_name=main_prot_name)\n",
    "    \n",
    "    print(\"we start shifts\")\n",
    "    #correct eventual shifts\n",
    "    shift_dict = parallel_shift_calculation(pdbfolder=path)\n",
    "    \n",
    "    #make new dirs within gene directory for higher oligomers if available.\n",
    "\n",
    "    #get_single_chains_pdbs(pdb_dict=chain_dict, path=path, \n",
    "    #                       domain_dict=domain_dict, shift_dict=shift_dict)\n",
    "    \n",
    "    print(\"we start renumber whole strucs\")\n",
    "    #try to renumber whole structures before building oligomers\n",
    "    parallel_renumbering(shift_dict=shift_dict, path=path)\n",
    "    \n",
    "    \n",
    "    #builds biological assemblies from asymmetric units\n",
    "    oligostates = get_biological_assemblies_atomium_1(path=path, \n",
    "                                            gene_name=main_prot_name,\n",
    "                                            main_iso_seq=main_prot_seq,\n",
    "                                            main_protein_seq=main_prot_seq)\n",
    "    \n",
    "    #print(oligostates)\n",
    "    \n",
    "    print(\"start make_oligo_dirs\")\n",
    "    \n",
    "    #lets make the required directories.\n",
    "    make_oligo_dirs(domain_dict, oligostates, path, domain_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #prepare templates for each domain and each oligomeric state respectively.\n",
    "    template_dict = prepare_references(\n",
    "                       main_gene_name=main_prot_name,\n",
    "                       templates=templates,\n",
    "                       seq_sim = seq_sim,\n",
    "                       query_start=query_start,\n",
    "                       query_end=query_end,\n",
    "                       temp_start =temp_start,\n",
    "                       temp_end =temp_end,\n",
    "                       path=path, oligodict=oligostates)  \n",
    "\n",
    "    #compute tm scores and rmsd\n",
    "    \n",
    "    \n",
    "    \n",
    "    min_seq_len_dict, templates_new_dict = USAlign(templates=template_dict, gene_name=main_prot_name, path=path, report=True)\n",
    "    \n",
    "    print(\"this is new templates dict:\")\n",
    "    print(f\"{templates_new_dict}\")\n",
    "    #now we run the PCA part.\n",
    "    \n",
    "    \"\"\"DO A MAJORITY VOTE TO DECIDE ON THE RELEVANT OLIGOMERIC STATE. WE ONLY CARRY ON THERE FROM HERE On.\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    #oligo_state_to_check = path/to/most/structures\n",
    "    print(\"this is relevant oligomers input\")\n",
    "    print(path, templates_new_dict)\n",
    "    oligo_state_to_check, template = _relevant_oligomers_1(path=path, templates_new_dict=templates_new_dict)\n",
    "    # FUNCTION CALL (ALL_DIRS_WITH_STRUCS):\n",
    "    # RETURN : MAJORITY CLASS \n",
    "    # CONTINUE with majority class ONLY.\n",
    "    \n",
    "    print(\"This is most common state:\")\n",
    "    print(oligo_state_to_check)\n",
    "    print(\"this is template\")\n",
    "    print(template)\n",
    "\n",
    "    #check first for conservation\n",
    "\n",
    "    # steps for conservation\n",
    "    try:\n",
    "        #run mmseq to find homolog sequences\n",
    "\n",
    "        out_dir = f\"{oligo_state_to_check}/mmseq\"\n",
    "        print(\"we start mmseq now!\")\n",
    "        mmseq_fasta_result = mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=out_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "\n",
    "        print(\"we got a df!\")\n",
    "        conserv_df.to_csv(f\"{out_dir}/conservation_df.csv\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        \n",
    "    #lets stop here.\n",
    "    \n",
    "    if run_PCA:\n",
    "        \n",
    "        pca_path = run_pca_for_all_oligomers(path=oligo_state_to_check, \n",
    "                              template=template,\n",
    "                              main_prot_seq=main_prot_seq, \n",
    "                              prot_name=main_prot_name)\n",
    "\n",
    "\n",
    "    #here is second stop point.\n",
    "\n",
    "    # clean_dir_path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA\"\n",
    "    \"\"\"\n",
    "    \n",
    "    path_pca = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/ATP2A1.mode_12.proj\"\n",
    "    path_ensemble = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/ensemble.txt\"\n",
    "    clean_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/clean_ensemble\"\n",
    "    ##path_pca = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_181/PCA/NUDT4B.mode_12.proj\"\n",
    "    ##path_ensemble = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_181/PCA/ensemble.txt\"\n",
    "    savepath = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/cluster_plot.png\"\n",
    "    basepath = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA\"\n",
    "    pca_df = get_cluster_df(PCA_12_proj_file=path_pca,\n",
    "                               prot_names=path_ensemble,\n",
    "                               save_path=savepath,\n",
    "                           ensemble_name=\"ATP2A1\")\n",
    "    \n",
    "    representative_df = get_representative_file_paths(clean_dir= clean_dir, \n",
    "                                          representative_df=pca_df)\n",
    "    \n",
    "    get_nma_domenico(representative_df, basepath)  # this function return representative_df and work dir!\n",
    "    \n",
    "    work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA\"\n",
    "    #get_bfactor_projections(representative_df, work_dir)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if run_NMA:\n",
    "        \n",
    "        #first we compute the normal modes\n",
    "        #NMA_pipeline(path=oligo_state_to_check)\n",
    "        try:\n",
    "            path_ensemble = os.path.join(pca_path, \"ensemble.txt\")\n",
    "            save_path =  os.path.join(pca_path, \"cluster_plot.png\")\n",
    "    \n",
    "            \n",
    "            #get from PCA the different clusters in order to select structures that are used in downstream NMA.\n",
    "            pca_df = get_cluster_df(PCA_12_proj_file=pca_path,\n",
    "                                   prot_names=path_ensemble,\n",
    "                                   save_path=savepath,\n",
    "                               ensemble_name=main_prot_name)\n",
    "    \n",
    "            clean_dir = os.path.join(pca_path, \"clean_ensemble\")\n",
    "    \n",
    "            #get all structures that are assigned -1 as label (i.e not assigned to any cluster) and from each assigned cluster 1 representative structure\n",
    "            #that represents the closest to the main ensemble.\n",
    "            \n",
    "            representative_df = get_representative_file_paths(clean_dir=clean_dir, \n",
    "                                              representative_df=pca_df)\n",
    "    \n",
    "            #get nma for selected structures\n",
    "            representative_df , work_dir = get_nma_domenico(representative_df, pca_path)\n",
    "            #extract b factors from the nma results.\n",
    "\n",
    "            get_bfactor_projections(representative_df, work_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    return\n",
    "    # Initialize variables to avoid errors\n",
    "    updated_clinvar_df = cbioport_df = cosmic_df = gnomad_df = clinvar_df = gnomad_mut_dict = gnomad_mutation_dict = None\n",
    "\n",
    "    \n",
    "    # Step 1: Cosmic mutations\n",
    "    try:\n",
    "        cosmic_df = get_cosmic_mutations(gene_name=main_prot_name)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    #we save it in the folder for the protein outside of monomer / pos at the base level. \n",
    "    save_dataframe_to_csv(cosmic_df, path, \"cosmic_mutations\")\n",
    "\n",
    "    # Step 2: Map gnomad mutations\n",
    "    try:\n",
    "        gnomad_table_path = map_gnomad(Gene_name=main_prot_name, outpath=oligo_state_to_check)\n",
    "        gnomad_df, gnomad_mutation_dict = gnomad_to_pandas(Gene_name=main_prot_name, path_to_tsv=gnomad_table_path, fasta_seq=main_prot_seq)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(gnomad_df, path, \"gnomad_mutations\")\n",
    "\n",
    "    # Step 3: Gather mutations from clinvar\n",
    "    try:\n",
    "        clinvar_df = map_clinvar(Gene_name=main_prot_name)\n",
    "        clinvar_map_outpath = f\"{path}/clinvar_intermediate.csv\"\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(clinvar_df, path, \"clinvar_intermediate\")\n",
    "\n",
    "    # Step 4: Map clinvar to gnomad\n",
    "    try:\n",
    "        list_to_be_searched, clinvar_df = map_clinvar_to_gnomad_1(Gene_name=main_prot_name, clinvar_df=clinvar_df,\n",
    "                                                                  gnomad_mut_dict=gnomad_mutation_dict, clinvar_mapped_df_path=clinvar_map_outpath)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        # Update clinvar muts that were found 1 step before.\n",
    "        try:\n",
    "            updated_clinvar_df = update_clinvar_muts_based_on_gnomad(clinvar_df=clinvar_df, gnomad_dict=gnomad_mutation_dict)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(updated_clinvar_df, path, \"clinvar_mutations\")\n",
    "\n",
    "    # Step 5: Fetch additional info from cbioportal\n",
    "    try:\n",
    "        gene_name = get_hugo_name(uniprot_id)\n",
    "        print(f\"This is gene name in hugo: {gene_name}\")\n",
    "        cbioport_df = get_cbioportal_info(gene_name=gene_name)\n",
    "        save_dataframe_to_csv(cbioport_df, path, \"cbioport_mutations\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    # Print shapes (if available)\n",
    "    dataframes = [cosmic_df, updated_clinvar_df, gnomad_df, cbioport_df]\n",
    "    for df in dataframes:\n",
    "        try:\n",
    "            print(f\"This is df shape: {df.shape}\")\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2e913e73-84ad-4a11-933e-4a702afc2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to handle the common task of saving a DataFrame to CSV\n",
    "def save_dataframe_to_csv(df, path, filename):\n",
    "    try:\n",
    "        df.to_csv(f\"{path}/{filename}.csv\", index=False)\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "70e13565-0393-4f26-a3d7-f71103ff291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cosmic = get_cosmic_mutations(gene_name=\"NUDT4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4c687669-cb03-4b1e-8ff4-4188e8aa12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cosmic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1bc97-83c5-4855-a4a0-679cd504eff7",
   "metadata": {},
   "source": [
    "# MUTATIONAL MAPPING NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "03ba8836-89b3-4301-becf-4ea7976ae9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosmic_mutations(gene_name:str):\n",
    "\n",
    "    path = \"/home/micnag/bioinformatics/cosmic/Cosmic_GenomeScreensMutant_v98_GRCh38.tsv\"\n",
    "    \n",
    "    usecols=['GENE_SYMBOL',\n",
    "         'MUTATION_AA', 'MUTATION_DESCRIPTION',\n",
    "       'MUTATION_ZYGOSITY', 'LOH', 'CHROMOSOME', 'GENOME_START', 'GENOME_STOP']\n",
    "\n",
    "    df = dd.read_csv(path, sep=\"\\t\", dtype={'CHROMOSOME': 'object',\n",
    "       'MUTATION_ZYGOSITY': 'object', 'GENOME_START': 'float64',\n",
    "       'GENOME_STOP': 'float64',\n",
    "       'LOH': 'object'}, usecols=usecols)\n",
    "\n",
    "    #we need to switch these tuples and then map the 1letter aa code to 3letter aa for later compatibility.\n",
    "    lst =  [('Val',\"V\"), ('Ile',\"I\"), ('Leu',\"L\"), ('Glu',\"E\"), ('Gln',\"Q\"),\n",
    "                    ('Asp',\"D\"), ('Asn',\"N\"), ('His',\"H\"), ('Trp',\"W\"), ('Phe',\"F\"), ('Tyr',\"Y\"), \n",
    "                    ('Arg',\"R\"), ('Lys',\"K\"), ('Ser',\"S\"), ('Thr',\"T\"), ('Met',\"M\"), ('Ala',\"A\"), \n",
    "                    ('Gly',\"G\"), ('Pro',\"P\"), ('Cys',\"C\")]\n",
    "\n",
    "    lst = [(y, x) for x, y in lst]\n",
    "\n",
    "    canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "\n",
    "    df_re = df[df[\"MUTATION_DESCRIPTION\"].str.contains(\"missense\")]\n",
    "    \n",
    "    df_re = df_re[df_re[\"GENE_SYMBOL\"] == f\"{gene_name}\"]\n",
    "\n",
    "    meta = ('Gene name', 'str') \n",
    "    df_re['CHROMOSOME'] = df_re['CHROMOSOME'].astype('object')\n",
    "    df_re['WT_AA'] = df_re['MUTATION_AA'].str[2].apply(lambda x: canonical_aas[x], meta=meta)\n",
    "    df_re['MUTATION_POSITION'] = df_re['MUTATION_AA'].str[3:-1]\n",
    "    df_re['MUTATED_AA'] = df_re['MUTATION_AA'].str[-1].apply(lambda x: canonical_aas[x], meta=meta)\n",
    "\n",
    "    df_re = df_re.drop(\"MUTATION_AA\", axis=1)\n",
    "    \n",
    "    cosmic_df = df_re.compute()\n",
    "\n",
    "    cosmic_df[\"GENOME_START\"] = cosmic_df[\"GENOME_START\"].astype(int)\n",
    "    cosmic_df[\"GENOME_STOP\"] = cosmic_df[\"GENOME_STOP\"].astype(int)\n",
    "    \n",
    "    return cosmic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cc889c2f-22c3-4aaa-b69e-f72e4646a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gnomad(Gene_name:str, outpath:str):\n",
    "\n",
    "    outpath = f\"{outpath}/gnomad_datatable.txt\"\n",
    "    #outpath=\"/home/micnag/bioinformatics/hail_trials/newline_test.tsv\"\n",
    "    \n",
    "    path = '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/gnomad_data.mt'\n",
    "    #path1 = \"/home/micnag/bioinformatics/gnomad_raw_data/gnomad_data_2.mt\"\n",
    "    mt = hl.read_matrix_table(path)\n",
    "    \n",
    "    substring1 = Gene_name\n",
    "    substring2 = \"missense\"\n",
    "    \n",
    "    \n",
    "    mt = mt.annotate_rows(Gene_names=mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[3]) ,\n",
    "                      type_of_change = mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[1]) , \n",
    "                      AA_change = mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[11]) , \n",
    "                      ENST_identifier= mt.info.vep.map(\n",
    "        lambda x: x.split(\"\\|\")[6])\n",
    "\n",
    "    ) \n",
    "             \n",
    "    filtered_mt_2 = mt.filter_rows(\n",
    "    \n",
    "    #hl.any(lambda x: hl.str(x).contains(substring3), mt.AA_change)\n",
    "    hl.any(lambda x: hl.str(x).contains(substring1), mt.info.vep) &\n",
    "    hl.any(lambda x: hl.str(x).contains(substring2), mt.info.vep)\n",
    "    \n",
    "    )\n",
    "                     \n",
    "    filtered_mt_3 = filtered_mt_2.annotate_rows(\n",
    "        Allele_count_int = filtered_mt_2.info.AC,\n",
    "        Allele_frequency_float = filtered_mt_2.info.AF,\n",
    "        Allele_number_int = filtered_mt_2.info.AN,\n",
    "        Gene_name_str = _replace_empty(filtered_mt_2.Gene_names), \n",
    "        Mutation_change_str = _replace_empty(filtered_mt_2.AA_change),\n",
    "        Type_of_change_str = _replace_empty(filtered_mt_2.type_of_change))\n",
    "    \n",
    "    \n",
    "    \n",
    "    rows_to_keep = [\"Gene_name_str\", \"Mutation_change_str\", \"Type_of_change_str\", \"Allele_count_int\",\n",
    "                \"Allele_frequency_float\", \"Allele_number_int\"]\n",
    "\n",
    "\n",
    "    selected_rows = filtered_mt_3.select_rows(\n",
    "        Allele_count_int=filtered_mt_3.Allele_count_int,\n",
    "        Allele_frequency_float=filtered_mt_3.Allele_frequency_float,\n",
    "        Allele_number_int=filtered_mt_3.Allele_number_int,\n",
    "        Gene_name_str=hl.str(filtered_mt_3.Gene_name_str),\n",
    "        Mutation_change_str=hl.str(filtered_mt_3.Mutation_change_str),\n",
    "        Type_of_change_str=hl.str(filtered_mt_3.Type_of_change_str)\n",
    "            )\n",
    "\n",
    "    save_buffer = selected_rows.select_rows(*rows_to_keep)\n",
    "    \n",
    "    select_rows_out = save_buffer.rows()\n",
    "    \n",
    "    select_rows_out.export(outpath)\n",
    "    \n",
    "    #this is the location where we save the results.\n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bd5de7fc-51bf-45af-bf9b-843945eb3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnomad_to_pandas(Gene_name:str,path_to_tsv:str, fasta_seq:str):\n",
    "    \n",
    "    df = pd.read_csv(path_to_tsv, sep=\"\\t\")\n",
    "    pd.set_option('display.float_format', '{:.2e}'.format)\n",
    "    \n",
    "    #modifying and streamlining input for further downstream compatibility.\n",
    "    df['Allele_frequency_float'] = df['Allele_frequency_float'].str.strip('[]').astype(float)\n",
    "    df['Allele_count_int'] = df['Allele_count_int'].str.strip('[]').astype(int)\n",
    "    \n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].astype(\"string\")\n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].str.replace(\"[\", \"\")\n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].str.replace(\"]\", \"\")\n",
    "    df[\"Type_of_change_str\"] = df[\"Type_of_change_str\"].str.replace(\"\\\"\", \"\")\n",
    "    \n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].astype(\"string\")\n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].str.replace(\"[\", \"\")\n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].str.replace(\"]\", \"\")\n",
    "    df[\"Mutation_change_str\"] = df[\"Mutation_change_str\"].str.replace(\"\\\"\",\"\")\n",
    "    \n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].astype(\"string\")\n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].str.replace(\"[\", \"\")\n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].str.replace(\"]\", \"\")\n",
    "    df[\"Gene_name_str\"] = df[\"Gene_name_str\"].str.replace(\"\\\"\",\"\")\n",
    "\n",
    "    #required to parse the info.\n",
    "    lst =  [('Val',\"V\"), ('Ile',\"I\"), ('Leu',\"L\"), ('Glu',\"E\"), ('Gln',\"Q\"),\n",
    "                    ('Asp',\"D\"), ('Asn',\"N\"), ('His',\"H\"), ('Trp',\"W\"), ('Phe',\"F\"), ('Tyr',\"Y\"), \n",
    "                    ('Arg',\"R\"), ('Lys',\"K\"), ('Ser',\"S\"), ('Thr',\"T\"), ('Met',\"M\"), ('Ala',\"A\"), \n",
    "                    ('Gly',\"G\"), ('Pro',\"P\"), ('Cys',\"C\")]\n",
    "    \n",
    "    canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "\n",
    "    Mut_dict = defaultdict()\n",
    "\n",
    "    for x, y, z, allele_cnt, allele_freq, allele_num \\\n",
    "        in zip(df[\"Gene_name_str\"], df[\"Mutation_change_str\"], df[\"Type_of_change_str\"],\n",
    "          df[\"Allele_count_int\"], df[\"Allele_frequency_float\"], df[\"Allele_number_int\"]):\n",
    "        \n",
    "        tmp = x.split(\",\")\n",
    "        tmp2 = y.split(\",\")\n",
    "        tmp3 = z.split(\",\")\n",
    "        #print(tmp, tmp2, tmp3)\n",
    "        #print(allele_cnt, allele_freq, allele_num)\n",
    "        for x1, y1, z1 in zip(tmp, tmp2, tmp3):\n",
    "            \n",
    "            #print(x1, y1, z1)\n",
    "            if x1 == Gene_name and y1[0:4] == \"ENSP\":\n",
    "                gene_code = x1\n",
    "                mutation_code = y1[20:]\n",
    "                type_change = z1\n",
    "                \n",
    "                result = _check_proper_transcript(fasta_seq=fasta_seq,\n",
    "                                                  mutation_code=mutation_code, \n",
    "                                                  dict_to_check=canonical_aas, allele_count=allele_cnt)\n",
    "                \n",
    "                #print(gene_code, mutation_code, type_change)\n",
    "                if result:\n",
    "                    Mut_dict[mutation_code] = (allele_cnt, allele_freq, allele_num)\n",
    "        \n",
    "    \n",
    "    #we return the dictionary with out mutations and their respective allele_cnts, allele_freqs, allele_nums\n",
    "    return df, Mut_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "61b7338c-1d57-4650-b3a4-61850e74884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clinvar(Gene_name:str):\n",
    "    \n",
    "    path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/clinvar_database/variant_summary.txt\"\n",
    "\n",
    "    use_cols = [\"Type\", \"Name\", \"GeneSymbol\",\n",
    "           \"ClinicalSignificance\", \"PhenotypeList\",\n",
    "           \"Assembly\", \"ChromosomeAccession\", \n",
    "           \"Chromosome\", \"Start\", \"Stop\"]\n",
    "\n",
    "    column_data_types = {\n",
    "    \"Type\": str,\n",
    "    \"Name\": str,\n",
    "    \"GeneSymbol\": str,\n",
    "    \"ClinicalSignificance\": str,\n",
    "    \"PhenotypeList\": str,\n",
    "    \"Assembly\": str,\n",
    "    \"ChromosomeAccession\": str,\n",
    "    \"Chromosome\": str,\n",
    "    \"Start\": int,\n",
    "    \"Stop\": int\n",
    "    }\n",
    "\n",
    "    df_work = pd.read_csv(path, sep=\"\\t\", usecols=use_cols, dtype=column_data_types)\n",
    "    \n",
    "    df_work.loc[:, \"AA_change\"] = df_work[\"Name\"].str.split().str.get(-1)\n",
    "    df_work.loc[:, \"AA_change\"] = df_work[\"AA_change\"].str.replace(\"(\", \"\")\n",
    "    df_work.loc[:, \"AA_change\"] = df_work[\"AA_change\"].str.replace(\")\", \"\")\n",
    "    \n",
    "    df_work.loc[:,\"Original_AA\"] = df_work[\"AA_change\"].str[2:5]\n",
    "    df_work.loc[:,\"Modified_AA\"] = df_work[\"AA_change\"].str[-3:]\n",
    "    df_work['Position'] = pd.to_numeric(df_work['AA_change'].str[5:-3], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN values in the 'Position' column\n",
    "    df_work.dropna(subset=['Position'], inplace=True)\n",
    "    df_work['Position'] = df_work['Position'].astype(int)\n",
    "    \n",
    "    df_work[\"Genomic_location\"] = df_work[\"Chromosome\"] + \":\" + df_work[\"Start\"].astype(str)\n",
    "    df_work[\"gnomad_aa_change\"] = \"p.\" + df_work[\"Original_AA\"] + df_work[\"Position\"].astype(str) + df_work[\"Modified_AA\"]\n",
    "    \n",
    "    df_work = df_work.drop(\"AA_change\", axis=1)\n",
    "    df_work = df_work.drop(\"Name\", axis=1)\n",
    "    df_work = df_work.drop(\"Chromosome\", axis=1)\n",
    "    df_work = df_work.drop(\"Start\", axis=1)\n",
    "    df_work = df_work.drop(\"Stop\", axis=1)\n",
    "    \n",
    "    df_work[\"Allele_count\"] = [np.nan] * len(df_work)\n",
    "    df_work[\"Allele_number\"] = [np.nan] * len(df_work)\n",
    "    df_work[\"Allele_frequency\"] = [np.nan] * len(df_work)\n",
    "    \n",
    "    \n",
    "    accepted_residues = [\"Ala\", \"Gly\", \"Ser\", \"Leu\", \"Pro\",\n",
    "                    \"Ile\", \"Val\", \"Phe\", \"Tyr\", \"Trp\",\n",
    "                     \"His\", \"Thr\", \"Asn\", \"Gln\", \"Asp\", \n",
    "                     \"Glu\",\"Cys\", \"Met\", \"Lys\", \"Arg\"]\n",
    "    \n",
    "    \n",
    "    #filtering based on our Gene name.\n",
    "    df_filtered = df_work[(df_work[\"Type\"] == \"single nucleotide variant\") & \n",
    "        (df_work[\"GeneSymbol\"] == Gene_name) &\n",
    "        (df_work[\"Assembly\"] == \"GRCh37\") & \n",
    "        (df_work['Original_AA'].isin(accepted_residues)) &\n",
    "        (df_work['Modified_AA'].isin(accepted_residues)) ]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "28b8b170-5a1c-484d-b9ec-83eafb7f1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clinvar_to_gnomad_1(Gene_name: str, clinvar_df: pd.DataFrame,\n",
    "                         gnomad_mut_dict: dict,\n",
    "                         clinvar_mapped_df_path: str):\n",
    "\n",
    "    path = '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/gnomad_data.mt'\n",
    "    mt = hl.read_matrix_table(path)\n",
    "\n",
    "    hits = set(gnomad_mut_dict.keys())\n",
    "\n",
    "    found = defaultdict(tuple)\n",
    "    to_be_searched = []\n",
    "\n",
    "    for x, y in zip(clinvar_df[\"gnomad_aa_change\"], clinvar_df[\"Genomic_location\"]):\n",
    "        if x[2:] in hits:\n",
    "            found[y] = gnomad_mut_dict[x[2:]]\n",
    "        else:\n",
    "            to_be_searched.append((x, y))\n",
    "        \n",
    "\n",
    "    # now we try to search for those hits that were not found in the gnomad dataset.\n",
    "    hit_test = [x[1] for x in to_be_searched]\n",
    "\n",
    "    if not hit_test:\n",
    "        return None\n",
    "\n",
    "    loci_to_filter_hail = hl.set(hit_test)\n",
    "\n",
    "    mt = mt.annotate_rows(Gene_names=mt.info.vep.map(\n",
    "        lambda x: x.split(\"|\")[3]),\n",
    "        type_of_change=mt.info.vep.map(\n",
    "            lambda x: x.split(\"|\")[1]),\n",
    "        AA_change=mt.info.vep.map(\n",
    "            lambda x: x.split(\"|\")[11]),\n",
    "        ENST_identifier=mt.info.vep.map(\n",
    "            lambda x: x.split(\"|\")[6]),\n",
    "        Allele_count_int=mt.info.AC,\n",
    "        Allele_frequency_float=mt.info.AF,\n",
    "        Allele_number_int=mt.info.AN\n",
    "    )\n",
    "\n",
    "    filtered_mt_2 = mt.filter_rows(\n",
    "        loci_to_filter_hail.contains(mt.locus)\n",
    "    )\n",
    "\n",
    "    selected_row_fields = [\"Allele_count_int\", \"Allele_number_int\", \"Allele_frequency_float\"]\n",
    "\n",
    "    selected_rows = filtered_mt_2.rows()\n",
    "\n",
    "    sel_output = selected_rows.select(*selected_row_fields)\n",
    "\n",
    "    # Convert Hail table to Pandas DataFrame\n",
    "    sel_output_df = sel_output.to_pandas()\n",
    "\n",
    "    # Merge the DataFrame with the original clinvar_df\n",
    "    merged_df = pd.merge(clinvar_df, sel_output_df, left_on=[\"gnomad_aa_change\", \"Genomic_location\"], \n",
    "                         right_on=[\"Allele_count_int\", \"Allele_number_int\"], how=\"left\")\n",
    "\n",
    "    # Export the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(clinvar_mapped_df_path, index=False)\n",
    "\n",
    "    # we return the outpath for further downstream work\n",
    "    return to_be_searched, clinvar_mapped_df_path\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2ecbd603-ea49-4368-b579-22506be2a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clinvar_to_gnomad(Gene_name:str, clinvar_df:pd.DataFrame,\n",
    "                         gnomad_mut_dict:dict,\n",
    "                         clinvar_mapped_df_path:str):\n",
    "    \n",
    "    path = '/home/micnag/bioinformatics/gnomad_raw_data/gnomad_data.mt'\n",
    "    #path1 = \"/home/micnag/bioinformatics/gnomad_raw_data/gnomad_data_2.mt\"\n",
    "    mt = hl.read_matrix_table(path)\n",
    "    \n",
    "    hits = []\n",
    "\n",
    "    to_be_searched = []\n",
    "    found = defaultdict(tuple)\n",
    "\n",
    "    for keys, vals in gnomad_mut_dict.items():\n",
    "        #print(keys, vals)\n",
    "        hits.append(keys)\n",
    "\n",
    "    for x, y in zip(clinvar_df[\"gnomad_aa_change\"], clinvar_df[\"Genomic_location\"]):\n",
    "        if x[2:] in hits:\n",
    "            found[y] = (gnomad_mut_dict[x[2:]])\n",
    "        else:\n",
    "            to_be_searched.append((x, y))\n",
    "            #print(x,\"not found\")\n",
    "\n",
    "    #now we try to search for those hits that were not found in the gnomad dataset.\n",
    "    hit_test = [x[1] for x in to_be_searched]\n",
    "\n",
    "\n",
    "    if len(hit_test) == 0:\n",
    "        return None\n",
    "        \n",
    "    #print(len(hit_test)) #126 but only 26 annotated in gnomad.\n",
    "    loci_to_filter_hail = [hl.parse_locus(locus) for locus in hit_test]\n",
    "\n",
    "    #we annotate these rows that bare information that is interesting for us.\n",
    "    mt = mt.annotate_rows(Gene_names=mt.info.vep.map(\n",
    "        \n",
    "        lambda x: x.split(r\"|\")[3]) ,\n",
    "                          \n",
    "                      type_of_change = mt.info.vep.map(\n",
    "        lambda x: x.split(r\"|\")[1]) , \n",
    "                          \n",
    "                      AA_change = mt.info.vep.map(\n",
    "        lambda x: x.split(r\"|\")[11]) , \n",
    "                          \n",
    "                      ENST_identifier= mt.info.vep.map(\n",
    "        lambda x: x.split(r\"|\")[6]),\n",
    "        \n",
    "                      Allele_count_int = mt.info.AC,\n",
    "                          \n",
    "                      Allele_frequency_float = mt.info.AF,\n",
    "                          \n",
    "                      Allele_number_int = mt.info.AN\n",
    "    ) \n",
    "\n",
    "    \n",
    "    filtered_mt_2 = mt.filter_rows(\n",
    "    \n",
    "        #hl.any(lambda x: hl.str(x).contains(substring3), mt.AA_change)\n",
    "        #hl.any(lambda x: hl.str(x).contains(substring1), mt.info.vep) &\n",
    "        #hl.any(lambda x: hl.str(x).contains(substring2), mt.info.vep) &\n",
    "        hl.literal(loci_to_filter_hail).contains(mt.locus)\n",
    "    \n",
    "    ) \n",
    "    \n",
    "    selected_row_fields = [\"Allele_count_int\", \"Allele_number_int\", \"Allele_frequency_float\"]\n",
    "\n",
    "    #selected_rows = selected_rows.rows()\n",
    "\n",
    "    selected_row_field_1 = filtered_mt_2.select_rows(*selected_row_fields)\n",
    "\n",
    "    sel_output = selected_row_field_1.rows()\n",
    "\n",
    "    sel_output.export(clinvar_mapped_df_path)\n",
    "    \n",
    "    #we return the outpath for further downstream work\n",
    "    return to_be_searched, clinvar_mapped_df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "08d6f3d6-ac77-4a77-bb52-295c4a2a1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_clinvar_muts_based_on_gnomad(clinvar_df:pd.DataFrame,\n",
    "                                       gnomad_dict:dict):\n",
    "\n",
    "    clinvar_rev_path = \"/home/micnag/bioinformatics/hail_trials/reverse_mapping_test.tsv\"\n",
    "    \n",
    "    df_reload = pd.read_csv(clinvar_rev_path,\n",
    "                            sep=\"\\t\")\n",
    "\n",
    "    #df_reload.shape\n",
    "\n",
    "    #df_reload.columns\n",
    "    df_reload['Allele_frequency_float'] = df_reload['Allele_frequency_float'].str.strip('[]').astype(float)\n",
    "    df_reload['Allele_count_int'] = df_reload['Allele_count_int'].str.strip('[]').astype(int)\n",
    "\n",
    "    for x, allele_c, allele_num, allele_freq in zip(df_reload[\"locus\"],\n",
    "                                                    df_reload[\"Allele_count_int\"],\n",
    "                                                    df_reload[\"Allele_number_int\"],\n",
    "                                                    df_reload['Allele_frequency_float']):\n",
    "        #print(x, allele_c, allele_freq, allele_num)\n",
    "        if x not in gnomad_dict.keys():\n",
    "            gnomad_dict[x] = (allele_c, allele_freq, allele_num)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    #setup dummies for mapping.\n",
    "    clinvar_df[\"Allele_count\"] = [np.nan] * len(clinvar_df)\n",
    "    clinvar_df[\"Allele_number\"] = [np.nan] * len(clinvar_df)\n",
    "    clinvar_df[\"Allele_frequency\"] = [np.nan] * len(clinvar_df)\n",
    "    \n",
    "    for k, v in gnomad_dict.items():\n",
    "    \n",
    "        #k = chr:position e.g 16:17464758  and v = (Allele_cnt, Allele_freq, Allele_num)\n",
    "        clinvar_df.loc[clinvar_df[\"Genomic_location\"] == k, \"Allele_count\"] = int(v[0])\n",
    "        clinvar_df.loc[clinvar_df[\"Genomic_location\"] == k, \"Allele_frequency\"] = v[1]\n",
    "        clinvar_df.loc[clinvar_df[\"Genomic_location\"] == k, \"Allele_number\"] = int(v[2])\n",
    "    \n",
    "    \n",
    "    return clinvar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "364578b7-0b1e-4b82-9d21-0734512e1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_proper_transcript(fasta_seq:str, mutation_code:str,\n",
    "                             dict_to_check:dict, allele_count:int):\n",
    "    \n",
    "    #print(mutation_code)\n",
    "    try:\n",
    "        position = int(mutation_code[3:-3])\n",
    "        \n",
    "    except Exception as error:\n",
    "        return False\n",
    "    \n",
    "    #quality filter to return ONLY variants that passed the filter criteria.\n",
    "    if allele_count == 0:\n",
    "        return False\n",
    "    \n",
    "    aa_according_to_gnomad = mutation_code[0:3]\n",
    "    \n",
    "    aa_to_check = dict_to_check[aa_according_to_gnomad]\n",
    "    \n",
    "    aa_according_to_fasta = fasta_seq[position-1] #-1 because of python starting with 0\n",
    "    \n",
    "    #print(aa_to_check, aa_according_to_fasta)\n",
    "    if aa_to_check == aa_according_to_fasta:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "26077e38-3d9a-4512-9efa-6f337546f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_empty(arr):\n",
    "    return hl.map(lambda x: hl.if_else(x == \"\", \"empty\", x), arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271790be-a65f-43b5-affd-f8117565b80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5031d-9884-438d-a2a9-8c6fea5c58d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5e07c586-4cf7-4e7e-a86c-fec78325fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relevant_oligomers_1(path:str, templates_new_dict:dict):\n",
    "    \n",
    "    \"\"\"This function basically will call a bunch of other functions \n",
    "    for each potential oligomeric state and each position within them.\"\"\"\n",
    "    \n",
    "    oligodirectories = [\"monomer\",\n",
    "                        \"dimer\",\n",
    "                        \"trimer\",\n",
    "                        \"tetramer\",\n",
    "                        \"pentamer\",\n",
    "                        \"hexamer\",\n",
    "                        \"heptamer\",\n",
    "                        \"oktamer\",\n",
    "                        \"nonamer\",\n",
    "                        \"decamer\",\n",
    "                        \"undecamer\",\n",
    "                        \"dodecamer\",\n",
    "                        \"tridecamer\",\n",
    "                        \"tetradecamer\",\n",
    "                        \"pentadecamer\",\n",
    "                        \"hexadecamer\",\n",
    "                        \"heptadecamer\",\n",
    "                        \"oktadecamer\",\n",
    "                        \"nonadecamer\",\n",
    "                        \"eicosamer\"\n",
    "    ]\n",
    "    \n",
    "    relevant_dirs = [os.path.join(path, file) for file in oligodirectories if os.path.isdir(os.path.join(path, file))]\n",
    "    \n",
    "    dir_dictionary = {dirs: os.listdir(dirs) for dirs in relevant_dirs}\n",
    "\n",
    "    path_to_check = [os.path.join(dirs, subdir) for dirs, vals in dir_dictionary.items() for subdir in vals]\n",
    "    \n",
    "    path_majority_vote = [\n",
    "        (check_paths, len(os.listdir(check_paths))) for check_paths in path_to_check if os.path.isdir(check_paths)\n",
    "    ]\n",
    "\n",
    "    majority_vote = sorted(path_majority_vote, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"this is majority vote\")\n",
    "    print(majority_vote)\n",
    "\n",
    "    #[('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_149', 14),\n",
    "    majority_path = majority_vote[0][0]\n",
    "\n",
    "    # Extract the relevant key from the majority_path\n",
    "    key_parts = majority_path.split('/')\n",
    "    relevant_key = f\"{key_parts[-2]}/{key_parts[-1]}\"\n",
    "\n",
    "    print(f\"this is relevant_key: {relevant_key} and this its templates_new_dict: {templates_new_dict}\")\n",
    "    # Now look up the template\n",
    "    template = templates_new_dict.get(relevant_key, [None])[0]\n",
    "\n",
    "    print(f\"this is majority path: {majority_path} and this is template: {template}\")\n",
    "    return majority_path, template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1251b2af-2a99-4fa5-ab9e-1b7ee95322a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_b_factors_0(path:str):\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    prot_name = f\"test\"\n",
    "    \n",
    "    fullpath = f\"{path}\"\n",
    "    \n",
    "    structure = parser.get_structure(prot_name, fullpath)\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    atom.set_bfactor(0.0)\n",
    "    \n",
    "    \n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    \n",
    "    savepath = fullpath\n",
    "        \n",
    "    #we save all structures to the monomeric category.\n",
    "    io.save(savepath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c38f4622-48d6-4aa3-8692-a33dfbd89c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_for_all_oligomers(path:str, \n",
    "                             template:str,\n",
    "                             main_prot_seq:str,\n",
    "                             prot_name:str):\n",
    "    \n",
    "        \n",
    "    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f[-4:] == \".pdb\"]\n",
    "    \n",
    "    #if there are more than 5 structures we do PCA\n",
    "    if len(onlyfiles) >= 5:\n",
    "        #structure_based_cutting_1(path_to_pdbs:str, template:str, main_prot_seq:str):\n",
    "        \n",
    "        template_new = structure_based_cutting_1(path_to_pdbs=path, \n",
    "                               template=template,\n",
    "                               main_prot_seq=main_prot_seq)\n",
    "        \n",
    "        print(template_new)\n",
    "        #now if it worked we can run PCA\n",
    "        work_dir, protein, num_struc = pca_laura_pipeline_1(path=f\"{path}/PCA\",\n",
    "                           template=template_new,\n",
    "                           protein=prot_name)\n",
    "        \n",
    "        try:\n",
    "            _plot_PCA_new(input_dir=work_dir, protein=protein, num_struc=num_struc)\n",
    "        except Exception as error:\n",
    "            \n",
    "            print(\"we could not run _plot_PCA_new\")\n",
    "            print(error)\n",
    "\n",
    "\n",
    "    return work_dir\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "34f3e653-0194-400f-a30d-34ad0c9250c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_pca_for_all_oligomers(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ADH1G_HUMAN/dimer\",\n",
    "#                         template_dict=None,\n",
    "#                         main_prot_seq=None,\n",
    "#                         prot_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9b1e9-860d-4ed8-87f7-c873a6e70cce",
   "metadata": {},
   "source": [
    "1. map_gnomad()  #we map all gnomad mutations to a hailtable and select only those relevant gene muts.\n",
    "2. gnomad_to_pandas()  #we convert the found mutations of gnomad to a pandas df.\n",
    "3. map_clinvar()     #we search for all clinvar mutations based on the same gene_name. \n",
    "4. map_clinvar_to_gnomad()  # we search for those mutations that dont already have associated frequencies if they might be present in gnomad\n",
    "5. update_clinvar_muts_based_on_gnomad() #now we update our previous clinvar df with those new found frequencies (if found)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72c337-8e25-49fe-96cb-b34797567f36",
   "metadata": {},
   "source": [
    "# testblock to retrieve mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d42cd5d3-ff4f-4c71-b2aa-3be827874a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 gather mutations from cosmic:\n",
    "\n",
    "#main_prot_name = \"FLNC\"\n",
    "#main_prot_seq = get_gene_fasta(main_prot_name)\n",
    "#\n",
    "#path = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/AT2A1_HUMAN/monomer/pos_1_994\"\n",
    "#oligo_state_to_check = path\n",
    "#\n",
    "#clinvar_mapped_df_path = f\"{path}/clinvar_map.txt\"\n",
    "#\n",
    "#cosmic_df = get_cosmic_mutations(gene_name=main_prot_name) \n",
    "#\n",
    "#cosmic_df.head()\n",
    "#\n",
    "#try:\n",
    "#    cosmic_df.to_csv(f\"{path}/cosmic_mutations.csv\")\n",
    "#except Exception as error:\n",
    "#    print(error)\n",
    "##    \n",
    "##step 2 map gnomad mutations.\n",
    "#\n",
    "#gnomad_table_path = map_gnomad(Gene_name=main_prot_name, outpath=oligo_state_to_check)\n",
    "##\n",
    "###step 3 extract mutations from gnomad\n",
    "#gnomad_df, gnomad_mutation_dict = gnomad_to_pandas(Gene_name=main_prot_name,\n",
    "#             path_to_tsv=gnomad_table_path, \n",
    "#             fasta_seq=main_prot_seq)\n",
    "##\n",
    "##\n",
    "###print(gnomad_mutation_dict)\n",
    "#gnomad_df.head()\n",
    "#try:\n",
    "#    gnomad_df.to_csv(f\"{path}/gnomad_mutations.csv\")\n",
    "#except Exception as error:\n",
    "#    print(error)\n",
    "##\n",
    "##step 4 gather mutations from clinvar\n",
    "#clinvar_df = map_clinvar(Gene_name=main_prot_name)\n",
    "### Search for mutations in gnomad if they are present so we obtain info about allele freq ect.\n",
    "#list_to_be_searched, clinvar_mapped_df_path = map_clinvar_to_gnomad(Gene_name=main_prot_name, clinvar_df=clinvar_df,\n",
    "#                     gnomad_mut_dict=gnomad_mutation_dict,clinvar_mapped_df_path=clinvar_mapped_df_path)\n",
    "#\n",
    "#\n",
    "#clinvar_df.head()\n",
    "#\n",
    "### Update clinvar muts that were found 1 step before.\n",
    "#updated_clinvar_df = update_clinvar_muts_based_on_gnomad(clinvar_df=clinvar_df, gnomad_dict=gnomad_mutation_dict)\n",
    "##try:\n",
    "##    updated_clinvar_df.to_csv(f\"{path}/clinvar_mutations.csv\")\n",
    "##except Exception as error:\n",
    "##    print(error)\n",
    "#\n",
    "#updated_clinvar_df.head()\n",
    "#\n",
    "##print(f\"This is cosmic df: {cosmic_df.shape}\")\n",
    "##\n",
    "##print(f\"This is updated_clinvar df: {updated_clinvar_df.shape}\")\n",
    "##\n",
    "##print(f\"This is gnomad df: {gnomad_df.shape}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe44db-72f8-4c00-8c7a-99526917540a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8aed7a0d-be53-461d-b95e-2b29cbc0af61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#_for_all_oligomers(path=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\", template_dict=template_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0033513-b595-4f5b-a561-f923ee000ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "df9d1f14-4d11-4b14-9158-5e80404eabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repair flnc strucs\n",
    "\n",
    "def FLNC_repair(path_to_pdb:str, stop_pos:int, main_prot_seq:str):\n",
    "    \"\"\"Function should take only those proteins that have\n",
    "    a) gaps with less than 7 residues missing per gap.\n",
    "    \n",
    "    Input:\n",
    "    path to pdb_folder\n",
    "    gap_dict file\n",
    "    \n",
    "    Output:\n",
    "    produces repaired structures\n",
    "    \"\"\"\n",
    "\n",
    "    #onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    \n",
    "    #pdbs = [corr_file for corr_file in onlyfiles if corr_file[-4:] == \".pdb\"]\n",
    "    \n",
    "    log.none() #no stdout spam\n",
    "    env = Environ() #setup env for modelling\n",
    "    \n",
    "    aln = Alignment(env) # setup alignment within the env.\n",
    "    \n",
    "    mdl = Model(env)  #setup the model object where we will append our pdbs into.\n",
    "     \n",
    "    current_pth = os.getcwd()\n",
    "    \n",
    "    #print(\"we are inside mini repair\")\n",
    "    #print(pdb_basep)\n",
    "\n",
    "    #['', 'home', 'micnag', 'result_test_struc_align', 'original_7e7s_A.pdb']\n",
    "    #print(pdb_basep)\n",
    "    #/home/micnag/result_test_struc_align\n",
    "    #print(pdb_id_target)\n",
    "    #7e7s_A\n",
    "    #print(pdb_id_chain)\n",
    "    #A\n",
    "    \n",
    "    # we need this for downstream split.\n",
    "    pdb_id_target = \"FLNc_ABD\"\n",
    "    \n",
    "    pdb_id_chain = \"A\"\n",
    "    \n",
    "    \n",
    "    #this is the new version which grabs correct seq per chain.\n",
    "    \n",
    "    #if original ensemble: we fetch the right uniprot sequence else we fetch whatever other homolog protein it is.\n",
    "\n",
    "    #either we grab this!\n",
    "    fasta_seq = main_prot_seq\n",
    "        \n",
    "    print(fasta_seq)\n",
    "    print(len(fasta_seq))\n",
    "    \n",
    "    os.chdir(\"/home/micnag/bioinformatics/FLNC_project/new_structures_modeller_repaired\")\n",
    "    print(os.getcwd())\n",
    "    \n",
    "    \n",
    "    #start stop grab:\n",
    "    \n",
    "    pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "        \n",
    "    #sample struc\n",
    "    sample_structure = pdb_parser.get_structure(\"sample\", path_to_pdb)\n",
    "    \n",
    "    sample_res = sample_structure.get_residues()\n",
    "    \n",
    "    sample_list = [x.get_id()[1] for x in sample_res]\n",
    "    \n",
    "    start = sample_list[0] #first residue.\n",
    "    stop = len(fasta_seq) #max amount of fasta seq.\n",
    "    \n",
    "    print(start, stop)\n",
    "    \n",
    "    env.io.atom_files_directory = ['.','../.']\n",
    "\n",
    "\n",
    "    #write fasta\n",
    "    #grab start + end pos.\n",
    "    \n",
    "    #start, stop, chain = _start_stop_fasta(pdb_id_target=pdb_id_target, \n",
    "    #                                                   path=f\"{pdb_basep}/{pdb_id_target}\")\n",
    "    \n",
    "    code = f\"{pdb_id_target}\"\n",
    "    \n",
    "    #print(\"this will be start and stop\")\n",
    "    #print(start, stop)\n",
    "    \n",
    "    \n",
    "    mdl.read(file=code, model_segment=(f\"{start}:{pdb_id_chain}\", f\"{stop}:{pdb_id_chain}\"))\n",
    "    \n",
    "    aln.append_model(mdl, align_codes=code, atom_files=code)\n",
    "    \n",
    "    #print(\"this is pdb_id_target and chain\")\n",
    "    #print(f\"{pdb_id_target[0:4]}\", chain)\n",
    "    \n",
    "    #this is the old version which grabs full fasta.\n",
    "    #fasta_seq = get_gene_fasta_from_pdb_id(f\"{pdb_id_target[0:4]}\", chain=chain)\n",
    "    \n",
    "    #print(\"this is fasta seq\")\n",
    "    #print(fasta_seq)\n",
    "    \n",
    "    with open(f\"./{pdb_id_target}x.fasta\", \"w\") as fastaout:\n",
    "        fastaout.write(f\">{pdb_id_target}x\\n\")\n",
    "        fastaout.write(fasta_seq)\n",
    "    \n",
    "    \n",
    "    aln_code = f\"{pdb_id_target}x\"\n",
    "\n",
    "\n",
    "    aln.append(file=f\"./{pdb_id_target}x.fasta\", align_codes=aln_code, \n",
    "               alignment_format=\"fasta\")\n",
    "    \n",
    "    aln.salign(overhang=30, gap_penalties_1d=(-450, -50),\n",
    "    alignment_type=\"tree\", output=\"ALIGNMENT\")\n",
    "    \n",
    "    ##aln.malign(gap_penalties_1d=(-500, -300))\n",
    "    ##aln.malign3d(gap_penalties_3d=(0.0, 2.0))\n",
    "    aln.write(file=f\"{pdb_id_target}.ali\")\n",
    "\n",
    "    #lets model\n",
    "    \n",
    "    #we will fish one out of the pdb list and align it against all others.\n",
    "    \n",
    "    a = AutoModel(env,\n",
    "                  alnfile = f\"{pdb_id_target}.ali\",\n",
    "                  knowns = f\"{pdb_id_target}\",\n",
    "                  sequence = aln_code)\n",
    "    \n",
    "    a.starting_model = 1\n",
    "    a.ending_model = 1\n",
    "    a.make()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aacd8c9f-9f6a-43fc-a774-dc84b8e9a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_prot_seq = \"MMNNSGYSDAGLGLGDETDEMPSTEKDLAEDAPWKKIQQNTFTRWCNEHLKCVGKRLTDLQRDLSDGLRLIALLEVLSQKRMYRKFHPRPNFRQMKLENVSVALEFLEREHIKLVSIDSKAIVDGNLKLILGLIWTLILHYSISMPMWEDEDDEDARKQTPKQRLLGWIQNKVPQLPITNFNRDWQDGKALGALVDNCAPGLCPDWEAWDPNQPVENAREAMQQADDWLGVPQVIAPEEIVDPNVDEHSVMTYLSQFPKAKL\"\n",
    "\n",
    "#path = \"/home/micnag/bioinformatics/FLNC_project/new_structures_modeller_repaired/FLNc_ABD.pdb\"\n",
    "\n",
    "#FLNC_repair(path_to_pdb=path, stop_pos=262, main_prot_seq=main_prot_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a9ea1-8db2-465a-86a3-31e78d70eb6e",
   "metadata": {},
   "source": [
    "# integrate cbioportal for patient information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ca2af421-2722-4f56-9feb-59ae0ab058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_name = get_hugo_name(\"P00533\")\n",
    "#print(gene_name)\n",
    "#result_df = get_cbioportal_info(gene_name=gene_name)\n",
    "#result_df.head(10)\n",
    "\n",
    "#print(result_df.shape)\n",
    "#['alleleSpecificCopyNumber', 'aminoAcidChange', 'center', 'chr', 'driverFilter', \n",
    "# 'driverFilterAnnotation', 'driverTiersFilter', 'driverTiersFilterAnnotation', \n",
    "# 'endPosition', 'entrezGeneId', 'gene', 'keyword', 'molecularProfileId', 'mutationStatus', \n",
    "# 'mutationType', 'namespaceColumns', 'ncbiBuild', 'normalAltCount', 'normalRefCount', \n",
    "# 'patientId', 'proteinChange', 'proteinPosEnd', 'proteinPosStart', 'referenceAllele', \n",
    "# 'refseqMrnaId', 'sampleId', 'startPosition', 'studyId', 'tumorAltCount', 'tumorRefCount', \n",
    "# 'uniquePatientKey', 'uniqueSampleKey', 'validationStatus', 'variantAllele', 'variantType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f11e0c8f-1955-4279-9bf1-a65a604a0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq alignment through mmseq2 and retrieval of multi pdb fasta.\n",
    "\n",
    "def mmseq_multi_fasta(uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "\n",
    "    \"\"\"\n",
    "    uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "\n",
    "    outdir: location where result files will be stored.\n",
    "\n",
    "    sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "\n",
    "    filter_msa = 0 default. if 1 hits are stricter.\n",
    "\n",
    "    query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #we blast with this fasta as query.\n",
    "    trgt_fasta_seq = get_gene_fasta(uniprot_id)\n",
    "\n",
    "\n",
    "    #Make outdir for all required files.\n",
    "    try:\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    #we need to write it out to file.\n",
    "    with open(f\"{outdir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "        fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "        fasta_out.write(trgt_fasta_seq)\n",
    "    \n",
    "    #fetch pre downloaded database from a parent folder.\n",
    "\n",
    "    msa_file = None\n",
    "    new_location = None\n",
    "    \n",
    "    try:\n",
    "        DB_storage_location = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/swissprot_DB\"\n",
    "        #shutil.copy(previous_path, savepath)\n",
    "        \n",
    "        bash_curl_cmd = f\"mmseqs createdb {outdir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\"\n",
    "    \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "\n",
    "        print(bash_curl_cmd)\n",
    "        print(\"we start now\")\n",
    "        \n",
    "        #run first cmd which setups query database based on our input fasta file\n",
    "        result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "\n",
    "        print(\"this worked\")\n",
    "        print(result_setup_query_db.stderr)\n",
    "\n",
    "        bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"\n",
    "    \n",
    "        bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "        \n",
    "        #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "        result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "\n",
    "        print(\"this worked as well\")\n",
    "        print(result_setup_blast_db.stderr)\n",
    "        #mmseqs result2flat resultDB resultDB_flat --threads 4\n",
    "        #mmseqs result2fasta pdb70_mmseqs2.fasta resultDB_flat resultDB.fasta --threads 4\n",
    "\n",
    "        # mmseqs convertalis queryDB targetDB resultDB result.fasta\n",
    "\n",
    "        #mmseqs result2flat queryDB targetDB resultDB result_flat.txt\n",
    "\n",
    "        #bash_curl_cmd_4 = f\"mmseqs result2flat {DB_storage_location}/result_DB {DB_storage_location}/result_DB_flat --use-fasta-header TRUE\"\n",
    "\n",
    "        #bash_curl_cmd_rdy_4 = bash_curl_cmd_4.split()\n",
    "\n",
    "        #result_setup_flat_convert = run(bash_curl_cmd_rdy_4, stdout=PIPE, stderr=PIPE, \n",
    "        #                     universal_newlines=True)\n",
    "        \n",
    "        #print(\"here works still\")\n",
    "        #bash_curl_cmd_3 = f\"mmseqs convertalis {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_result.m8\"\n",
    "        #mmseqs convertalis query_fastaDB PDB_DB result_DB \"$input_fasta\"_result.m8\n",
    "        #bash_curl_cmd_rdy_3 = bash_curl_cmd_3.split()\n",
    "        \n",
    "        #result_setup_flat_convert = run(bash_curl_cmd_rdy_3, stdout=PIPE, stderr=PIPE, \n",
    "        #                     universal_newlines=True)\n",
    "\n",
    "        #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "        bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "\n",
    "        bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "\n",
    "        result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "        #print(result_setup_flat_convert.stderr)\n",
    "\n",
    "        #delete last line.. required.\n",
    "        sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'\n",
    "        \n",
    "        bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "\n",
    "        #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "        with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "            result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                             universal_newlines=True)\n",
    "\n",
    "        # Specify the path to your MSA file\n",
    "        msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "\n",
    "\n",
    "        #transfer the msa file to another location and delete useless files.\n",
    "        # we need to delete : all uniprot* files. \n",
    "        # all query*. All result* \n",
    "\n",
    "        new_location = f\"{outdir}/{uniprot_id}.fasta\"\n",
    "\n",
    "        \n",
    "        shutil.copy(msa_file, new_location)\n",
    "\n",
    "        remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "    #we want the path to msa_file for downstream analysis.\n",
    "    return new_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "31abe1c3-0a7d-484c-98ff-60c4a6f9e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_and_dirs_msa(directory, uniprot_id):\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Check if the item is a file and matches the specified patterns\n",
    "        if os.path.isfile(file_path) and (filename.startswith(\"query\") or filename.startswith(\"result\") or filename.startswith(uniprot_id)):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed file: {file_path}\")\n",
    "        \n",
    "        # Check if the item is a directory and has the name \"tmp\"\n",
    "        elif os.path.isdir(file_path) and filename == \"tmp\":\n",
    "            shutil.rmtree(file_path)\n",
    "            print(f\"Removed directory and its contents: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dd87c061-9d6a-4426-a96e-5f6e05818fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conservation(path_to_msa:str):\n",
    "\n",
    "    '''\n",
    "    path_to_msa:str    path to the multiple sequence alignment file. The first entry is expected to be used as reference.\n",
    "    canal: object that contains the msa seq. Will be used with canal.analysis to compute conservation scores.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "    result_cons = canal.analysis(method=\"all\")\n",
    "    #this is a pandas df that contains 3 cols : 3 diff conservation score computations.\n",
    "    return result_cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f168fe3-7cc7-415c-a990-08115ab7c8d4",
   "metadata": {},
   "source": [
    "# steps for conservation\n",
    "\n",
    "1) mmseq_multi_fasta\n",
    "2) remove_files_and_dirs_msa\n",
    "3) get_conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d70ab2a5-fe83-41ea-b75f-b653ec72be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outdir = \"/home/micnag/bioinformatics/mmseq_dir_pipe_test\"\n",
    "#uniprot_id = \"Q14315\"\n",
    "\n",
    "#out_path_msa = mmseq_multi_fasta(uniprot_id =uniprot_id, \n",
    "#                outdir=outdir)\n",
    "\n",
    "#get_conservation(path_to_msa=out_path_msa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e936d-78b5-4c48-8bdd-3f447a82f5f5",
   "metadata": {},
   "source": [
    "# PCA cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0878241e-9988-4aa9-ab52-ce3c1ef759b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_df(PCA_12_proj_file:str, prot_names:str, save_path:str, ensemble_name:str):\n",
    "    \n",
    "    '''\n",
    "\n",
    "    Input:\n",
    "\n",
    "    PCA_12_proj_file: Path to the file that contains the projected PC1 and PC2 values as column entries.\n",
    "\n",
    "    prot_names: The path to the file that contains the names of the associated proteins (4 digit pdb code) that are used in the PCA.\n",
    "\n",
    "    Output:\n",
    "\n",
    "    pandas.DataFrame ready for downstream applications. \n",
    "\n",
    "    Cols: prot_name , PC1, PC2\n",
    "    \n",
    "    '''\n",
    "\n",
    "    PC_lst = []\n",
    "    prot_lst = []\n",
    "    #lets store the PC 1 2 coords as a list consisting of tuples [(PC1, PC2), (PC1, PC2),.....n] for all n structures.\n",
    "    with open(PCA_12_proj_file, \"r\") as pca_input:\n",
    "        for lines in pca_input:\n",
    "            lines = lines.split()\n",
    "            PC_lst.append((lines[2], lines[3]))\n",
    "\n",
    "    #here we have the associated names + chains so we also add them to a list for later retrieval.\n",
    "    with open(prot_names, \"r\") as prot_input:\n",
    "        for pdb_chain in prot_input:\n",
    "            pdb_chain = pdb_chain.replace(\"\\n\",\"\")\n",
    "            prot_lst.append(pdb_chain)\n",
    "\n",
    "    pca_df = pd.DataFrame()\n",
    "    pca_df[\"prot_name\"] = prot_lst\n",
    "    pca_df[\"PC1\"] = [float(x[0]) for x in PC_lst]\n",
    "    pca_df[\"PC2\"] = [float(x[1]) for x in PC_lst]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    PC1 = pca_df['PC1'].values\n",
    "    PC2 = pca_df['PC2'].values\n",
    "    X = np.column_stack((PC1, PC2))\n",
    "    #y = pca_df.iloc[:,0]\n",
    "    \n",
    "    PC_normalized = scaler.fit_transform(X)\n",
    "    #print(PC_normalized)\n",
    "    pca_df[\"PC1_norm\"] = PC_normalized[:, 0]\n",
    "    pca_df[\"PC2_norm\"] = PC_normalized[:, 1]\n",
    "    \n",
    "     # Perform DBSCAN\n",
    "    clusters = cluster_pca_dbscan(pca_df)\n",
    "    pca_df[\"clusters\"] = clusters\n",
    "    \n",
    "    #this df contains : all -1 labels and from each other label 1 mean structure (corresponding to the structure closest to the mean of PC1 and PC2 over the whole cluster)\n",
    "    representative_df = extract_representative_strucs(pca_df)\n",
    "\n",
    "    \n",
    "    unique_clusters = pca_df['clusters'].unique()\n",
    "    custom_cmap = plt.get_cmap('Set1', len(unique_clusters))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(pca_df['PC1_norm'], pca_df['PC2_norm'], c=pca_df['clusters'], cmap=custom_cmap, s=50)\n",
    "    \n",
    "    # Add labels with jitter\n",
    "    jitter_amount = 0.02  # Adjust as needed\n",
    "    for i, row in representative_df.iterrows():\n",
    "        x_jitter = np.random.uniform(-jitter_amount, jitter_amount)\n",
    "        y_jitter = np.random.uniform(-jitter_amount, jitter_amount)\n",
    "        plt.text(row['PC1_norm'] + x_jitter, row['PC2_norm'] + y_jitter, row['prot_name'], alpha=1)\n",
    "\n",
    "    plt.title(f'DBSCAN automated PCA clustering of {ensemble_name}')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    \n",
    "\n",
    "     # Set colorbar ticks to only include used cluster labels\n",
    "    colorbar = plt.colorbar(scatter, ticks=unique_clusters, label='Cluster Labels')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    \n",
    "    return representative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d79fe3a6-1919-40a2-b724-3181fc8c2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_pca_dbscan(pca_df:pd.DataFrame):\n",
    "    # Assuming pca_df has columns 'PC1' and 'PC2'\n",
    "    X = pca_df[['PC1_norm', 'PC2_norm']].values\n",
    "    y = pca_df.iloc[:,0]\n",
    "\n",
    "    dbscan_model = DBSCAN(eps=0.5, min_samples=5)  # You may need to adjust eps and min_samples\n",
    "    dbscan_labels = dbscan_model.fit_predict(X)\n",
    "    \n",
    "    return dbscan_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3a84c5d5-95fd-415e-9def-51fcd8ef9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_representative_strucs(pca_df:pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "   # Extract rows where 'clusters' column is not -1\n",
    "    non_minus_one_df = pca_df[pca_df['clusters'] != -1].copy()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the representative structures\n",
    "    representative_df = pd.DataFrame()\n",
    "\n",
    "    # Calculate mean values for each label group\n",
    "    mean_values = non_minus_one_df.groupby('clusters')[['PC1', 'PC2']].mean()\n",
    "\n",
    "    # Iterate over unique labels and find the row closest to the mean for each label\n",
    "    for label in non_minus_one_df['clusters'].unique():\n",
    "        label_rows = non_minus_one_df[non_minus_one_df['clusters'] == label]\n",
    "        label_mean = mean_values.loc[label]\n",
    "\n",
    "        # Calculate the distance to the mean for each row in the label group\n",
    "        distances = np.linalg.norm(label_rows[['PC1', 'PC2']].values - label_mean.values, axis=1)\n",
    "\n",
    "        # Find the row with the minimum distance\n",
    "        closest_row_index = distances.argmin()\n",
    "        closest_row = label_rows.iloc[closest_row_index:closest_row_index + 1].copy()\n",
    "\n",
    "        # Append the closest row to the representative DataFrame\n",
    "        representative_df = pd.concat([representative_df, closest_row], ignore_index=True)\n",
    "\n",
    "    # Append all rows with label -1 to the representative DataFrame\n",
    "    label_minus_one_rows = pca_df[pca_df['clusters'] == -1].copy()\n",
    "    representative_df = pd.concat([representative_df, label_minus_one_rows], ignore_index=True)\n",
    "\n",
    "    return representative_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c522f622-ffb4-4e3f-8af5-5911f3e2bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy_files(representative_df:pd.DataFrame, basepath:str):\n",
    "\n",
    "    try:\n",
    "        os.mkdir(f\"{basepath}/NMA\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    file_destination = f'{basepath}/NMA/'\n",
    "    \n",
    "    for filep in representative_df.loc[representative_df['file_path_exists'], 'file_path']:\n",
    "        try:\n",
    "            shutil.copy(filep, file_destination)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return file_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "70949a2e-38e7-4ff0-b75e-a207de825295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement domenico NMA for those structures that are found representative for the ensemble.\n",
    "\n",
    "def get_nma_domenico(representative_df:pd.DataFrame, basepath:str):\n",
    "    #print(os.getcwd())\n",
    "    #os.chdir(\"/home/micnag/bioinformatics/domenico_nma\")\n",
    "\n",
    "    work_dir = _copy_files(representative_df, basepath)\n",
    "\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    print(os.getcwd())\n",
    "\n",
    "    #copy executable from baselocation to this folder.\n",
    "    \n",
    "    try:\n",
    "        baselocation = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/ENM_NMA_general\"\n",
    "        shutil.copy(baselocation, work_dir)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    nma_commands = []\n",
    "    \n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(work_dir):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(work_dir, filename)\n",
    "        \n",
    "        # Check if the path is a regular file and has a \".pdb\" extension\n",
    "        if os.path.isfile(file_path) and filename.endswith('.pdb'):\n",
    "            # Your code to process each \".pdb\" file goes here\n",
    "            pdb_without_ending = filename[:-4]\n",
    "\n",
    "            pdb_chain = pdb_without_ending.split(\"_\")[-1]\n",
    "\n",
    "            print(f\"we append now {pdb_without_ending, pdb_chain}\")\n",
    "            nma_commands.append((pdb_without_ending, pdb_chain, work_dir))\n",
    "\n",
    "    print(nma_commands)\n",
    "    # Using concurrent processing to parallelize the execution\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "         # Use as_completed to wait for completion and print results\n",
    "        futures = {executor.submit(run_nma_command, *args): args for args in nma_commands}\n",
    "        for future in as_completed(futures):\n",
    "            args = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Exception for {args}: {e}\")\n",
    "    \n",
    "\n",
    "    return work_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "56c54ace-89e9-475b-bde4-3307b69b94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nma_command(pdb_code, chain, work_dir):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        bash_curl_cmd = f\"./ENM_NMA_general {pdb_code} {chain} 10 50\"\n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "    \n",
    "        #print(\"we start now\")\n",
    "        print(bash_curl_cmd_rdy)\n",
    "        result = subprocess.run(bash_curl_cmd_rdy, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "    \n",
    "        print(f\"Results for {pdb_code}_{chain}:\")\n",
    "        print(result.stderr)\n",
    "        print(result.stdout)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    try:\n",
    "        base_filename = f\"{pdb_code}_{chain}\"\n",
    "\n",
    "        new_dir = f\"{work_dir}/{pdb_code}\"\n",
    "        os.mkdir(new_dir)\n",
    "\n",
    "        res_lst = [\"bfact.txt\", \"evecs.txt\", \"freq_GHz.txt\"]\n",
    "        for results in res_lst:\n",
    "            move_path = os.path.join(work_dir, f\"{base_filename}_{results}\")\n",
    "            shutil.move(move_path, new_dir)\n",
    "\n",
    "        \n",
    "        shutil.move(os.path.join(work_dir, f\"{pdb_code}.pdb\"), new_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2276eca0-884f-4cf3-a0a8-58ce4660b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bfactor_projections(pdb_file_path:str):\n",
    "\n",
    "    target_files = \"bfact.txt\"\n",
    "    \n",
    "    pdb_name = os.path.basename(pdb_file_path).split(\".\")[0]\n",
    "    \n",
    "    pdb_basep = os.path.dirname(pdb_file_path)\n",
    "    \n",
    "    pdb_suffix = pdb_name.split(\"_\")[1:]\n",
    "    \n",
    "    chain = pdb_name.split(\"_\")[-1] #this fetches the whole chains.\n",
    "    \n",
    "    file_bfac = os.path.join(pdb_basep,f\"{pdb_name}_{chain}_{target_files}\")\n",
    "    #'/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa7_A/_A_bfact.txt'\n",
    "    #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa7_A/_/A/_bfact.txt'\n",
    "    result_df = _retrieve_b_fac(file_bfac, pdb_file_path)\n",
    "    \n",
    "    outf = os.path.join(pdb_basep, f\"{pdb_name}.csv\")\n",
    "    \n",
    "    result_df.to_csv(outf)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5d18419d-b4ab-48d4-b834-0055d0ff0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get bfactor projections for a given PDB file\n",
    "def process_b_fac_parallel(dir_path):\n",
    "    try:\n",
    "        all_files = os.listdir(os.path.join(work_dir, dir_path))\n",
    "        pdb_file = next((f for f in all_files if f.endswith(\".pdb\")), None)\n",
    "        if pdb_file:\n",
    "            print(os.path.join(work_dir, dir_path, pdb_file))\n",
    "            get_bfactor_projections(os.path.join(work_dir, dir_path, pdb_file))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing directory {dir_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "86f661d4-601b-4b4e-87f1-95a2c91d4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_b_fac(path_to_b_fac:str, pdb_path:str):\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    exp_bf = []\n",
    "    comp_bf = []\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    with open(path_to_b_fac, \"r\") as pdb_bfac:\n",
    "        for lines in pdb_bfac:\n",
    "            if i > 0:\n",
    "                if lines[0] != \" \":\n",
    "                    break\n",
    "                lines = lines.replace(\"\\n\", \" \")\n",
    "                lines = lines.split(\" \")\n",
    "                # Flatten the list of lists\n",
    "                flat_data = [item for item in lines if item]  # Remove empty strings\n",
    "                try:\n",
    "                    exp_bf.append(float(flat_data[0]))\n",
    "                    comp_bf.append(float(flat_data[1]))\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #print(lines)\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "    result_df[\"exp_bf\"] = exp_bf\n",
    "    result_df[\"comp_bf\"] = comp_bf\n",
    "    \n",
    "\n",
    "    copy_struc_for_mod_b_facs(pdb_path=pdb_path, comp_bf=comp_bf)\n",
    "\n",
    "    result_df[\"RMSF_comp_bfac\"] = np.sqrt(result_df.loc[:, \"comp_bf\"] / (8*np.pi))\n",
    "    \n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "62d23860-74d1-4f1b-b042-049813dd1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_file_paths(clean_dir:str, representative_df:pd.DataFrame):\n",
    "\n",
    "    #print(representative_df)\n",
    "    # Assuming 'prot_name' is the column containing protein names\n",
    "    representative_df['file_path'] = f'{clean_dir}/original_' + representative_df['prot_name'] + '.pdb'\n",
    "\n",
    "    # Check if the files exist\n",
    "    representative_df['file_path_exists'] = representative_df['file_path'].apply(os.path.exists)\n",
    "\n",
    "    return representative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fa7fa857-b94d-49e3-a5c9-0d42933501e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_struc_for_mod_b_facs(pdb_path:str, comp_bf:list):\n",
    "    #this function will simply just change the experimental B factors with modified B factors computed from NMA.\n",
    "\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    structure = parser.get_structure(\"default\", pdb_path)\n",
    "\n",
    "    # Extract file information\n",
    "    pdb_name = os.path.basename(pdb_path)\n",
    "    pdb_basep = os.path.dirname(pdb_path)\n",
    "    pdb_suffix = pdb_name.split(\"_\")[1:]\n",
    "\n",
    "    # Modify B-factors\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue, new_bfactor in zip(chain, comp_bf):\n",
    "                for atom in residue:\n",
    "                    atom.set_bfactor(new_bfactor)\n",
    "\n",
    "    # Save the modified structure to a new PDB file\n",
    "    output_path = os.path.join(pdb_basep, f\"comp_bfac_{'_'.join(pdb_suffix)}\")\n",
    "    io = PDB.PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "112214c9-9d6e-4a06-9243-5898b43b738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_directories(directory):\n",
    "    # Get a list of all entries in the directory\n",
    "    all_entries = os.listdir(directory)\n",
    "\n",
    "    # Filter out only directories\n",
    "    all_directories = [entry for entry in all_entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "\n",
    "    return all_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c265210b-3528-461a-8701-5b55090d298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hinge_parallelize(file_path):\n",
    "    \n",
    "    try:\n",
    "        result_dict = {}\n",
    "        total_hinge_res_local = []\n",
    "        pdb_name = file_path.split(\"_\")[-1]\n",
    "        chain = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "        hinges = hinge_pred(path_to_pdb=file_path, chain=chain)\n",
    "        result_dict[pdb_name] = hinges\n",
    "        total_hinge_res_local.extend(hinges)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    return result_dict, total_hinge_res_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6bd8bc2f-459a-42ae-a2a2-ca7550a1f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_pred(path_to_pdb:str, chain:str):\n",
    "\n",
    "\n",
    "    hinge_lst = []\n",
    "\n",
    "    mol = molecule.load_structure(path_to_pdb)\n",
    "\n",
    "    backbone = [j for i in mol[0][chain].get_backbone() for j in i if j is not None]\n",
    "\n",
    "    alpha_start, alpha_stop, step_size = 2, 8, 0.5\n",
    "\n",
    "    for i in np.arange(alpha_start, alpha_stop, step_size):\n",
    "        i = np.around(i, decimals=1)\n",
    "        try:\n",
    "            # here we can either store output in a sep file or we just trash it.\n",
    "            predict_hinge(backbone, Alpha=i, outputfile=open(f\"{path_to_pdb}_{i}.txt\", \"w\"))\n",
    "            hinge_res = mol[0][chain].get_hinges()\n",
    "\n",
    "            for a in hinge_res:\n",
    "                if a.get_pvalue() < 0.05:\n",
    "                    hits = [hinge_res.get_id() for hinge_res in a.get_elements()]\n",
    "                    hinge_lst.extend(hits)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    sorted_hinges = sorted(set(hinge_lst), reverse=False)\n",
    "    return sorted_hinges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe0e66-235a-4537-96b1-9f7845d60583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa8_A/comp_bfac_5xa8_A.pdb/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_7w7w_A/comp_bfac_7w7w_A.pdb/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_3ar5_A/comp_bfac_3ar5_A.pdb/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_7e7s_A/original_7e7s_A.pdb/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_1su4_A/comp_bfac_1su4_A.pdb/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_2c9m_A/original_2c9m_A.pdb/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5ncq_A/comp_bfac_5ncq_A.pdb/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa7_A/comp_bfac_5xa7_A.pdb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error processing directory original_7w7w_A: [Errno 2] No such file or directory: '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_7w7w_A/comp_bfac_7w7w_A_A_bfact.txt'Error processing directory original_5xa8_A: [Errno 2] No such file or directory: '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa8_A/comp_bfac_5xa8_A_A_bfact.txt'Error processing directory original_3ar5_A: [Errno 2] No such file or directory: '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_3ar5_A/comp_bfac_3ar5_A_A_bfact.txt'Error processing directory original_1su4_A: [Errno 2] No such file or directory: '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_1su4_A/comp_bfac_1su4_A_A_bfact.txt'Error processing directory original_5xa7_A: [Errno 2] No such file or directory: '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5xa7_A/comp_bfac_5xa7_A_A_bfact.txt'\n",
      "\n",
      "\n",
      "\n",
      "Error processing directory original_5ncq_A: [Errno 2] No such file or directory: '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_5ncq_A/comp_bfac_5ncq_A_A_bfact.txt'\n",
      "\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by KMeans.\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connectedFound infinite path length because the graph is not connected\n",
      "\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connected\n",
      "Found infinite path length because the graph is not connected\n"
     ]
    }
   ],
   "source": [
    "path_pca = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/ATP2A1.mode_12.proj\"\n",
    "path_ensemble = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/ensemble.txt\"\n",
    "clean_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/clean_ensemble\"\n",
    "###path_pca = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_181/PCA/NUDT4B.mode_12.proj\"\n",
    "###path_ensemble = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/monomer/pos_1_181/PCA/ensemble.txt\"\n",
    "savepath = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/cluster_plot.png\"\n",
    "basepath = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA\"\n",
    "pca_df = get_cluster_df(PCA_12_proj_file=path_pca,\n",
    "                           prot_names=path_ensemble,\n",
    "                           save_path=savepath,\n",
    "                       ensemble_name=\"ATP2A1\")\n",
    "#\n",
    "representative_df = get_representative_file_paths(clean_dir= clean_dir, \n",
    "                                      representative_df=pca_df)\n",
    "#\n",
    "#work_dir = get_nma_domenico(representative_df, basepath)  # this function return representative_df and work dir!\n",
    "#\n",
    "work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA\"\n",
    "lst_of_dirs_to_parse = get_all_directories(work_dir)\n",
    "\n",
    "# Your main loop to process directories\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(process_b_fac_parallel, lst_of_dirs_to_parse)\n",
    "\n",
    "\n",
    "subdirectories = [d for d in os.listdir(work_dir) if os.path.isdir(os.path.join(work_dir, d))]\n",
    "\n",
    "pdb_paths = []\n",
    "\n",
    "# Loop through each subdirectory\n",
    "for subdirectory in subdirectories:\n",
    "    subdirectory_path = os.path.join(work_dir, subdirectory)\n",
    "\n",
    "    # Get the list of PDB files in the subdirectory\n",
    "    pdb_files = [f for f in os.listdir(subdirectory_path) if f.endswith(\".pdb\")]\n",
    "\n",
    "    # Add the paths to the PDB files to the result list\n",
    "    pdb_paths.extend([os.path.join(subdirectory_path, pdb_file) for pdb_file in pdb_files])\n",
    "\n",
    "# Print or use the list of PDB file paths\n",
    "orig_b_fac = []\n",
    "comp_b_fac = []\n",
    "\n",
    "for pdb_path in pdb_paths:\n",
    "    final_p = pdb_path.split(\"/\")[-1]\n",
    "    if final_p.startswith(\"original\"):\n",
    "        orig_b_fac.append(pdb_path)\n",
    "    else:\n",
    "        comp_b_fac.append(pdb_path)\n",
    "\n",
    "#print(orig_b_fac)\n",
    "#print(\"-\")\n",
    "#print(comp_b_fac)\n",
    "\n",
    "#works.. now use these 16 outpaths as input for SINGLE USEAGE CALCULATIONS. So we can parallelize them.\n",
    "\n",
    "result_dicts = []\n",
    "total_hinge_res = []\n",
    "\n",
    "try:\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(process_hinge_parallelize, orig_b_fac)\n",
    "\n",
    "        for result_dict, hinge_res_local in results:\n",
    "            result_dicts.append(result_dict)\n",
    "            total_hinge_res.append(hinge_res_local)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "print(result_dicts)\n",
    "print(\"this is the unique set\")\n",
    "flattened_total_hinge_res = [item for sublist in total_hinge_res for item in sublist]\n",
    "\n",
    "# Count the frequency of each value\n",
    "frequency_dict = Counter(flattened_total_hinge_res)\n",
    "\n",
    "# Sort the frequency dictionary by values in descending order\n",
    "sorted_frequency = dict(sorted(frequency_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print or use the frequency dictionary\n",
    "for value, frequency in sorted_frequency.items():\n",
    "    print(f\"{value}: {frequency/len(total_hinge_res)}\")\n",
    "\n",
    "\n",
    "result_dicts_2 = []\n",
    "total_hinge_res_2 = []\n",
    "\n",
    "#now with computed b fac\n",
    "\n",
    "try:\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(process_hinge_parallelize, comp_b_fac)\n",
    "\n",
    "        for result_dict, hinge_res_local in results:\n",
    "            result_dicts.append(result_dict)\n",
    "            total_hinge_res_2.append(hinge_res_local)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "print(result_dicts_2)\n",
    "print(\"this is the unique set\")\n",
    "flattened_total_hinge_res = [item for sublist in total_hinge_res_2 for item in sublist]\n",
    "\n",
    "# Sort the frequency dictionary by values in descending order\n",
    "sorted_frequency_2 = dict(sorted(frequency_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print or use the frequency dictionary\n",
    "for value, frequency in sorted_frequency_2.items():\n",
    "    print(f\"{value}: {frequency/len(total_hinge_res)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find common keys\n",
    "common_keys = set(sorted_frequency_2.keys()) & set(sorted_frequency.keys())\n",
    "\n",
    "# Create a new dictionary for the overlap\n",
    "overlap_dict = {}\n",
    "\n",
    "# Iterate over common keys\n",
    "for key in common_keys:\n",
    "    value1 = sorted_frequency[key]\n",
    "    value2 = sorted_frequency_2[key]\n",
    "    \n",
    "    # Check if both dictionaries have the entry\n",
    "    if key in sorted_frequency and key in sorted_frequency_2:\n",
    "        overlap_dict[key] = (value1, value2)\n",
    "    elif key in freq_dict1:\n",
    "        overlap_dict[key] = (value1,)\n",
    "    elif key in freq_dict2:\n",
    "        overlap_dict[key] = (value2,)\n",
    "\n",
    "# Print or use the overlap dictionary sorted by the sum of values[0] and values[1]\n",
    "for key, values in sorted(overlap_dict.items(), key=lambda item: sum(item[1]), reverse=True):\n",
    "    sorted_keys_within_group = sorted(values)\n",
    "    print(f\"{key}: {sorted_keys_within_group}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#pdb_exp = [os.path.join(work_dir, f) for f in os.listdir(work_dir) if f.startswith(\"original\") and f.endswith(\".pdb\")]\n",
    "#pdb_comp = [os.path.join(work_dir, f) for f in os.listdir(work_dir) if f.startswith(\"comp\") and f.endswith(\".pdb\")]\n",
    "\n",
    "#for pdb in pdb_exp:\n",
    "#    chain = pdb.split(\"_\")[-1]\n",
    "#    chain = chain.split(\".\")[0]\n",
    "#    hinge_pred(path_to_pdb=pdb, chain=chain)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e229f3-f28e-4e03-96da-7fe4ce5d0352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4b751a61-887e-4d96-a7f5-9552c26a8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hinge_pred('/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/ATP2A1/monomer/pos_1_994/PCA/NMA/original_1su4_A.pdb', \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b9081-927c-4d09-bc12-2296c4f42b38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RUN MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "46a75496-938d-45a9-939b-b044a6bb6d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n"
     ]
    }
   ],
   "source": [
    "#First: download all pdb.. /home/cond... do the tm score and rmsc comp...then select based on that...\n",
    "#then we will repair gaps for those retrieved structures.\n",
    "\n",
    "path = \"/home/micnag/bioinformatics/uniprot/mmseq_protein_hits_raw.json\" #testcase\n",
    "\n",
    "hits = open(path, \"r\")\n",
    "testcase = json.load(hits)\n",
    "key_lst = []\n",
    "val_lst = []\n",
    "i = 0\n",
    "for keys, vals in testcase.items():\n",
    "    i +=1\n",
    "    if keys == \"P00533\":\n",
    "        print(i)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3e7864a7-4f5b-47a7-88c7-59b5c82d2a8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2204\n",
      "14913\n",
      "O14983\n"
     ]
    }
   ],
   "source": [
    "## First: download all pdb.. /home/cond... do the tm score and rmsc comp...then select based on that...\n",
    "#then we will repair gaps for those retrieved structures.\n",
    "\n",
    "path = \"/home/micnag/bioinformatics/uniprot/mmseq_protein_hits_raw.json\" #testcase\n",
    "\n",
    "hits = open(path, \"r\")\n",
    "testcase = json.load(hits)\n",
    "key_lst = []\n",
    "val_lst = []\n",
    "i = 0\n",
    "for keys, vals in testcase.items():\n",
    "    i +=1\n",
    "    if keys == \"P04049\":\n",
    "        print(i)\n",
    "        pass\n",
    "    key_lst.append(keys)\n",
    "    val_lst.append(vals)\n",
    "\n",
    "\n",
    "#we parse from here through all.\n",
    "hit_list = list(zip(key_lst,val_lst))\n",
    "\n",
    "#2024:2025 egfr\n",
    "\n",
    "#try 6:7 good case.\n",
    "#so is 202:203\n",
    "#24-25 is faulty. so is 34-35 and 37:38\n",
    "\n",
    "#555-556 seems good case to illustrate.\n",
    "#5803 = ACSM2A gene\n",
    "\n",
    "\n",
    "#O14983 SERCA 744:745\n",
    "#O75311 GLRA 1401:1402\n",
    "#P48167 GLRB 4400:4401\n",
    "#P00326 ADHG 2004:2005\n",
    "#P06756 Integrin checknumbers 2356:2357  THIS case is totally catastrophic. #final stuff to fix for mixed heteromers.\n",
    "i = 0\n",
    "#Q8N142 is Adenylosuccinate synthetase isozyme 1 9096:9097\n",
    "#2569:2570 = LOX5\n",
    "#calmodulin 1 2689:2690  !rerun again when NMR is properly working.\n",
    "\n",
    "#P02788 Lactotransferin 2175:2176\n",
    "#P02911 LAO-binding protein try as well. \n",
    "#Q8TEX9 Importin-4 10086:10087\n",
    "#Q14974 Real importin subunit beta with strucs: 6418:6419\n",
    "\n",
    "#Q16774 guanylate kinase 6766:6767   ! rerun again when NMR is properly working! seems interesting.\n",
    "#raf1 + complexes\n",
    "print(len(hit_list))\n",
    "#744:745 serca\n",
    "#2203:2204 kraf\n",
    "for keys, entries in hit_list[744:745]:\n",
    "    #print(keys, entries)\n",
    "    query, templates, seq_sim, query_start, query_end, temp_start, temp_end = keys, entries[0],\\\n",
    "    entries[1],entries[2],entries[3],entries[4],entries[5] \n",
    "    \n",
    "    print(query)\n",
    "    #print(sorted(templates))\n",
    "    os.chdir(\"/home/micnag/bioinformatics\") #just to start properly.\n",
    "    \n",
    "    #2203:2204 == RAF1\n",
    "    #fetch_struc_1(query, templates, seq_sim, query_start, query_end, temp_start, temp_end, run_NMA=True, run_PCA=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813bb3b-19b0-4caf-a6ff-b1ff380b38a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TO DO LOG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9ad97-852d-4f1b-80d3-181d67cd1650",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### repair and check for repairability.\n",
    "\n",
    "+ repair modeller pipeline should work : done\n",
    "+ rechaining / renumbering after modelling: done\n",
    "+ funnelling repaired chains into full biological ensembles: done\n",
    "\n",
    "\n",
    "### NOW WE NEED TO RENUMBER AFTER REPAIR!\n",
    "\n",
    "REPAIR IS DONE AND WORKS.\n",
    "\n",
    "\n",
    "## NEXT STEP:\n",
    "\n",
    "remove duplicate TEMPLATE from MSA USALIGN.\n",
    "example whats wrong:\n",
    "\n",
    ">2q9p.pdb:A\tL=132\td0=4.26\tseqID=0.874\tTM-score=0.93282\n",
    "---RTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIF-ENQERKHRTYVYVLIVTEVLEDWEDSV-NIGRKREWFKIEDAIKVLQYHKPVQASYFE----------\n",
    ">2q9p.pdb:A\tL=132\td0=4.26\tseqID=0.874\tTM-score=0.93282\t*\n",
    "---RTYDGDGYKKRAACLCFRSESEEEVLLVSSSRHPDRWIVPGGGMEPEEEPSVAAVREVCEEAGVKGTLGRLVGIF-ENQERKHRTYVYVLIVTEVLEDWEDSV-NIGRKREWFKIEDAIKVLQYHKPVQASYFE----------\n",
    ">3mcf.pdb:A\tL=136\td0=4.33\tseqID=0.801\tTM-score=0.85092\n",
    "FK----------KRAACLCFRSEREDEVLLVSSSRYPDRWIVPGGGMEPEEEPGGAAVREVYEEAGVKGKLGRLLGVFEQNQDPKHRTYVYVLTVTELLEDWEDSV-SIGRKREWFKVEDAIKVLQCHKPVHAEYLEKLKLGGSPTN\n",
    "...\n",
    "\n",
    "2q9p is twice there\n",
    "\n",
    "\n",
    "+ **ABOVE BUG IS ALREADY FIXED** - 12/06/2023\n",
    "\n",
    "\n",
    "### Implement according structural cutting based on 1:1 correspondence residues (i.e remove all that dont have 1:1 correspondence)\n",
    "\n",
    "+ do the structural alignment and cut all residues that dont have 1:1 correspondence: **done**\n",
    "+ prepare structures to only have those residues i.e all structures will end up with the same amount of residues : **done**\n",
    "+ run domenicos PCA pipeline and integrate it into workflow: **done**\n",
    "+ plot pca results (PCA1 PCA2): **done**\n",
    "\n",
    "\n",
    "## Assemble FULL pipeline and run preliminary test runs.\n",
    "\n",
    "\n",
    "pipeline assembled:\n",
    "\n",
    "bugfixes:\n",
    "\n",
    "+ fixed fetch fasta during repair:\n",
    "\n",
    "previous fetch fasta would always grab full length protein fasta which would lead to full length protein repair reconstruction. This is not desired. We want only to restore the structure present (irrespective if the structure is only a fragment of the full length protein)\n",
    "\n",
    "TBD:\n",
    "\n",
    "+ multichain alignment in msa\n",
    "Currently we only align chain A. Multichain alignment are not possible but we can run for all chains against all respective chains e.g chain A vs A / B vs B etc? Done\n",
    "\n",
    "\n",
    "\n",
    "establish 1 to 1 full length alignments with USAlign and then cut according to the results of n - 1 seq alignments against the reference structure.\n",
    "\n",
    "+ for multichain alignment, output file looks different. \n",
    "come up with a correct way to parse output file and make sure that chains are properly aligned!\n",
    "\n",
    "\n",
    "+ set up proper PCA plots for each oligomer against its respective oligomers.\n",
    "#\n",
    "\n",
    "\n",
    "+ for NMR modells, consider that high seq similarity is equivalently good and means same protein but Calmodulin 1 e.g 6y95 and 6y94 are seen as different with low tm score ! mistake !they are same sequence and protein.\n",
    "\n",
    "\n",
    "+ NOW NMR modells are taken as separate PDBS into account and will also be part of the ensemble!\n",
    "fixed on 19.7.2023\n",
    "\n",
    "\n",
    "# <b> TODO: </b>\n",
    "NEED FOR OLIGOMERS TO DO SEQUENCE BASED GROUPING FOR EACH CHAIN AGAINST EACH CHAIN... A VS A... B VS B\n",
    "\n",
    "check EGFR case for repair mutated residues. there is an issue with a CY0 modified residue that will result in a weird alanine.\n",
    "_mutate function needs to be checked.\n",
    "\n",
    "2j5e.pdb shows what is wrong...\n",
    "\n",
    "\n",
    "+ check the alignment if it cuts properly the residues that it SHOULD CUT. There might be an issue with the residue selection that are KEPT FOR PCA.\n",
    "\n",
    "\n",
    "+ pipeline mistakes small ligand bounds in other chains as oligomers... should be monomeric still.. \n",
    "THIS ERROR IS FIXED ALREADY.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NEXT THURSDAY:\n",
    "\n",
    "Continue with HAIL and incorporation of GNOMAD DATABASE INFO.\n",
    "Mutational mapping new is construction site now.\n",
    "https://hail.is/tutorial.html\n",
    "Check NUMBERING on the pdb structures. SEEMS TO BE OFF (maybe for repaired structures)????\n",
    "\n",
    "DECOUPLE NMA RUNS FROM PCA... PCA IS A WHOLE OWN STORY...\n",
    "WE RUN NMA ANALYSIS ON THE FRESH POPULATION OF STRUCTURES, irrespective of gaps. TAKE STRUCS FROM THE POS X-Y DIRECTORY. DONT REPAIR (AT FIRST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
