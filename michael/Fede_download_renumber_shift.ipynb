{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b90007-cb85-483b-990d-c88ba5d53dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadPipe:\n",
    "    '''Class object containing the download function that will download all pdbs \n",
    "    which we need for downstream analysis of a particular uniprot ID'''\n",
    "\n",
    "    def __init__(self, input_df, work_dir, script_dir, seq_id_cutoff=None, download_type=\"pdb\", logging=True):\n",
    "        self.work_dir = work_dir # Storage of seq identity useful later for temp selection.\n",
    "        self.script_dir = script_dir #here we store all scripts\n",
    "        self.seq_id_cutoff = seq_id_cutoff\n",
    "        self.download_type = download_type # Download PDB or also mmCIF (currently only PDB)\n",
    "        self.input_df = input_df\n",
    "        self.pdbs_to_download = input_df.loc[:, \"Target_id\"]\n",
    "        self.seq_id = input_df.loc[:, \"Seq_identity\"]\n",
    "        # The bash script location which will download the pdbs. \n",
    "        self.download_script = os.path.join(script_dir, \"batch_download_modified.sh\") #modify for script location\n",
    "        self.download_tmp = os.path.join(work_dir, \"pdb_list.csv\") # The location for the temporary file that is required for the download_script as input.\n",
    "        self.log_dir = os.path.join(work_dir, \"log_files\")\n",
    "        # The list of chains that will be used later to fetch correct structures.\n",
    "        self.chain_seqid_dict = self._setup_download_list()\n",
    "        self.temp_seqid_dict = {template: seq_id for template, seq_id in zip(self.pdbs_to_download, self.seq_id)}\n",
    "        # We store also meta info as a json dict\n",
    "        self.meta_dict = None\n",
    "        #we store high resolution structures as a list if the user wants to separate based on resolution.\n",
    "        self.high_resolution = None\n",
    "        # set a flag that stops redownloading.\n",
    "        self.already_downloaded = None\n",
    "        # collect conservation\n",
    "        self.conservation_df = None\n",
    "        #filtered structures based on meta resolution\n",
    "        self.filtered_structures = None\n",
    "        #store shifts.\n",
    "        self.shift_dict = None\n",
    "        self.logging = logging # for report purpose.\n",
    "            \n",
    "    def paralellized_download(self):\n",
    "        '''\n",
    "        This function is going to call _download_files n times to parallelize download. \n",
    "        It is going to pass the function call itself **_download_file**,\n",
    "        self.download_tmp (the location of the tmp file which is pdb_id comma separated), \n",
    "        p (an additional parameter specifying that \n",
    "        we want to download pdbs, and self.work_dir(the current work dir)\n",
    "        '''\n",
    "\n",
    "        self.already_downloaded = self._check_for_pdbs_present()\n",
    "        # ThreadPoolExecutor\n",
    "        print(f\"{self.already_downloaded=}\")\n",
    "        if self.already_downloaded == False:\n",
    "            print(\"we start downloading now:\")\n",
    "\n",
    "            # now for debugging.\n",
    "\n",
    "            return 0\n",
    "            \n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                # Submit your tasks to the executor.\n",
    "                futures_pdb = [executor.submit(self._download_files, self.download_tmp, 'p', self.work_dir)]\n",
    "                # Optionally, you can use as_completed to wait for and retrieve completed results.\n",
    "                for future in as_completed(futures_pdb):\n",
    "                    result = future.result()\n",
    "            self.already_downloaded = True\n",
    "        else:\n",
    "            print(\"we already have pdbs from the templates downloaded\")\n",
    "    \n",
    "    def _setup_download_list(self):\n",
    "        '''Helper function to setup the list of comma-separated pdb\n",
    "        ids for the download_files function'''\n",
    "        \n",
    "        \n",
    "        if not self.input_df.empty:\n",
    "            pdbs = self.input_df.loc[:, \"Target_id\"]\n",
    "            seq_ids = self.input_df.loc[:, \"Seq_identity\"]\n",
    "        else:\n",
    "            # we cant proceed\n",
    "            return\n",
    "\n",
    "        #initialize dict\n",
    "        chain_seqid_dict = defaultdict(list)\n",
    "\n",
    "        self.pdbs_to_download = [] # overwrite to set it blank for seq_id filtering.\n",
    "\n",
    "        original_pdbs = len(set([x[0:4] for x in pdbs])) #for logging purpose. tells us how many pdbs originally were there before cutoff\n",
    "        for pdb, seq_id in zip(pdbs, seq_ids):\n",
    "            if float(seq_id) > float(self.seq_id_cutoff):\n",
    "                pdb_4_digit_id = pdb[:4] # e.g 4CFR\n",
    "                chain = pdb[-1] # e.g A\n",
    "                chain_seqid_dict[pdb_4_digit_id].append((chain, seq_id)) #map chains and seq id to pdb id\n",
    "                # We only want to download pdb files once. \n",
    "                # No reason to download a PDB-file 4 times just because we need chain [A, B, C, D]\n",
    "                self.pdbs_to_download.append(pdb_4_digit_id) # we store it for a later check \n",
    "                \n",
    "        unique_pdbs = chain_seqid_dict.keys() # Keys : PDB-IDs, Vals: Chains, seq_id\n",
    "        # Create download_files input list\n",
    "        \n",
    "        if unique_pdbs:\n",
    "            with open(self.download_tmp, \"w\") as pdb_tar:\n",
    "                pdb_tar.write(\",\".join(unique_pdbs))\n",
    "            \n",
    "            #return dict {key: pdb_id = [(chain, seq_id)]}\n",
    "            print(f\"Before applying cutoff: {original_pdbs} Structures\\nAfter applying cutoff: {len(unique_pdbs)} Structures\")\n",
    "            return chain_seqid_dict\n",
    "        else:\n",
    "            print(f\"No structures available for cutoff {self.seq_id_cutoff}. Try lowering cutoff.\")\n",
    "\n",
    "    \n",
    "    def _download_files(self, download_tmp, download_type, path)->list:\n",
    "        \"\"\"This helper function runs inside paralellized_download \n",
    "        and will be used to get the PDB files that we require for downstream analysis.\"\"\"\n",
    "        results = []\n",
    "        # Input for subprocess\n",
    "        bash_curl_cmd = f\"{self.download_script} -f {download_tmp} -o {path} -{download_type}\"\n",
    "        # split into list \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "        \n",
    "        try:\n",
    "            # Run subprocess\n",
    "            result = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            # Append result output to results\n",
    "            results.append(result.stdout.split(\"\\n\")[:-1])  # Skip the last empty element\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error downloading: {e}\")\n",
    "\n",
    "        return results    \n",
    "\n",
    "    def _check_for_pdbs_present(self):\n",
    "        '''\n",
    "        Could be good to improve so that if we miss SOME structures we fetch them as well and download ONLY those.\n",
    "        For those structures we also need seqid per chain and then also update the seqid_chain dict for the whole directory after\n",
    "        successful download.\n",
    "        Currently we only check if pdbs are present and if yes we dont download anything further.\n",
    "        '''\n",
    "        pdbs_to_retrieve = {f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")}  # Use a set for efficient lookups\n",
    "        template_codes = {f[:4] for f in self.pdbs_to_download}  # Convert list to set for efficient intersection operation\n",
    "\n",
    "        print(f\"{pdbs_to_retrieve=}, {template_codes=}\")\n",
    "        # Check for any overlap between the two sets\n",
    "        overlap = pdbs_to_retrieve.intersection(template_codes)\n",
    "        \n",
    "        print(f\"This is overlap in the directory: {overlap}\")\n",
    "        # Return 1 if there is an overlap, else 0\n",
    "        return True if overlap else False\n",
    "\n",
    "    \n",
    "    def retrieve_meta(self, dict_location=None, human_readable=True)->dict:\n",
    "        '''\n",
    "        We also want to store meta information about resolution etc.\n",
    "        This function takes each pdb file and retrieves the following information:\n",
    "        - Title\n",
    "        - Keywords\n",
    "        - PDBcode\n",
    "        - Authors\n",
    "        - Deposition date\n",
    "        - Technique\n",
    "        - Resolution\n",
    "        - R_value : If crystallography else None\n",
    "        - R_free : If crystallographe else None\n",
    "        - Classification\n",
    "        - Organism\n",
    "        - Expression System\n",
    "        - Number of amino acids in the asymmetric unit\n",
    "        - Mass of amino acids in the asymmetric unit (Da)\n",
    "        - Number of amino acids in the biological unit\n",
    "        - Mass of amino acids in the biological unit (Da)\n",
    "        '''\n",
    "        \n",
    "        json_file_path = os.path.join(self.log_dir, 'meta_dictionary.json')\n",
    "\n",
    "        for path in [json_file_path, dict_location]: #check first the supposed location alternatively the user supplied location.\n",
    "            if path and os.path.exists(path):\n",
    "                try:\n",
    "                    with open(path, 'r') as json_fh:\n",
    "                        self.meta_dict = json.load(json_fh)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {path}: {e}\")\n",
    "                \n",
    "        #little helper function to deal with date data\n",
    "        def _date_encoder(obj):\n",
    "            if isinstance(obj, date):\n",
    "                return obj.isoformat()  # Convert date to ISO format\n",
    "\n",
    "        #grab all PDB files which contain the meta information.\n",
    "        pdbs_to_retrieve = [f for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "        #here we store info about ALL pdbs.\n",
    "        meta_dictionary = dict()\n",
    "        \n",
    "        for pdbs in pdbs_to_retrieve:\n",
    "            if len(pdbs) == 8: #lets exclude preprocessed pdbs that are longer or shorter.\n",
    "                sub_dict = dict()\n",
    "                pdb_code = pdbs[:4]\n",
    "                try:\n",
    "                    fullp = os.path.join(self.work_dir, pdbs)\n",
    "                    pdb = atomium.open(fullp)\n",
    "                    sub_dict[\"title\"] = pdb.title\n",
    "                    sub_dict[\"key_words\"] = pdb.keywords\n",
    "                    sub_dict[\"code\"] = pdb.code\n",
    "                    sub_dict[\"authors\"] = pdb.authors\n",
    "                    #sub_dict[\"deposition_date\"] = pdb.deposition_date.isoformat()  #isoformat because it is a time object\n",
    "                    sub_dict[\"technique\"] = pdb.technique\n",
    "                    sub_dict[\"resolution\"] = pdb.resolution\n",
    "                    sub_dict[\"r_val\"] = pdb.rvalue\n",
    "                    sub_dict[\"r_free\"] = pdb.rfree\n",
    "                    sub_dict[\"classification\"] = pdb.classification\n",
    "                    sub_dict[\"organism\"] = pdb.source_organism\n",
    "                    sub_dict[\"expression_system\"] = pdb.expression_system\n",
    "                    sub_dict['number_of_residues_asymmetric_unit'] = len(pdb.model.residues())\n",
    "                    sub_dict['mass_dalton_asymetric_unit'] = f\"{pdb.model.mass:.2f}\" \n",
    "                    try:\n",
    "                        assembly = pdb.generate_assembly(1) #build the biological assembly \n",
    "                        sub_dict['number_of_residues_biological_unit'] = len(assembly.residues())\n",
    "                        sub_dict['mass_dalton_biological_unit'] = f\"{assembly.mass:.2f}\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"We could not build the assembly for: {pdb_code}\")\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"We had an error with file: {pdb_code}\")\n",
    "                # store meta info and return\n",
    "                meta_dictionary[pdb_code] = sub_dict\n",
    "\n",
    "\n",
    "        #lets store meta info as json dict\n",
    "        self.meta_dict = meta_dictionary\n",
    "        \n",
    "        # Code block to store meta info as a txt file.\n",
    "        self._save_meta_dict(self.meta_dict, human_readable=human_readable)\n",
    "\n",
    "\n",
    "    def _save_meta_dict(self, meta_dictionary, human_readable=True):\n",
    "        '''Helper function to store meta info as a txt file.'''\n",
    "        #check if log file dir exists, else make it.\n",
    "        \n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        #lets store the dict in json to read it in for later useage.\n",
    "        json_file_path = os.path.join(self.log_dir, 'meta_dictionary.json')\n",
    "        #convert defaultdict to normal dict.\n",
    "        \n",
    "        with open(json_file_path, 'w') as json_fh:\n",
    "            json.dump(meta_dictionary, json_fh, indent=4, default=str)  # Use default=str to handle non-serializable objects\n",
    "\n",
    "    \n",
    "    def conservation(self, uniprot_id):\n",
    "        '''Gets 3 different types of Conservation:\n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "\n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        \n",
    "        mmseq_fasta_result = self._mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=self.work_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = self._get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "        self.conservation_df = conserv_df\n",
    "\n",
    "        conserv_df.to_csv(f\"{self.log_dir}/conservation_df.csv\")\n",
    "        \n",
    "    def _mmseq_multi_fasta(self, uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "        \"\"\"\n",
    "        uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "        outdir: location where result files will be stored.\n",
    "        sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "        filter_msa = 0 default. if 1 hits are stricter.\n",
    "        query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "        \"\"\"\n",
    "\n",
    "        #we blast with this fasta as query.\n",
    "        trgt_fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "        #Make outdir for all required files.\n",
    "        #we need to write it out to file.\n",
    "        with open(f\"{self.work_dir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "            fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "            fasta_out.write(trgt_fasta_seq)\n",
    "\n",
    "        #fetch pre downloaded database from a parent folder.\n",
    "        msa_file = None\n",
    "        new_location = None\n",
    "        try:\n",
    "            DB_storage_location = f\"{work_dir}\"\n",
    "            #shutil.copy(previous_path, savepath)\n",
    "            bash_curl_cmd = f\"mmseqs createdb {self.work_dir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\" \n",
    "            bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "            #run first cmd which setups query database based on our input fasta file\n",
    "            result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"    \n",
    "            bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "            #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "            result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "            bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "            bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "            result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #delete last line.. required.\n",
    "            sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'        \n",
    "            bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "            #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "                result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            # Specify the path to your MSA file\n",
    "            msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            #transfer the meta file to another location and delete useless files.\n",
    "            # we need to delete : all uniprot* files. \n",
    "            # all query*. All result* \n",
    "            new_location = f\"{self.work_dir}/{uniprot_id}.fasta\"\n",
    "            shutil.copy(msa_file, new_location)\n",
    "            #remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        #we want the path to msa_file for downstream analysis.\n",
    "        return new_location\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "        '''\n",
    "        Helper function to grab the sequence \n",
    "        based on the Uniprot ID\n",
    "        '''\n",
    "        fields = \"sequence\"\n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        seq = \"\"\n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "            i += 1\n",
    "        return seq\n",
    "\n",
    "    def _get_conservation(self, path_to_msa:str):    \n",
    "        '''\n",
    "        Helper function to compute 3 different types of conservation.\n",
    "        \n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "        canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "        result_cons = canal.analysis(method=\"all\")\n",
    "        return result_cons\n",
    "\n",
    "    def _get_url(self, url):\n",
    "        '''Helper function that uses requests for Downloads.'''\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit() \n",
    "        return response\n",
    "    \n",
    "    def setup_cutoff(self, cutoff=10, apply_filter=False):\n",
    "        '''If we want to setup a resolution cutoff filter for further downstream analysis, \n",
    "        this function helps with it.'''\n",
    "        # If there is no meta dict we cant proceed and filter based on resolution.\n",
    "        if self.meta_dict:\n",
    "            #here we store the pdb codes that we keep\n",
    "            pdbs_to_keep = []\n",
    "            #Now lets parse through the whole meta dict and fetch the cutoffs for structures.\n",
    "            for _, single_pdbs in self.meta_dict.items():\n",
    "                try:\n",
    "                    if single_pdbs['resolution'] <= cutoff:\n",
    "                        pdbs_to_keep.append(single_pdbs['code'].lower()) #normalize to lower in order to have uniform list members.   \n",
    "                except:\n",
    "                    # 'technique': 'SOLUTION NMR' check for that.\n",
    "                    print(f\"we allow for now {single_pdbs=} because no resolution! check if NMR\")\n",
    "                    pass\n",
    "\n",
    "            \n",
    "            self.filtered_structures = pdbs_to_keep\n",
    "            #now if we directly want to apply the filter to remove files that dont match our criteria.\n",
    "            if apply_filter:\n",
    "                #check for union between files and kept structures.\n",
    "                pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "                #lets fetch the intersect between the 2 sets which corresponds to the pdbs we want to keep.\n",
    "                common_pdb = set(pdbs_to_retrieve) & set(pdbs_to_keep) #intersection\n",
    "                intersect_lst = list(common_pdb)\n",
    "                self.filtered_structures = intersect_lst\n",
    "                if self.chain_seqid_dict:\n",
    "                    #now we need to update the chain_dict as well:\n",
    "                    filtered_dict = {pdb: v for pdb, v in self.chain_seqid_dict.items() if pdb[:4] in self.filtered_structures}\n",
    "                    self.filtered_structures = filtered_dict\n",
    "                    \n",
    "        else:\n",
    "            print(\"We have no meta dict to implement a cutoff\")\n",
    "            #In this case we take all.\n",
    "            print(f\"{self.chain_seqid_dict=}\")\n",
    "            # this also needs to take into account the seq id to be useful.\n",
    "            pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\") and len(f) == 8] #exclude non original files. Only store pdb + _ + chains.\n",
    "            \n",
    "            self.filtered_structures = pdbs_to_retrieve\n",
    "\n",
    "    def parallel_shift_calculation(self):\n",
    "        '''Here we compute the shift according to uniprot or authors\n",
    "        in order to be in line with UNIPROT numbering which is crucial for later renumbering.'''\n",
    "        \n",
    "        pdbs_to_retrieve = [f[0:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]  \n",
    "        pdbs_to_retrieve = set(pdbs_to_retrieve) & set(x[:4] for x in self.oligodict.keys()) #here we check the first 4 which is pdb code\n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        shift_dict = defaultdict()\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            calculate_shift_bound = partial(self._calculate_shift)\n",
    "            tasks = ((link_path, pdb) for pdb in pdbs_to_retrieve)\n",
    "            # Map the bound function to the arguments in parallel\n",
    "            results = executor.map(calculate_shift_bound, tasks)\n",
    "            for result in results:\n",
    "                for keys, vals in result.items():\n",
    "                    shift_dict[keys] = vals\n",
    "                    \n",
    "        self.shifts = shift_dict\n",
    "\n",
    "    def _calculate_shift(self, args):\n",
    "        '''\n",
    "        Helper function to compute the shift.\n",
    "        Args: link_path to UNIPROT page and the pdb path.\n",
    "        '''\n",
    "        link_path, pdb = args\n",
    "        shift_dict = defaultdict()\n",
    "        searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "        resp = self._get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                for mapping in uniprot_info['mappings']:\n",
    "                    chain_id = mapping['chain_id']\n",
    "                    unp_start = mapping['unp_start']\n",
    "                    unp_end = mapping['unp_end']\n",
    "                    author_start = mapping['start']['author_residue_number']\n",
    "                    author_end = mapping['end']['author_residue_number']\n",
    "    \n",
    "                    if author_start is None:\n",
    "                        author_start = unp_start\n",
    "                    if author_end is None:\n",
    "                        author_end = unp_end\n",
    "                    shift_start = unp_start - author_start\n",
    "                    shift_end = unp_end - author_end\n",
    "                    shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "                    \n",
    "        self.shift_dict = shift_dict\n",
    "        return shift_dict\n",
    "\n",
    "    \n",
    "    def parallel_renumbering(self):\n",
    "        '''\n",
    "        Helper function to do parallelized renumbering.\n",
    "        If already renumbered, don't do it again.\n",
    "        '''\n",
    "        if self.renumbered:\n",
    "            print(\"You already renumbered your structures based on shift.\")\n",
    "            return  # Exit the function early\n",
    "\n",
    "        if not self.shifts:\n",
    "            print(\"You first need to obtain shifts which will be used as reference in order to start renumbering.\\nCall .parallel_shift_calculation() first.\")\n",
    "            return  # Exit the function if no shifts are available\n",
    "\n",
    "        # At this point, we know renumbering has not been done and shifts are available\n",
    "        relevant_files = self.chain_seq_dict.keys()\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Using partial to create a function with fixed parameters (shift_dict, path)\n",
    "            renumber_structure_partial = partial(self._renumber_structure, shift_dict=self.shifts, path=self.work_dir)\n",
    "            # Map the renumbering function to each relevant file in parallel\n",
    "            executor.map(renumber_structure_partial, relevant_files)\n",
    "\n",
    "        self.renumbered = True\n",
    "\n",
    "    \n",
    "    def _renumber_structure(self, files, shift_dict, path):\n",
    "        '''Function that is going to apply pdb_shiftres_by_chain.py to each pdb file that is shifted.\n",
    "        Will apply renumbering to ALL structures if you did not set a cutoff previously and applied filter. \n",
    "        If filter applied for resolution will only renumber those structures that are left after filtering.'''\n",
    "        for keys, vals in shift_dict.items():\n",
    "            #dont renumber if there is not shift\n",
    "            if files == keys[0:4] and vals != str(0):\n",
    "                chain = keys[-1]\n",
    "                shift = int(vals)\n",
    "                filepath = f\"{self.work_dir}/{files}.pdb\"\n",
    "                # Should we really shift by shift + 1??? or just shift?\n",
    "                bash_cmd = f\"python {self.script_dir}/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "            \n",
    "                with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                    result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, universal_newlines=True)\n",
    "                    # Now replace the original one with the temp file.\n",
    "                    os.replace(f\"{filepath}_tmp\", filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
