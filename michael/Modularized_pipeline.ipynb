{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be60346-a603-46d9-a77a-d05e88d76a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdb\n",
    "import requests\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil\n",
    "import statistics\n",
    "from datetime import date\n",
    "#import hail as hl\n",
    "import glob\n",
    "import time\n",
    "import pytrimal\n",
    "# Import from installed package\n",
    "#from pypdb.clients.pdb.pdb_client import *\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import pandas as pd\n",
    "#import plotly as px\n",
    "from Bio.PDB import PDBParser, PDBIO, Select, MMCIFParser, Structure, Chain, Atom\n",
    "from Bio.PDB import Model as Bio_Model\n",
    "from Bio.PDB import Chain as Bio_chain\n",
    "from Bio.SeqIO import PirIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO, Align, PDB, Seq, AlignIO\n",
    "#from Bio import pairwise2\n",
    "from io import StringIO\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "from modeller.parallel import job, local_slave\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import logging\n",
    "import subprocess\n",
    "import shlex\n",
    "from subprocess import PIPE, run\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup  #required later to download SIFT files.\n",
    "import atomium\n",
    "from itertools import compress\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.gridspec as gridspe\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from bravado.client import SwaggerClient\n",
    "from pycanal import Canal\n",
    "#import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from threading import Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from packman import molecule\n",
    "from packman.apps import predict_hinge\n",
    "\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "#logging.getLogger(\"requests\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fd73c953-08ee-4ab2-ae2f-b9e660a0e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadPipe:\n",
    "    '''Class object containing the download function that will download all pdbs \n",
    "    which we need for downstream analysis of a particular uniprot ID'''\n",
    "    \n",
    "    def __init__(self, templates, work_dir, download_type=\"pdb\"):\n",
    "\n",
    "        # Storage for potential use.\n",
    "        self.work_dir = work_dir \n",
    "        # Download PDB or also mmCIF (currently only PDB)\n",
    "        self.download_type = download_type\n",
    "        # All pdb files that we need to download\n",
    "        self.pdbs_to_download = templates \n",
    "        # The bash script location which will download the pdbs. \n",
    "        self.download_script = os.path.join(work_dir, \"batch_download_modified.sh\")\n",
    "        # The location for the temporary file that is required for the download_script as input.\n",
    "        self.download_tmp = os.path.join(work_dir, \"pdb_list.csv\")     \n",
    "        # The list of chains that will be used later to fetch correct structures.\n",
    "        self.chain_dict = self._setup_download_list()\n",
    "        # We store also meta info as a json dict\n",
    "        self.meta_dict = None\n",
    "        #we store high resolution structures as a list if the user wants to separate based on resolution.\n",
    "        self.high_resolution = None\n",
    "            \n",
    "    def paralellized_download(self):\n",
    "        '''This function is going to call _download_files n times to parallelize download. It is going to pass\n",
    "        the function call itself **_download_file**, self.download_tmp (the location of the tmp file which is pdb_id comma separated), p (an additional parameter specifying that \n",
    "        we want to download pdbs, and self.work_dir(the current work dir)'''\n",
    "        \n",
    "        # ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Submit your tasks to the executor.\n",
    "            futures_pdb = [executor.submit(self._download_files, self.download_tmp, 'p', self.work_dir)]\n",
    "\n",
    "            # Optionally, you can use as_completed to wait for and retrieve completed results.\n",
    "            for future in as_completed(futures_pdb):\n",
    "                result = future.result()\n",
    "                \n",
    "            \n",
    "    def _setup_download_list(self):\n",
    "        '''Helper function to setup the list of comma-separated pdb ids for the download_files function'''\n",
    "        \n",
    "        chain_dict = defaultdict(list)\n",
    "        #Parse through all PDBs and their associated chains which we need to download.\n",
    "        for pdb in self.pdbs_to_download:\n",
    "            # Extract the 4-digit PDB-ID\n",
    "            pdb_4_digit_id = pdb[:4]\n",
    "            # Extract the Chain\n",
    "            chain = pdb[-1]\n",
    "            # Map the chains that we need to each unique PDB-ID\n",
    "            chain_dict[pdb_4_digit_id].append(chain)\n",
    "\n",
    "        # We only want to download pdb files once. \n",
    "        # No reason to download a PDB-file 4 times just because we need chain [A, B, C, D]\n",
    "        unique_pdbs = chain_dict.keys() # Keys : PDB-IDs, Vals: Chains\n",
    "        \n",
    "        # Create download_files input list\n",
    "        with open(self.download_tmp, \"w\") as pdb_tar:\n",
    "            pdb_tar.write(\",\".join(unique_pdbs))\n",
    "\n",
    "        print(chain_dict)\n",
    "        return chain_dict\n",
    "        \n",
    "    \n",
    "    def _download_files(self, download_tmp, download_type, path)->list:\n",
    "        \"\"\"This helper function runs inside paralellized_download \n",
    "        and will be used to get the PDB files that we require for downstream analysis.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "    \n",
    "        # Input for subprocess\n",
    "        bash_curl_cmd = f\"{self.download_script} -f {download_tmp} -o {path} -{download_type}\"\n",
    "        # split into list \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "        \n",
    "        try:\n",
    "            # Run subprocess\n",
    "            result = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            # Append result output to results\n",
    "            results.append(result.stdout.split(\"\\n\")[:-1])  # Skip the last empty element\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error downloading: {e}\")\n",
    "\n",
    "        return results    \n",
    "    \n",
    "    def retrieve_meta(self)->dict:\n",
    "        '''We also want to store meta information about resolution etc.\n",
    "        This function takes each pdb file and retrieves the following information:\n",
    "        \n",
    "        - Title\n",
    "        - Keywords\n",
    "        - PDBcode\n",
    "        - Authors\n",
    "        - Deposition date\n",
    "        - Technique\n",
    "        - Resolution\n",
    "        - R_value : If crystallography else None\n",
    "        - R_free : If crystallographe else None\n",
    "        - Classification\n",
    "        - Organism\n",
    "        - Expression System\n",
    "        - Number of amino acids in the asymmetric unit\n",
    "        - Mass of amino acids in the asymmetric unit (Da)\n",
    "        - Number of amino acids in the biological unit\n",
    "        - Mass of amino acids in the biological unit (Da)\n",
    "        '''\n",
    "        \n",
    "        #little helper function to deal with date data\n",
    "        def _date_encoder(obj):\n",
    "            if isinstance(obj, date):\n",
    "                return obj.isoformat()  # Convert date to ISO format\n",
    "\n",
    "        #grab all PDB files which contain the meta information.\n",
    "        pdbs_to_retrieve = [f for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "\n",
    "        #here we store info about ALL pdbs.\n",
    "        meta_dictionary = defaultdict()\n",
    "        \n",
    "        for pdbs in pdbs_to_retrieve:\n",
    "\n",
    "            # We store meta data here for EACH pdb.\n",
    "            sub_dict = defaultdict()\n",
    "            #we store the 4-digit code for easier access\n",
    "            pdb_code = pdbs[:4]\n",
    "            # We need the fullpath to fetch meta data.\n",
    "            fullp = os.path.join(self.work_dir, pdbs)\n",
    "\n",
    "            #open file through atomium\n",
    "            pdb = atomium.open(fullp)\n",
    "            sub_dict[\"title\"] = pdb.title\n",
    "            sub_dict[\"key_words\"] = pdb.keywords\n",
    "            sub_dict[\"code\"] = pdb.code\n",
    "            sub_dict[\"authors\"] = pdb.authors\n",
    "            sub_dict[\"deposition_date\"] = pdb.deposition_date.isoformat()  #isoformat because it is a time object\n",
    "            sub_dict[\"technique\"] = pdb.technique\n",
    "            sub_dict[\"resolution\"] = pdb.resolution\n",
    "            sub_dict[\"r_val\"] = pdb.rvalue\n",
    "            sub_dict[\"r_free\"] = pdb.rfree\n",
    "            sub_dict[\"classification\"] = pdb.classification\n",
    "            sub_dict[\"organism\"] = pdb.source_organism\n",
    "            sub_dict[\"expression_system\"] = pdb.expression_system\n",
    "            sub_dict['number_of_residues_asymmetric_unit'] = len(pdb.model.residues())\n",
    "            sub_dict['mass_dalton_asymetric_unit'] = f\"{pdb.model.mass:.2f}\" \n",
    "            #sub_dict[\"file_type\"] = pdb.filetype\n",
    "            #sub_dict[\"missing_res\"] = pdb.missing_residues\n",
    "\n",
    "            try:\n",
    "                #build the biological assembly \n",
    "                assembly = pdb.generate_assembly(1)\n",
    "                sub_dict['number_of_residues_biological_unit'] = len(assembly.residues())\n",
    "                sub_dict['mass_dalton_biological_unit'] = f\"{assembly.mass:.2f}\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"We could not build the assembly for: {pdb_code}\")\n",
    "                \n",
    "            meta_dictionary[pdb_code] = sub_dict\n",
    "        \n",
    "        # Code block to store meta info as a txt file.\n",
    "        with open(f\"{work_dir}/human_meta_data.txt\", \"w\") as human_fh:\n",
    "            for _, entries in meta_dictionary.items():\n",
    "                for info , val in entries.items():\n",
    "                    human_fh.write(info)\n",
    "                    human_fh.write(\":\")\n",
    "                    if isinstance(val, list):\n",
    "                        val = \", \".join(map(str,val))\n",
    "                    human_fh.write(str(val))\n",
    "                    human_fh.write(\"\\n\")\n",
    "                    \n",
    "                human_fh.write(\"-\"*80)\n",
    "\n",
    "        #lets store meta info as json dict\n",
    "        self.meta_dict = meta_dictionary\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3ace5f9c-d10f-4beb-8044-cf617c1e9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBCleaning:\n",
    "    '''Class object containing the cleaning functions that will work on all PDBs that we fetched.'''\n",
    "    \n",
    "    def __init__(self, work_dir, chain_dict=None, meta_dict=None):\n",
    "        self.work_dir = work_dir\n",
    "        self.chain_dict = chain_dict\n",
    "        self.filtered_chain_dict = None\n",
    "        self.meta_dict = meta_dict\n",
    "        self.shifts = None\n",
    "        self.filtered_structures = None\n",
    "        self.renumbered = False\n",
    "\n",
    "    \n",
    "    def setup_cutoff(self, cutoff, apply_filter=False):\n",
    "        '''If we want to setup a resolution cutoff filter for further downstream analysis, \n",
    "        this function helps with it.'''\n",
    "        \n",
    "        # If there is no meta dict we cant proceed and filter based on resolution.\n",
    "        if self.meta_dict:\n",
    "\n",
    "            #here we store the pdb codes that we keep\n",
    "            pdbs_to_keep = []\n",
    "            #Now lets parse through the whole meta dict and fetch the cutoffs for structures.\n",
    "            for _, single_pdbs in self.meta_dict.items():\n",
    "                if single_pdbs['resolution'] <= cutoff:\n",
    "                    pdbs_to_keep.append(single_pdbs['code'].lower()) #normalize to lower in order to have uniform list members.    \n",
    "\n",
    "\n",
    "            self.filtered_structures = pdbs_to_keep\n",
    "\n",
    "            #now if we directly want to apply the filter to remove files that dont match our criteria.\n",
    "            if apply_filter:\n",
    "                #check for union between files and kept structures.\n",
    "                pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "\n",
    "                #lets fetch the intersect between the 2 sets which corresponds to the pdbs we want to keep.\n",
    "                common_pdb = set(pdbs_to_retrieve) & set(pdbs_to_keep) #intersection\n",
    "                intersect_lst = list(common_pdb)\n",
    "                self.filtered_structures = pdbs_to_keep\n",
    "                \n",
    "                if self.chain_dict:\n",
    "                    #now we need to update the chain_dict as well:\n",
    "                    filtered_dict = {pdb: v for pdb, v in self.chain_dict.items() if pdb[:4] in pdbs_to_keep}\n",
    "                    \n",
    "                    self.chain_dict = filtered_dict\n",
    "                    \n",
    "        \n",
    "        else:\n",
    "            print(\"We have no meta dict to implement a cutoff\")\n",
    "\n",
    "\n",
    "    \n",
    "    def parallel_shift_calculation(self):\n",
    "        '''Here we compute the shift according to uniprot or authors in order to be in line with UNIPROT numbering which is crucial for later renumbering.'''\n",
    "        \n",
    "        pdbs_to_retrieve = [f[0:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "\n",
    "        #print(pdbs_to_retrieve)\n",
    "        #print(set(x[:4] for x in self.chain_dict.keys()))\n",
    "        \n",
    "        pdbs_to_retrieve = set(pdbs_to_retrieve) & set(x[:4] for x in self.chain_dict.keys()) #here we check the first 4 which is pdb code\n",
    "\n",
    "        #print(pdbs_to_retrieve)\n",
    "        \n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        shift_dict = defaultdict()\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Using partial to create a function with fixed parameters (link_path)\n",
    "            calculate_shift_bound = partial(self._calculate_shift)\n",
    "            \n",
    "            tasks = ((link_path, pdb) for pdb in pdbs_to_retrieve)\n",
    "\n",
    "            # Map the bound function to the arguments in parallel\n",
    "            results = executor.map(calculate_shift_bound, tasks)\n",
    "\n",
    "            # Combine the results\n",
    "            for result in results:\n",
    "                for keys, vals in result.items():\n",
    "                    shift_dict[keys] = vals\n",
    "\n",
    "        self.shifts = shift_dict\n",
    "\n",
    "    \n",
    "    def _calculate_shift(self, args):\n",
    "\n",
    "        link_path, pdb = args\n",
    "        shift_dict = defaultdict()\n",
    "        \n",
    "        searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "        resp = self._get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        \n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                for mapping in uniprot_info['mappings']:\n",
    "                    chain_id = mapping['chain_id']\n",
    "                    unp_start = mapping['unp_start']\n",
    "                    unp_end = mapping['unp_end']\n",
    "                    \n",
    "                    author_start = mapping['start']['author_residue_number']\n",
    "                    author_end = mapping['end']['author_residue_number']\n",
    "    \n",
    "                    if author_start is None:\n",
    "                        author_start = unp_start\n",
    "                    if author_end is None:\n",
    "                        author_end = unp_end\n",
    "    \n",
    "                    shift_start = unp_start - author_start\n",
    "                    shift_end = unp_end - author_end\n",
    "    \n",
    "                    shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "\n",
    "        return shift_dict\n",
    "\n",
    "    \n",
    "    def parallel_renumbering(self):\n",
    "\n",
    "        if self.shifts:\n",
    "            \n",
    "            #needs a list to apply it to \n",
    "            relevant_files = self.chain_dict.keys()\n",
    "            \n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                # Using partial to create a function with fixed parameters (shift_dict, path)\n",
    "                renumber_structure_partial = partial(self._renumber_structure, shift_dict=self.shifts, path=self.work_dir)\n",
    "                # Map the renumbering function to each relevant file in parallel\n",
    "                executor.map(renumber_structure_partial, relevant_files)\n",
    "\n",
    "            self.renumbered = True\n",
    "        else:\n",
    "            print(\"You first need to obtain shifts which will be used as reference in order to start renumbering.\\nCall first .parallel_shift_calculation()\")\n",
    "    \n",
    "    def _renumber_structure(self, files, shift_dict, path):\n",
    "        '''Function that is going to apply pdb_shiftres_by_chain.py to each pdb file that is shifted.\n",
    "        Will apply renumbering to ALL structures if you did not set a cutoff previously and applied filter. \n",
    "        If filter applied for resolution will only renumber those structures that are left after filtering.'''\n",
    "        \n",
    "        for keys, vals in shift_dict.items():\n",
    "            #dont renumber if there is not shift\n",
    "            if files == keys[0:4] and vals != str(0):\n",
    "\n",
    "                chain = keys[-1]\n",
    "                shift = int(vals)\n",
    "                filepath = f\"{self.work_dir}/{files}.pdb\"\n",
    "                # Should we really shift by shift + 1??? or just shift?\n",
    "                bash_cmd = f\"python {self.work_dir}/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "            \n",
    "                with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                    result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, universal_newlines=True)\n",
    "                    # Now replace the original one with the temp file.\n",
    "                    os.replace(f\"{filepath}_tmp\", filepath)\n",
    "    \n",
    "    \n",
    "    def _get_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit()\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ebf74f3d-61e6-42f7-a1d5-5f5f20eaa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdbBuilder:\n",
    "    def __init__(self, work_dir, structures):\n",
    "        self.work_dir = work_dir\n",
    "        #Here we store the structures that need to be built.\n",
    "        self.structures = structures\n",
    "        \n",
    "\n",
    "    \n",
    "    def build_assembly(self):\n",
    "\n",
    "        # These files need to be opened, rechained and assemblies built.\n",
    "        full_pdb_paths = [os.path.join(self.work_dir, f\"{file}.pdb\") for file in self.structures]\n",
    "\n",
    "        \n",
    "        oligostates = defaultdict(str)\n",
    "    \n",
    "        #this letterdict is used for rechaining.\n",
    "        letterdict = {i: chr(65 + i) for i in range(26)}\n",
    "        \n",
    "        #changed this here from threadpool to process pool\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Define your processing function, partially applied with gene_name and main_protein_seq\n",
    "            process_func = partial(self._process_pdb, letterdict=letterdict)\n",
    "    \n",
    "            results = executor.map(process_func, full_pdb_paths)\n",
    "    \n",
    "            for result in results:\n",
    "                #print(f\"this is result: {result}\")\n",
    "                oligostates.update(result)\n",
    "                \n",
    "        return oligostates\n",
    "\n",
    "\n",
    "    def _process_pdb(self, path:str,letterdict:dict)->dict:\n",
    "        #helper function to split between nmr and xray / cryoem\n",
    "        try:\n",
    "        \n",
    "            pdb_file_name = os.path.basename(path)\n",
    "        \n",
    "            pdb_file = atomium.open(path)\n",
    "            # We filter according to model length.. if there are more than 5 models deposited its NMR\n",
    "            model_len = pdb_file.models\n",
    "            \n",
    "            if len(model_len) > 5:\n",
    "\n",
    "                print(\"we go into _NMR_ensemble\")\n",
    "                #this gives back a dictionary with all nmr structures and their oligomeric state (mostly monomer.)\n",
    "                return {pdb_file_name: self._NMR_ensemble(path=path, letterdict=letterdict)}\n",
    "            else:\n",
    "                return {pdb_file_name: self._non_NMR_structures(path=path, letterdict=letterdict)}\n",
    "    \n",
    "        except Exception as error:\n",
    "            print(\"process pdb did not work\")\n",
    "            print(error)\n",
    "        \n",
    "            return {}\n",
    "    \n",
    "    #helper function for XRAY and CRYO-EM ensembles.\n",
    "    def _non_NMR_structures(self, path:str, letterdict:dict):\n",
    "    \n",
    "        \"\"\"This function takes in the the pdb file that is xray or cryoem and rechains each chain. \n",
    "        Additionally, we merge the new labelled chains into a merged_pdb file for further use.\"\"\"\n",
    "    \n",
    "        #store base dir.\n",
    "        base_dir = os.path.dirname(path)\n",
    "        pdb_name = os.path.basename(path)[:4]\n",
    "    \n",
    "        #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95.pdb\n",
    "        pdb_file = atomium.open(path)\n",
    "    \n",
    "        assemblies = [pdb_file.generate_assembly(n + 1) for n in range(len(pdb_file.assemblies))]\n",
    "    \n",
    "        #we take the first one(this is the biological unit built from the asymmetric unit)\n",
    "        assembly = assemblies[0]\n",
    "\n",
    "        #tuple containing chain ID and LEN of each chain.\n",
    "        seq_chains = [(chain.id, len(chain.sequence)) for chain in assembly.chains()]\n",
    "\n",
    "        sorted_lens = sorted(seq_chains, key= lambda x: x[1], reverse=True) #reverse = true :largest first.\n",
    "\n",
    "        accepted_chains = []  #this will be used to store and evalute oligomeric state.\n",
    "    \n",
    "        min_accepted_length = float('inf')   # Minimum length of accepted chains init as pos inf.\n",
    "\n",
    "        for chain, length in seq_chains:\n",
    "            #for each chain and length.\n",
    "            if not accepted_chains or length > 0.8 * min_accepted_length:\n",
    "                #we accept it if either we have no other chain so far OR length > 80% of the chain we have so far.\n",
    "            \n",
    "                accepted_chains.append(chain)\n",
    "                #first chain will be accepted and then will be the standard for the next chains to follow.\n",
    "                min_accepted_length = min(min_accepted_length, length)\n",
    "\n",
    "\n",
    "\n",
    "        oligostate = len(accepted_chains)  #this excludes small peptides ect from being mistaken as oligomers.\n",
    "\n",
    "        \"\"\"Part 2. We investigate oligomeric state.\"\"\"\n",
    "    \n",
    "        accepted_chains_set = set(accepted_chains)\n",
    "\n",
    "        oligomeric_status = None\n",
    "    \n",
    "        if len(accepted_chains_set) != len(accepted_chains):\n",
    "            #this means we have a homo-oligomer!\n",
    "            #e.g A vs A A A .. len(1) != len(0)\n",
    "            #hetero-mers are not caught here.. A B C == A B C == len(3)\n",
    "            oligomeric_status = \"homo_oligomer\"\n",
    "        \n",
    "        elif len(accepted_chains) == 1:\n",
    "        \n",
    "            #this means we deal with a monomer.\n",
    "            oligomeric_status = \"monomer\"\n",
    "        \n",
    "        elif len(accepted_chains) > 1 and len(accepted_chains) == len(accepted_chains_set):\n",
    "        \n",
    "            #this means its a mixed heteromer. becaue len(1) > AND set == list aka no redundancy ergo heteromer.\n",
    "            oligomeric_status = \"hetero_oligomer\"\n",
    "\n",
    "        \"\"\"Part 3: We follow through and now save individual chains + send them to proper rechaining. \"\"\"    \n",
    "        path_list = []\n",
    "\n",
    "        for idx, chain in enumerate(assembly.chains()):\n",
    "            chain_label = chain.id\n",
    "            if chain_label in accepted_chains_set:\n",
    "                path_to_pdb = f\"{self.work_dir}/{pdb_name}_{idx}.pdb\"\n",
    "                #save it here.\n",
    "                path_list.append(path_to_pdb)\n",
    "                #and also save the structure in its wrong chain state first.\n",
    "                chain.save(path_to_pdb)\n",
    "        \n",
    "            path_list = sorted(path_list, key=lambda x: int(x[-5]))\n",
    "\n",
    "    \n",
    "        \"\"\"Part 4: We now deal with all kind of oligomers, and also save all single chains in the procedure.\n",
    "        Normal monomers are also simply saved and rechained. Everything according to a general schema for efficient\n",
    "        downstream processing.\"\"\"\n",
    "    \n",
    "        self._merge_pdb_chains(path_list, pdb_name=pdb_name, oligomeric_status=oligomeric_status, \n",
    "                      letterdict=letterdict, accepted_chains = accepted_chains, accepted_chains_set=accepted_chains_set)\n",
    "    \n",
    "        \n",
    "        #we return the oligostate of this file and merge it into dict as return value.\n",
    "        return oligostate\n",
    "\n",
    "\n",
    "    #helper function for NMR ensembles.\n",
    "    def _NMR_ensemble(self, path:str, letterdict:dict):\n",
    "\n",
    "        \"\"\"This function takes in the NMR ensemble and splits each state into a respective PDB file.\"\"\"\n",
    "    \n",
    "        #open the pdb file\n",
    "        print(f\"we currently open with atomium: {path}\")\n",
    "    \n",
    "        pdb_name = os.path.basename(path)[:4] #4 digit ID\n",
    "        base_dir = os.path.dirname(path) #base dir name\n",
    "    \n",
    "        pdb_file = atomium.open(path)\n",
    "    \n",
    "        oligostate = 1 #default initialized\n",
    "\n",
    "        path_list = []\n",
    "    \n",
    "        for i, model in enumerate(pdb1.models):\n",
    "            #now iterate through each model and its respective chains.\n",
    "\n",
    "            chain_len = len([x.id for x in model.chains()]) > 1 # True if multiple chains.\n",
    "            #if larger than 1 : we need to merge.\n",
    "            chain_paths = []\n",
    "            new_chains = []\n",
    "        \n",
    "            for j, chain in enumerate(model.chains()): #enumerate because there are NMR models with MULTIPLE CHAINS\n",
    "            \n",
    "                #here we save the structure. as number.. we need to check the chain id.\n",
    "            \n",
    "                new_chain = chain.copy(id=letterdict[j]) #new chain ID.\n",
    "            \n",
    "                #this effectively rechained the chain.\n",
    "                save_location = f\"{base_dir}/{pdb_name}_{i}_{letterdict[j]}.pdb\"\n",
    "\n",
    "                print(f\"We save NMR structure chain at : {save_location=}\")\n",
    "                new_chain.save(save_location) # e.g 4ND5_0_A.pdb 4ND5_1_A etc..\n",
    "            \n",
    "                #for multichain this would be : 5NR2_0_A.pdb 5NR2_0_B.pdb 5NR2_0_C.pdb etc.\n",
    "            \n",
    "                if chain_len:\n",
    "                    chain_paths.append(save_location)\n",
    "                    new_chains.append(new_chain.id)\n",
    "        \n",
    "            #if done: check if there are multiple chains. if yes. merge.\n",
    "            if chain_len:\n",
    "\n",
    "                oligostate = len(new_chains)\n",
    "            \n",
    "                #this is still inside the i enumerate one so we utilize this one.\n",
    "                #we merge the chains.\n",
    "\n",
    "                print(f\"This is {new_chains=}\")\n",
    "                save_nmr_oligomer = f\"{base_dir}/{pdb_name}_{i}_{''.join(new_chains)}.pdb\"\n",
    "            \n",
    "                merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(chain_paths)}\"\n",
    "    \n",
    "                merge_command_rdy = merge_command.split()\n",
    "    \n",
    "                merge_output_file = f\"{save_nmr_oligomer}_tmp.pdb\"  #tmp\n",
    "    \n",
    "                with open(merge_output_file, \"w\") as fh_out:\n",
    "                    result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "                # Run tidy on the merged PDB\n",
    "                tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "    \n",
    "                tidy_command_rdy = tidy_command.split()\n",
    "    \n",
    "                tidy_output_file = f\"{save_nmr_oligomer}.pdb\"\n",
    "    \n",
    "                with open(tidy_output_file, \"w\") as fh_out2:\n",
    "                    results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "                #we remove tmp intermediate files.\n",
    "                os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "\n",
    "        return oligostate\n",
    "    \n",
    "    \n",
    "    def _merge_pdb_chains(self, path_list:list, pdb_name:str, oligomeric_status:str, letterdict:dict,\n",
    "                     accepted_chains:list, accepted_chains_set:set):\n",
    "\n",
    "        \"\"\"DEBUG THIS FUNCTION. NEEDS TO SAVE ACCORDINGLY ALL STRUCTURES AT MERGED_CLEANED_FILES.\"\"\"\n",
    "\n",
    "        if oligomeric_status == \"homo_oligomer\":\n",
    "        \n",
    "            print(\"we move into pure oligomer now!\")\n",
    "\n",
    "            #this is ok. lets continue afterwards with the rest.\n",
    "        \n",
    "            self._pure_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "        \n",
    "        elif oligomeric_status == \"hetero_oligomer\":\n",
    "        \n",
    "            print(\"we move into hetero oligomer now!\")\n",
    "\n",
    "            self._mixed_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name,\n",
    "                                  accepted_chains=accepted_chains, \n",
    "                                   accepted_chains_set=accepted_chains_set)\n",
    "\n",
    "        elif oligomeric_status == \"monomer\":\n",
    "        \n",
    "            #print(\"we move into monomer now!\")\n",
    "\n",
    "            #print(f\"{path_list=}, {letterdict=}, {pdb_name=}\")\n",
    "            self._monomeric_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "\n",
    "        else:\n",
    "            print(f\"There was an issue with: {oligomeric_status=}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def _pure_oligomer_rechaining(self, path_list:list, letterdict:dict, pdb_name:str): \n",
    "        #path_list = ['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_0.pdb',\n",
    "        # '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_1.pdb']\n",
    "\n",
    "\n",
    "        directory = os.path.dirname(path_list[0]) #this does not change. so no reason to constantly evaluate it in the loop\n",
    "\n",
    "        #store path to chains here.\n",
    "        lst_to_merge_paths = []\n",
    "        #store lists here for later merge.\n",
    "        chain_lst = []\n",
    "\n",
    "        for path_to_pdb in path_list:\n",
    "            \n",
    "            filename = os.path.basename(path_to_pdb) #same as above.\n",
    "        \n",
    "            new_chain_digit = filename[5]  # Get the single chain id (e.g., '0')\n",
    "\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            individual_structure = parser.get_structure(\"default\", path_to_pdb)\n",
    "\n",
    "        \n",
    "            new_chain = letterdict[int(new_chain_digit)]\n",
    "            #print(f\"this is new chain: {new_chain}\")\n",
    "        \n",
    "            for models in individual_structure:\n",
    "            \n",
    "                for chain in models:\n",
    "                \n",
    "                    if chain.id == new_chain:\n",
    "                        #then we simply save it under its original chain.\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        chain_lst.append(chain.id)\n",
    "                        io.save(save_location)\n",
    "                    else:\n",
    "                        chain.id = new_chain\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        chain_lst.append(chain.id)\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        io.save(save_location)\n",
    "                    \n",
    "\n",
    "        #now we run the merge script to merge the single chains into a merged pdb.\n",
    "        #print(f\"WE are at oligomer_rechaining pure: {lst_to_merge_paths=}\")\n",
    "        #print(f\"WE are at oligomer_rechaining pure: {chain_lst=}\")\n",
    "    \n",
    "        merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "    \n",
    "        merge_command_rdy = merge_command.split()\n",
    "    \n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}_tmp.pdb\"  #tmp\n",
    "    \n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        # Run tidy on the merged PDB\n",
    "        tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "    \n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "    \n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}.pdb\"\n",
    "    \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "        #we remove tmp intermediate files.\n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _mixed_oligomer_rechaining(self, accepted_chains:list,\n",
    "                               accepted_chains_set:set,\n",
    "                               path_list:list,\n",
    "                               letterdict:dict, pdb_name:str):\n",
    "\n",
    "\n",
    "        seen_chains = sorted(accepted_chains, reverse=False)\n",
    "    \n",
    "        chain_seq_len = len(seen_chains) #e.g 6\n",
    "    \n",
    "        shift = len(accepted_chains_set) # e.g 3\n",
    "    \n",
    "        blocksize = chain_seq_len // shift # e.g 2\n",
    "\n",
    "        block_count = int(chain_seq_len/blocksize) \n",
    "    \n",
    "        # A A B B C C becomes A D B E C F\n",
    "    \n",
    "        # B B C C becomes A C B D \n",
    "    \n",
    "        #i = 1\n",
    "    \n",
    "        # A D \n",
    "    \n",
    "        # block 1 2 3 for A A B B C C \n",
    "    \n",
    "        # 0 2 1 3\n",
    "        j = 0\n",
    "        new_chain_seq = []\n",
    "        lst_to_merge_paths = []\n",
    "    \n",
    "        for blocks in range(0, block_count):\n",
    "\n",
    "            for i in range(0, blocksize):\n",
    "                # first iteration A D\n",
    "                # second iteration B E\n",
    "                # third iteration C F\n",
    "                new_chain = letterdict[blocks+i*shift]\n",
    "    \n",
    "                #print(f\"this is new_chain in mixed oligomer rechaining: {new_chain=}\")\n",
    "                new_chain_seq.append(new_chain)\n",
    "    \n",
    "                path_to_pdb = path_list[j]\n",
    "            \n",
    "                directory = os.path.dirname(path_to_pdb)\n",
    "            \n",
    "                j += 1\n",
    "    \n",
    "                parser = PDBParser(QUIET=True)\n",
    "                \n",
    "                prot_name = f\"default\"\n",
    "                \n",
    "                #open the correct pdb and rechain it.\n",
    "                structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "                \n",
    "                for models in structure_template:\n",
    "                    for chain in models:\n",
    "                        if chain.id != new_chain:\n",
    "                            chain.id = new_chain\n",
    "                \n",
    "                        io = PDBIO()\n",
    "            \n",
    "                        io.set_structure(chain)\n",
    "                        #print(f\"This is single chain save inside oligomer rechain: {directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        io.save(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "        \n",
    "        merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "        \n",
    "        merge_command_rdy = merge_command.split()\n",
    "        \n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}_tmp.pdb\"  #tmp\n",
    "        \n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        # Run tidy on the merged PDB\n",
    "        tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "        \n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "        \n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}.pdb\"\n",
    "        \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "        #we remove tmp intermediate files.\n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.  \n",
    "\n",
    "    def _monomeric_rechaining(self, path_list:list,\n",
    "                          letterdict:dict,\n",
    "                          pdb_name:str):\n",
    "\n",
    "        #path_list=['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2q9p_0.pdb'], \n",
    "        #letterdict={0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K',\n",
    "        #11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21:\n",
    "        #'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}, \n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = \"default\"\n",
    "        #We only have 1 path in this list.\n",
    "        pdb = path_list[0]\n",
    "        # Open the correct PDB and rechain it.\n",
    "        dir_name = os.path.dirname(pdb)\n",
    "        structure_template = parser.get_structure(prot_name, pdb) \n",
    "        # Get the new chain ID\n",
    "        new_chain = \"A\"  #always... A\n",
    "        for model in structure_template:\n",
    "            for original_chain in model:\n",
    "                if original_chain.id != new_chain:\n",
    "                    original_chain.id = new_chain\n",
    "        save_path = os.path.join(dir_name, f\"{pdb_name}_{new_chain}.pdb\")\n",
    "        #print(f\"This is save path: {save_path=}\")\n",
    "        # Save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure_template)\n",
    "    \n",
    "        #print(\"we save now:\")\n",
    "        io.save(save_path)\n",
    "        os.remove(pdb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "282e8d55-bc73-49ef-8821-54f134bf8019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'5ltu': ['A', 'B'], '7nnj': ['A', 'B'], '2duk': ['A', 'B'], '3mcf': ['A', 'B'], '7tn4': ['A'], '2q9p': ['A'], '2fvv': ['A'], '6pck': ['A'], '6pcl': ['A'], '6wo7': ['A'], '6wo8': ['A'], '6wo9': ['A'], '6woa': ['A'], '6wob': ['A'], '6woc': ['A'], '6wod': ['A'], '6woe': ['A'], '6wof': ['A'], '6wog': ['A'], '6woh': ['A'], '6woi': ['A'], '7aut': ['A'], '7aui': ['A'], '7auk': ['A'], '7aul': ['A'], '7aum': ['A'], '7aun': ['A'], '7auo': ['A'], '7aup': ['A'], '7auq': ['A'], '7aur': ['A'], '7aus': ['A'], '7auu': ['A'], '7auj': ['A'], '3h95': ['A'], '3i7u': ['A', 'B', 'C', 'D'], '3i7v': ['A', 'B'], '4hfq': ['A', 'B']})\n"
     ]
    }
   ],
   "source": [
    "templates = ['5ltu_A', '5ltu_B', '7nnj_A', '7nnj_B', '2duk_A', '2duk_B', '3mcf_A', '3mcf_B', '7tn4_A', '2q9p_A', '2fvv_A', '6pck_A', '6pcl_A', '6wo7_A', '6wo8_A', '6wo9_A', '6woa_A', '6wob_A', '6woc_A', '6wod_A', '6woe_A', '6wof_A', '6wog_A', '6woh_A', '6woi_A', '7aut_A', '7aui_A', '7auk_A', '7aul_A', '7aum_A', '7aun_A', '7auo_A', '7aup_A', '7auq_A', '7aur_A', '7aus_A', '7auu_A', '7auj_A', '3h95_A', '3i7u_A', '3i7u_B', '3i7u_C', '3i7u_D', '3i7v_A', '3i7v_B', '4hfq_A', '4hfq_B']\n",
    "work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline\"\n",
    "\n",
    "PdbEnsemble = DownloadPipe(templates=templates, work_dir=work_dir)\n",
    "PdbEnsemble.paralellized_download()\n",
    "PdbEnsemble.retrieve_meta()\n",
    "PdbEnsemble_meta = PdbEnsemble.meta_dict\n",
    "PdbEnsemble_chains = PdbEnsemble.chain_dict\n",
    "#shifts = PdbEnsemble.parallel_shift_calculation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "bc3e2a62-c0e1-4f3c-a414-72b8b922d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_Cleaner = PDBCleaning(work_dir=work_dir, meta_dict=PdbEnsemble_meta, chain_dict=PdbEnsemble_chains)\n",
    "PDB_Cleaner.setup_cutoff(cutoff=2.5, apply_filter=True)  #apply filter to only include structures that are of good quality\n",
    "PDB_Cleaner.parallel_shift_calculation()  # compute shift for each structure\n",
    "PDB_Cleaner.parallel_renumbering()  # renumber based on shifts.\n",
    "#PDB_cleaned_ensemble.chain_dict\n",
    "\n",
    "struct = PDB_Cleaner.filtered_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e7db6e76-bb78-471c-87b5-5fef2b9e698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3i7v', '2q9p', '7auu', '3mcf', '7aur', '7aup', '3h95', '7auq', '6woa', '6woi', '4hfq', '6wod', '7auk', '6pck', '6wob', '7aun', '7aut', '7auj', '6wof', '6wo9', '5ltu', '7nnj', '6wo8', '7aui', '6wo7', '7aum', '3i7u', '2fvv', '6woc', '7aul', '7aus', '6pcl', '6wog', '7tn4', '6woh', '6woe']\n"
     ]
    }
   ],
   "source": [
    "print(struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6046009-6b90-4960-a58f-e84425eb5947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f206e949-8850-4e5a-85a5-fd69b6f90125",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_Builder = PdbBuilder(work_dir=work_dir, structures=struct) #structures that are filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6bcee9d6-9a9e-4c5e-a90c-edc604022b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we move into pure oligomer now!\n",
      "we move into hetero oligomer now!\n",
      "we move into hetero oligomer now!\n",
      "we move into hetero oligomer now!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'3i7v.pdb': 1,\n",
       "             '2q9p.pdb': 1,\n",
       "             '7auu.pdb': 1,\n",
       "             '3mcf.pdb': 1,\n",
       "             '7aur.pdb': 1,\n",
       "             '7aup.pdb': 1,\n",
       "             '3h95.pdb': 2,\n",
       "             '7auq.pdb': 1,\n",
       "             '6woa.pdb': 1,\n",
       "             '6woi.pdb': 1,\n",
       "             '4hfq.pdb': 2,\n",
       "             '6wod.pdb': 1,\n",
       "             '7auk.pdb': 1,\n",
       "             '6pck.pdb': 1,\n",
       "             '6wob.pdb': 1,\n",
       "             '7aun.pdb': 1,\n",
       "             '7aut.pdb': 1,\n",
       "             '7auj.pdb': 1,\n",
       "             '6wof.pdb': 1,\n",
       "             '6wo9.pdb': 1,\n",
       "             '5ltu.pdb': 2,\n",
       "             '7nnj.pdb': 1,\n",
       "             '6wo8.pdb': 1,\n",
       "             '7aui.pdb': 1,\n",
       "             '6wo7.pdb': 1,\n",
       "             '7aum.pdb': 1,\n",
       "             '3i7u.pdb': 4,\n",
       "             '2fvv.pdb': 1,\n",
       "             '6woc.pdb': 1,\n",
       "             '7aul.pdb': 1,\n",
       "             '7aus.pdb': 1,\n",
       "             '6pcl.pdb': 1,\n",
       "             '6wog.pdb': 1,\n",
       "             '7tn4.pdb': 1,\n",
       "             '6woh.pdb': 1,\n",
       "             '6woe.pdb': 1})"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDB_Builder.build_assembly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa857a-7aca-4a02-96e9-fc9e8ddccdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d2633-96a6-4a40-bc36-b451a7d5b9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e751d02-81a8-4d6d-a3cd-2ed505869a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe6755-31b3-4625-80d2-d5fe6f72770a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b9384-085d-488b-801d-b1ff26656cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa047f5a-39aa-4096-b60f-a75d6106dd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a6f85-d399-4400-b1bd-f3a613b51ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1429f1f-3a0a-454f-aa21-259e6722280c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9169f64-fdac-413d-a327-9b90ec03fd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e39fdc-f448-43d2-a352-59293c3b05b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09789c11-5420-4464-abd1-3c5b12e7c2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf31e48-db6e-4ed8-9803-2c3027b362ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e0a1a-b6fa-4a78-a902-e3a42426176b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614c4b0-b1c7-4069-b746-5bca97620321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81521527-400c-45d0-b49b-bec452fac237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bb0bb-7cfe-4ce1-b2d0-cde888f3029e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcdf9f-caaa-4d5e-9cdd-1ba317478f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad68230-5c43-4ca3-92f6-e9fcdb6c3a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2735516-ab1a-4c87-ae06-4a623907efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac38b3f-b54f-4051-b5b3-a8a2902af69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb825eb4-2ab6-4584-9c1d-8913a20c21fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910e871-c075-446b-8186-e051e0ea8d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ff7b5-72cf-44b5-b088-825ab691322e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8894f40-9733-44d6-bf9c-bb85a6b16544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e09dd-c2aa-45c1-abf3-6a22a6a95087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b374a-70f6-4562-b425-1342f3399170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b68735-2013-4f25-ad03-857ab72e9466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec6037-a0f3-4823-bc9d-fe3346d369b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4121fe-cc4e-437b-9ae6-76e07ad8e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
