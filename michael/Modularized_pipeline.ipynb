{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1be60346-a603-46d9-a77a-d05e88d76a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdb\n",
    "import requests\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict, OrderedDict\n",
    "from collections import Counter\n",
    "import re\n",
    "import shutil\n",
    "import statistics\n",
    "from datetime import date\n",
    "#import hail as hl\n",
    "import glob\n",
    "import time\n",
    "import pytrimal\n",
    "# Import from installed package\n",
    "#from pypdb.clients.pdb.pdb_client import *\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import pandas as pd\n",
    "#import plotly as px\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO, Align, PDB, Seq, AlignIO\n",
    "from Bio.PDB import PDBParser, PDBIO, Select, MMCIFParser, Structure, Chain, Atom\n",
    "from Bio.PDB import Model as Bio_Model\n",
    "from Bio.PDB import Chain as Bio_chain\n",
    "from Bio.SeqIO import PirIO\n",
    "\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align import substitution_matrices\n",
    "#from Bio import pairwise2\n",
    "from io import StringIO\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "from modeller.parallel import job, local_slave\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import logging\n",
    "import subprocess\n",
    "import shlex\n",
    "from subprocess import PIPE, run\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup  #required later to download SIFT files.\n",
    "import atomium\n",
    "from itertools import compress\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.gridspec as gridspe\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from bravado.client import SwaggerClient\n",
    "from pycanal import Canal\n",
    "#import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from threading import Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from packman import molecule\n",
    "from packman.apps import predict_hinge\n",
    "\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "#logging.getLogger(\"requests\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd73c953-08ee-4ab2-ae2f-b9e660a0e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadPipe:\n",
    "    '''Class object containing the download function that will download all pdbs \n",
    "    which we need for downstream analysis of a particular uniprot ID'''\n",
    "\n",
    "    def __init__(self, templates, work_dir, seq_id=None, download_type=\"pdb\"):\n",
    "\n",
    "        # Storage for potential use.\n",
    "        self.work_dir = work_dir\n",
    "        # Storage of seq identity useful later for temp selection.\n",
    "        self.seq_id = seq_id\n",
    "        # Download PDB or also mmCIF (currently only PDB)\n",
    "        self.download_type = download_type\n",
    "        # All pdb files that we need to download\n",
    "        self.pdbs_to_download = templates \n",
    "        # The bash script location which will download the pdbs. \n",
    "        self.download_script = os.path.join(work_dir, \"batch_download_modified.sh\")\n",
    "        # The location for the temporary file that is required for the download_script as input.\n",
    "        self.download_tmp = os.path.join(work_dir, \"pdb_list.csv\")     \n",
    "        # The list of chains that will be used later to fetch correct structures.\n",
    "        self.chain_seqid_dict = self._setup_download_list()\n",
    "        # We store also meta info as a json dict\n",
    "        self.meta_dict = None\n",
    "        #we store high resolution structures as a list if the user wants to separate based on resolution.\n",
    "        self.high_resolution = None\n",
    "        # set a flag that stops redownloading.\n",
    "        self.already_downloaded = None\n",
    "        # collect conservation\n",
    "        self.conservation_df = None\n",
    "            \n",
    "    def paralellized_download(self):\n",
    "        '''This function is going to call _download_files n times to parallelize download. It is going to pass\n",
    "        the function call itself **_download_file**, self.download_tmp (the location of the tmp file which is pdb_id comma separated), p (an additional parameter specifying that \n",
    "        we want to download pdbs, and self.work_dir(the current work dir)'''\n",
    "\n",
    "        # ThreadPoolExecutor\n",
    "        if not self.already_downloaded:\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                # Submit your tasks to the executor.\n",
    "                futures_pdb = [executor.submit(self._download_files, self.download_tmp, 'p', self.work_dir)]\n",
    "\n",
    "                # Optionally, you can use as_completed to wait for and retrieve completed results.\n",
    "                for future in as_completed(futures_pdb):\n",
    "                    result = future.result()\n",
    "\n",
    "        self.already_downloaded = True\n",
    "            \n",
    "    def _setup_download_list(self):\n",
    "        '''Helper function to setup the list of comma-separated pdb ids for the download_files function'''\n",
    "        \n",
    "        chain_seqid_dict = defaultdict(list)\n",
    "        #Parse through all PDBs and their associated chains which we need to download. store also the seq id.\n",
    "        for pdb, seq_id in zip(self.pdbs_to_download, self.seq_id):\n",
    "            # Extract the 4-digit PDB-ID\n",
    "            pdb_4_digit_id = pdb[:4]\n",
    "            # Extract the Chain\n",
    "            chain = pdb[-1]\n",
    "            # Map the chains that we need to each unique PDB-ID\n",
    "            chain_seqid_dict[pdb_4_digit_id].append((chain, seq_id))\n",
    "\n",
    "        # We only want to download pdb files once. \n",
    "        # No reason to download a PDB-file 4 times just because we need chain [A, B, C, D]\n",
    "        unique_pdbs = chain_seqid_dict.keys() # Keys : PDB-IDs, Vals: Chains, seq_id\n",
    "        \n",
    "        # Create download_files input list\n",
    "        with open(self.download_tmp, \"w\") as pdb_tar:\n",
    "            pdb_tar.write(\",\".join(unique_pdbs))\n",
    "\n",
    "        #print(chain_dict)\n",
    "        return chain_seqid_dict\n",
    "        \n",
    "    \n",
    "    def _download_files(self, download_tmp, download_type, path)->list:\n",
    "        \"\"\"This helper function runs inside paralellized_download \n",
    "        and will be used to get the PDB files that we require for downstream analysis.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "    \n",
    "        # Input for subprocess\n",
    "        bash_curl_cmd = f\"{self.download_script} -f {download_tmp} -o {path} -{download_type}\"\n",
    "        # split into list \n",
    "        bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "        \n",
    "        try:\n",
    "            # Run subprocess\n",
    "            result = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            # Append result output to results\n",
    "            results.append(result.stdout.split(\"\\n\")[:-1])  # Skip the last empty element\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error downloading: {e}\")\n",
    "\n",
    "        return results    \n",
    "    \n",
    "    def retrieve_meta(self)->dict:\n",
    "        '''We also want to store meta information about resolution etc.\n",
    "        This function takes each pdb file and retrieves the following information:\n",
    "        \n",
    "        - Title\n",
    "        - Keywords\n",
    "        - PDBcode\n",
    "        - Authors\n",
    "        - Deposition date\n",
    "        - Technique\n",
    "        - Resolution\n",
    "        - R_value : If crystallography else None\n",
    "        - R_free : If crystallographe else None\n",
    "        - Classification\n",
    "        - Organism\n",
    "        - Expression System\n",
    "        - Number of amino acids in the asymmetric unit\n",
    "        - Mass of amino acids in the asymmetric unit (Da)\n",
    "        - Number of amino acids in the biological unit\n",
    "        - Mass of amino acids in the biological unit (Da)\n",
    "        '''\n",
    "        \n",
    "        #little helper function to deal with date data\n",
    "        def _date_encoder(obj):\n",
    "            if isinstance(obj, date):\n",
    "                return obj.isoformat()  # Convert date to ISO format\n",
    "\n",
    "        #grab all PDB files which contain the meta information.\n",
    "        pdbs_to_retrieve = [f for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "\n",
    "        #here we store info about ALL pdbs.\n",
    "        meta_dictionary = defaultdict()\n",
    "        \n",
    "        for pdbs in pdbs_to_retrieve:\n",
    "            # We store meta data here for EACH pdb.\n",
    "            sub_dict = defaultdict()\n",
    "            #we store the 4-digit code for easier access\n",
    "            pdb_code = pdbs[:4]\n",
    "            # We need the fullpath to fetch meta data.\n",
    "\n",
    "            try:\n",
    "                fullp = os.path.join(self.work_dir, pdbs)\n",
    "    \n",
    "                #open file through atomium\n",
    "                pdb = atomium.open(fullp)\n",
    "                sub_dict[\"title\"] = pdb.title\n",
    "                sub_dict[\"key_words\"] = pdb.keywords\n",
    "                sub_dict[\"code\"] = pdb.code\n",
    "                sub_dict[\"authors\"] = pdb.authors\n",
    "                sub_dict[\"deposition_date\"] = pdb.deposition_date.isoformat()  #isoformat because it is a time object\n",
    "                sub_dict[\"technique\"] = pdb.technique\n",
    "                sub_dict[\"resolution\"] = pdb.resolution\n",
    "                sub_dict[\"r_val\"] = pdb.rvalue\n",
    "                sub_dict[\"r_free\"] = pdb.rfree\n",
    "                sub_dict[\"classification\"] = pdb.classification\n",
    "                sub_dict[\"organism\"] = pdb.source_organism\n",
    "                sub_dict[\"expression_system\"] = pdb.expression_system\n",
    "                sub_dict['number_of_residues_asymmetric_unit'] = len(pdb.model.residues())\n",
    "                sub_dict['mass_dalton_asymetric_unit'] = f\"{pdb.model.mass:.2f}\" \n",
    "                #sub_dict[\"file_type\"] = pdb.filetype\n",
    "                #sub_dict[\"missing_res\"] = pdb.missing_residues\n",
    "    \n",
    "                try:\n",
    "                    #build the biological assembly \n",
    "                    assembly = pdb.generate_assembly(1)\n",
    "                    sub_dict['number_of_residues_biological_unit'] = len(assembly.residues())\n",
    "                    sub_dict['mass_dalton_biological_unit'] = f\"{assembly.mass:.2f}\"\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"We could not build the assembly for: {pdb_code}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"We had an error with file: {pdb_code}\")\n",
    "            meta_dictionary[pdb_code] = sub_dict\n",
    "        \n",
    "        # Code block to store meta info as a txt file.\n",
    "        with open(f\"{work_dir}/human_meta_data.txt\", \"w\") as human_fh:\n",
    "            for _, entries in meta_dictionary.items():\n",
    "                for info , val in entries.items():\n",
    "                    human_fh.write(info)\n",
    "                    human_fh.write(\":\")\n",
    "                    if isinstance(val, list):\n",
    "                        val = \", \".join(map(str,val))\n",
    "                    human_fh.write(str(val))\n",
    "                    human_fh.write(\"\\n\")\n",
    "                    \n",
    "                human_fh.write(\"-\"*80)\n",
    "\n",
    "        #lets store meta info as json dict\n",
    "        self.meta_dict = meta_dictionary\n",
    "\n",
    "    \n",
    "    def conservation(self, uniprot_id):\n",
    "        \n",
    "        #run mmseq to find homolog sequences based on the majority path.\n",
    "\n",
    "        out_dir = f\"{self.work_dir}\"\n",
    "        #print(\"we start mmseq now!\")\n",
    "        mmseq_fasta_result = self._mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=out_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = self._get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "\n",
    "        #store it in this variable.\n",
    "        self.conservation_df = conserv_df\n",
    "\n",
    "        #and lets also save it.\n",
    "        conserv_df.to_csv(f\"{out_dir}/conservation_df.csv\")\n",
    "    \n",
    "\n",
    "    def _mmseq_multi_fasta(self, uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "        \"\"\"\n",
    "        uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "        outdir: location where result files will be stored.\n",
    "        sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "        filter_msa = 0 default. if 1 hits are stricter.\n",
    "        query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "        \"\"\"\n",
    "\n",
    "        #we blast with this fasta as query.\n",
    "        trgt_fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "        #Make outdir for all required files.\n",
    "        #we need to write it out to file.\n",
    "        with open(f\"{self.work_dir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "            fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "            fasta_out.write(trgt_fasta_seq)\n",
    "        \n",
    "        #fetch pre downloaded database from a parent folder.\n",
    "        msa_file = None\n",
    "        new_location = None\n",
    "        try:\n",
    "            DB_storage_location = f\"{work_dir}\"\n",
    "            #shutil.copy(previous_path, savepath)\n",
    "            bash_curl_cmd = f\"mmseqs createdb {self.work_dir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\" \n",
    "            bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "            \n",
    "            #run first cmd which setups query database based on our input fasta file\n",
    "            result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "\n",
    "            bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"    \n",
    "            bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "            \n",
    "            #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "            result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "    \n",
    "            #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "            bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "            bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "            result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "    \n",
    "            #delete last line.. required.\n",
    "            sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'        \n",
    "            bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "    \n",
    "            #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "                result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "    \n",
    "            # Specify the path to your MSA file\n",
    "            msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "    \n",
    "    \n",
    "            #transfer the msa file to another location and delete useless files.\n",
    "            # we need to delete : all uniprot* files. \n",
    "            # all query*. All result* \n",
    "    \n",
    "            new_location = f\"{self.work_dir}/{uniprot_id}.fasta\"\n",
    "            shutil.copy(msa_file, new_location)\n",
    "            #remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "        #we want the path to msa_file for downstream analysis.\n",
    "        return new_location\n",
    "\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "\n",
    "        #print(\"we are in get gene fasta\")\n",
    "        \"this is already overworked. should work.\"\n",
    "        #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "        fields = \"sequence\"\n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        seq = \"\"\n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "                #print(lines)\n",
    "            i += 1\n",
    "        #print(seq)\n",
    "        return seq\n",
    "\n",
    "    def _get_conservation(self, path_to_msa:str):    \n",
    "        \n",
    "        canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "        result_cons = canal.analysis(method=\"all\")\n",
    "        #this is a pandas df that contains 3 cols : 3 diff conservation score computations.\n",
    "        return result_cons\n",
    "\n",
    "    def _get_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit() \n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ace5f9c-d10f-4beb-8044-cf617c1e9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBCleaning:\n",
    "    '''Class object containing the cleaning functions that will work on all PDBs that we fetched.'''\n",
    "    \n",
    "    def __init__(self, work_dir, chain_seq_dict=None, meta_dict=None):\n",
    "        self.work_dir = work_dir\n",
    "        self.chain_seq_dict = chain_seq_dict\n",
    "        self.filtered_chain_dict = None\n",
    "        #self.filtered_seq_ids = None\n",
    "        self.meta_dict = meta_dict\n",
    "        self.shifts = None\n",
    "        self.filtered_structures = None\n",
    "        self.renumbered = False\n",
    "        self.unfiltered = False\n",
    "\n",
    "    def get_unfiltered_strucs(self):\n",
    "\n",
    "        # this does not correvtly retrieve 5ltu because of upstream problems. check later. Seems to be an issue with the files that have more than 1 chain. \n",
    "        pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\") and len(f) != 8] #exclude original raw files. Only store pdb + _ + chains.\n",
    "        self.unfiltered = pdbs_to_retrieve\n",
    "        return self.unfiltered\n",
    "\n",
    "    \n",
    "    def setup_cutoff(self, cutoff, apply_filter=False):\n",
    "        '''If we want to setup a resolution cutoff filter for further downstream analysis, \n",
    "        this function helps with it.'''\n",
    "        \n",
    "        # If there is no meta dict we cant proceed and filter based on resolution.\n",
    "        if self.meta_dict:\n",
    "\n",
    "            #here we store the pdb codes that we keep\n",
    "            pdbs_to_keep = []\n",
    "            #Now lets parse through the whole meta dict and fetch the cutoffs for structures.\n",
    "            for _, single_pdbs in self.meta_dict.items():\n",
    "                if single_pdbs['resolution'] <= cutoff:\n",
    "                    pdbs_to_keep.append(single_pdbs['code'].lower()) #normalize to lower in order to have uniform list members.    \n",
    "\n",
    "            \n",
    "            self.filtered_structures = pdbs_to_keep\n",
    "\n",
    "            #now if we directly want to apply the filter to remove files that dont match our criteria.\n",
    "            if apply_filter:\n",
    "                #check for union between files and kept structures.\n",
    "                pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "\n",
    "                #lets fetch the intersect between the 2 sets which corresponds to the pdbs we want to keep.\n",
    "                common_pdb = set(pdbs_to_retrieve) & set(pdbs_to_keep) #intersection\n",
    "                intersect_lst = list(common_pdb)\n",
    "                self.filtered_structures = intersect_lst\n",
    "                \n",
    "                if self.chain_seq_dict:\n",
    "                    #now we need to update the chain_dict as well:\n",
    "                    filtered_dict = {pdb: v for pdb, v in self.chain_seq_dict.items() if pdb[:4] in self.filtered_structures}\n",
    "                    \n",
    "                    self.chain_seq_dict = filtered_dict\n",
    "                    \n",
    "        \n",
    "        else:\n",
    "            print(\"We have no meta dict to implement a cutoff\")\n",
    "\n",
    "            #In this case we take all.\n",
    "            pdbs_to_retrieve = [f[:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\") and len(f) != 8] #exclude original raw files. Only store pdb + _ + chains.\n",
    "            self.filtered_structures = pdbs_to_retrieve\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "    def parallel_shift_calculation(self):\n",
    "        '''Here we compute the shift according to uniprot or authors in order to be in line with UNIPROT numbering which is crucial for later renumbering.'''\n",
    "        \n",
    "        pdbs_to_retrieve = [f[0:4] for f in os.listdir(self.work_dir) if f.endswith(\".pdb\")]\n",
    "\n",
    "        #print(pdbs_to_retrieve)\n",
    "        #print(set(x[:4] for x in self.chain_dict.keys()))\n",
    "        \n",
    "        pdbs_to_retrieve = set(pdbs_to_retrieve) & set(x[:4] for x in self.chain_seq_dict.keys()) #here we check the first 4 which is pdb code\n",
    "\n",
    "        #print(pdbs_to_retrieve)\n",
    "        \n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        shift_dict = defaultdict()\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Using partial to create a function with fixed parameters (link_path)\n",
    "            calculate_shift_bound = partial(self._calculate_shift)\n",
    "            \n",
    "            tasks = ((link_path, pdb) for pdb in pdbs_to_retrieve)\n",
    "\n",
    "            # Map the bound function to the arguments in parallel\n",
    "            results = executor.map(calculate_shift_bound, tasks)\n",
    "\n",
    "            # Combine the results\n",
    "            for result in results:\n",
    "                for keys, vals in result.items():\n",
    "                    shift_dict[keys] = vals\n",
    "\n",
    "        self.shifts = shift_dict\n",
    "\n",
    "    \n",
    "    def _calculate_shift(self, args):\n",
    "\n",
    "        link_path, pdb = args\n",
    "        shift_dict = defaultdict()\n",
    "        \n",
    "        searchp = f\"{link_path}/{pdb[0:4]}\"\n",
    "        resp = self._get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        \n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                for mapping in uniprot_info['mappings']:\n",
    "                    chain_id = mapping['chain_id']\n",
    "                    unp_start = mapping['unp_start']\n",
    "                    unp_end = mapping['unp_end']\n",
    "                    \n",
    "                    author_start = mapping['start']['author_residue_number']\n",
    "                    author_end = mapping['end']['author_residue_number']\n",
    "    \n",
    "                    if author_start is None:\n",
    "                        author_start = unp_start\n",
    "                    if author_end is None:\n",
    "                        author_end = unp_end\n",
    "    \n",
    "                    shift_start = unp_start - author_start\n",
    "                    shift_end = unp_end - author_end\n",
    "    \n",
    "                    shift_dict[f\"{pdb_id}_{chain_id}\"] = shift_start \n",
    "\n",
    "        return shift_dict\n",
    "\n",
    "    \n",
    "    def parallel_renumbering(self):\n",
    "\n",
    "        if self.shifts:\n",
    "            \n",
    "            #needs a list to apply it to \n",
    "            relevant_files = self.chain_seq_dict.keys()\n",
    "            \n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                # Using partial to create a function with fixed parameters (shift_dict, path)\n",
    "                renumber_structure_partial = partial(self._renumber_structure, shift_dict=self.shifts, path=self.work_dir)\n",
    "                # Map the renumbering function to each relevant file in parallel\n",
    "                executor.map(renumber_structure_partial, relevant_files)\n",
    "\n",
    "            self.renumbered = True\n",
    "        else:\n",
    "            print(\"You first need to obtain shifts which will be used as reference in order to start renumbering.\\nCall first .parallel_shift_calculation()\")\n",
    "    \n",
    "    def _renumber_structure(self, files, shift_dict, path):\n",
    "        '''Function that is going to apply pdb_shiftres_by_chain.py to each pdb file that is shifted.\n",
    "        Will apply renumbering to ALL structures if you did not set a cutoff previously and applied filter. \n",
    "        If filter applied for resolution will only renumber those structures that are left after filtering.'''\n",
    "        \n",
    "        for keys, vals in shift_dict.items():\n",
    "            #dont renumber if there is not shift\n",
    "            if files == keys[0:4] and vals != str(0):\n",
    "\n",
    "                chain = keys[-1]\n",
    "                shift = int(vals)\n",
    "                filepath = f\"{self.work_dir}/{files}.pdb\"\n",
    "                # Should we really shift by shift + 1??? or just shift?\n",
    "                bash_cmd = f\"python {self.work_dir}/pdb_shiftres_by_chain.py {filepath} {shift} {chain}\"\n",
    "                bash_cmd_rdy = bash_cmd.split()\n",
    "            \n",
    "                with open(f\"{filepath}_tmp\", \"w\") as fh_tmp:\n",
    "                    result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, universal_newlines=True)\n",
    "                    # Now replace the original one with the temp file.\n",
    "                    os.replace(f\"{filepath}_tmp\", filepath)\n",
    "    \n",
    "    \n",
    "    def _get_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit()\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf74f3d-61e6-42f7-a1d5-5f5f20eaa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBBuilder:\n",
    "    \n",
    "    def __init__(self, work_dir, structures, remove_intermediates=False):\n",
    "        self.work_dir = work_dir\n",
    "        #Here we store the structures that need to be built.\n",
    "        self.structures = structures\n",
    "        #we store intermediate files per default.\n",
    "        self.remove_intermediates = remove_intermediates\n",
    "        #we set oligo to None but store later the oligodict\n",
    "        self.oligodict = None\n",
    "        \n",
    "    def build_assembly(self):\n",
    "\n",
    "        # These files need to be opened, rechained and assemblies built.\n",
    "        full_pdb_paths = [os.path.join(self.work_dir, f\"{file}.pdb\") for file in self.structures]\n",
    "\n",
    "        oligostates = defaultdict(str)\n",
    "    \n",
    "        #this letterdict is used for rechaining.\n",
    "        letterdict = {i: chr(65 + i) for i in range(26)}\n",
    "        \n",
    "        #changed this here from threadpool to process pool\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Define your processing function, partially applied with gene_name and main_protein_seq\n",
    "            process_func = partial(self._process_pdb, letterdict=letterdict)\n",
    "    \n",
    "            results = executor.map(process_func, full_pdb_paths)\n",
    "    \n",
    "            for result in results:\n",
    "                #print(f\"this is result: {result}\")\n",
    "                oligostates.update(result)\n",
    "\n",
    "        #beautifully chaotic.... basically we update... 4 digit ID + chains i.e 4 means ABCD 2 means AB 1 means A, and concatenate it with .pdb to get 4rt4_ABCD.pdb\n",
    "        new_oligostates = {f\"{k[0:4]}_{''.join(letterdict[i] for i in range(v))}.pdb\": v for k, v in oligostates.items()}\n",
    "        self.oligodict = new_oligostates\n",
    "\n",
    "    def _process_pdb(self, path:str,letterdict:dict)->dict:\n",
    "        #helper function to split between nmr and xray / cryoem\n",
    "        try:\n",
    "        \n",
    "            pdb_file_name = os.path.basename(path)\n",
    "        \n",
    "            pdb_file = atomium.open(path)\n",
    "            # We filter according to model length.. if there are more than 5 models deposited its NMR\n",
    "            model_len = pdb_file.models\n",
    "            \n",
    "            if len(model_len) > 5:\n",
    "\n",
    "                print(\"we go into _NMR_ensemble\")\n",
    "                #this gives back a dictionary with all nmr structures and their oligomeric state (mostly monomer.)\n",
    "                return {pdb_file_name: self._NMR_ensemble(path=path, letterdict=letterdict)}\n",
    "            else:\n",
    "                return {pdb_file_name: self._non_NMR_structures(path=path, letterdict=letterdict)}\n",
    "\n",
    "            #now we remove the original file.\n",
    "            os.remove(path)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(\"process pdb did not work\")\n",
    "            print(error)\n",
    "        \n",
    "            return {}\n",
    "    \n",
    "    #helper function for XRAY and CRYO-EM ensembles.\n",
    "    def _non_NMR_structures(self, path:str, letterdict:dict):\n",
    "    \n",
    "        \"\"\"This function takes in the the pdb file that is xray or cryoem and rechains each chain. \n",
    "        Additionally, we merge the new labelled chains into a merged_pdb file for further use.\"\"\"\n",
    "    \n",
    "        #store base dir.\n",
    "        base_dir = os.path.dirname(path)\n",
    "        pdb_name = os.path.basename(path)[:4]\n",
    "        #/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95.pdb\n",
    "        pdb_file = atomium.open(path)\n",
    "        assemblies = [pdb_file.generate_assembly(n + 1) for n in range(len(pdb_file.assemblies))]\n",
    "        #we take the first one(this is the biological unit built from the asymmetric unit)\n",
    "        assembly = assemblies[0]\n",
    "        #tuple containing chain ID and LEN of each chain.\n",
    "        seq_chains = [(chain.id, len(chain.sequence)) for chain in assembly.chains()]\n",
    "        sorted_lens = sorted(seq_chains, key= lambda x: x[1], reverse=True) #reverse = true :largest first.\n",
    "        accepted_chains = []  #this will be used to store and evalute oligomeric state.\n",
    "        min_accepted_length = float('inf')   # Minimum length of accepted chains init as pos inf.\n",
    "\n",
    "        for chain, length in seq_chains:\n",
    "            #for each chain and length.\n",
    "            if not accepted_chains or length > 0.8 * min_accepted_length:\n",
    "                #we accept it if either we have no other chain so far OR length > 80% of the chain we have so far.\n",
    "                accepted_chains.append(chain)\n",
    "                #first chain will be accepted and then will be the standard for the next chains to follow.\n",
    "                min_accepted_length = min(min_accepted_length, length)\n",
    "\n",
    "        oligostate = len(accepted_chains)  #this excludes small peptides ect from being mistaken as oligomers.\n",
    "        \"\"\"Part 2. We investigate oligomeric state.\"\"\"\n",
    "        accepted_chains_set = set(accepted_chains)\n",
    "        oligomeric_status = None\n",
    "    \n",
    "        if len(accepted_chains_set) != len(accepted_chains):\n",
    "            #this means we have a homo-oligomer!\n",
    "            #e.g A vs A A A .. len(1) != len(0)\n",
    "            #hetero-mers are not caught here.. A B C == A B C == len(3)\n",
    "            oligomeric_status = \"homo_oligomer\"\n",
    "        \n",
    "        elif len(accepted_chains) == 1:\n",
    "            #this means we deal with a monomer.\n",
    "            oligomeric_status = \"monomer\"\n",
    "        \n",
    "        elif len(accepted_chains) > 1 and len(accepted_chains) == len(accepted_chains_set):\n",
    "            #this means its a mixed heteromer. becaue len(1) > AND set == list aka no redundancy ergo heteromer.\n",
    "            oligomeric_status = \"hetero_oligomer\"\n",
    "\n",
    "        \"\"\"Part 3: We follow through and now save individual chains + send them to proper rechaining. \"\"\"    \n",
    "        path_list = []\n",
    "\n",
    "        for idx, chain in enumerate(assembly.chains()):\n",
    "            chain_label = chain.id\n",
    "            if chain_label in accepted_chains_set:\n",
    "                path_to_pdb = f\"{self.work_dir}/{pdb_name}_{idx}.pdb\"\n",
    "                #save it here.\n",
    "                path_list.append(path_to_pdb)\n",
    "                #and also save the structure in its wrong chain state first.\n",
    "                chain.save(path_to_pdb)\n",
    "        \n",
    "            path_list = sorted(path_list, key=lambda x: int(x[-5]))\n",
    "    \n",
    "        \"\"\"Part 4: We now deal with all kind of oligomers, and also save all single chains in the procedure.\n",
    "        Normal monomers are also simply saved and rechained. Everything according to a general schema for efficient\n",
    "        downstream processing.\"\"\"\n",
    "    \n",
    "        self._merge_pdb_chains(path_list, pdb_name=pdb_name, oligomeric_status=oligomeric_status, \n",
    "                      letterdict=letterdict, accepted_chains = accepted_chains, accepted_chains_set=accepted_chains_set)\n",
    "        \n",
    "        #we return the oligostate of this file and merge it into dict as return value.\n",
    "        return oligostate\n",
    "\n",
    "    #helper function for NMR ensembles.\n",
    "    def _NMR_ensemble(self, path:str, letterdict:dict):\n",
    "\n",
    "        \"\"\"This function takes in the NMR ensemble and \n",
    "        splits each state into a respective PDB file.\"\"\"\n",
    "    \n",
    "        #open the pdb file\n",
    "        print(f\"we currently open with atomium: {path}\")\n",
    "        pdb_name = os.path.basename(path)[:4] #4 digit ID\n",
    "        base_dir = os.path.dirname(path) #base dir name\n",
    "        pdb_file = atomium.open(path)\n",
    "        oligostate = 1 #default initialize\n",
    "        path_list = []\n",
    "    \n",
    "        for i, model in enumerate(pdb1.models):\n",
    "            #now iterate through each model and its respective chains.\n",
    "            chain_len = len([x.id for x in model.chains()]) > 1 # True if multiple chains.\n",
    "            #if larger than 1 : we need to merge.\n",
    "            chain_paths = []\n",
    "            new_chains = []\n",
    "        \n",
    "            for j, chain in enumerate(model.chains()): #enumerate because there are NMR models with MULTIPLE CHAINS\n",
    "                #here we save the structure. as number.. we need to check the chain id.\n",
    "                new_chain = chain.copy(id=letterdict[j]) #new chain ID.\n",
    "                #this effectively rechained the chain.\n",
    "                save_location = f\"{base_dir}/{pdb_name}_{i}_{letterdict[j]}.pdb\"\n",
    "                new_chain.save(save_location) # e.g 4ND5_0_A.pdb 4ND5_1_A etc..\n",
    "                  \n",
    "                if chain_len:\n",
    "                    chain_paths.append(save_location)\n",
    "                    new_chains.append(new_chain.id)\n",
    "        \n",
    "            #if done: check if there are multiple chains. if yes. merge.\n",
    "            if chain_len:\n",
    "                oligostate = len(new_chains)\n",
    "                #we merge the chains.\n",
    "                save_nmr_oligomer = f\"{base_dir}/{pdb_name}_{i}_{''.join(new_chains)}.pdb\"\n",
    "                merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(chain_paths)}\"\n",
    "                merge_command_rdy = merge_command.split()\n",
    "                merge_output_file = f\"{save_nmr_oligomer}_tmp.pdb\"  #tmp\n",
    "    \n",
    "                with open(merge_output_file, \"w\") as fh_out:\n",
    "                    result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "                # Run tidy on the merged PDB\n",
    "                tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "                tidy_command_rdy = tidy_command.split()\n",
    "                tidy_output_file = f\"{save_nmr_oligomer}.pdb\"\n",
    "    \n",
    "                with open(tidy_output_file, \"w\") as fh_out2:\n",
    "                    results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "                #we remove tmp intermediate files.\n",
    "                if self.remove_intermediates:\n",
    "                    print(\"we remove\", merge_output_file)\n",
    "                    os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "\n",
    "        return oligostate\n",
    "    \n",
    "    def _merge_pdb_chains(self, path_list:list, pdb_name:str, oligomeric_status:str, letterdict:dict,\n",
    "                     accepted_chains:list, accepted_chains_set:set):\n",
    "\n",
    "        if oligomeric_status == \"homo_oligomer\":\n",
    "            self._pure_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "            \n",
    "        elif oligomeric_status == \"hetero_oligomer\":\n",
    "            self._mixed_oligomer_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name,\n",
    "                                  accepted_chains=accepted_chains, accepted_chains_set=accepted_chains_set)\n",
    "            \n",
    "        elif oligomeric_status == \"monomer\":\n",
    "            self._monomeric_rechaining(path_list=path_list, letterdict=letterdict, pdb_name=pdb_name)\n",
    "            \n",
    "        else:\n",
    "            print(f\"There was an issue with: {oligomeric_status=}\")\n",
    "            \n",
    "        return\n",
    "\n",
    "    def _pure_oligomer_rechaining(self, path_list:list, letterdict:dict, pdb_name:str): \n",
    "        #path_list = ['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_0.pdb',\n",
    "        # '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/3h95_1.pdb']\n",
    "        directory = os.path.dirname(path_list[0]) #this does not change. so no reason to constantly evaluate it in the loop\n",
    "        #store path to chains here.\n",
    "        lst_to_merge_paths = []\n",
    "        #store lists here for later merge.\n",
    "        chain_lst = []\n",
    "\n",
    "        for path_to_pdb in path_list:\n",
    "            \n",
    "            filename = os.path.basename(path_to_pdb) #same as above.\n",
    "            new_chain_digit = filename[5]  # Get the single chain id (e.g., '0')\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            individual_structure = parser.get_structure(\"default\", path_to_pdb)\n",
    "            new_chain = letterdict[int(new_chain_digit)]\n",
    "\n",
    "            for models in individual_structure:\n",
    "                for chain in models:\n",
    "                    if chain.id == new_chain:\n",
    "                        #then we simply save it under its original chain.\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        chain_lst.append(chain.id)\n",
    "                        io.save(save_location)\n",
    "                    else:\n",
    "                        chain.id = new_chain\n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        save_location = os.path.join(directory, f\"{pdb_name}_{chain.id}.pdb\")\n",
    "                        chain_lst.append(chain.id)\n",
    "                        lst_to_merge_paths.append(save_location)\n",
    "                        io.save(save_location)\n",
    "            \n",
    "            if self.remove_intermediates:\n",
    "                os.remove(path_to_pdb) #we no longer need the original file\n",
    "\n",
    "        #prepare subprocess for pdb_merge.py\n",
    "        merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "        merge_command_rdy = merge_command.split()\n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}_tmp.pdb\"  #tmp\n",
    "    \n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        # prepare subprocess for pdb_tidy.py\n",
    "        tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(chain_lst)}.pdb\"\n",
    "    \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "\n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.\n",
    "        #lets remove artifacts if the user wishes\n",
    "        if self.remove_intermediates:\n",
    "            for artifacts in lst_to_merge_paths:\n",
    "                os.remove(artifacts)\n",
    "\n",
    "    def _mixed_oligomer_rechaining(self, accepted_chains:list,\n",
    "                               accepted_chains_set:set,\n",
    "                               path_list:list,\n",
    "                               letterdict:dict, pdb_name:str):\n",
    "\n",
    "\n",
    "        seen_chains = sorted(accepted_chains, reverse=False)\n",
    "        chain_seq_len = len(seen_chains) #e.g 6\n",
    "        shift = len(accepted_chains_set) # e.g 3\n",
    "        blocksize = chain_seq_len // shift # e.g 2\n",
    "        block_count = int(chain_seq_len/blocksize) \n",
    "        '''\n",
    "        # A A B B C C becomes A D B E C F\n",
    "        # B B C C becomes A C B D \n",
    "        # i = 1\n",
    "        # A D \n",
    "        # block 1 2 3 for A A B B C C \n",
    "        # 0 2 1 3\n",
    "        '''\n",
    "        j = 0\n",
    "        new_chain_seq = []\n",
    "        lst_to_merge_paths = []\n",
    "    \n",
    "        for blocks in range(0, block_count):\n",
    "            for i in range(0, blocksize):\n",
    "                '''\n",
    "                # first iteration A D\n",
    "                # second iteration B E\n",
    "                # third iteration C F\n",
    "                '''\n",
    "                new_chain = letterdict[blocks+i*shift]\n",
    "                new_chain_seq.append(new_chain)\n",
    "                path_to_pdb = path_list[j]\n",
    "                directory = os.path.dirname(path_to_pdb)\n",
    "                j += 1\n",
    "                parser = PDBParser(QUIET=True)\n",
    "                prot_name = f\"default\"\n",
    "                #open the correct pdb and rechain it.\n",
    "                structure_template = parser.get_structure(prot_name, path_to_pdb)\n",
    "                \n",
    "                for models in structure_template:\n",
    "                    for chain in models:\n",
    "                        if chain.id != new_chain:\n",
    "                            chain.id = new_chain\n",
    "                \n",
    "                        io = PDBIO()\n",
    "                        io.set_structure(chain)\n",
    "                        #print(f\"This is single chain save inside oligomer rechain: {directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        io.save(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "                        lst_to_merge_paths.append(f\"{directory}/{pdb_name}_{chain.id}.pdb\")\n",
    "\n",
    "                #remove intermediate file\n",
    "                os.remove(path_to_pdb)\n",
    "\n",
    "        # Prepare subprocess for pdb_merge.py\n",
    "        merge_command = f\"python {self.work_dir}/pdb_merge.py {' '.join(lst_to_merge_paths)}\"\n",
    "        merge_command_rdy = merge_command.split()\n",
    "        merge_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}_tmp.pdb\"  #tmp\n",
    "        \n",
    "        with open(merge_output_file, \"w\") as fh_out:\n",
    "            result_pdbs = run(merge_command_rdy, stdout=fh_out, stderr=PIPE, universal_newlines=True)\n",
    "            \n",
    "        # Prepare subprocess for pdb_tidy.py\n",
    "        tidy_command = f\"python {self.work_dir}/pdb_tidy.py {merge_output_file}\"\n",
    "        tidy_command_rdy = tidy_command.split()\n",
    "        tidy_output_file = f\"{self.work_dir}/{pdb_name}_{''.join(new_chain_seq)}.pdb\"\n",
    "        \n",
    "        with open(tidy_output_file, \"w\") as fh_out2:\n",
    "            results_tidy = run(tidy_command_rdy, stdout=fh_out2, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "        os.remove(merge_output_file) #this is the tmp file that is not tidy.  \n",
    "        #lets remove artifacts if the user wishes\n",
    "        if self.remove_intermediates:\n",
    "            for artifacts in lst_to_merge_paths:\n",
    "                os.remove(artifacts)\n",
    "    \n",
    "    def _monomeric_rechaining(self, path_list:list,\n",
    "                          letterdict:dict,\n",
    "                          pdb_name:str):\n",
    "\n",
    "        #path_list=['/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUDT4B/2q9p_0.pdb'], \n",
    "        #letterdict={0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K',\n",
    "        #11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21:\n",
    "        #'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}, \n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = \"default\"\n",
    "        #We only have 1 path in this list.\n",
    "        pdb = path_list[0]\n",
    "        # Open the correct PDB and rechain it.\n",
    "        dir_name = os.path.dirname(pdb)\n",
    "        structure_template = parser.get_structure(prot_name, pdb) \n",
    "        # Get the new chain ID\n",
    "        new_chain = \"A\"  #always... A\n",
    "        for model in structure_template:\n",
    "            for original_chain in model:\n",
    "                if original_chain.id != new_chain:\n",
    "                    original_chain.id = new_chain\n",
    "        save_path = os.path.join(dir_name, f\"{pdb_name}_{new_chain}.pdb\")\n",
    "        #print(f\"This is save path: {save_path=}\")\n",
    "        # Save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure_template)\n",
    "    \n",
    "        #print(\"we save now:\")\n",
    "        io.save(save_path)\n",
    "        if self.remove_intermediates:\n",
    "            #print(\"we remove\", pdb)\n",
    "            os.remove(pdb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ab30756-8286-4eff-b69f-68411c6b169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDBEnsemblePrep:\n",
    "    \"\"\"This class deals with splitting the PDB into domains and \n",
    "    grouping associated domains together for automated throughput analysis downstream.\"\"\"\n",
    "    \n",
    "    def __init__(self, work_dir, oligo_dict, chain_seq_dict=None, main_prot_seq=None, save=True):\n",
    "        self.work_dir = work_dir\n",
    "        self.oligo_dict = oligo_dict\n",
    "        self.range_dict = None\n",
    "        self.established_domain_dict = None\n",
    "        self.oligostates_filtered_paths = None\n",
    "        self.most_common_filtered_oligostates = None\n",
    "        self.chain_seq_dict = chain_seq_dict\n",
    "        self.main_prot_seq = main_prot_seq\n",
    "        self.templates_for_oligos = None\n",
    "        self.save = save\n",
    "        self.top_templates = None\n",
    "\n",
    "    \n",
    "    #master function to be called which calls internally all _functions inside.\n",
    "    def create_domain_boundaries(self, save=True):\n",
    "        \"\"\"\n",
    "        Process PDB data by getting real ranges and then splitting domains.\n",
    "        This is the single entry point for users to execute the workflow.\n",
    "        \"\"\"\n",
    "        # Step 1: Get Real Ranges\n",
    "        self._get_real_ranges()\n",
    "\n",
    "        # Step 2: Split Domains\n",
    "        # Ensure that range_dict is not None or handle the case if it is\n",
    "        if self.range_dict is not None:\n",
    "            domain_dict = self._split_domains_pdb()\n",
    "            self.established_domain_dict = domain_dict\n",
    "            self._save_human_readable(save)\n",
    "        else:\n",
    "            print(\"Range dictionary is empty. Cannot proceed with splitting domains.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_real_ranges(self):\n",
    "\n",
    "        real_range_dict = defaultdict()\n",
    "        pdbs = [os.path.join(self.work_dir, k) for k in self.oligo_dict]\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = list(executor.map(self._get_range_parallelized, pdbs))\n",
    "        # Process the results\n",
    "        for hit in results:\n",
    "            for pdb_path, chains, min_num, max_num in hit:\n",
    "                real_range_dict[pdb_path] = (min_num, max_num)\n",
    "\n",
    "        self.range_dict = real_range_dict\n",
    "\n",
    "    \n",
    "    def _get_range_parallelized(self, pdb_path:str):\n",
    "        \"\"\"This function gets called by get_real_ranges and leverages parallelization to speed up domain boundary computations\"\"\"\n",
    "        hits = []\n",
    "        try:\n",
    "            parser = PDBParser()\n",
    "            structure = parser.get_structure(\"none\", pdb_path)\n",
    "            chains_residues = []\n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    chain_id = chain.id\n",
    "                    # Filter the residues within the chain\n",
    "                    residues_in_chain = [res for res in chain if res.get_id()[0] == \" \"]\n",
    "                    result = [res.get_id()[1] for res in residues_in_chain]\n",
    "                    chains_residues.append((chain_id, result))\n",
    "\n",
    "            #now we have all chains and their ids\n",
    "            if len(chains_residues) == 1: #means its monomer \n",
    "                for chains, resids in chains_residues:\n",
    "                    min_num, max_num = min(resids), max(resids)\n",
    "                    #print(pdb_path, min_num, max_num)\n",
    "                    hits.append((pdb_path, chains, min_num, max_num))\n",
    "                \n",
    "            elif len(chains_residues) > 1: # oligomer.\n",
    "                #real chains is in the name of the pdb.\n",
    "                #'/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/\n",
    "                #NUDT4B/merged_cleaned_files/3i7u_ABCD.pdb'\n",
    "                chain_name = os.path.basename(pdb_path)[5:-4]  #this should be ABCD in the case above\n",
    "                min_max_list = []\n",
    "                for chains, resids in chains_residues:\n",
    "                    chains += chains\n",
    "                    min_num, max_num = min(resids), max(resids)\n",
    "                    min_max_list.append((min_num, max_num))\n",
    "\n",
    "                lowest_start = min([x[0] for x in min_max_list])\n",
    "                highest_stop = max([x[1] for x in min_max_list])\n",
    "            \n",
    "                hits.append((pdb_path, chain_name, lowest_start, highest_stop))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        return hits\n",
    "\n",
    "    \n",
    "    def _split_domains_pdb(self):\n",
    "\n",
    "        #pdb ranges is a dict containing of key: path  val:(start, stop) \n",
    "        pdb_ranges = self.range_dict \n",
    "        # lets first do a quick merge\n",
    "        #print(f\"{pdb_ranges=}\")\n",
    "        merged_dict = self._merge_paths_within_interval(pdb_ranges)\n",
    "        \n",
    "        #print(f\"This is merged_dict_length: {len(merged_dict.values())}\")\n",
    "    \n",
    "        #now we start iterative merging of overlaps.\n",
    "        domain_dict = self._make_groups_iter_pdb(merged_dict, num_of_iterations=0 , break_point=10)\n",
    "    \n",
    "        #print(f\"This is domain_dict_length after make_groups_iter: {len(domain_dict.values())}\")\n",
    "        cleaned_dict = defaultdict()\n",
    "    \n",
    "        for ranges, twisted_path_lists in domain_dict.items():\n",
    "            #now we need to flatten domain_dict\n",
    "        \n",
    "            flattened_data = self._flatten_nested_lists(twisted_path_lists)\n",
    "\n",
    "            #print(flattened_data)\n",
    "            # Use a set to remove duplicates\n",
    "            unique_flattened_data = set(flattened_data)\n",
    "        \n",
    "            unique_flattened_list = list(unique_flattened_data)\n",
    "            #this works.\n",
    "            cleaned_dict[ranges] = unique_flattened_list\n",
    "\n",
    "\n",
    "        #print(f\"This is cleaned_dict_length: {len(cleaned_dict.values())}\")\n",
    "        #now lets merge in the last step overlap\n",
    "        prot_length = 10 #len(main_prot_seq) test purpose\n",
    "\n",
    "\n",
    "        #lets try this first.\n",
    "        if self.main_prot_seq:\n",
    "            if len(self.main_prot_seq) > 300:\n",
    "                tolerance = 0.3 * prot_length\n",
    "            else:\n",
    "                tolerance = 80\n",
    "        else:\n",
    "            tolerance = 80\n",
    "\n",
    "        #print(f\"This is cleaned_dict_length before merge_overlapping_int: {len(cleaned_dict.values())}\")\n",
    "        domain_dict = self._merge_overlapping_intervals(cleaned_dict, tolerance=tolerance)\n",
    "    \n",
    "        #print(f\"This is cleaned_dict_length after merge_overlapping_int: {len(domain_dict.values())}\")\n",
    "    \n",
    "        return domain_dict\n",
    "\n",
    "\n",
    "\n",
    "    def _make_groups_iter_pdb(self, merged_dict:dict, num_of_iterations, break_point):\n",
    "\n",
    "        #print(f\"This is iteration : {num_of_iterations}\")\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            if num_of_iterations > break_point:\n",
    "                 #that means we cant progress.\n",
    "                 break\n",
    "    \n",
    "            #print(f\"iteration number: {num_of_iterations}\")\n",
    "            if len(merged_dict) == 1:\n",
    "                return merged_dict\n",
    "    \n",
    "            num_of_iterations += 1\n",
    "            ranges_1 = []\n",
    "            ranges_2 = []\n",
    "    \n",
    "            #attach key ranges to both lists\n",
    "            for keys, vals in merged_dict.items():\n",
    "                ranges_1.append(keys)\n",
    "                ranges_2.append(keys)\n",
    "            \n",
    "            union_dict = defaultdict(list)        \n",
    "            \n",
    "            for range_to_check in ranges_1:  #lets parse through the ranges.\n",
    "                union_new = False  #default false for start.\n",
    "                for range_to_compare in ranges_2: #we compare our hit against all potential ranges.\n",
    "                    if range_to_check != range_to_compare:  #means they are different.\n",
    "                        \n",
    "                        range_to_check_paths = merged_dict[range_to_check]\n",
    "                        range_to_comp_paths = merged_dict[range_to_compare]\n",
    "                        \n",
    "                        range_abs_check = np.abs(int(range_to_check[1])-int(range_to_check[0]))\n",
    "                        range_abs_compare = np.abs(int(range_to_compare[1])-int(range_to_compare[0]))\n",
    "    \n",
    "                        intersect_between_both = self._get_intersect(range_to_check, range_to_compare)# gets abs length of intersect.\n",
    "    \n",
    "                        #range abs check and range abs comp are integers. \n",
    "                        #intersect is int and corresponds to length between intersection of both.\n",
    "    \n",
    "                        \n",
    "                        #cond 1: if the intersection between the 2 ranges is LESS than the absolute length of range to check and \n",
    "                        #the intersection is also GREATER than 80% of the first interval.\n",
    "                        \n",
    "                        condition_1 = intersect_between_both <= range_abs_check and intersect_between_both >= 0.8* range_abs_check\n",
    "                        \n",
    "                        #cond 2: if the intersection between the 2 ranges is LESS than the absolute length of range to compare and \n",
    "                        #the intersection is also GREATER than 80% of the 2nd interval.\n",
    "                        condition_2 = intersect_between_both <= range_abs_compare and intersect_between_both >= 0.8* range_abs_compare\n",
    "    \n",
    "                        #this means the intersect is less than the original range and the intersect is also larger than 80% of the orignal range.\n",
    "                        #as well as the same for the second range to compare.\n",
    "                        if condition_1 and condition_2:\n",
    "                            union_new = self._merge_into_union(range_to_check, range_to_compare)\n",
    "    \n",
    "                if union_new:\n",
    "                    if union_new in union_dict:\n",
    "                        \n",
    "                        union_dict[union_new].append(range_to_check_paths + range_to_comp_paths)\n",
    "                    else:\n",
    "                        union_dict[union_new] = range_to_check_paths + range_to_comp_paths\n",
    "    \n",
    "                else:\n",
    "                    union_dict[range_to_check].extend(range_to_check_paths)\n",
    "                \n",
    "            merged_dict = union_dict\n",
    "    \n",
    "            merged_dict = self._make_groups_iter_pdb(merged_dict, num_of_iterations, break_point)\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "    def _get_intersect(self, range_to_check, range_to_compare):\n",
    "\n",
    "        # range_to_check = (start, stop)\n",
    "        # range_to_compare = (start, stop)\n",
    "    \n",
    "        start_check = int(range_to_check[0])\n",
    "        stop_check = int(range_to_check[1])\n",
    "    \n",
    "        start_comp = int(range_to_compare[0])\n",
    "        stop_comp = int(range_to_compare[1])\n",
    "        \n",
    "        \n",
    "        full_range_to_check = set([x for x in range(start_check, stop_check+1)])\n",
    "        full_range_to_compare = set([x for x in range(start_comp, stop_comp+1)])\n",
    "    \n",
    "        #now lets grab set for both\n",
    "        \n",
    "        intersection = full_range_to_check.intersection(full_range_to_compare)\n",
    "    \n",
    "        #we are interested in the length of the intersect\n",
    "        return len(intersection)\n",
    "\n",
    "\n",
    "    \n",
    "    def _merge_dicts_range_seqid(self, range_dict, seqid_dict):\n",
    "        merged_dict = {}\n",
    "    \n",
    "        for range_key, pdb_entries in range_dict.items():\n",
    "            merged_list = []\n",
    "            for pdb_path, chain in pdb_entries:\n",
    "                pdb_id = os.path.basename(pdb_path).split('_')[0]  # Extracts '6woh' from '/path/6woh_A.pdb'\n",
    "                # Some PDB entries might have multiple chains, handle them separately\n",
    "                chains = list(chain)\n",
    "                for c in chains:\n",
    "                    # Check if the PDB ID and chain is in the score_dict\n",
    "                    if pdb_id in seqid_dict and c in dict(seqid_dict[pdb_id]):\n",
    "                        seq_id = dict(seqid_dict[pdb_id])[c]\n",
    "                        merged_list.append((pdb_path, seq_id))\n",
    "                    else:\n",
    "                        print(f\"Warning: No seqid found for {pdb_id} chain {c}\")\n",
    "            merged_dict[range_key] = merged_list\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _merge_into_union(self, range_to_check, range_to_compare):\n",
    "\n",
    "        start_check = int(range_to_check[0])\n",
    "        stop_check = int(range_to_check[1])\n",
    "    \n",
    "        start_comp = int(range_to_compare[0])\n",
    "        stop_comp = int(range_to_compare[1])\n",
    "    \n",
    "        full_range_to_check = set([x for x in range(start_check, stop_check+1)])\n",
    "        full_range_to_compare = set([x for x in range(start_comp, stop_comp+1)])\n",
    "    \n",
    "        union_merge = full_range_to_check.union(full_range_to_compare)\n",
    "        \n",
    "        sorted_union = sorted(union_merge)\n",
    "        result = (sorted_union[0], sorted_union[-1])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def _merge_overlapping_intervals(self, path_interval_dict, tolerance):\n",
    "\n",
    "        #input: key: ranges values = list of paths.\n",
    "        \n",
    "        intervals = list(path_interval_dict.keys())\n",
    "    \n",
    "        #[(5, 500), (1, 507), (3, 507), (1, 535), (8, 500), (4, 507)]\n",
    "        #print(f\"these are the intervals we deal with in merging: {intervals}\")\n",
    "        \n",
    "        merged_intervals = self._merge_intervals(intervals, tolerance)\n",
    "    \n",
    "        merged_dict = {}\n",
    "    \n",
    "        #print(f\"this is path_interval_dict : {path_interval_dict}\")\n",
    "        for merged_interval in merged_intervals:\n",
    "            merged_paths = []\n",
    "                \n",
    "            for interval, path_chain_list in path_interval_dict.items():\n",
    "                if self._is_within_tolerance(merged_interval, interval, tolerance):\n",
    "                    #print(f\"{path_chain_list=}\")\n",
    "                    for path, chain in path_chain_list: # list of lists consisting of tuples a (path / chain)\n",
    "                        merged_paths.append((path, chain))\n",
    "            merged_dict[merged_interval] = merged_paths\n",
    "    \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _merge_intervals(self, intervals, tolerance):\n",
    "\n",
    "        merged = [intervals[0]]  # Initialize with the first interval\n",
    "    \n",
    "        for start, end in intervals[1:]:\n",
    "            merged_interval = None\n",
    "    \n",
    "            for i, (merged_start, merged_end) in enumerate(merged):\n",
    "                if abs(start - merged_start) <= tolerance and abs(end - merged_end) <= tolerance:\n",
    "                    # Merge the interval into the existing one\n",
    "                    merged_interval = (min(start, merged_start), max(end, merged_end))\n",
    "                    merged[i] = merged_interval\n",
    "                    break\n",
    "    \n",
    "            if merged_interval is None:\n",
    "                # No suitable merged interval found, create a new one\n",
    "                merged.append((start, end))\n",
    "    \n",
    "        return merged\n",
    "\n",
    "\n",
    "    def _merge_paths_within_interval(self, path_start_stop_dict):\n",
    "        \n",
    "        merged_dict = {}  # Create a new dictionary to store merged paths\n",
    "        #print(f\"{path_start_stop_dict=}\")\n",
    "        for pdb_path, (start, stop) in path_start_stop_dict.items():\n",
    "            pdb_location, chain = pdb_path, os.path.basename(pdb_path)[5:-4]  #location + chain\n",
    "\n",
    "            #print(f\"{pdb_location=}, {chain=}\")\n",
    "            \n",
    "            interval = (start, stop)\n",
    "            \n",
    "            if interval in merged_dict:\n",
    "                merged_dict[interval].append((pdb_path, chain))\n",
    "            else:\n",
    "                merged_dict[interval] = [(pdb_path, chain)]\n",
    "            \n",
    "        return merged_dict\n",
    "\n",
    "\n",
    "    def _flatten_nested_lists(self, lst):\n",
    "        flattened = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                flattened.extend(self._flatten_nested_lists(item))  #recursive call\n",
    "            else:\n",
    "                flattened.append(item)\n",
    "        return flattened\n",
    "\n",
    "    def _is_within_tolerance(self, interval1, interval2, tolerance):\n",
    "    \n",
    "        # Calculate the differences in starts and stops for both intervals\n",
    "        start_diff = abs(interval1[0] - interval2[0])\n",
    "        stop_diff = abs(interval1[1] - interval2[1])\n",
    "\n",
    "        # Check if both differences are within the tolerance\n",
    "        return start_diff <= tolerance and stop_diff <= tolerance\n",
    "\n",
    "\n",
    "    def _save_human_readable(self, save=False):\n",
    "        #helper function to save the result in human readable format.\n",
    "        with open(os.path.join(self.work_dir, \"domain_boundaries.txt\"), \"w\") as db_out:\n",
    "            for keys, vals in Preper.established_domain_dict.items():\n",
    "                db_out.write(str(keys))\n",
    "                db_out.write(\":\")\n",
    "                db_out.write(\"\\n\")\n",
    "                for pdb in vals:\n",
    "                    pdb_name = os.path.basename(pdb[0])[:4]\n",
    "                    pdb_chain = pdb[1]\n",
    "                    db_out.write(pdb_name)\n",
    "                    db_out.write(\"_\")\n",
    "                    db_out.write(pdb_chain)\n",
    "                    db_out.write(\"\\n\")\n",
    "\n",
    "\n",
    "    \"\"\"DEBUG FROM HERE.\"\"\"\n",
    "\n",
    "    def get_oligostates(self, num_most_common_oligostates=2):\n",
    "        # First, we split into groups based on boundaries\n",
    "        domains_grouped_dict = self.established_domain_dict\n",
    "        # lets merge seq id and paths so we can handle them later\n",
    "        domains_merge_dict = self._merge_dicts_range_seqid(domains_grouped_dict, self.chain_seq_dict)\n",
    "\n",
    "        grouped_dict = self._split_and_group_by_chain(domains_merge_dict)\n",
    "\n",
    "        # Initialize a dictionary to store the Counters for each range\n",
    "        most_common_oligostates = {}\n",
    "\n",
    "        # Initialize a dictionary to store the oligostate paths for each range\n",
    "        oligostate_paths = {}\n",
    "\n",
    "        for ranges, vals in grouped_dict.items():\n",
    "            oligostate_counter = Counter()\n",
    "\n",
    "            # Count occurrences for each oligostate\n",
    "            for oligostate, entries in vals.items():\n",
    "                oligostate_counter[oligostate] += len(entries)\n",
    "\n",
    "            # Get the top oligostates up to the number specified by num_most_common_oligostates\n",
    "            top_oligostates = oligostate_counter.most_common(num_most_common_oligostates)\n",
    "\n",
    "            # Store the top oligostates and their counts\n",
    "            most_common_oligostates[ranges] = top_oligostates\n",
    "\n",
    "            # Store all paths associated with the top oligostates\n",
    "            oligostate_paths[ranges] = [entry for oligostate, count in top_oligostates for entry in vals[oligostate]]\n",
    "\n",
    "        self.most_common_filtered_oligostates = most_common_oligostates\n",
    "        self.oligostates_filtered_paths = oligostate_paths\n",
    "\n",
    "        # Display the major oligostates and their entries\n",
    "        if self.save:\n",
    "            with open(os.path.join(self.work_dir, \"filtered_oligostates.txt\"), \"w\") as rg_fh:\n",
    "                for range, top_oligos in most_common_oligostates.items():\n",
    "                    rg_fh.write(f\"Range {range}:\\n\")\n",
    "                    for oligostate, count in top_oligos:\n",
    "                        rg_fh.write(f\"Oligostate {oligostate}: {count} occurrences\\n\")\n",
    "                    rg_fh.write(\"Associated PDB paths:\\n\")\n",
    "                    for path in oligostate_paths[range]:\n",
    "                        rg_fh.write(f\"{path}\\n\")\n",
    "\n",
    "        # Optionally, return the major oligostates and their entries if needed for further processing\n",
    "        #return most_common_oligostates, oligostate_paths\n",
    "    \n",
    "    def process_templates(self):\n",
    "\n",
    "        #first we split into groups based on boundaries\n",
    "        if self.oligostates_filtered_paths:\n",
    "            domains_grouped_dict = self.oligostates_filtered_paths\n",
    "            \n",
    "        else:\n",
    "            print(\"we have no self.oligostates_filtered_paths\")\n",
    "            return\n",
    "\n",
    "        sorted_oligos_by_range = defaultdict()\n",
    "        self.top_templates = defaultdict()\n",
    "        \n",
    "        for keys, vals in self.oligostates_filtered_paths.items():\n",
    "            oligostate_dict = defaultdict(list)\n",
    "\n",
    "            for (path, seqid) in vals:\n",
    "                oligostate = len(os.path.basename(path)[5:-4])  # Extract oligostate\n",
    "                oligostate_dict[oligostate].append((path, seqid))\n",
    "\n",
    "            for oligostate in oligostate_dict:\n",
    "                # Sort the list of tuples for each oligostate by seq_id\n",
    "                oligostate_dict[oligostate] = sorted(oligostate_dict[oligostate], key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "            sorted_oligostate_dict = OrderedDict(sorted(oligostate_dict.items()))\n",
    "            \n",
    "            sorted_oligos_by_range[keys] = sorted_oligostate_dict\n",
    "            self.templates_for_oligos = sorted_oligos_by_range\n",
    "\n",
    "            # Store the first hit of each oligostate for each range\n",
    "            for oligostate, templates in sorted_oligostate_dict.items():\n",
    "                # Initialize a dictionary for 'keys' if it does not exist\n",
    "                if keys not in self.top_templates:\n",
    "                    self.top_templates[keys] = {}\n",
    "                    self.top_templates[keys][oligostate] = templates[0]  # Take the first template\n",
    "            \n",
    "        if self.save:\n",
    "            self._write_templates_for_oligos()\n",
    "\n",
    "\n",
    "    \n",
    "    def _split_and_group_by_chain(self, input_dict):\n",
    "        grouped_dict = {}\n",
    "        for interval, pdb_list in input_dict.items():\n",
    "            # Initialize a sub-dictionary for each interval\n",
    "            grouped_dict[interval] = {}\n",
    "\n",
    "            for pdb_path, seq_id in pdb_list:\n",
    "                chain_length = len(os.path.basename(pdb_path)[5:-4])\n",
    "                \n",
    "                if chain_length not in grouped_dict[interval]:\n",
    "                    grouped_dict[interval][chain_length] = []\n",
    "\n",
    "                grouped_dict[interval][chain_length].append((pdb_path, seq_id))\n",
    "\n",
    "        return grouped_dict\n",
    "\n",
    "    def _write_templates_for_oligos(self):\n",
    "\n",
    "        with open(os.path.join(self.work_dir,\"templates_oligos.txt\"), \"w\") as oligo_fh:\n",
    "            for keys, vals in self.templates_for_oligos.items():\n",
    "                oligo_fh.write(str(keys))\n",
    "                oligo_fh.write(\"\\n\")\n",
    "                for oligo, pdbs in vals.items():\n",
    "                    oligo_fh.write(str(oligo))\n",
    "                    oligo_fh.write(\"\\n\")\n",
    "                    for pdb, seqid in pdbs:\n",
    "                        oligo_fh.write(pdb)\n",
    "                        oligo_fh.write(\" \")\n",
    "                        oligo_fh.write(str(seqid))\n",
    "                        oligo_fh.write(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c54a8682-7d4c-4d3d-b766-a332247fb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USAlign:\n",
    "\n",
    "    def __init__(self, work_dir,\n",
    "                 structure_seqid_dict,\n",
    "                 script_dir=None,\n",
    "                 top_templates=None,\n",
    "                 template_min_identity=None, num_top_clusters_per_range=None,\n",
    "                 save=True):\n",
    "\n",
    "        self.work_dir = work_dir\n",
    "        self.script_dir = script_dir\n",
    "        self.template_dict = structure_seqid_dict\n",
    "        self.template_min_identity = template_min_identity\n",
    "        self.num_top_clusters_per_range = num_top_clusters_per_range\n",
    "        self.transformed_input = None\n",
    "        self.filtered_input = None\n",
    "        self.filtered_injector_dict = None\n",
    "        self.list_of_sizes_per_cluster = None\n",
    "        self.save_results = save\n",
    "        self.results = None\n",
    "        self.result_dict = None\n",
    "        self.multiseq_alignment = None\n",
    "\n",
    "    def USAlign_run(self):\n",
    "        \n",
    "        #this function converts the input to a dict with a value list.\n",
    "        self._convert_input_to_value_list()\n",
    "        if self.template_min_identity:\n",
    "            self._filter_subclusters()\n",
    "        #lets run USAlign\n",
    "        self._prep_clusters_USAlign()\n",
    "        #now lets inject the filtered_injector_dict files\n",
    "        self._parallelized_execution_USAlign()\n",
    "\n",
    "        #if save:\n",
    "        if self.save_results:\n",
    "            self._save_results()\n",
    "\n",
    "\n",
    "    def run_multiseq_alignment(self, directory, oligomer=False):\n",
    "        \"\"\"This function takes as input a pdb directory and will perform structure based superposition. \n",
    "        The results will be stored in a dict.\"\"\"\n",
    "\n",
    "        #grab all pdbs from the directory.\n",
    "        pdb_to_check = [f[:-4] for f in os.listdir(directory) if f.endswith(\".pdb\")]\n",
    "        \n",
    "        #print(pdb_to_check)\n",
    "        pdb_list = os.path.join(directory, \"pdb_list.txt\")\n",
    "        \n",
    "        with open(pdb_list, \"w\") as pdb_fh:\n",
    "            for pdbs in pdb_to_check:\n",
    "                pdb_fh.write(pdbs)\n",
    "                pdb_fh.write(\"\\n\")\n",
    "        \n",
    "        bash_tm_and_rmsd_calc = f\"{self.work_dir}/USalign -dir {directory}/ {pdb_list} -suffix .pdb -mm 4 -outfmt 1\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        #print(bash_command)\n",
    "        try:\n",
    "            fasta_path = os.path.join(directory, \"multiseq_fasta_output.txt\")\n",
    "            with open(fasta_path, \"w\") as fh_results:\n",
    "                result = run(bash_command, stdout=fh_results, stderr=PIPE, universal_newlines=True)\n",
    "        \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "\n",
    "        sequences = defaultdict(str)\n",
    "        \n",
    "        if os.path.exists(fasta_path):\n",
    "            with open(fasta_path, \"r\") as fh_inp:\n",
    "                for line in fh_inp:\n",
    "                    # Ignore lines that start with \"#\" or empty lines\n",
    "                    if not line.startswith(\">\") and not line.startswith(\"#\") and line.strip():\n",
    "                        # The previous line contained the PDB ID, process it here\n",
    "                        # Ensure there's a current PDB ID to process and sequence data exists\n",
    "                        if current_pdb_id and line.strip():\n",
    "                            sequences[current_pdb_id] += line.strip()\n",
    "                    elif line.startswith(\">\"):\n",
    "                        # Extract the PDB ID, remove \":A\" part, and strip whitespace\n",
    "                        current_pdb_id = line.split()[0][1:].split(':')[0].strip()\n",
    "                    else:\n",
    "                        # Reset current PDB ID for non-sequence lines or new entries\n",
    "                        current_pdb_id = None\n",
    "        \n",
    "            \n",
    "            self.multiseq_alignment = sequences\n",
    "            self._write_fasta(sequences, outdir=directory)\n",
    "            # Print the sequences dictionary\n",
    "            #for pdb_id, sequence in sequences.items():\n",
    "            #    print(f\"{pdb_id}: {sequence}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"File {fasta_path} does not exist.\")\n",
    "        \n",
    "\n",
    "\n",
    "    def _write_fasta(self, sequence_dict:dict, outdir=None):\n",
    "\n",
    "        if outdir is None:\n",
    "            outdir = self.work_dir\n",
    "            \n",
    "        outfile = os.path.join(outdir, \"multiseq_fasta.fa\")\n",
    "        #Here we store our output as fastas for downstream analysis.\n",
    "        seq_records = []\n",
    "        for pdb_id, seq in sequence_dict.items():\n",
    "            seq_record = SeqRecord(Seq.Seq(seq), id=pdb_id, description=\"\")\n",
    "            seq_records.append(seq_record)\n",
    "\n",
    "        with open(outfile, \"w\") as output_handle:\n",
    "            SeqIO.write(seq_records, output_handle, \"fasta\")\n",
    "        \n",
    "    \n",
    "    def filter_results(self, tm_cutoff=0.5, rmsd_min_cutoff=0.1, log_file=True):\n",
    "        # Check if results are available\n",
    "        if self.results:\n",
    "            tmp_dict = defaultdict(dict)\n",
    "            for ranges, values in self.results.items():\n",
    "                for oligo, val in values.items():\n",
    "                    filtered_results = []\n",
    "                    for path, tm_str, rmsd_str in val:\n",
    "                        try:\n",
    "                            tm = float(tm_str)\n",
    "                            rmsd = float(rmsd_str)\n",
    "                            if tm > tm_cutoff and rmsd > rmsd_min_cutoff:\n",
    "                                filtered_results.append((path, tm, rmsd))\n",
    "                        except ValueError:\n",
    "                            # Handle the case where tm or rmsd is not a number\n",
    "                            print(\"we have no tm and rmsd error\")\n",
    "                            pass\n",
    "                    if filtered_results:\n",
    "                        tmp_dict[ranges][oligo] = filtered_results\n",
    "                    \n",
    "            #save log file\n",
    "            if log_file:\n",
    "                self._logger_us_results(tmp_dict)\n",
    "\n",
    "            self.result_dict = tmp_dict\n",
    "            return tmp_dict\n",
    "            \n",
    "        else:\n",
    "            tmp_dict = {}\n",
    "            if log_file:\n",
    "                self._logger_us_results(tmp_dict)\n",
    "                \n",
    "            self.result_dict = tmp_dict\n",
    "            return tmp_dict\n",
    "\n",
    "    def setup_oligo_directories(self):\n",
    "        #setup directories from here.\n",
    "        if self.result_dict:\n",
    "            self._separate_oligostates()\n",
    "        else:\n",
    "            print(\"You must first run filter_result()\")\n",
    "    \n",
    "    def _logger_us_results(self, result_dict:dict):\n",
    "\n",
    "        if len(result_dict) == 0:\n",
    "            with open(os.path.join(self.work_dir, \"filtered_usalign_log.txt\"), \"w\") as us_fh:\n",
    "                us_fh.write(\"No structures after filtering left.\")\n",
    "                \n",
    "        with open(os.path.join(self.work_dir, \"filtered_usalign_log.txt\"), \"w\") as us_fh:\n",
    "            for keys, vals in result_dict.items():\n",
    "                us_fh.write(str(keys))\n",
    "                us_fh.write(\"\\n\")\n",
    "                for oligo, lst in vals.items():\n",
    "                    us_fh.write(str(oligo))\n",
    "                    us_fh.write(\"\\n\")\n",
    "                    for p, t, r in lst:\n",
    "                        us_fh.write(p)\n",
    "                        us_fh.write(\" \")\n",
    "                        us_fh.write(str(t))\n",
    "                        us_fh.write(\" \")\n",
    "                        us_fh.write(str(r))\n",
    "                        us_fh.write(\"\\n\")\n",
    "\n",
    "    \n",
    "    def _convert_input_to_value_list(self):\n",
    "        \"\"\"Helper function to uniformize all input dict to one standard for downstream functions.\"\"\"\n",
    "        all_potential_pdbs_dict = self.template_dict\n",
    "        converted_dict = defaultdict(list)\n",
    "        for keys, vals in all_potential_pdbs_dict.items():\n",
    "            #here transform keys to remove spaces\n",
    "            formatted_key = f\"({keys[0]},{keys[1]})\"\n",
    "            converted_tmp_dict = defaultdict(list)\n",
    "            for oligo, pdb_seqid_lst in vals.items():\n",
    "                for (pdb, seqid) in pdb_seqid_lst:        \n",
    "                    converted_tmp_dict[oligo].append((pdb, seqid))\n",
    "            converted_dict[formatted_key] = converted_tmp_dict\n",
    "        self.transformed_input = converted_dict\n",
    "        \n",
    "    def _filter_subclusters(self):\n",
    "        #print(self.cleaned_input)\n",
    "        filtered_dict = defaultdict(dict)\n",
    "        for range_key, oligostate_dict in self.transformed_input.items():\n",
    "            for oligostate_key, templates in oligostate_dict.items():\n",
    "                if templates and templates[0][1] > self.template_min_identity:\n",
    "                    filtered_dict[range_key][oligostate_key] = templates\n",
    "                    \n",
    "        # Removing any range keys that no longer have any oligostates after filtering\n",
    "        filtered_dict = {k: v for k, v in filtered_dict.items() if v}\n",
    "        self.filtered_input = filtered_dict\n",
    "\n",
    "    def _prep_clusters_USAlign(self):\n",
    "        result_lst_usalign = []\n",
    "        injector_dict = defaultdict()\n",
    "        list_of_sizes_per_range_and_cluster = defaultdict()\n",
    "        \n",
    "        for ranges, oligo_states in self.filtered_input.items():\n",
    "            #print(ranges, oligo_states)\n",
    "            sub_injector_dict = defaultdict()\n",
    "            list_of_sizes_per_cluster = defaultdict()\n",
    "            for oligo, pdb in oligo_states.items():\n",
    "                oligo_collection = []\n",
    "                if len(pdb) != 1:\n",
    "                    for x, seqidentity in pdb:\n",
    "                        oligo_collection.append((x, seqidentity))\n",
    "\n",
    "                    oligo_collection = list(set(oligo_collection)) # lets get rid of duplicated stuff.\n",
    "                    ordered_oligo_collection = sorted(oligo_collection, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "                if len(ordered_oligo_collection) > 1:\n",
    "                    sub_injector_dict[oligo] = [x[0] for x in ordered_oligo_collection] # only the paths\n",
    "                    list_of_sizes_per_cluster[oligo] = len(ordered_oligo_collection)\n",
    "\n",
    "            list_of_sizes_per_range_and_cluster[ranges] = list_of_sizes_per_cluster\n",
    "            list_of_sizes_per_range_and_cluster_cleaned = {k: v for k, v in list_of_sizes_per_range_and_cluster.items() if v}\n",
    "            \n",
    "            injector_dict[ranges] = sub_injector_dict\n",
    "            filtered_injector_dict = {k: v for k, v in injector_dict.items() if v}  #remove empty\n",
    "        \n",
    "        self.list_of_sizes_per_cluster = list_of_sizes_per_range_and_cluster_cleaned\n",
    "        # THIS IS WHAT WE NEED. KEYS = Ranges... \n",
    "        # VALS = DICT: key: oligostate: values = pdbpaths. FIRST IS best template. use 1rst and query it against all others.\n",
    "        self.filtered_injector_dict = filtered_injector_dict\n",
    "\n",
    "\n",
    "    def _parallelized_execution_USAlign(self):\n",
    "        \n",
    "        #print(self.num_top_clusters_per_range)\n",
    "        self._get_top_clusters_per_range()\n",
    "        # comment\n",
    "        self._extract_files_per_cluster()\n",
    "        #now we want to send this dict into a function that does something\n",
    "        self.results = self._run_usalign_singles() \n",
    "        \n",
    "\n",
    "    def _run_usalign_singles(self):\n",
    "        range_results = defaultdict(dict)\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            for range_key, oligo_path in self.filtered_injector_dict.items():\n",
    "                for oligo_state, pdb_list in oligo_path.items():\n",
    "                    template, queries = pdb_list[0], pdb_list[1:]\n",
    "\n",
    "                    # Initialize the futures list for this oligo_state\n",
    "                    futures = []\n",
    "                    if oligo_state == 1:  # Monomeric\n",
    "                        for query in queries:\n",
    "                            future = executor.submit(self._run_usalign_monomer, template, query)\n",
    "                            futures.append(future)\n",
    "                    else:  # Oligomeric\n",
    "                        for query in queries:\n",
    "                            future = executor.submit(self._run_usalign_oligomer, template, query)\n",
    "                            futures.append(future)\n",
    "\n",
    "                    # Collecting results for each oligo_state\n",
    "                    oligo_state_results = []\n",
    "                    for future in as_completed(futures):\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            if result:\n",
    "                                oligo_state_results.append(result)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                            \n",
    "                    # Store results in the dictionary\n",
    "                    range_results[range_key][oligo_state] = oligo_state_results\n",
    "\n",
    "        # Print or process range_results as needed\n",
    "        return range_results\n",
    "\n",
    "\n",
    "    def _run_usalign_monomer(self, template, path_struc_1):\n",
    "    \n",
    "        bash_tm_and_rmsd_calc = f\"{self.work_dir}/USalign {path_struc_1} {template} -outfmt 2 -mol prot\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        #print(bash_command)\n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            tm_2, rmsd = self._get_tm_scores_and_rmsd(result.stdout)\n",
    "        \n",
    "            # TM-score=0.5, it corresponds to a P-value of 5.5  107  taken from \n",
    "            #figure 3 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2913670/ \n",
    "        \n",
    "            if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "            else:\n",
    "                print(f\"we remove {path_struc_1} because of tm: {tm_2} and rmsd: {rmsd}\")\n",
    "                #os.remove(path_struc_1)\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def _run_usalign_oligomer(self, template, path_struc_1):\n",
    "\n",
    "        # -outfmt 2 : tsv outfile, \n",
    "        # -mol prot : only consider protein\n",
    "        # -ter 0:align all chains from all models (recommended for aligning biological assemblies, i.e. biounits)\n",
    "        # -mm 1: alignment of two multi-chain oligomeric structures\n",
    "    \n",
    "        bash_tm_and_rmsd_calc = f\"{self.work_dir}/USalign {path_struc_1} {template} -outfmt 2 -mol prot -ter 0 -mm 1\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        #print(bash_command)\n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            tm_2, rmsd = self._get_tm_scores_and_rmsd(result.stdout)\n",
    "            if float(tm_2) > 0.5:  # Adjust the threshold as needed\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "            else:\n",
    "                print(f\"we remove {path_struc_1} because of tm: {tm_2} and rmsd: {rmsd}\")\n",
    "                return path_struc_1, tm_2, rmsd\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    def _get_tm_scores_and_rmsd(self, results:str):\n",
    "        \"\"\" helper function to retrieve tm scores and rmsd\n",
    "        We are only interested in TM_2 and RMSD.\n",
    "        IF RMSD is HIGH and TM_2 HIGH that means we have a conformer.\n",
    "        IF RMSD is LOW and TM_2 HIGH that means we have the same structure in the same conformer\n",
    "        IF RMSD is HIGH and TM_2 LOW that means the structures are not related.\"\"\"\n",
    "    \n",
    "        \"\"\"['#PDBchain1', 'PDBchain2', 'TM1', 'TM2', 'RMSD', 'ID1', 'ID2', 'IDali', 'L1', 'L2', 'Lali\\n/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/2duk_A.pdb:A', '/home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN/5ltu_A.pdb:A', \n",
    "        '0.8833', '0.9565', '0.94', '0.920', '1.000', '1.000', '138', '127', '127\\n']\"\"\"\n",
    "    \n",
    "        res_list = results.split(\"\\t\")\n",
    "        #this one is from the mobile protein\n",
    "        tm_1 = res_list[12]\n",
    "        #this one belongs to the target protein (the one we superimpose the mobile protein onto)\n",
    "        tm_2 = res_list[13]\n",
    "        #rmsd used to judge cutoff for trashing structures.\n",
    "        rmsd = res_list[14]\n",
    "        #we return tm_2 and rmsd\n",
    "        return ((tm_2, rmsd))\n",
    "    \n",
    "\n",
    "    def _extract_files_per_cluster(self):\n",
    "        \n",
    "        files_per_cluster = {}\n",
    "        for range_key, clusters in self.list_of_sizes_per_cluster.items():\n",
    "            # Check if the range key exists in the filtered_injector_dict\n",
    "            if range_key in self.filtered_injector_dict:\n",
    "                files_per_cluster[range_key] = {}\n",
    "                for cluster_id in clusters.keys():\n",
    "                    # Fetch the file paths for each cluster id\n",
    "                    files_per_cluster[range_key][cluster_id] = self.filtered_injector_dict[range_key][cluster_id]\n",
    "\n",
    "        self.filtered_injector_dict = files_per_cluster\n",
    "\n",
    "    def _get_top_clusters_per_range(self):\n",
    "    \n",
    "        top_clusters = {}\n",
    "        for range_key, clusters in self.list_of_sizes_per_cluster.items():\n",
    "            # Sort clusters based on size, and in case of a tie, use larger key\n",
    "            sorted_clusters = sorted(clusters.items(), key=lambda x: (-x[1], -x[0]))\n",
    "            # Take the top n clusters, where n is specified by num_top_clusters_per_range\n",
    "            top_n_clusters = dict(sorted_clusters[:self.num_top_clusters_per_range])\n",
    "            top_clusters[range_key] = top_n_clusters\n",
    "\n",
    "        self.list_of_sizes_per_cluster = top_clusters\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def _separate_oligostates(self):\n",
    "\n",
    "        #lets separate our found results and make new directories.   \n",
    "        lst = [(\"1\",'monomer'),(\"2\", 'dimer'),(\"3\", 'trimer'),(\"4\", 'tetramer'),(\"5\", 'pentamer'),\n",
    "           (\"6\", 'hexamer'),(\"7\", 'heptamer'),(\"8\", 'oktamer'),(\"9\", 'nonamer'),(\"10\", 'decamer'),\n",
    "           (\"11\", 'undecamer'),(\"12\", 'dodecamer'),(\"13\", 'tridecamer'),\n",
    "           (\"14\", 'tetradecamer'),(\"15\", 'pentadecamer'),(\"16\", 'hexadecamer'),\n",
    "           (\"17\", 'heptadecamer'),(\"18\", 'oktadecamer'),(\"19\", 'nonadecamer'),(\"20\", 'eicosamer'),\n",
    "           (\"21\", 'eicosameundamer'),(\"22\", 'eicosadodamer'),(\"23\", 'eicosatrimer'),(\"24\", 'eicosatetramer')]\n",
    "           \n",
    "        oligodirdict = defaultdict(lambda: \"X-mer\", lst)\n",
    "        \n",
    "        oligos = defaultdict(dict)\n",
    "        #for each range we go through all potential oligos.\n",
    "        for ranges, dicts in self.result_dict.items():\n",
    "            ranges = ranges.split(\",\")\n",
    "            start, stop = ranges[0][1:], ranges[1][:-1]\n",
    "            ranges = start + \"-\" + stop\n",
    "            for key, value in dicts.items():  #dicts = dict with : key = oligostate, value = paths \n",
    "                oligos[ranges][oligodirdict[str(key)]] = value\n",
    "                    \n",
    "        self.oligomers = oligos\n",
    "        #now lets make the dirs.\n",
    "        self._shuffle(oligos)\n",
    "\n",
    "\n",
    "    def _shuffle(self, dict_to_shuffle):\n",
    "        # Initialize the new dictionary structure for oligostates, ranges, and PDB paths\n",
    "        new_dict = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        for range, oligo_data in dict_to_shuffle.items():\n",
    "            for oligo, path_tm_rmsd in oligo_data.items():\n",
    "                # Create the main directory for each \"oligostate\" if it doesn't exist\n",
    "                oligo_path = os.path.join(work_dir, oligo)\n",
    "                if not os.path.exists(oligo_path):\n",
    "                    os.mkdir(oligo_path)\n",
    "        \n",
    "                # Create a subdirectory for each \"range\" within the \"oligostate\" directory\n",
    "                range_path = os.path.join(oligo_path, range)\n",
    "                if not os.path.exists(range_path):\n",
    "                    os.mkdir(range_path)\n",
    "            \n",
    "                # Copy files associated with the current range into its directory\n",
    "                for (path, tm, rsmd) in path_tm_rmsd:\n",
    "                    shutil.copy(path, range_path)  # Copy file to the new location\n",
    "                    \n",
    "                    # Append only the PDB paths to the list associated with each range within each oligostate\n",
    "                    new_dict[oligo][range].append((path, tm, rsmd))\n",
    "        \n",
    "        # Convert the defaultdict to a regular dict for the final output, if preferred\n",
    "        final_dict = {oligo: dict(ranges) for oligo, ranges in new_dict.items()}\n",
    "        \n",
    "        # Now 'final_dict' contains the structure you want, and can be used as needed\n",
    "        self.result_dict = final_dict\n",
    "        #return final_dict\n",
    "\n",
    "\n",
    "    def _save_results(self):\n",
    "\n",
    "        with open(os.path.join(self.work_dir, \"USalign_log.txt\"), \"w\") as us_fh:\n",
    "            for ranges, vals in self.results.items():\n",
    "                us_fh.write(str(ranges))\n",
    "                us_fh.write(\"\\n\")\n",
    "                for oligo, path_score_rmsd in vals.items():\n",
    "                    us_fh.write(str(oligo))\n",
    "                    us_fh.write(\"\\n\")\n",
    "                    for path, score, rmsd in path_score_rmsd:\n",
    "                        us_fh.write(str(path))\n",
    "                        us_fh.write(\" \")\n",
    "                        us_fh.write(str(score))\n",
    "                        us_fh.write(\" \")\n",
    "                        us_fh.write(str(rmsd))\n",
    "                        us_fh.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dca8664-fb74-4634-ac35-ab8017695ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModellerRepairEnsemble:\n",
    "\n",
    "    def __init__(self, work_dir, ensemble_dict):\n",
    "\n",
    "        self.repair_ensemble = ensemble_dict\n",
    "        self.work_dir = work_dir\n",
    "        self.monomers = None\n",
    "        self.oligomers = None\n",
    "        self.monomeric_templates= None\n",
    "        self.oligomeric_templates = None\n",
    "        self.usalign_exe = os.path.join(work_dir, \"USalign\")\n",
    "        self.repairable_structures = None\n",
    "        self.repaired_monomers = None\n",
    "        self.repaired_oligomers = None\n",
    "\n",
    "    \n",
    "    def _separate_oligostates(self):\n",
    "\n",
    "        lst = [(\"1\",'monomer'),(\"2\", 'dimer'),(\"3\", 'trimer'),(\"4\", 'tetramer'),(\"5\", 'pentamer'),\n",
    "           (\"6\", 'hexamer'),(\"7\", 'heptamer'),(\"8\", 'oktamer'),(\"9\", 'nonamer'),(\"10\", 'decamer'),\n",
    "           (\"11\", 'undecamer'),(\"12\", 'dodecamer'),(\"13\", 'tridecamer'),\n",
    "           (\"14\", 'tetradecamer'),(\"15\", 'pentadecamer'),(\"16\", 'hexadecamer'),\n",
    "           (\"17\", 'heptadecamer'),(\"18\", 'oktadecamer'),(\"19\", 'nonadecamer'),(\"20\", 'eicosamer'),\n",
    "           (\"21\", 'eicosameundamer'),(\"22\", 'eicosadodamer'),(\"23\", 'eicosatrimer'),(\"24\", 'eicosatetramer')]\n",
    "\n",
    "        oligodirdict = defaultdict(lambda: \"X-mer\", lst)\n",
    "\n",
    "        \n",
    "        oligos = defaultdict(dict)\n",
    "        monos = defaultdict(dict)\n",
    "        for ranges, dicts in self.repair_ensemble.items():\n",
    "            for key, value in dicts.items():\n",
    "                if key == 1:\n",
    "                    monos[ranges][oligodirdict[str(key)]] = value\n",
    "                else:\n",
    "                    oligos[ranges][oligodirdict[str(key)]] = value\n",
    "\n",
    "        self.monomers, self.oligomers = monos, oligos\n",
    "\n",
    "    def _shuffle(self, dict_to_shuffle):\n",
    "\n",
    "\n",
    "        lst = [(\"1\",'monomer'),(\"2\", 'dimer'),(\"3\", 'trimer'),(\"4\", 'tetramer'),(\"5\", 'pentamer'),\n",
    "           (\"6\", 'hexamer'),(\"7\", 'heptamer'),(\"8\", 'oktamer'),(\"9\", 'nonamer'),(\"10\", 'decamer'),\n",
    "           (\"11\", 'undecamer'),(\"12\", 'dodecamer'),(\"13\", 'tridecamer'),\n",
    "           (\"14\", 'tetradecamer'),(\"15\", 'pentadecamer'),(\"16\", 'hexadecamer'),\n",
    "           (\"17\", 'heptadecamer'),(\"18\", 'oktadecamer'),(\"19\", 'nonadecamer'),(\"20\", 'eicosamer'),\n",
    "           (\"21\", 'eicosameundamer'),(\"22\", 'eicosadodamer'),(\"23\", 'eicosatrimer'),(\"24\", 'eicosatetramer')]\n",
    "\n",
    "        oligodirdict = defaultdict(lambda: \"X-mer\", lst)\n",
    "\n",
    "\n",
    "        \n",
    "        dirs_to_check = []\n",
    "        \n",
    "        for keys, vals in dict_to_shuffle.items():\n",
    "            print(keys)\n",
    "            keys = keys.split(\",\") #CONVERT tuple to a string.\n",
    "            start, stop = keys[0][1:], keys[1][:-1]\n",
    "            print(start)\n",
    "            print(stop)\n",
    "            new_range_obj = start+\"-\"+stop\n",
    "            dir_path = os.path.join(work_dir, new_range_obj)\n",
    "            if not os.path.exists(dir_path):\n",
    "                #get rid of whitespace which makes issues later downstream\n",
    "                os.mkdir(dir_path)\n",
    "                \n",
    "            for oligo, path_tm_rmsd in vals.items():\n",
    "                print(oligo)\n",
    "                oligo_path = os.path.join(work_dir, new_range_obj, oligodirdict[str(oligo)])\n",
    "                if not os.path.exists(oligo_path):\n",
    "                    os.mkdir(oligo_path)\n",
    "                dirs_to_check.append(oligo_path) #this is a list that contains the locations of all directories that will be visited during repair.\n",
    "                for path, _, _ in path_tm_rmsd:\n",
    "                    shutil.copy(path, oligo_path) #new location\n",
    "        \n",
    "        return dirs_to_check\n",
    "\n",
    "    \n",
    "    def repair_structures(self):\n",
    "        # first the monomeric case.\n",
    "        self._separate_oligostates()\n",
    "        \n",
    "        # run now repairs on both.\n",
    "        if self.monomers:\n",
    "            #we need a separate environment for modeller etc.\n",
    "            self.monomer_dirs = self._shuffle(self.monomers)\n",
    "            oligo_dir_dict = defaultdict()\n",
    "            if self.monomer_dirs:\n",
    "                for directory in self.monomer_dirs:\n",
    "                    #check which pdbs need repair.\n",
    "                    pdb_to_check = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".pdb\")] \n",
    "                    #first get the gap ranges\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        gap_list = list(executor.map(self._gap_localization_1, pdb_to_check))\n",
    "                    #each gaplist member is a tuple of (path, list of gaps)\n",
    "                    # keys: templates:   inner keys: the targets inner vals: their seqid and their TM score. which we will now filter and take the top X target to repair our structure.\n",
    "                    \n",
    "                    #print(f\"{gap_list=}\")\n",
    "                    \n",
    "                    suitable_templates = self._get_templates_for_monomeric_multitemplate_modelling(gap_list)\n",
    "\n",
    "                    #print(f\"{suitable_templates=}\")\n",
    "                    # now lets go through each and every of them an check if there are gaps.\n",
    "                    \n",
    "                    potential_template_dict = self._check_repairability(suitable_templates, gap_list)\n",
    "\n",
    "                    #print(f\"{potential_template_dict=}\")\n",
    "                    \n",
    "                    #next step is repair. if empty we skip. else we go through all repairable structures.\n",
    "                    if potential_template_dict:\n",
    "                        with ProcessPoolExecutor() as executor:\n",
    "                            key, value = zip(*potential_template_dict.items()) #unpack first \n",
    "                            try:\n",
    "                                print(\"we try intelligent_monomeric_repair. \")\n",
    "                                results = list(executor.map(self._intelligent_monomeric_repair, key, value))\n",
    "                            except Exception as e:\n",
    "                                results = []\n",
    "                                print(e)\n",
    "                        oligo_dir_dict[directory] = results\n",
    "                        \n",
    "                    else:\n",
    "                        print(f\"We have no repaired structures for directory: {directory}\")\n",
    "                        oligo_dir_dict[directory] = []\n",
    "                        \n",
    "            #this is a dict of paths for each oligostate and range as a instance of the class saved.\n",
    "            self.repaired_monomers =  oligo_dir_dict\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.oligomers:\n",
    "            #we need a separate environment for modeller etc.\n",
    "            self.oligomer_dirs = self._shuffle(self.oligomers)\n",
    "\n",
    "            oligo_dir_dict = defaultdict()\n",
    "            if self.oligomer_dirs:\n",
    "                for directory in self.oligomer_dirs:\n",
    "                    #check which pdbs need repair.\n",
    "                    pdb_to_check = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".pdb\")] \n",
    "\n",
    "\n",
    "                    \"\"\"Lets implement this later. First we just take all structures as they are and continue.\"\"\"\n",
    "                    oligo_dir_dict[directory] = pdb_to_check\n",
    "\n",
    "                    \n",
    "                    #first get the gap ranges\n",
    "                    with ProcessPoolExecutor() as executor:\n",
    "                        gap_list = list(executor.map(self._gap_localization_1_oligomeric, pdb_to_check))\n",
    "        \n",
    "\n",
    "                    suitable_templates = self._get_templates_for_oligo_multitemplate_modelling(template_path=template_path,\n",
    "                                                                    potential_templates=template_backup_list,\n",
    "                                                                    gap_dict=gap_dict)\n",
    "\n",
    "\n",
    "            self.repaired_oligos = oligo_dir_dict\n",
    "\n",
    "\n",
    "    def _check_repairability(self, template_dict: dict, gap_list: list, min_seq_id: float = 0.6) -> dict:\n",
    "        \"\"\"\n",
    "        Checks each structure's eligibility for repair based on gap alignment and minimum sequence identity.\n",
    "    \n",
    "        Args:\n",
    "        - template_dict (dict): A dictionary where keys are template paths and values are dicts containing template details.\n",
    "        - gap_list (list): A list of tuples representing the paths and gaps in the structure to be repaired (path, [(start, stop), ...]).\n",
    "        - min_seq_id (float): The minimum sequence identity required for a template to be considered suitable.\n",
    "    \n",
    "        Returns:\n",
    "        - dict: A dictionary of potential templates that can be used for repair.\n",
    "        \"\"\"\n",
    "        \n",
    "        potential_template_dict = defaultdict(list)\n",
    "        for (temp_path, temp_gaps) in gap_list:\n",
    "            for template_path, details in template_dict.items():\n",
    "                # Assume details structure is a dict with keys as some identifiers and values as tuples of details\n",
    "                for key, val in details.items():\n",
    "                    try:\n",
    "                        # Attempt to unpack expecting 3 elements; adjust according to your data structure\n",
    "                        tm_score, seq_id, template_gaps = val[0][0], val[0][1], val[1]\n",
    "                    except ValueError:\n",
    "                        # Handle cases where unpacking fails due to unexpected structure\n",
    "                        print(f\"Skipping {key} due to unpacking error: expected 3 values, got {len(val)}\")\n",
    "                        continue\n",
    "                        \n",
    "                    #print(seq_id, min_seq_id)\n",
    "                    if float(seq_id) > min_seq_id and not self._has_large_gap_overlap(temp_gaps, template_gaps):\n",
    "                        #print(seq_id, min_seq_id)\n",
    "                        #print(temp_path, template_path)\n",
    "                        potential_template_dict[(temp_path, temp_gaps)].append((template_path, (tm_score, seq_id, template_gaps)))\n",
    "        \n",
    "        return potential_template_dict\n",
    "\n",
    "    def _has_large_gap_overlap(self, gaps1: list, gaps2: list) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if there is a large overlap in gaps between two structures.\n",
    "    \n",
    "        Args:\n",
    "        - gaps1 (list): The gaps in the first structure as a list of tuples (start, stop).\n",
    "        - gaps2 (list): The gaps in the second structure as a list of tuples (start, stop).\n",
    "    \n",
    "        Returns:\n",
    "        - bool: True if there is a large overlap, False otherwise.\n",
    "        \"\"\"\n",
    "        for start1, end1 in gaps1:\n",
    "            for start2, end2 in gaps2:\n",
    "                if start1 <= end2 and start2 <= end1:  # Check if gaps overlap\n",
    "                    return True  # Overlapping gaps found\n",
    "        return False  # No overlapping gaps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _gap_localization_1(self, pdb_path: str):\n",
    "        \"\"\"Helper function to compute the start and stops of gaps \n",
    "        for later potential reconstruction.\"\"\"\n",
    "    \n",
    "        gap_ranges = []\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "        \n",
    "        test_res = test_structure.get_residues()\n",
    "        start = end = None\n",
    "    \n",
    "        for res in test_res:\n",
    "            res_id = int(res.get_id()[1])\n",
    "            if end is None:\n",
    "                start = end = res_id\n",
    "            elif res_id == end + 1:\n",
    "                end = res_id\n",
    "            else:\n",
    "                if start != end:\n",
    "                    gap_ranges.append((start, end))\n",
    "                start = end = res_id\n",
    "    \n",
    "        if start is not None and start != end:\n",
    "            gap_ranges.append((start, end))\n",
    "\n",
    "        if not gap_ranges:\n",
    "            return (pdb_path, [])  # Return an empty list if there are no gaps\n",
    "\n",
    "        # Convert the list of gap ranges to a list of gap tuples\n",
    "        gap_tuples = [(start, end) for start, end in gap_ranges]\n",
    "        merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "        return (pdb_path, merged_gaps)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_templates_for_monomeric_multitemplate_modelling(self, pdb_gap_list):\n",
    "        parser = PDBParser()\n",
    "        suitable_templates = {}\n",
    "\n",
    "        for template_index, (template_path, tmp_gap) in enumerate(pdb_gap_list):\n",
    "            template_results = {}\n",
    "\n",
    "            for target_index, (target_path, gaps) in enumerate(pdb_gap_list):\n",
    "                if target_index != template_index:  # Avoid self-comparison\n",
    "                    score = self._align(target_path, template_path)\n",
    "                    template_results[target_path] = (score, gaps)\n",
    "\n",
    "            suitable_templates[template_path] = template_results\n",
    "\n",
    "        #print(suitable_templates)\n",
    "        # outer keys: templates, inner keys: target_structures. inner values: (seq_id, tm., gaps) \n",
    "        return suitable_templates\n",
    "    \n",
    "    \n",
    "    def _gap_localization_1_oligomeric(self,pdb_path: str):\n",
    "        \"\"\"Helper function to compute the start and stops of gaps for later potential reconstruction.\"\"\"\n",
    "        \n",
    "        full_gaps_dict = defaultdict()\n",
    "        gap_dict = defaultdict(list)\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        test_structure = parser.get_structure(\"test\", pdb_path)\n",
    "        \n",
    "        for model in test_structure:\n",
    "            for chain in model:\n",
    "                for residues in chain:\n",
    "                    resnum = residues.get_id()\n",
    "                    gap_dict[chain.id].append(resnum[1])\n",
    "    \n",
    "        #now we have all chains and all sequence nums from start to finish\n",
    "        chain_to_query = []\n",
    "    \n",
    "        for chains, residues in gap_dict.items():\n",
    "            chain_to_query.append(chains)\n",
    "            \n",
    "            start = end = None\n",
    "            gap_ranges = []\n",
    "            for res in residues:\n",
    "                if end is None:\n",
    "                    start = end = res\n",
    "                elif res == end + 1:\n",
    "                    end = res\n",
    "                else:\n",
    "                    if start != end:\n",
    "                        gap_ranges.append((start, end))\n",
    "                    start = end = res\n",
    "    \n",
    "            if start is not None and start != end:\n",
    "                gap_ranges.append((start, end))\n",
    "    \n",
    "            gap_tuples = [(start, end) for start, end in gap_ranges]\n",
    "            merged_gaps = [(1, gap_tuples[0][0])] + [(gap_tuples[i][1], gap_tuples[i + 1][0]) for i in range(len(gap_tuples) - 1)]\n",
    "            full_gaps_dict[chains] = merged_gaps\n",
    "        \n",
    "        return full_gaps_dict, chain_to_query\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _align(self, target_path, template_path):\n",
    "        \n",
    "        # Implement the logic to align the target_path with the template_path\n",
    "        # focusing on the region defined by 'gap', and return a scoring metric\n",
    "        bash_tm_and_rmsd_calc = f\"{self.usalign_exe} {target_path} {template_path} -TMscore 0 -outfmt 1\"\n",
    "        bash_command = bash_tm_and_rmsd_calc.split()\n",
    "        \n",
    "        result_scores_for_templates = defaultdict()\n",
    "        \n",
    "        try:\n",
    "            result = run(bash_command, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "            #print(result.stdout)\n",
    "            result_list = self._get_aligned_fastas(result.stdout)  # gap list not used here... can be removed in future version.\n",
    "\n",
    "            #print(result_list)\n",
    "            return result_list\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e)\n",
    "            return {}\n",
    "            \n",
    "\n",
    "\n",
    "    def _get_aligned_fastas(self, result):\n",
    "\n",
    "        lines = result.strip().split('\\n')\n",
    "        #this is gap_list : [(1, 3), (263, 276), (396, 407)]\n",
    "        #print(\"this is gap_list :\", gap_list)\n",
    "        result_list = []\n",
    "        #print(lines)\n",
    "        # Initialize an empty dictionary\n",
    "        template_header = lines[2].split(\"\\t\")\n",
    "        #template_full_seq = lines[3]  #replace - to get the full seq.\n",
    "        template_seq_id = float(template_header[3].split(\"=\")[-1])\n",
    "        template_tm_score = float(template_header[4].split(\"=\")[-1])\n",
    "        template_pdb_path = template_header[0].replace(\">\",\"\")\n",
    "        template_pdb_path = template_pdb_path.split(\":\")[0]\n",
    "    \n",
    "        potential_template_header = lines[0].split(\"\\t\")\n",
    "        #potential_template_full_seq = lines[1]\n",
    "        potential_template_seq_id = float(potential_template_header[3].split(\"=\")[-1])\n",
    "        potential_template_tm_score = float(potential_template_header[4].split(\"=\")[-1])\n",
    "        potential_template_pdb_path = potential_template_header[0].replace(\">\",\"\")\n",
    "        potential_template_pdb_path = potential_template_pdb_path.split(\":\")[0]\n",
    "        #print(template_seq_id, template_tm_score)\n",
    "        #print(potential_template_seq_id, potential_template_tm_score)\n",
    "        #result_dict[potential_template_pdb_path] = (potential_template_seq_id, potential_template_tm_score, potential_template_full_seq)\n",
    "        #result_dict[template_pdb_path] = (template_seq_id, template_tm_score, template_full_seq)\n",
    "        return template_seq_id, template_tm_score\n",
    "    \n",
    "    def _score_meets_criteria(self, score):\n",
    "        # Implement the logic to check if the score meets your criteria\n",
    "        # for a suitable template\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _intelligent_oligomeric_repair(self, path_to_pdb:str, gap_list:list, selected_templates:list,chains_query:list):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function repairs structures with gaps less than 8 residues per gap.\n",
    "    \n",
    "        Args:\n",
    "        - path_to_pdb (str): Path to the folder containing PDB files.\n",
    "        - stop_pos (int): Stop position.\n",
    "        - main_prot_seq (str): Main protein sequence.\n",
    "        - use_main (bool): Whether to use the main protein.\n",
    "    \n",
    "        Output:\n",
    "        Repaired structures.\n",
    "        \"\"\"\n",
    "    \n",
    "        #new_template_path='/home/micnag/test_modeller_oligomer/6hyr/6hyr.pdb'\n",
    "        #,gap_list_all_chains=[[(1, 5), (96, 116)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)]], \n",
    "        #repair_templates_non_redundant={'/home/micnag/test_modeller_oligomer/6hz3_A.pdb'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"inside oligomeric_repair\")\n",
    "        log.none()  # no stdout spam\n",
    "        env = Environ()  # setup env for modelling\n",
    "        aln = Alignment(env)  # setup the alignment\n",
    "        mdl = Model(env)  # setup the model\n",
    "    \n",
    "        #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "        # current working directory\n",
    "        pdb_id_target = os.path.basename(path_to_pdb) #results in 2duk_A.pdb\n",
    "    \n",
    "        #pdb_id_target='6hyr.pdb'\n",
    "        #print(f\"{pdb_id_target=}\")\n",
    "        \n",
    "        pdb_id_chain = chains_query #this corresponds to all chains in oligomeric struc. We shall also pass this\n",
    "        #to our automodell derived child class.\n",
    "    \n",
    "        #we need first and last chain to set boundaries for our model \n",
    "        first_chain, last_chain = pdb_id_chain[0], pdb_id_chain[-1]\n",
    "        \n",
    "        #pdb_code_name is passed to modeller later.\n",
    "        pdb_code_name = pdb_id_target[:-4] #2duk_ABCDEF\n",
    "        \n",
    "        pdb_4_digit_id = pdb_id_target[0:4] #2duk\n",
    "    \n",
    "        #print(f\"{pdb_4_digit_id=}\")\n",
    "        \n",
    "        #get uniprot_id second. HERE CHECK HOW WE GET X UNIPROT IDS FOR X PROTEINS IN A HETERO_OLIGOMER\n",
    "        uniprot_id = return_uniprot_id_from_rcsb(pdb_4_digit_id)\n",
    "    \n",
    "        #print(f\"{uniprot_id=}\")\n",
    "        #get associated fasta from uniprot id.\n",
    "        fasta_seq = get_gene_fasta(uniprot_id)\n",
    "    \n",
    "        #print(f\"{fasta_seq=}\")\n",
    "    \n",
    "        #first_chain='A', last_chain='E', pdb_4_digit_id='6hyr', pdb_code_name='6hyr',uniprot_id='Q7NDN8', pdb_id_chain=['A', 'B', 'C', 'D', 'E']\n",
    "        #print(f\"{first_chain=}, {last_chain=}, {pdb_4_digit_id=}, {pdb_code_name=},{uniprot_id=}, {pdb_id_chain=}, {fasta_seq=}\")\n",
    "        \"\"\"HERE WE NEED TO CHECK HOW TO MERGE OUR FASTAS.. ESPECIALLY CRUCIAL IF WE HAVE A MIXED OLIGOMER.\n",
    "        SUBSEQUENT CHAINS NEEDS TO BE SEPARATED BY / \"\"\"\n",
    "        \n",
    "        #if mixed oligomer this needs to be taken into account. \n",
    "        \"\"\"WE NEED TO CHECK THIS IN THE FUTURE... ADAPT GET_GENE_FASTA for multiple seqs in hetero X mers.\"\"\"\n",
    "        \n",
    "        merged_fasta = '/'.join([fasta_seq] * len(pdb_id_chain))\n",
    "        \n",
    "        #first we check what is the first ID in our struc. because otherwise we can miss structures that simply miss \n",
    "        #N terminus e.g gap 1-21 but they in fact start with residue 21 and are otherwise good or might just have small\n",
    "        # gaps to fix otherwhere in the structure.\n",
    "    \n",
    "        start_stop_dict = defaultdict()\n",
    "    \n",
    "        temp_codes = []\n",
    "        temp_abs_paths = []\n",
    "        temp_chains = []\n",
    "    \n",
    "    \n",
    "    \n",
    "        #print(f\"{merged_fasta=}\")\n",
    "    \n",
    "        for structure in set(selected_templates):  #no duplicates here.\n",
    "            #print(f\"Structure: {structure} selected as template\")\n",
    "            \n",
    "            temp_codes.append(structure.split(\"/\")[-1][:-4]) #extract codes for all strucs. this works for oligomers.\n",
    "            \n",
    "            temp_abs_paths.append(structure) #grab abspath\n",
    "            \n",
    "            temp_chains.append(structure.split(\"/\")[-1][5:-4]) #this are the chains we need.\n",
    "    \n",
    "    \n",
    "        #print(f\"{temp_codes=}, {temp_abs_paths=}, {temp_chains}\")\n",
    "    \n",
    "        for temp_code, temp_paths, chains in zip(temp_codes, temp_abs_paths, temp_chains):\n",
    "            if len(temp_code) <= 6:\n",
    "                #append all start stop and shifts here.\n",
    "                start_struc_temp, stop_struc_temp = get_struc_stop_oligomer(temp_paths)  #this works also for oligomers in theory. But it is questionable.. if chain A is 100 res and chain B is 40.\n",
    "                #how should we give this info to modeller... 1:A until B:40 problably. include \n",
    "                \n",
    "                start_stop_dict[temp_code] = ((start_struc_temp, stop_struc_temp, chains))\n",
    "    \n",
    "        start_struc_query, stop_struc_query = get_struc_stop_oligomer(path_to_pdb) \n",
    "        \n",
    "        start_stop_dict[pdb_code_name] = ((start_struc_query, stop_struc_query, pdb_id_chain))\n",
    "    \n",
    "    \n",
    "        #print(f\"{start_stop_dict=}\")\n",
    "        \n",
    "        #now the dict contains all paths and start stops.\n",
    "        \n",
    "        #this is the only shift we care about!\n",
    "    \n",
    "        #careful here.. shifts need to be taken into account FOR EACH CHAIN!\n",
    "        \n",
    "        shift = abs(1-start_struc_query)  #e.g 1- 8 abs means 7 shift.\n",
    "        \n",
    "        #if we find that the first end of gap corresponds to the start resi number of the pdb... we skip this gap.\n",
    "    \n",
    "        #would be better to check if one of the templates might cover this gap. But currently not implemented.\n",
    "    \n",
    "        #print(f\"{shift=}\")\n",
    "    \n",
    "    \n",
    "        #gap_list=[((96, 116), 'A')]\n",
    "        #print(f\"{gap_list=}\")\n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        gap_list_all_chains=[[(1, 5), (96, 116)], [(1, 5)], [(1, 5)], [(1, 5)], [(1, 5)]]\n",
    "        we enter oligomeric repair with: new_template_path='/home/micnag/test_modeller_oligomer/6hyr/6hyr.pdb',repairable_gaps=[((96, 116), 'A')], repair_templates_non_redundant={'/home/micnag/test_modeller_oligomer/6hz3_A.pdb'}\n",
    "        inside oligomeric_repair\n",
    "        Structure: /home/micnag/test_modeller_oligomer/6hz3_A.pdb selected as template\n",
    "        temp_codes=['6hz3_A'], temp_abs_paths=['/home/micnag/test_modeller_oligomer/6hz3_A.pdb'], ['A']\n",
    "        start_stop_dict=defaultdict(None, {'6hz3_A': (5, 315, 'A'), '6hyr': (5, 315, ['A', 'B', 'C', 'D', 'E'])})\n",
    "        shift=4\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        \n",
    "        \"\"\"CONTINUE HERE AFTER COURSE TO PROCEED WITH HETEROMERIC OLIGOMER REPAIRS.  NOV 17 2023  18:33 \"\"\"\n",
    "        \n",
    "    \n",
    "        for gaps, chains in gap_list:\n",
    "            #print(gaps, chains)\n",
    "            \n",
    "            found_N_terminus = False\n",
    "            found_C_terminus = False\n",
    "    \n",
    "            #can only concatenate str (not \"int\") to str#\n",
    "    \n",
    "            #    n_terminus = [x for x in range(gap_list[0][0], gap_list[0][1]+1)] #including the last residue.\n",
    "            #    c_terminus = [x for x in range(gap_list[-1][0], gap_list[-1][1]+1)] #including the last residue.\n",
    "        \n",
    "            #(96, 116) A\n",
    "    \n",
    "            if len(gaps) > 2: #means we have more than 1 gap. each gap = len 2\n",
    "                n_terminus = [x for x in range(gaps[0][0], gaps[0][1]+1)] #including the last residue.\n",
    "                c_terminus = [x for x in range(gaps[-1][0], gaps[-1][1]+1)] #including the last residue.\n",
    "            else:\n",
    "                n_terminus = None\n",
    "                c_terminus = None\n",
    "                #we dont deal with n or c terminus here if we only have 1 gap.\n",
    "    \n",
    "            #print(f\"this is {n_terminus=}\")\n",
    "            #print(f\"this is {c_terminus=}\")\n",
    "    \n",
    "            max_n_terminus_found = None\n",
    "            max_c_terminus_found = None\n",
    "    \n",
    "            #print(f\"works until here inside intelligent oligomeric repair!!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "            for paths, (start, stop, chain) in start_stop_dict.items():\n",
    "                # paths='6hz3_A',start=5, stop=315, chain='A'\n",
    "                # paths='6hyr',start=5, stop=315, chain=['A', 'B', 'C', 'D', 'E']\n",
    "                \n",
    "                #print(f\"{paths=},{start=}, {stop=}, {chain=}\")\n",
    "                #lets check for the n-terminus if we find something that can be repaired.\n",
    "                \n",
    "                if n_terminus:\n",
    "                    if start < n_terminus[-1]:\n",
    "                    \n",
    "                        #then we set n terminus found because there is something to repair\n",
    "                        found_N_terminus = True\n",
    "                    \n",
    "                        #but if we have no template... we cant repair beyond what we have and we stay with start.\n",
    "                        if max_n_terminus_found == None or start < max_n_terminus_found:\n",
    "                            max_n_terminus_found = start\n",
    "                            \n",
    "                #lets check for the c terminus in the same fashion\n",
    "                if c_terminus: \n",
    "                    if stop > c_terminus[0]:\n",
    "                    \n",
    "                        #that means we have something that goes beyond the max available struc len.\n",
    "                        found_C_terminus = True\n",
    "                    \n",
    "                        #but if we have no template, we still cant repair beyond.\n",
    "                        if max_c_terminus_found == None or stop > max_c_terminus_found:\n",
    "                            max_c_terminus_found = stop\n",
    "                    \n",
    "    \n",
    "    \n",
    "            #now lets check outside the loop\n",
    "    \n",
    "            if found_N_terminus == False and len(gaps) != 2: #means its not our only hit.\n",
    "    \n",
    "                if len(gaps) > 2:\n",
    "    \n",
    "                    gap_list = gaps[1:] #skip first tuple because we cant repair N terminus\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"We could not repair the only gap because we have no template. this is {gap_list=}\")         \n",
    "                    return\n",
    "            \n",
    "            if found_C_terminus == False and len(gaps) != 2:\n",
    "                #this means we have no suitable template and we skip it.\n",
    "                \n",
    "                if len(gap_list) > 2:\n",
    "                    \n",
    "                    gap_list = gap_list[:-1]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"We could not repair the only gap because we have no template. this is {gap_list=}\")\n",
    "                    return\n",
    "    \n",
    "            \n",
    "            #lets grab the max range of gaps in our structure.\n",
    "            if len(gaps) != 2:\n",
    "                max_gap_range = sorted([y - x for x, y in gaps], reverse=True)\n",
    "            else:\n",
    "                max_gap_range = [gaps[1] - gaps[0]] #simple 1 gap, list for downstream compatibility\n",
    "    \n",
    "    \n",
    "            if len(selected_templates) == 0 and max_gap_range[0] > 8:  \n",
    "                #this last part checks for the case we have\n",
    "                #no templates found but only small gaps of lenght < 8 and still want to repair.\n",
    "                print(\"no suitable templates found.\")\n",
    "                return\n",
    "    \n",
    "        #print(\"we went through this chaos.\")\n",
    "    \n",
    "        \"\"\"Modeller aln.salign treats the FIRST structure provided as QUERY. so we need to load this one FIRST.\"\"\"\n",
    "        #code to be passed to mdl.read\n",
    "    \n",
    "        #start struc query is already above computed at the beginning so we re use it again here.\n",
    "        #CONTINUE HERE\n",
    "    \n",
    "        \"\"\"model_segment specifies the range we look into. Ideally we look for start - stop based on majority vote. Chain is always the same in monomeric\"\"\"\n",
    "        mdl.read(file=pdb_code_name, model_segment=(f\"{start_struc_query}:{pdb_id_chain[0]}\", f\"{stop_struc_query}:{pdb_id_chain[-1]}\"))\n",
    "    \n",
    "        #pdb_code_name='6hyr',start_struc_query=5,pdb_id_chain[0]='A',stop_struc_query=315,pdb_id_chain[-1]='E'\n",
    "        #print(f\"{pdb_code_name=},{start_struc_query=},{pdb_id_chain[0]=},{stop_struc_query=},{pdb_id_chain[-1]=}\")\n",
    "        #append model object to alignment object.\n",
    "    \n",
    "        aln.append_model(mdl, align_codes=pdb_code_name, atom_files=pdb_code_name)\n",
    "    \n",
    "         \n",
    "        for codes, (start_temp, stop_temp, chains) in start_stop_dict.items():\n",
    "            #dont add our query again to the stack.\n",
    "            if codes == pdb_code_name:\n",
    "                continue\n",
    "    \n",
    "            #rest of templates...add to the stack.\n",
    "            mdl.read(file=codes, model_segment=(f\"{start_temp}:{chains[0]}\", f\"{stop_temp}:{chains[-1]}\"))\n",
    "            \n",
    "            aln.append_model(mdl, align_codes=codes, atom_files=codes)\n",
    "    \n",
    "    \n",
    "        #now add fasta sequence as last entry to the align model.\n",
    "        with open(f\"./{pdb_code_name}x.fasta\", \"w\") as fastaout:\n",
    "            fastaout.write(f\">{pdb_code_name}x\\n\")\n",
    "            fastaout.write(merged_fasta)\n",
    "    \n",
    "        aln_code = f\"{pdb_code_name}x\"\n",
    "    \n",
    "        #align fasta file to our alignment object which contains now a fasta sequence and a structure object.\n",
    "        aln.append(file=f\"./{pdb_code_name}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        #Overwrite MyModel and Inherit from AutoModel.\n",
    "        class MyModel(AutoModel):\n",
    "            def __init__(self, env, alnfile, knowns, sequence, gaps, chains, shift, **kwargs):\n",
    "                super(MyModel, self).__init__(env, alnfile, knowns, sequence, **kwargs)\n",
    "                self.gaps = gaps\n",
    "                self.chain = chains\n",
    "                self.shift = shift #shift because modeller always renumbers all stuff to be 1 based.\n",
    "                \n",
    "            def select_atoms(self):\n",
    "                selections = []\n",
    "                chain = self.chain\n",
    "                #this needs to be adjusted for multi chain modells.\n",
    "                #this is self.gaps: [((96, 116), 'A')]\n",
    "                #print(f\"this is self.gaps: {self.gaps}\")\n",
    "                for (start, end), chain in self.gaps:\n",
    "                    if start > 1:\n",
    "                        \n",
    "                        #print(f\"Selecting atoms for range {start-self.shift}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start-self.shift}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                        \n",
    "                    else:\n",
    "                        #print(f\"Selecting atoms for range {start}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "    \n",
    "                selected_atoms = Selection(*selections)\n",
    "                \n",
    "                # Combine all selections into a single Selection object\n",
    "                print(f\"Selected {len(selected_atoms)} atoms for optimization\")\n",
    "                \n",
    "                return selected_atoms\n",
    "    \n",
    "    \n",
    "        #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "        env.io.atom_files_directory = ['.','../.']\n",
    "        \n",
    "        #aln.malign3d(fit=True)\n",
    "    \n",
    "    \n",
    "        # Additional debugging: Print alignment content\n",
    "        #print(\"Alignment content before salign:\")\n",
    "        #for record in aln:\n",
    "            #print(record.code)\n",
    "    \n",
    "        #align sequence to structure.\n",
    "    \n",
    "        #overhang = 0 because we are confident the structures as templates are suitable candidates (seq id > 0.8)\n",
    "        # gap penalties are default.\n",
    "        # alignment_type = progressive : each template pairwise against query comparison.\n",
    "        # \n",
    "    \n",
    "        \n",
    "        aln.salign(overhang=0, gap_penalties_1d=(-450, -50), alignment_type=\"progressive\", output=\"ALIGNMENT\")\n",
    "    \n",
    "        #write out alignmentfile for automodell usage later\n",
    "        aln.write(file=f\"{pdb_code_name}.ali\")\n",
    "        \n",
    "        #add template codes to the code list.\n",
    "        alternate_templates = [x for x in temp_codes]\n",
    "    \n",
    "        # merge with our codes.\n",
    "        full_knowns = (pdb_code_name, *alternate_templates) #empty list from alternate_templates\n",
    "        \n",
    "        #print(f\"{full_knowns=}\")\n",
    "        #custom class\n",
    "    \n",
    "        # selected atoms do not feel the neighborhood\n",
    "        #env.edat.nonbonded_sel_atoms = 2\n",
    "    \n",
    "        #here we need to rechain again and make sure that this is what modeller sees.. e.g BCD Will need to be ABC\n",
    "    \n",
    "        pdb_original = None #this is considered false by default == 1 == False\n",
    "    \n",
    "        letters = [chr(ord('A') + i) for i in range(26)]\n",
    "        \n",
    "        \n",
    "        if pdb_id_chain != letters[0:len(pdb_id_chain)]:  # ABC for len 3 e.g\n",
    "            #print(f\"we are inside pdb_id_chain not fitting with modeller. : {pdb_id_chain=}\")\n",
    "            pdb_original = list(pdb_id_chain)\n",
    "            #we set it to A for repair.. but afterwards we swap it back!\n",
    "            pdb_id_chain = letters[0:len(pdb_id_chain)]\n",
    "    \n",
    "        #print(f\"{pdb_id_chain=}\")\n",
    "    \n",
    "        #print(\"works until here.\")\n",
    "    \n",
    "        a = MyModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code,\n",
    "                    gaps=gap_list, shift=shift, chains=pdb_id_chain)\n",
    "    \n",
    "    \n",
    "        #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "        #a = AutoModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code)\n",
    "    \n",
    "        a.starting_model = 1\n",
    "        a.ending_model = 1\n",
    "    \n",
    "        # Thorough MD optimization:\n",
    "        #a.md_level = refine.slow\n",
    "    \n",
    "        # Repeat the whole cycle 2 times and do not stop unless obj.func. > 1E6\n",
    "        #a.repeat_optimization = 2\n",
    "    \n",
    "        #env.libs.topology.make(aln)\n",
    "        #env.libs.parameters.read(file='$(LIB)/par.lib')\n",
    "        #mdl.generate_topology(aln[pdb_code_name])\n",
    "        #a.generate_topology(aln[pdb_code_name])\n",
    "        # Assign the average of the equivalent template coordinates to MODEL:\n",
    "        #a.transfer_xyz(aln) #lets try this.\n",
    "        \n",
    "        # Get the remaining undefined coordinates from internal coordinates:\n",
    "        #a.build(initialize_xyz=True, build_method='INTERNAL_COORDINATES')\n",
    "    \n",
    "        try:\n",
    "            # Build the model(s)\n",
    "            a.make();\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during modeling: {e}\")\n",
    "    \n",
    "        #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            #remove artifacts.\n",
    "            #remove_repair_artefacts(pdb_basep=path_to_pdb, pdb_code_name=pdb_code_name)\n",
    "    \n",
    "            #ok now we need to check what the updated start stop range is!\n",
    "    \n",
    "            if max_n_terminus_found and max_n_terminus_found < start_struc_query:\n",
    "                \n",
    "                keep_start = max_n_terminus_found\n",
    "                if not max_c_terminus_found:\n",
    "                    keep_stop = stop_struc_query\n",
    "                \n",
    "                    \n",
    "            if max_c_terminus_found and max_c_terminus_found > stop_struc_query:\n",
    "                keep_stop = max_n_terminus_found\n",
    "    \n",
    "                if not max_n_terminus_found:\n",
    "                    keep_start = start_struc_query\n",
    "                    \n",
    "            if not max_n_terminus_found and not max_c_terminus_found:\n",
    "                keep_start, keep_stop = start_struc_query, stop_struc_query\n",
    "            \n",
    "    \n",
    "            #here we can switch chain back to original if required:\n",
    "    \n",
    "            \n",
    "            #print(f\"{keep_start=}, {keep_stop=}\")\n",
    "            \n",
    "            #this should select only from the start to end and excludes potentially repaired N and C termini that would only introduce more noise.\n",
    "            select_c_alpha_and_correct_range(path_to_pdb, start=keep_start, stop=keep_stop)\n",
    "    \n",
    "            \n",
    "            \n",
    "            #not required!\n",
    "            #renumber_structure_monomeric(path_to_pdb, start=start_struc, chain=pdb_id_chain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _intelligent_monomeric_repair(self, repair_struc_gap_lst, temp_paths_tm_score_seq_id_template_gaps):\n",
    "    \n",
    "        \"\"\"\n",
    "        Function repairs structures with gaps less than 7 residues per gap.\n",
    "    \n",
    "        Args:\n",
    "        - template_dict: key = (pdb to be repaired, temp_gaps), vals = (path_for_template_structure,  (tm_score, seq_id, template_gaps)) \n",
    "\n",
    "        We need to make sure that the selection of residues used for repair does not fall into the region of template_gaps.\n",
    "\n",
    "        \n",
    "        Attention:\n",
    "    \n",
    "        Modeller INTERNALLY RECHAINS AND RENUMBERS EVERY QUERY.\n",
    "        ALL SINGLE CHAINS == A chain. irrespective of natural chain.. e.g if you supply chain B single chain it will go back to rechaining it to chain A.\n",
    "        Also dangerous... it will renumber structures from 1... so you need to correct for this shift based on your desired selection target. \n",
    "        If you structure starts with residue 30... and you want to remodell a gap between 40-50 you need to take this into account. shift = abs(1-start_struc_query)\n",
    "        \n",
    "        Output:\n",
    "        Repaired structures.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(repair_struc_gap_lst)\n",
    "        print(temp_paths_tm_score_seq_id_template_gaps)\n",
    "\n",
    "        #lets unpack everything properly.\n",
    "\n",
    "        pdb_id_target_path = repair_struc_gap_lst[0] #the path\n",
    "        pdb_id_target_gaps = repair_struc_gap_lst[1] #the list of gaps to be checked\n",
    "\n",
    "        template_paths = temp_paths_tm_score_seq_id_template_gaps[0] #the path to the templates\n",
    "        template_gaps = temp_paths_tm_score_seq_id_template_gaps[1][2] # the list of gaps in the templates\n",
    "\n",
    "\n",
    "        if len(pdb_id_target_gaps) == 0:\n",
    "            #means we have nothing to add.\n",
    "            return pdb_id_target_path \n",
    "        \n",
    "        log.none()  # no stdout spam\n",
    "        env = Environ()  # setup env for modelling\n",
    "        aln = Alignment(env)  # setup the alignment\n",
    "        mdl = Model(env)  # setup the model\n",
    "        #path : /home/micnag/bioinformatics/.../monomer/pos/2duk_A.pdb\n",
    "        # current working directory\n",
    "        #print(\"works until here step 1:\")\n",
    "        pdb_id_target = os.path.basename(pdb_id_target_path) #results in 2duk_A.pdb\n",
    "        pdb_id_chain = pdb_id_target[-5] #this corresponds to chain in single monomeric struc. We shall also pass this\n",
    "        #to our automodell derived child class.\n",
    "        #pdb_code_name is passed to modeller later.\n",
    "        pdb_code_name = pdb_id_target[:6] #2duk_A\n",
    "        pdb_4_digit_id = pdb_id_target[0:4]\n",
    "        #print(f\"this is {pdb_4_digit_id=}, {pdb_id_chain=}, {pdb_code_name=}\")\n",
    "        #print(\"works until here step 2:\")\n",
    "        #get uniprot_id second.\n",
    "        uniprot_id = self._return_uniprot_id_from_rcsb(pdb_4_digit_id)\n",
    "        #get associated fasta from uniprot id.\n",
    "        fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "    \n",
    "        #first we check what is the first ID in our struc. because otherwise we can miss structures that simply miss \n",
    "        #N terminus e.g gap 1-21 but they in fact start with residue 21 and are otherwise good or might just have small\n",
    "        # gaps to fix otherwhere in the structure.\n",
    "    \n",
    "\n",
    "        start_stop_dict = self._setup_and_prep_templates(template_paths)\n",
    "        start_struc_query, stop_struc_query = self._get_struc_stop(pdb_id_target_path) \n",
    "\n",
    "        #leave this as this adds the QUERY to the start_stop_dic\n",
    "        start_stop_dict[pdb_code_name] = ((start_struc_query, stop_struc_query, pdb_id_chain))\n",
    "\n",
    "        \n",
    "        #now the dict contains all paths and start stops.\n",
    "        #this is the only shift we care about!\n",
    "        shift = abs(1-start_struc_query)  #e.g 1- 8 abs means 7 shift.\n",
    "        \n",
    "        #if we find that the first end of gap corresponds to the start resi number of the pdb... we skip this gap.\n",
    "        #would be better to check if one of the templates might cover this gap. But currently not implemented.\n",
    "        #[(275, 284), (882, 889)]\n",
    "        #print(f\"this is {gap_list=}\")\n",
    "        \n",
    "        found_N_terminus, found_C_terminus, pdb_id_target_gaps = self._check_terminus_coverage(pdb_id_target_gaps, start_stop_dict)\n",
    "            \n",
    "        #print(f\"gaplist after cutting: {gap_list=}\")\n",
    "        \n",
    "        #lets grab the max range of gaps in our structure.\n",
    "        #max_gap_range = sorted([y - x for x, y in pdb_id_target_gaps], reverse=True)\n",
    "    \n",
    "        #check if we found suitable templates... IF NO and there are still only small gaps < 8 then we use the structure itself as template and still repair.\n",
    "        #if len(selected_templates) == 0 and max_gap_range[0] > 8:  \n",
    "            #this last part checks for the case we have\n",
    "            #no templates found but only small gaps of lenght < 8 and still want to repair.\n",
    "            #print(\"no suitable templates found.\")\n",
    "        #    return\n",
    "            \n",
    "        #elif len(selected_templates) == 0 and max_gap_range[0] < 8:\n",
    "            #nothing to do since we always append our own structure to repair ensemble. But in this case we can continue.\n",
    "            #print(\"we found gaps but they are small so we shall continue\")\n",
    "        #    pass\n",
    "        \n",
    "    \n",
    "        \"\"\"Modeller aln.salign treats the FIRST structure provided as QUERY. so we need to load this one FIRST.\"\"\"\n",
    "        #code to be passed to mdl.read\n",
    "    \n",
    "        #start struc query is already above computed at the beginning so we re use it again here.\n",
    "        #CONTINUE HERE\n",
    "    \n",
    "        \"\"\"model_segment specifies the range we look into. Ideally we look for start - stop based on majority vote. Chain is always the same in monomeric\"\"\"\n",
    "        mdl.read(file=pdb_code_name, model_segment=(f\"{start_struc_query}:{pdb_id_chain}\", f\"{stop_struc_query}:{pdb_id_chain}\"))\n",
    "        #append model object to alignment object.\n",
    "        aln.append_model(mdl, align_codes=pdb_code_name, atom_files=pdb_code_name)\n",
    "    \n",
    "        #now lets add all the templates.\n",
    "        #now we need to append all template strucs.\n",
    "        \n",
    "        for codes, (start_temp, stop_temp, chains) in start_stop_dict.items():\n",
    "            #dont add our query again to the stack.\n",
    "            if codes == pdb_code_name:\n",
    "                continue\n",
    "    \n",
    "            #rest of templates...add to the stack.\n",
    "            #print(f\"{start_temp}:{chains}\", f\"{stop_temp}:{chains}\")\n",
    "            mdl.read(file=codes, model_segment=(f\"{start_temp}:{chains}\", f\"{stop_temp}:{chains}\"))  \n",
    "            aln.append_model(mdl, align_codes=codes, atom_files=codes)\n",
    "    \n",
    "        #now add fasta sequence as last entry to the align model.\n",
    "        with open(f\"./{pdb_code_name}x.fasta\", \"w\") as fastaout:\n",
    "            fastaout.write(f\">{pdb_code_name}x\\n\")\n",
    "            fastaout.write(fasta_seq)\n",
    "    \n",
    "        aln_code = f\"{pdb_code_name}x\"\n",
    "        #align fasta file to our alignment object which contains now a fasta sequence and a structure object.\n",
    "        aln.append(file=f\"./{pdb_code_name}x.fasta\", align_codes=aln_code, alignment_format=\"fasta\")\n",
    "        \n",
    "        class MyModel(AutoModel):\n",
    "            def __init__(self, env, alnfile, knowns, sequence, gaps, chain, shift, **kwargs):\n",
    "                super(MyModel, self).__init__(env, alnfile, knowns, sequence, **kwargs)\n",
    "                self.gaps = gaps\n",
    "                self.chain = chain\n",
    "                self.shift = shift #shift because modeller always renumbers all stuff to be 1 based.\n",
    "                \n",
    "            def select_atoms(self):\n",
    "                selections = []\n",
    "                chain = self.chain\n",
    "    \n",
    "                #print(f\"this is self.gaps: {self.gaps}\")\n",
    "                for start, end in self.gaps:\n",
    "                    if start > 1:  #negative indx make problems so we start from 1 in this cases.\n",
    "                        #print(f\"Selecting atoms for range {start-self.shift}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start-self.shift}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                    else:\n",
    "                        #print(f\"Selecting atoms for range {start}:{chain} to {end-self.shift}:{chain}\")\n",
    "                        selection = self.residue_range(f'{start}:{chain}', f'{end-self.shift}:{chain}')\n",
    "                        selections.append(selection)\n",
    "                \n",
    "                selected_atoms = Selection(*selections)\n",
    "                # Combine all selections into a single Selection object\n",
    "                #print(f\"Selected {len(selected_atoms)} atoms for optimization\")\n",
    "                return selected_atoms\n",
    "    \n",
    "    \n",
    "        #setup environment dir for MODELLER. ACCEPTED current dir and previous dir.\n",
    "        env.io.atom_files_directory = [f'{self.work_dir}', f'../.']\n",
    "\n",
    "        #aln.malign3d(fit=True)\n",
    "        # Additional debugging: Print alignment content\n",
    "        #print(\"Alignment content before salign:\")\n",
    "        #for record in aln:\n",
    "            #print(record.code)\n",
    "        #align sequence to structure.\n",
    "        #overhang = 0 because we are confident the structures as templates are suitable candidates (seq id > 0.8)\n",
    "        # gap penalties are default.\n",
    "        # alignment_type = progressive : each template pairwise against query comparison.\n",
    "        #    \n",
    "        aln.salign(overhang=0, gap_penalties_1d=(-450, -50), alignment_type=\"progressive\", output=\"ALIGNMENT\")\n",
    "        #write out alignmentfile for automodell usage later\n",
    "        aln.write(file=f\"{pdb_code_name}.ali\")\n",
    "        #add template codes to the code list.\n",
    "        alternate_templates = [x for x in temp_codes]\n",
    "        # merge with our codes.\n",
    "        full_knowns = (pdb_code_name, *alternate_templates) #empty list from alternate_templates\n",
    "        #print(full_knowns)\n",
    "        #custom class\n",
    "        # selected atoms do not feel the neighborhood\n",
    "        #env.edat.nonbonded_sel_atoms = 2\n",
    "        #check chain if not A... take A and afterwards change it back to original chain. \n",
    "        pdb_original = None #this is considered false by default == 1 == False\n",
    "        if pdb_id_chain != \"A\":\n",
    "            #print(f\"we are inside pdb_id_chain != A: {pdb_id_chain=}\")\n",
    "            pdb_original = list(pdb_id_chain)\n",
    "            #we set it to A for repair.. but afterwards we swap it back!\n",
    "            pdb_id_chain = \"A\"\n",
    "    \n",
    "        a = MyModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code,\n",
    "                    gaps=pdb_id_target_gaps, shift=shift, chain=pdb_id_chain) #pdb_id_chain\n",
    "     \n",
    "\n",
    "        #print(f\"ALIGN_CODES(1) = {a.alignment_codes[0]}\")\n",
    "        #a = AutoModel(env, alnfile=f\"{pdb_code_name}.ali\", knowns=full_knowns, sequence=aln_code)\n",
    "    \n",
    "        a.starting_model = 1\n",
    "        a.ending_model = 1\n",
    "    \n",
    "        # Thorough MD optimization:\n",
    "        #a.md_level = refine.slow\n",
    "    \n",
    "        # Repeat the whole cycle 2 times and do not stop unless obj.func. > 1E6\n",
    "        #a.repeat_optimization = 2\n",
    "    \n",
    "        #env.libs.topology.make(aln)\n",
    "        #env.libs.parameters.read(file='$(LIB)/par.lib')\n",
    "        #mdl.generate_topology(aln[pdb_code_name])\n",
    "        #a.generate_topology(aln[pdb_code_name])\n",
    "        # Assign the average of the equivalent template coordinates to MODEL:\n",
    "        #a.transfer_xyz(aln) #lets try this.\n",
    "        \n",
    "        # Get the remaining undefined coordinates from internal coordinates:\n",
    "        #a.build(initialize_xyz=True, build_method='INTERNAL_COORDINATES')\n",
    "        \n",
    "        try:\n",
    "            # Build the model(s)\n",
    "            a.make();\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during modeling: {e}\")\n",
    "    \n",
    "        #this worked.. now we need to clean all files that are no longer required\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            #remove artifacts.\n",
    "            self._remove_repair_artefacts(pdb_basep=pdb_id_target_path, pdb_code_name=pdb_code_name)\n",
    "    \n",
    "            #ok now we need to check what the updated start stop range is!\n",
    "    \n",
    "            #print(f\"{max_n_terminus_found=}, {max_n_terminus_found < start_struc_query=}\")\n",
    "            \n",
    "            #print(f\"{max_c_terminus_found=}, {max_c_terminus_found > start_struc_query=}\")\n",
    "    \n",
    "            #print(f\"{start_struc_query=}\")\n",
    "            #print(f\"{stop_struc_query=}\")\n",
    "            \n",
    "            \"\"\"\n",
    "            max_n_terminus_found=-2, max_n_terminus_found < start_struc_query=True\n",
    "            max_c_terminus_found=994, max_c_terminus_found > start_struc_query=True\n",
    "            start_struc_query=1\n",
    "            stop_struc_query=992\n",
    "            keep_start=-2, keep_stop=-2\n",
    "            \"\"\"\n",
    "    \n",
    "            if pdb_original:\n",
    "                #print(f\"this is inside pdb_original {pdb_id_chain=}\")\n",
    "                #rechain back and set pdb_id_chain to original pdb chain.\n",
    "                pdb_id_chain = self._rechain_back(path_to_pdb=pdb_id_target_path, pdb_original=pdb_original)\n",
    "                \n",
    "            #should take both chains and pdb path.. then rechain back and return back the new chain.\n",
    "    \n",
    "    \n",
    "            keep_start = start_struc_query\n",
    "            keep_stop = stop_struc_query\n",
    "            \n",
    "            if max_n_terminus_found and max_n_terminus_found < start_struc_query:\n",
    "                keep_start = max_n_terminus_found\n",
    "                    \n",
    "            if max_c_terminus_found and max_c_terminus_found > stop_struc_query:\n",
    "                keep_stop = max_c_terminus_found\n",
    "            \n",
    "            #print(f\"{keep_start=}, {keep_stop=}\")\n",
    "            #this should select only from the start to end and excludes potentially repaired N and C termini that would only introduce more noise.\n",
    "            self._select_correct_range(pdb_id_target_path, start=keep_start, stop=keep_stop)\n",
    "    \n",
    "    \n",
    "            monomeric_chain = \"\".join(pdb_id_chain)\n",
    "            #print(f\"{monomeric_chain=}\")\n",
    "            #not required! ?? lets see later if it is .\n",
    "            self._renumber_structure_monomeric(pdb_id_target_path, start=keep_start, chain=pdb_id_chain)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        #if everyhing worked out we return the path!\n",
    "        return pdb_id_target_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _return_uniprot_id_from_rcsb(self, uniprot_id:str):\n",
    "    \n",
    "        link_path = \"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot\"\n",
    "        \n",
    "        searchp = f\"{link_path}/{uniprot_id}\"\n",
    "        #print(searchp)\n",
    "        resp = get_url(searchp)\n",
    "        resp = resp.json()\n",
    "        \n",
    "        for pdb_id, pdb_info in resp.items():\n",
    "            for uniprot_id, uniprot_info in pdb_info['UniProt'].items():\n",
    "                return uniprot_id\n",
    "\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "    \n",
    "        #print(\"we are in get gene fasta\")\n",
    "        \"this is already overworked. should work.\"\n",
    "        #uniprot_canonical_isoform = get_uniprot_id(uniprot_id=uniprot_id)\n",
    "        \n",
    "        fields = \"sequence\"\n",
    "        \n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        \n",
    "        seq = \"\"\n",
    "        \n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "                #print(lines)\n",
    "            i += 1\n",
    "        \n",
    "        #print(seq)\n",
    "        return seq\n",
    "\n",
    "\n",
    "\n",
    "    def _setup_and_prep_templates(self, template_paths)->dict:\n",
    "\n",
    "        #print(\"works until here step 3:\")\n",
    "        start_stop_dict = defaultdict()\n",
    "    \n",
    "        temp_codes = []\n",
    "        temp_abs_paths = []\n",
    "        temp_chains = []\n",
    "        \n",
    "        for structure in set(template_paths):  #no duplicates here.\n",
    "            #print(f\"Structure: {structure} selected as template\")\n",
    "            temp_codes.append(structure.split(\"/\")[-1][:-4])\n",
    "            temp_abs_paths.append(structure)\n",
    "            temp_chains.append(structure.split(\"/\")[-1][-5]) #this is the chain we need.\n",
    "    \n",
    "        for temp_code, temp_paths, chains in zip(temp_codes, temp_abs_paths, temp_chains):\n",
    "            if len(temp_code) <= 6:\n",
    "                #append all start stop and shifts here.\n",
    "                start_struc_temp, stop_struc_temp = get_struc_stop(temp_paths)\n",
    "                start_stop_dict[temp_code] = ((start_struc_temp, stop_struc_temp, chains))\n",
    "    \n",
    "        return start_stop_dict\n",
    "\n",
    "    \n",
    "    def _get_struc_stop(self, path_to_pdb):\n",
    "    \n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"none\", path_to_pdb)\n",
    "        seq_ids = [x.get_id()[1] for x in structure.get_residues()]\n",
    "        seq_ids = sorted(seq_ids)\n",
    "        return seq_ids[0], seq_ids[-1] #this corresponds to the last residue.\n",
    "\n",
    "\n",
    "    def _remove_repair_artefacts(self, pdb_basep, pdb_code_name):\n",
    "    \n",
    "        pattern = f\"{pdb_code_name}\"\n",
    "        #print(pattern)\n",
    "        # Remove original_****_*.ali file\n",
    "        ali_files = glob.glob(f\"{pattern}.ali\")\n",
    "        for file in ali_files:\n",
    "            os.remove(file)\n",
    "        # Remove original_****_A.pdb\n",
    "        old_pdb_file = f\"{pattern}.pdb\"\n",
    "        if os.path.exists(old_pdb_file):\n",
    "            os.remove(old_pdb_file)\n",
    "            \n",
    "        # Rename original_****_Ax.B99990001.pdb to original_****_A.pdb\n",
    "        repaired_pdb_file = f\"{pattern}x.B99990001.pdb\"\n",
    "        if os.path.exists(repaired_pdb_file):\n",
    "            new_pdb_file = repaired_pdb_file.replace('x.B99990001', '')\n",
    "            shutil.move(repaired_pdb_file, new_pdb_file)\n",
    "    \n",
    "        # Remove original_****_*x.D00000001\n",
    "        d_files = glob.glob(f\"{pattern}x.D00000001\")\n",
    "        for file in d_files:\n",
    "            os.remove(file)\n",
    "    \n",
    "        # Remove other files\n",
    "        extensions_to_remove = ['.fasta', '.ini', '.rsr', '.sch', '.V99990001']\n",
    "        for ext in extensions_to_remove:\n",
    "            files = glob.glob(f\"{pattern}x{ext}\")\n",
    "            for file in files:\n",
    "                os.remove(file)\n",
    "                \n",
    "\n",
    "    def _check_terminus_coverage(self, pdb_id_target_gaps, start_stop_dict):\n",
    "        \"\"\"\n",
    "        Checks if the N-terminus and C-terminus gaps in the target structure can be covered by any of the templates.\n",
    "    \n",
    "        Args:\n",
    "        - pdb_id_target_gaps: A list of tuples representing the gaps in the target structure.\n",
    "        - start_stop_dict: A dictionary with template start and stop residue numbers and chains.\n",
    "    \n",
    "        Returns:\n",
    "        - A tuple of booleans indicating whether suitable templates for the N-terminus and C-terminus were found.\n",
    "        - Updated pdb_id_target_gaps after excluding uncovered terminus gaps.\n",
    "        \"\"\"\n",
    "        found_N_terminus = False\n",
    "        found_C_terminus = False\n",
    "    \n",
    "        n_terminus_range = range(pdb_id_target_gaps[0][0], pdb_id_target_gaps[0][1] + 1)\n",
    "        c_terminus_range = range(pdb_id_target_gaps[-1][0], pdb_id_target_gaps[-1][1] + 1)\n",
    "    \n",
    "        max_n_terminus_found = None\n",
    "        max_c_terminus_found = None\n",
    "    \n",
    "        for _, (start, stop, _) in start_stop_dict.items():\n",
    "            if int(start) < n_terminus_range[-1]:\n",
    "                found_N_terminus = True\n",
    "                max_n_terminus_found = max(max_n_terminus_found, start) if max_n_terminus_found is not None else start\n",
    "    \n",
    "            if int(stop) > c_terminus_range[0]:\n",
    "                found_C_terminus = True\n",
    "                max_c_terminus_found = max(max_c_terminus_found, stop) if max_c_terminus_found is not None else stop\n",
    "    \n",
    "        # Adjust the gaps list based on the terminus coverage\n",
    "        if not found_N_terminus and len(pdb_id_target_gaps) > 1:\n",
    "            pdb_id_target_gaps = pdb_id_target_gaps[1:]\n",
    "        elif not found_N_terminus:\n",
    "            return False, False, []\n",
    "    \n",
    "        if not found_C_terminus and len(pdb_id_target_gaps) > 1:\n",
    "            pdb_id_target_gaps = pdb_id_target_gaps[:-1]\n",
    "        elif not found_C_terminus:\n",
    "            return False, False, []\n",
    "    \n",
    "        return found_N_terminus, found_C_terminus, pdb_id_target_gaps\n",
    "\n",
    "\n",
    "    def _rechain_back(self, path_to_pdb:str, pdb_original:list)-> None:\n",
    "\n",
    "        #first check if its multichain or single chain.\n",
    "        new_chains = \"\".join(pdb_original)\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"default\", path_to_pdb)\n",
    "\n",
    "        for model in structure:\n",
    "            for idx, chain in enumerate(model):\n",
    "                if chain.id != new_chains[idx]:\n",
    "                    chain.id = new_chains[idx]\n",
    "    \n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(path_to_pdb)\n",
    "        return pdb_original\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _select_correct_range(self, path:str, start:int, stop:int):\n",
    "    \n",
    "        #sel only c_alpha\n",
    "        class CAlphaOnlyInCorrectRange(Select):\n",
    "            def __init__(self, start, stop, *args):\n",
    "                super().__init__(*args)\n",
    "                self.start = start\n",
    "                self.stop = stop\n",
    "\n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] >= self.start and residue.id[1] <= self.stop else 0    \n",
    "            \n",
    "        #filelst    path\n",
    "        #5ltu_A.pdb /home/micnag/bioinformatics/rcsb_retrieved_pdbs/rcsb_fetched_structures/NUD4B_HUMAN\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        \n",
    "        #print(\"we are here\")\n",
    "        #print(fullpath)\n",
    "        structure = parser.get_structure(prot_name, path)\n",
    "        \n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(path, CAlphaOnlyInCorrectRange(start=start, stop=stop))\n",
    "\n",
    "\n",
    "\n",
    "    def _renumber_structure_monomeric(self, path_to_pdb:str, start:int, chain:str):\n",
    "\n",
    "        shiftres_location = f\"{self.work_dir}/pdb_shiftres_by_chain.py\"\n",
    "        bash_cmd = f\"python {shiftres_location} {path_to_pdb} {start-1} {chain}\"\n",
    "        bash_cmd_rdy = bash_cmd.split()\n",
    "        \n",
    "        with open(f\"{path_to_pdb}_tmp\", \"w\") as fh_tmp:\n",
    "            result = run(bash_cmd_rdy, stdout=fh_tmp, stderr=PIPE, \n",
    "                 universal_newlines=True)\n",
    "        \n",
    "        #now replace the original one with the temp file.\n",
    "        os.replace(f\"{path_to_pdb}_tmp\", f\"{path_to_pdb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "89962721-bbcd-4489-9b4a-72a2c0c15293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrincipalComponentAnalysis:\n",
    "\n",
    "    def __init__(self, script_dir, pca_dir, work_dir, msa_fasta,multiseq_dict=None,store_original=True):\n",
    "        self.work_dir = work_dir\n",
    "        self.script_dir = script_dir\n",
    "        self.store_full_atomistic = store_original\n",
    "        self.stored_dir = None\n",
    "        self.seqalignment_dict = multiseq_dict\n",
    "        self.seqalignment_fasta = msa_fasta\n",
    "        self.prepared_dir = None\n",
    "        self.multi_pdb_codes = None\n",
    "        self.reference_pdb = None\n",
    "        self.pca_dir = pca_dir\n",
    "\n",
    "    \n",
    "    def _copy_original_pdbs(self):\n",
    "        if self.store_full_atomistic:\n",
    "            # List all PDB files in the working directory\n",
    "            pdbs = [f for f in os.listdir(self.work_dir) if os.path.isfile(os.path.join(self.work_dir, f)) and f.endswith(\".pdb\")]\n",
    "        \n",
    "            # Define the directory where PDB files will be stored\n",
    "            store_dir = os.path.join(self.work_dir, \"full_pdb_models\")\n",
    "            # Create the directory if it doesn't exist\n",
    "            if not os.path.exists(store_dir):\n",
    "                os.mkdir(store_dir)\n",
    "            # Copy each PDB file to the storage directory\n",
    "            for pdb in pdbs:\n",
    "                src = os.path.join(self.work_dir, pdb)  # Source file path\n",
    "                dst = os.path.join(store_dir, pdb)  # Destination file path\n",
    "                shutil.copy(src, dst)  # Copy file\n",
    "            # Save the location where files were stored\n",
    "            self.stored_dir = store_dir\n",
    "            # Optionally, return the path to the directory where files were stored\n",
    "            #return self.stored_dir\n",
    "\n",
    "\n",
    "    \n",
    "    def prepare_ensemble(self, strict_cutting=True):\n",
    "        #here we run single directory wise PCA.\n",
    "        #first save full original pdbs:\n",
    "        self._copy_original_pdbs()\n",
    "        \n",
    "        #1 we select CA only. / we obtain info about potential non canonical aas.\n",
    "        pdbs = [os.path.join(self.work_dir,f) for f in os.listdir(self.work_dir) if os.path.isfile(os.path.join(self.work_dir, f)) and f.endswith(\".pdb\")]\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            remove_het_results = list(executor.map(self._select_c_alpha_and_check_non_canonical, pdbs))\n",
    "        \n",
    "        #2 remove non canonical aas.\n",
    "        for non_canonical, pdb_path in remove_het_results:\n",
    "            if non_canonical:\n",
    "                for keys, vals in non_canonical.items():\n",
    "                    self._mutate_non_standard_aa(pdb_path, non_standard_residue=vals[0], residue=keys, chain=vals[1])\n",
    "\n",
    "        #now we leverage the seq alignment from USAlign.\n",
    "        if strict_cutting:\n",
    "            #print(\"we move into strict cutting\")\n",
    "            residues_to_keep = self._hard_trimmer(self.seqalignment_fasta)\n",
    "            \n",
    "            save_dir = os.path.join(self.work_dir, \"trimmed_strucs\")\n",
    "            print(f\"{save_dir=}\")\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.mkdir(save_dir)\n",
    "\n",
    "            #this will be used to run PCA.\n",
    "            self.prepared_dir = save_dir\n",
    "            \n",
    "            self._select_proper_residues_after_trim(residues_to_keep, save_dir)\n",
    "\n",
    "            #print(residues_to_keep)\n",
    "\n",
    "        else:\n",
    "            #these strucs need to be removed.\n",
    "            strucs_to_remove = self._soft_trimmer(self.seqalignment_fasta)\n",
    "            #and then we rerun usalign.\n",
    "            self._hard_trimmer(self.seqalignment_fasta)\n",
    "            \n",
    "    \n",
    "    def _soft_trimmer(self, result_fasta:str):\n",
    "        \n",
    "        #maybe better idea is instead of removing so harshly to set a treshold. if its a long prot seq \n",
    "        #we dont want to kick out stuff that introduces 5 gaps e.g serca because mean is already 2... so very low..\n",
    "        #in this case we are perfectly fine in introducing a gap of up to 10% of total len\n",
    "        gap_counter = defaultdict()\n",
    "        ali = pytrimal.Alignment.load(result_fasta)\n",
    "        \n",
    "        trimmer = pytrimal.AutomaticTrimmer(\"gappyout\")\n",
    "        trimmed = trimmer.trim(ali)\n",
    "        \n",
    "        for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "            structure_name = name.decode().rjust(6)\n",
    "            gap_count = seq.count(\"-\")\n",
    "            #print(seq)\n",
    "            gap_counter[structure_name] = gap_count\n",
    "    \n",
    "        \n",
    "        gap_values = list(gap_counter.values())\n",
    "        mean_gap_count = statistics.mean(gap_values)\n",
    "        median_gap_count = statistics.median(gap_values)\n",
    "    \n",
    "        print(mean_gap_count)\n",
    "        print(median_gap_count)\n",
    "        high_gap_count_sequence_ids = [seq_id for seq_id, gap_count in gap_counter.items() if gap_count > mean_gap_count]\n",
    "    \n",
    "        print(high_gap_count_sequence_ids)\n",
    "        \n",
    "        return high_gap_count_sequence_ids\n",
    "    \n",
    "\n",
    "    def _hard_trimmer(self, result_fasta:str):\n",
    "\n",
    "        keep_res_dict = defaultdict()\n",
    "        ali = pytrimal.Alignment.load(result_fasta)\n",
    "        #trimm all gaps unless this leaves 10% of the original alignment.\n",
    "        trimmer = pytrimal.ManualTrimmer(gap_threshold=1) #changed to 80% now as test.\n",
    "        trimmed = trimmer.trim(ali)\n",
    "        for name, seq in zip(trimmed.names, trimmed.sequences):\n",
    "            keep_res_dict[name.decode().rjust(6)] = seq\n",
    "\n",
    "        return keep_res_dict\n",
    "\n",
    "    \n",
    "\n",
    "    def _select_proper_residues_after_trim(self, keep_residue_dict:dict, save_dir:dir):\n",
    "        \n",
    "        for pdb, seq in keep_residue_dict.items():\n",
    "            #seq in keep_residue_dict is extracted from hard trimmer.\n",
    "        \n",
    "            full_p = os.path.join(self.work_dir, pdb)\n",
    "            result_seq = self._grab_sequence_from_struc(full_p)\n",
    "            positions_to_keep = self._find_positions(result_seq[0], seq)\n",
    "            #print(f\"this is seq to keep: {seq}, this is full_seq from structure: {result_seq}\")\n",
    "            #print(f\"this is our savepath for cutted struc: {full_p}\")\n",
    "            self._select_trimmed_positions(path_to_pdb=full_p, list_to_keep=positions_to_keep, save_dir=save_dir)\n",
    "            \n",
    "\n",
    "    def _select_trimmed_positions(self, path_to_pdb:str, list_to_keep:str, save_dir):\n",
    "        #sel only c_alpha\n",
    "        \n",
    "        class CAlphaTrimmedPositions(Select):\n",
    "            def __init__(self, list_to_keep_shifts, *args):\n",
    "                super().__init__(*args)\n",
    "                self.list_to_keep_shifts = list_to_keep_shifts\n",
    "                \n",
    "            #overload accept_residue inherited from Select with this conditional return\n",
    "            def accept_atom(self, atom):\n",
    "                return 1 if atom.id == \"CA\" else 0\n",
    "            \n",
    "            #overloaded to only accept positive residue numbering.\n",
    "            def accept_residue(self, residue):      \n",
    "                return 1 if residue.id[1] in self.list_to_keep_shifts else 0    \n",
    "\n",
    "        #print(f\"{path_to_pdb=}, {list_to_keep=}\")\n",
    "\n",
    "        #lets make a new dir where we store the trimmed strucs.\n",
    "        pdb_name = os.path.basename(path_to_pdb)\n",
    "        new_location = os.path.join(save_dir, pdb_name) #here we save\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        prot_name = f\"default\"\n",
    "        structure = parser.get_structure(prot_name, path_to_pdb)\n",
    "        shift = [x.get_id()[1] for x in structure.get_residues()][0] #first residue number = shift\n",
    "        list_to_keep_shifted = [x+shift-1 for x in list_to_keep] #correct for shift. lets try with shift\n",
    "        #print(f\"this is structure: {path_to_pdb} for list to keep shifted: {list_to_keep_shifted}\")\n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(new_location, CAlphaTrimmedPositions(list_to_keep_shifts=list_to_keep_shifted))\n",
    "    \n",
    "\n",
    "    def _find_positions(self, full_sequence, partial_sequence):\n",
    "        i = 0  # Index for full_sequence\n",
    "        j = 0  # Index for partial_sequence\n",
    "        found_positions = []\n",
    "    \n",
    "        while i < len(full_sequence) and j < len(partial_sequence):\n",
    "            if partial_sequence[j] == '-':  # Skip gap positions in partial_sequence\n",
    "                j += 1\n",
    "            elif partial_sequence[j] == full_sequence[i]:  # Match found\n",
    "                found_positions.append(i)\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1  # Advance in full_sequence if no match\n",
    "    \n",
    "        found_pos = [x + 1 for x in found_positions]  # Adjust for 1-based indexing\n",
    "        return found_pos\n",
    "\n",
    "    \n",
    "\n",
    "    def _grab_sequence_from_struc(self, pdb):\n",
    "\n",
    "        lst =  [('VAL',\"V\"), ('ILE',\"I\"), ('LEU',\"L\"), ('GLU',\"E\"), ('GLN',\"Q\"),\n",
    "                        ('ASP',\"D\"), ('ASN',\"N\"), ('HIS',\"H\"), ('TRP',\"W\"), ('PHE',\"F\"), ('TYR',\"Y\"), \n",
    "                        ('ARG',\"R\"), ('LYS',\"K\"), ('SER',\"S\"), ('THR',\"T\"), ('MET',\"M\"), ('ALA',\"A\"), \n",
    "                        ('GLY',\"G\"), ('PRO',\"P\"), ('CYS',\"C\")]\n",
    "        \n",
    "        canonical_aas = defaultdict(lambda: \"X\", lst)\n",
    "    \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        pdb_name = os.path.basename(pdb)\n",
    "        prot_name = f\"default\"\n",
    "    \n",
    "        structure = parser.get_structure(prot_name, pdb)\n",
    "        struc_seq = [canonical_aas[x.get_resname()] for x in structure.get_residues()]\n",
    "        struc_seq = \"\".join(struc_seq)\n",
    "    \n",
    "        return (struc_seq, pdb_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def run_PCA(self):\n",
    "\n",
    "        if not self.prepared_dir:\n",
    "            return\n",
    "\n",
    "        self.multi_pdb_codes, self.reference_pdb = self._create_multi_pdb()\n",
    "        \n",
    "        print(f\"{self.multi_pdb_codes=}, {self.reference_pdb=}\")\n",
    "        if self.multi_pdb_codes and self.reference_pdb:\n",
    "            #now lets run it.\n",
    "            print(\"we shuffle and go\")\n",
    "            self._shuffle_exe_and_run_pca()\n",
    "        \n",
    "    \n",
    "    def _create_multi_pdb(self, reference=None):\n",
    "        \n",
    "        #this function acts as an helper function in pca_laura_pipeline_1\n",
    "        pdb_files = [f for f in os.listdir(self.prepared_dir) if f.endswith(\".pdb\")]\n",
    "        #lets do first a check of length so we only keep strucs that are pca-able.\n",
    "        count_dict = Counter()\n",
    "        for file in pdb_files:\n",
    "            location = os.path.join(self.prepared_dir, file)\n",
    "            prot_name = file[:-4]\n",
    "            try:\n",
    "                structure = PDBParser(QUIET=True).get_structure(prot_name, location)\n",
    "                struc_len = len([x for x in structure.get_residues()])\n",
    "                count_dict[struc_len] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "        most_common_lengths = count_dict.most_common()\n",
    "        most_common_length = most_common_lengths[0][0]\n",
    "        print(most_common_length)\n",
    "        ms = Structure.Structure(\"master\")\n",
    "        i = 0\n",
    "        \n",
    "        list_of_pdb_codes = []\n",
    "        for file in pdb_files:\n",
    "            location = os.path.join(self.prepared_dir, file)\n",
    "            prot_name = file[:-4]\n",
    "    \n",
    "            try:\n",
    "                structure = PDBParser(QUIET=True).get_structure(prot_name, location)\n",
    "                struc_len = len([x for x in structure.get_residues()])\n",
    "    \n",
    "                if struc_len != most_common_length:\n",
    "                    print(f\"This structure has wrong amount of residues: {location=} {struc_len=} instead of {most_common_length=}\")\n",
    "                    #dont proceed in this case.\n",
    "                    continue    \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            list_of_pdb_codes.append(file)\n",
    "            for model in structure:\n",
    "                new_model = model.copy()\n",
    "                new_model.id = i\n",
    "                new_model.serial_num = i + 1\n",
    "    \n",
    "                # Iterate through atoms and set B-factor to 0 because this causes issues with pcatoolS\n",
    "                for chain in new_model:\n",
    "                    for residue in chain:\n",
    "                        for atom in residue:\n",
    "                            atom.set_bfactor(0)\n",
    "    \n",
    "                i += 1\n",
    "                ms.add(new_model)\n",
    "    \n",
    "        pdb_io = PDBIO()\n",
    "        pdb_io.set_structure(ms)\n",
    "        pdb_io.save(f\"{self.prepared_dir}/multi_ensemble.pdb\")\n",
    "\n",
    "        if reference:\n",
    "            path_of_ref = None\n",
    "        else:\n",
    "            path_of_ref = os.path.join(self.prepared_dir, list_of_pdb_codes[0])\n",
    "\n",
    "        \n",
    "        return list_of_pdb_codes, path_of_ref \n",
    "\n",
    "\n",
    "    def _move_executables(self, basepath, work_dir):\n",
    "        #this helper function is required for domenico and laura stuff to work.\n",
    "        os.chdir(work_dir) #i dislike this solution but its fine\n",
    "        try:\n",
    "            files_to_move = [f for f in os.listdir(basepath) if os.path.isfile(os.path.join(basepath, f))]\n",
    "            for file in files_to_move:\n",
    "                shutil.copy(os.path.join(basepath, file), work_dir)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _shuffle_exe_and_run_pca(self, protein=\"test\"):\n",
    "\n",
    "        #THIS IS TERRIBLE BUT ONLY LIKE THAT PCATOOLS WORKS\n",
    "        self._move_executables(self.pca_dir, self.prepared_dir)\n",
    "        template = os.path.basename(self.reference_pdb) # this is the template pdb file name\n",
    "        \n",
    "        #_move_executables(\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/PCA_pipeline\", work_dir)\n",
    "        # Part 1: PCA\n",
    "        bash_cmd1 = f\"./getpca.sh multi_ensemble.pdb {template} refpdb_ref\"\n",
    "        \n",
    "        print(f\"this is bashcmd 1: {bash_cmd1}\")\n",
    "        self._run_command(bash_cmd1, \"Part 1\")\n",
    "        \n",
    "        # Part 2: Convert evec files\n",
    "        bash_cmd2 = f\"./convert.sh refpdb_ref_pca.evec\"\n",
    "    \n",
    "        \n",
    "        print(f\"this is bashcmd 2: {bash_cmd2}\")\n",
    "        self._run_command(bash_cmd2, \"Part 2\")\n",
    "    \n",
    "        # Part 3: Projections\n",
    "        bash_cmd3 = f\"./getproj.sh {template} refpdb_ref_pca.evec {protein} multi_ensemble.pdb\"\n",
    "        \n",
    "        print(f\"this is bashcmd 3: {bash_cmd3}\")\n",
    "        self._run_command(bash_cmd3, \"Part 3\")\n",
    "        \n",
    "\n",
    "\n",
    "    def _run_command(self, bash_cmd, part_name):\n",
    "        try:\n",
    "            result = run(bash_cmd.split(), stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            print(f\"{part_name} output:\\n{result.stdout}\")\n",
    "            print(f\"{part_name} errors:\\n{result.stderr}\")\n",
    "        except Exception:\n",
    "            print(f\"Script did not work. Parameters: {bash_cmd.split()}\")\n",
    "\n",
    "    \n",
    "    def _select_c_alpha_and_check_non_canonical(self, pdb_path):\n",
    "\n",
    "        #sel only c_alpha\n",
    "        class CAlphaOnlyInCorrectRange(Select):\n",
    "\n",
    "            #overload accept_residue inherited from Select with this conditional return to save only CA\n",
    "            def accept_atom(self, atom):\n",
    "                return 1 if atom.id == \"CA\" else 0\n",
    "\n",
    "            def accept_residue(self, residue):\n",
    "                return 1 if residue.id[0] == \" \" else 0\n",
    "\n",
    "\n",
    "        non_canonical_aas = defaultdict()\n",
    "\n",
    "        canonical_aas = {'VAL', 'ILE', 'LEU', 'GLU', 'GLN',\n",
    "                     'ASP', 'ASN', 'HIS', 'TRP', 'PHE', 'TYR',\n",
    "                     'ARG', 'LYS', 'SER', 'THR', 'MET', 'ALA',\n",
    "                     'GLY', 'PRO', 'CYS'}\n",
    "        \n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure('default', pdb_path)\n",
    "\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                for residue in chain:\n",
    "                    if residue.id[0] == \" \": \n",
    "                        curr_res = residue.get_resname()\n",
    "                        curr_pos = residue.get_id()[1]\n",
    "\n",
    "                        if curr_res not in canonical_aas:\n",
    "                            non_canonical_aas[curr_pos] = (curr_res, chain.get_id())\n",
    "\n",
    "            \n",
    "        # Select C-alpha atoms and save the modified structure\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        #print(f\"we save {pdb_path=}\")\n",
    "        io.save(pdb_path, CAlphaOnlyInCorrectRange())\n",
    "\n",
    "        return non_canonical_aas, pdb_path\n",
    "\n",
    "\n",
    "    def _count_unique_values_in_dict(self, input_dict):\n",
    "        start_counts = Counter(val[0] for val in input_dict.values())\n",
    "        stop_counts = Counter(val[1] for val in input_dict.values())\n",
    "        return start_counts, stop_counts\n",
    "\n",
    "    \n",
    "    def _mutate_non_standard_aa(self, path_to_pdb):\n",
    "        \"\"\"Mutates a non-standard amino acid in a PDB file.\"\"\"\n",
    "        \n",
    "        path_to_script = f\"{self.script_dir}/pdb_mutate.py\"\n",
    "        path_to_error_script = f\"{self.script_dir}/pdb_delresname.py\"\n",
    "        path_to_tidy = f\"{self.script_dir}/pdb_tidy.py\"\n",
    "\n",
    "        input_file = f\"{path_to_pdb}_new\" #temp name to not overwrite files we currently read from\n",
    "        try:\n",
    "            with open(input_file, \"w\") as pdb_out:\n",
    "                bash_code = f\"python {path_to_script} {path_to_pdb} {chain} {residue} {non_standard_residue} ALA\"\n",
    "                run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "                \n",
    "            resulting_success = True\n",
    "    \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            print(\"Attempting to delete non-standard residue...\")\n",
    "            \n",
    "            with open(input_file, \"w\") as pdb_out:\n",
    "                bash_code = f\"python {path_to_error_script} -{non_standard_residue} {path_to_pdb}\"\n",
    "                run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "    \n",
    "        #now we tidy the file to adhere to the most common pdb standard\n",
    "        try:\n",
    "            with open(path_to_pdb, \"w\") as pdb_out:\n",
    "                bash_code = f\"python {path_to_tidy} {input_file}\"\n",
    "                run(bash_code.split(), stdout=pdb_out, stderr=PIPE, universal_newlines=True)\n",
    "                \n",
    "            #print(\"Cleaned the file. Outfile is at\", path_to_pdb)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "    \n",
    "        #we dont need the intermediate file that was only created to prevent read/write from same file.\n",
    "        try:\n",
    "            os.remove(input_file)  # Remove intermediate file\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "        return resulting_success\n",
    "\n",
    "\n",
    "    def _get_struc_stop(self, path_to_pdb):\n",
    "\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"none\", path_to_pdb)\n",
    "        seq_ids = [x.get_id()[1] for x in structure.get_residues()]\n",
    "        seq_ids = sorted(seq_ids)\n",
    "        return seq_ids[0], seq_ids[-1] #this corresponds to the last residue.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c52894-a149-4aca-8a54-502ba3784c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c90d2-edd3-4be3-8f57-4dbdd3942fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930e257-3936-493e-8403-7f588f530e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932ef9e-bd31-4b78-b032-8f0fd251f270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b2b3f-ce20-419f-975c-7a557cc3c8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9d0f5-5a1c-4885-a855-ae2646a7c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bda26-3394-40a8-b6e4-00360fce9c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c7472-aaed-41e6-8a9b-a27fb4492c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dbf07-2989-42fd-a806-0e226a51f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5b551-f8b4-4fae-acf6-631e935ea35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a8d1e-f1bb-4163-ac82-44fb0b82a786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8356b1-99e5-4718-8677-91b96ab98faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a8f85b3-1994-433e-9ebe-ff92658da97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = ['5ltu_A', '5ltu_B', '7nnj_A', '7nnj_B', '2duk_A', '2duk_B', '3mcf_A', '3mcf_B', '7tn4_A', '2q9p_A', '2fvv_A', '6pck_A', '6pcl_A', '6wo7_A', '6wo8_A', '6wo9_A', '6woa_A', '6wob_A', '6woc_A', '6wod_A', '6woe_A', '6wof_A', '6wog_A', '6woh_A', '6woi_A', '7aut_A', '7aui_A', '7auk_A', '7aul_A', '7aum_A', '7aun_A', '7auo_A', '7aup_A', '7auq_A', '7aur_A', '7aus_A', '7auu_A', '7auj_A', '3h95_A', '3i7u_A', '3i7u_B', '3i7u_C', '3i7u_D', '3i7v_A', '3i7v_B', '4hfq_A', '4hfq_B']\n",
    "\n",
    "seq_id = [0.96, 0.96, 0.956, 0.956, 0.967, 0.967, 0.952, 0.952, 0.787, 0.787, 0.787, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.4, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.395, 0.419, 0.388, 0.295, 0.438, 0.438, 0.438, 0.438, 0.438, 0.438, 0.355, 0.355]\n",
    "query_start = [1, 1, 7, 7, 9, 9, 18, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 29, 30, 30, 30, 30, 30, 30, 31, 31]\n",
    "query_end = [181, 181, 147, 147, 147, 147, 145, 145, 157, 157, 157, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 85, 91, 128, 82, 82, 82, 82, 82, 82, 97, 97]\n",
    "temp_start = [1, 1, 1, 1, 1, 1, 2, 2, 1, 23, 23, 1, 1, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 36, 13, 13, 13, 13, 13, 13, 78, 78]\n",
    "temp_end = [180, 180, 140, 140, 138, 138, 129, 129, 156, 178, 178, 148, 148, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 102, 108, 139, 63, 63, 63, 63, 63, 63, 148, 148]\n",
    "\n",
    "\n",
    "work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline\"\n",
    "\n",
    "\n",
    "# in the near future: supply here with a uniprot id! and then fetch from there the conservation\n",
    "Downloader = DownloadPipe(templates=templates, work_dir=work_dir, seq_id=seq_id)\n",
    "Downloader.paralellized_download()\n",
    "Downloader.retrieve_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa88d07a-a796-4ebb-9fd0-ff73a334d9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shannon</th>\n",
       "      <th>relative</th>\n",
       "      <th>lockless</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.064300</td>\n",
       "      <td>4.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.064300</td>\n",
       "      <td>4.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.603282</td>\n",
       "      <td>2.603282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.093375</td>\n",
       "      <td>2.296558</td>\n",
       "      <td>4.049312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.603282</td>\n",
       "      <td>2.603282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.954259</td>\n",
       "      <td>2.954259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.782409</td>\n",
       "      <td>2.782409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.782409</td>\n",
       "      <td>2.782409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.673012</td>\n",
       "      <td>2.148936</td>\n",
       "      <td>3.155803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.669422</td>\n",
       "      <td>2.669422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      shannon  relative  lockless\n",
       "0   -0.000000  4.064300  4.064300\n",
       "1   -0.000000  4.064300  4.064300\n",
       "2   -0.000000  2.603282  2.603282\n",
       "3    1.093375  2.296558  4.049312\n",
       "4   -0.000000  2.603282  2.603282\n",
       "..        ...       ...       ...\n",
       "175 -0.000000  2.954259  2.954259\n",
       "176 -0.000000  2.782409  2.782409\n",
       "177 -0.000000  2.782409  2.782409\n",
       "178  0.673012  2.148936  3.155803\n",
       "179 -0.000000  2.669422  2.669422\n",
       "\n",
       "[180 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PdbEnsemble_meta = Downloader.meta_dict\n",
    "PdbEnsemble_chains_seq_id = Downloader.chain_seqid_dict\n",
    "\n",
    "Downloader.conservation(uniprot_id=\"Q9NZJ9\")\n",
    "#shifts = PdbEnsemble.parallel_shift_calculation()\n",
    "Downloader.conservation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43495d81-2216-4a19-82ba-e0140cfceaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline\n"
     ]
    }
   ],
   "source": [
    "print(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc3e2a62-c0e1-4f3c-a414-72b8b922d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_Cleaner = PDBCleaning(work_dir=work_dir, meta_dict=PdbEnsemble_meta, chain_seq_dict=PdbEnsemble_chains_seq_id)\n",
    "PDB_Cleaner.setup_cutoff(cutoff=3, apply_filter=True)  #apply filter to only include structures that are of good quality\n",
    "#PDB_Cleaner.parallel_shift_calculation()  # compute shift for each structure\n",
    "#PDB_Cleaner.parallel_renumbering()  # renumber based on shifts.\n",
    "#PDB_cleaned_ensemble.chain_dict\n",
    "\n",
    "#PDB_Cleaner.filtered_structures\n",
    "struct = PDB_Cleaner.filtered_structures\n",
    "#struct = PDB_Cleaner.get_unfiltered_strucs()\n",
    "#chain_seqid = PDB_Cleaner.chain_seq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a74e32ee-4977-414c-b264-ac0d0490ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7auq', '6pcl', '3i7u', '7aur', '6wof', '7tn4', '7auo', '3h95', '6pck', '6woh', '3mcf', '6wod', '2duk', '6woi', '7aun', '3i7v', '7nnj', '7aui', '7auj', '6woc', '5ltu', '7aus', '6wo7', '6woe', '7aul', '6wo8', '2q9p', '7auk', '6woa', '6wo9', '6wog', '7auu', '7aum', '4hfq', '6wob', '7aup', '7aut', '2fvv']\n"
     ]
    }
   ],
   "source": [
    "print(struct) #get_unfiltered_strucs() misses out on all files with multi chains\n",
    "#['3i7v_A', '6woh_A', '6wog_A', '4hfq_AB', '6wob_A', '3h95_AB', '7aui_A', '6woe_A', '6pcl_A', '2q9p_A', '7aul_A', '6wod_A', '6wo8_A', '6woa_A', '6wo9_A', '6wof_A', '3i7u_ABCD', '6pck_A', '7aup_A', '7auk_A', '7aus_A', '7aut_A', '2fvv_A', '6wo7_A', '7nnj_A', '7tn4_A', '6woi_A', '7aun_A', '6woc_A', '3mcf_A', '7aur_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c1b7c97-8a19-4cba-a3f6-e29c1e2ff3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chain_seqid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f206e949-8850-4e5a-85a5-fd69b6f90125",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_Builder = PDBBuilder(work_dir=work_dir, structures=struct, remove_intermediates=True) #structures that are filtered\n",
    "PDB_Builder.build_assembly()\n",
    "oligo_dict = PDB_Builder.oligodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8c60a70-471f-4a6e-8425-f9c3a3c7cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'7auq_A.pdb': 1, '6pcl_A.pdb': 1, '3i7u_ABCD.pdb': 4, '7aur_A.pdb': 1, '6wof_A.pdb': 1, '7tn4_A.pdb': 1, '7auo_A.pdb': 1, '3h95_AB.pdb': 2, '6pck_A.pdb': 1, '6woh_A.pdb': 1, '3mcf_A.pdb': 1, '6wod_A.pdb': 1, '2duk_A.pdb': 1, '6woi_A.pdb': 1, '7aun_A.pdb': 1, '3i7v_A.pdb': 1, '7nnj_A.pdb': 1, '7aui_A.pdb': 1, '7auj_A.pdb': 1, '6woc_A.pdb': 1, '5ltu_AB.pdb': 2, '7aus_A.pdb': 1, '6wo7_A.pdb': 1, '6woe_A.pdb': 1, '7aul_A.pdb': 1, '6wo8_A.pdb': 1, '2q9p_A.pdb': 1, '7auk_A.pdb': 1, '6woa_A.pdb': 1, '6wo9_A.pdb': 1, '6wog_A.pdb': 1, '7auu_A.pdb': 1, '7aum_A.pdb': 1, '4hfq_AB.pdb': 2, '6wob_A.pdb': 1, '7aup_A.pdb': 1, '7aut_A.pdb': 1, '2fvv_A.pdb': 1}\n"
     ]
    }
   ],
   "source": [
    "print(oligo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c96d2633-96a6-4a40-bc36-b451a7d5b9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No seqid found for 3h95 chain B\n"
     ]
    }
   ],
   "source": [
    "Preper = PDBEnsemblePrep(work_dir=work_dir, oligo_dict=oligo_dict, chain_seq_dict=chain_seqid, main_prot_seq=None)\n",
    "Preper.create_domain_boundaries()\n",
    "Preper.get_oligostates(num_most_common_oligostates=3)\n",
    "Preper.process_templates()\n",
    "top_templates = Preper.top_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85adc600-e911-41b6-a8b0-0639335e0f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f1429f1f-3a0a-454f-aa21-259e6722280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we remove /home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/4hfq_AB.pdb because of tm: 0.4132 and rmsd: 2.79\n"
     ]
    }
   ],
   "source": [
    "USAligner = USAlign(work_dir=work_dir, structure_seqid_dict=Preper.templates_for_oligos, \n",
    "                    template_min_identity=0.1,\n",
    "                   num_top_clusters_per_range=4)\n",
    "\n",
    "USAligner.USAlign_run()\n",
    "USAligner.filter_results(tm_cutoff=0.8,rmsd_min_cutoff=0.2, log_file=True)\n",
    "#USAligner.setup_oligo_directories()\n",
    "#result_dict = USAligner.result_dict\n",
    "USAligner.run_multiseq_alignment(directory=\"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b8ac3-5c3c-46e9-a95d-72b7b0d880ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys, vals in USAligner.multiseq_alignment.items():\n",
    "    print(keys, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf31e48-db6e-4ed8-9803-2c3027b362ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Repairstation = ModellerRepairEnsemble(ensemble_dict=result_dict, work_dir=work_dir)\n",
    "#print(Repairstation.repair_ensemble)\n",
    "#print(Repairstation.monomers)\n",
    "Repairstation.repair_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c49e0a1a-b6fa-4a78-a902-e3a42426176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir='/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203/trimmed_strucs'\n"
     ]
    }
   ],
   "source": [
    "script_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/scripts\"\n",
    "\n",
    "work_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203\"\n",
    "\n",
    "fasta = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/monomer/1-203/multiseq_fasta.fa\"\n",
    "\n",
    "pca_dir = \"/home/micnag/bioinformatics/rcsb_retrieved_pdbs/Test_OOP_pipeline/pca_part\"\n",
    "\n",
    "PCA_object = PrincipalComponentAnalysis(work_dir=work_dir, script_dir=script_dir,msa_fasta=fasta, \n",
    "                                        pca_dir=pca_dir,store_original=True)\n",
    "#PCA_object.run_PCA(work_dir)\n",
    "PCA_object.prepare_ensemble()\n",
    "#PCA_object.run_PCA()  # works but is crap because of forced path locations for executables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6614c4b0-b1c7-4069-b746-5bca97620321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3i7v_A.pdb',\n",
       " '6woh_A.pdb',\n",
       " '6wog_A.pdb',\n",
       " '6wob_A.pdb',\n",
       " '7aui_A.pdb',\n",
       " '6woe_A.pdb',\n",
       " '6pcl_A.pdb',\n",
       " '2q9p_A.pdb',\n",
       " '7aul_A.pdb',\n",
       " '6wod_A.pdb',\n",
       " '6wo8_A.pdb',\n",
       " '6woa_A.pdb',\n",
       " '6wo9_A.pdb',\n",
       " '6wof_A.pdb',\n",
       " '7auj_A.pdb',\n",
       " '6pck_A.pdb',\n",
       " '7aup_A.pdb',\n",
       " '7auk_A.pdb',\n",
       " '7aus_A.pdb',\n",
       " '7aum_A.pdb',\n",
       " '7aut_A.pdb',\n",
       " '2fvv_A.pdb',\n",
       " '6wo7_A.pdb',\n",
       " '7tn4_A.pdb',\n",
       " '6woi_A.pdb',\n",
       " '7aun_A.pdb',\n",
       " '6woc_A.pdb']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_object.multi_pdb_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81521527-400c-45d0-b49b-bec452fac237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bb0bb-7cfe-4ce1-b2d0-cde888f3029e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbcdf9f-caaa-4d5e-9cdd-1ba317478f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad68230-5c43-4ca3-92f6-e9fcdb6c3a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2735516-ab1a-4c87-ae06-4a623907efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac38b3f-b54f-4051-b5b3-a8a2902af69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb825eb4-2ab6-4584-9c1d-8913a20c21fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910e871-c075-446b-8186-e051e0ea8d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ff7b5-72cf-44b5-b088-825ab691322e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8894f40-9733-44d6-bf9c-bb85a6b16544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e09dd-c2aa-45c1-abf3-6a22a6a95087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b374a-70f6-4562-b425-1342f3399170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b68735-2013-4f25-ad03-857ab72e9466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec6037-a0f3-4823-bc9d-fe3346d369b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4121fe-cc4e-437b-9ae6-76e07ad8e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
