{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d76882f-629a-47ec-9fca-3e42eff41150",
   "metadata": {},
   "source": [
    "### List of Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b45735-f5e8-4081-a9c8-60a0729c7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from Bio.PDB import Selection, NeighborSearch\n",
    "from collections import defaultdict\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d958fc9-7d66-4fad-b288-da0b3ca7336e",
   "metadata": {},
   "source": [
    "# Class object that we utilize as framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f13819-7199-4034-82b6-ea0f04df25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutationalClusterer:\n",
    "    def __init__(self, work_dir, logging=True):\n",
    "        '''\n",
    "        General File structure should be something like:\n",
    "        https://tree.nathanfriend.io/?s=(%27opGHs!(%27fancy8~fullPath!fJse~trailingSlash8~rootDot8)~K(%27K%27work_dir5C0QD64Swissprot.DB4OthA_files_for_Mlookup_in_d6.DB--IntAmediates4F_protein_mulGple_seq2.Ji4summary_staGsGcs29quJity2_df93-Results4*Mdf9*MplotsL*4*additHJ4*5Pymol_output-Example4screenshotL35PDBsQ4D6_parsA_s7%204NeighbFhood_clustAA.py43-3-%27)~vAsiH!%271%27)*%20%20-5*0HsAvaGH2_Jignment3...%2F4-*5%5Cn*6atabase7cripts8!true9.csv4AerFourGtiHonJalKsFce!L.png4Mc0_Q-S7-%01QMLKJHGFA987654320-*\n",
    "        .\n",
    "        └── work_dir/\n",
    "            ├── Conservation/\n",
    "            │   ├── Scripts\n",
    "            │   ├── Database/\n",
    "            │   │   ├── Swissprot.DB\n",
    "            │   │   └── Other_files_for_conservation_lookup_in_database.DB\n",
    "            │   ├── Intermediates/\n",
    "            │   │   ├── our_protein_multiple_seq_alignment.ali\n",
    "            │   │   ├── summary_statistics_alignment.csv\n",
    "            │   │   ├── quality_alignment_df.csv\n",
    "            │   │   └── .../\n",
    "            │   └── Results/\n",
    "            │       ├── conservation_df.csv\n",
    "            │       ├── conservation_plots.png\n",
    "            │       └── additonal\n",
    "            ├── Pymol_output/\n",
    "            │   └── Example/\n",
    "            │       ├── screenshot.png\n",
    "            │       └── .../\n",
    "            └── PDBs/\n",
    "                ├── Scripts/\n",
    "                │   ├── Database_parser_scripts \n",
    "                │   ├── Neighbourhood_clusterer.py\n",
    "                │   └── .../\n",
    "                └── .../\n",
    "        \n",
    "        '''\n",
    "        self.work_dir = work_dir #the directory we want to work in\n",
    "        self.log_dir = logging # we want to store log results for whatever we do.\n",
    "        self.gene_name = None\n",
    "        #Your other features of the class that we need.\n",
    "        \n",
    "        \n",
    "\n",
    "    def _visualize_clusters_pymol(self, pdb:str) -> None:\n",
    "        \"\"\"\n",
    "        Utility function to visualize cluster in pymol.\n",
    "        \"\"\"\n",
    "        \n",
    "        #from pymol import cmd\n",
    "        pass\n",
    "\n",
    "    def _plot_clusters(self, pdb:str) -> object:\n",
    "        \"\"\"\n",
    "        Helper function to plot some statistics or quick interactive plots to investigate clustering.\n",
    "        Mostly thought about pyplot or plotly interactive plots i.e alignment where we can see the conservation etc.\n",
    "        https://plotly.com/python/alignment-chart/\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def compute_neighbours(self, pdb:str, cutoff=8.0) -> pd.DataFrame:\n",
    "\n",
    "        # Initialize parser and retrieve structure\n",
    "        structure = parser.get_structure(protname, pdbfile1)\n",
    "        atom_list = Selection.unfold_entities(structure, \"A\")  # Retrieve all atoms\n",
    "    \n",
    "        # Initialize NeighborSearch with all atoms and prepare to store results\n",
    "        ns = NeighborSearch(atom_list)\n",
    "        neighbor_dict = defaultdict(set)  # Use set to avoid duplicates\n",
    "    \n",
    "        # Define list of standard amino acids to exclude solvents and ligands\n",
    "        aa_lst = [\n",
    "            \"VAL\", \"ALA\", \"GLY\", \"TRP\", \"ARG\", \"LYS\", \"LEU\", \"ILE\", \"ASP\", \"ASN\",\n",
    "            \"GLN\", \"GLU\", \"PRO\", \"TYR\", \"PHE\", \"SER\", \"THR\", \"CYS\", \"MET\", \"HIS\"\n",
    "        ]\n",
    "    \n",
    "        # Search for neighboring residues for each atom\n",
    "        for atom in atom_list:\n",
    "            residue = atom.get_parent()\n",
    "            res_name = residue.get_resname()\n",
    "            res_id = residue.get_id()[1]\n",
    "    \n",
    "            # Skip non-amino acid residues\n",
    "            if res_name not in aa_lst:\n",
    "                continue\n",
    "    \n",
    "            # Search for neighboring residues within the cutoff distance\n",
    "            for neighbour in ns.search(atom.get_coord(), cutoff, \"R\"):\n",
    "                neighbour_id = neighbour.get_id()[1]\n",
    "                if neighbour_id != res_id:  # Exclude the residue itself\n",
    "                    neighbour_dict[res_name + str(res_id)].add(neighbour_id)\n",
    "    \n",
    "            \n",
    "        # Convert the neighbor dictionary to a list of tuples\n",
    "        neighbour_data = [(res_id, ' '.join(map(str, sorted(neighbours)))) for res_id, neighbours in neighbour_dict.items()]\n",
    "\n",
    "        # Create a pandas DataFrame\n",
    "        df_neighbours = pd.DataFrame(neighbour_data, columns=['Residue_ID', 'Neighbours'])\n",
    "        return df_neighbours\n",
    "    \n",
    "\n",
    "    def conservation(self, uniprot_id):\n",
    "        '''Gets 3 different types of Conservation:\n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "\n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        \n",
    "        mmseq_fasta_result = self._mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=self.work_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = self._get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "        self.conservation_df = conserv_df\n",
    "\n",
    "        conserv_df.to_csv(f\"{self.log_dir}/conservation_df.csv\")\n",
    "        \n",
    "    def _mmseq_multi_fasta(self, uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "        \"\"\"\n",
    "        uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "        outdir: location where result files will be stored.\n",
    "        sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "        filter_msa = 0 default. if 1 hits are stricter.\n",
    "        query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "        \"\"\"\n",
    "\n",
    "        #we blast with this fasta as query.\n",
    "        trgt_fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "        #Make outdir for all required files.\n",
    "        #we need to write it out to file.\n",
    "        with open(f\"{self.work_dir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "            fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "            fasta_out.write(trgt_fasta_seq)\n",
    "\n",
    "        #fetch pre downloaded database from a parent folder.\n",
    "        msa_file = None\n",
    "        new_location = None\n",
    "        try:\n",
    "            DB_storage_location = f\"{work_dir}\"\n",
    "            #shutil.copy(previous_path, savepath)\n",
    "            bash_curl_cmd = f\"mmseqs createdb {self.work_dir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\" \n",
    "            bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "            #run first cmd which setups query database based on our input fasta file\n",
    "            result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"    \n",
    "            bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "            #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "            result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "            bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "            bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "            result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #delete last line.. required.\n",
    "            sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'        \n",
    "            bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "            #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "                result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            # Specify the path to your MSA file\n",
    "            msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            #transfer the meta file to another location and delete useless files.\n",
    "            # we need to delete : all uniprot* files. \n",
    "            # all query*. All result* \n",
    "            new_location = f\"{self.work_dir}/{uniprot_id}.fasta\"\n",
    "            shutil.copy(msa_file, new_location)\n",
    "            #remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        #we want the path to msa_file for downstream analysis.\n",
    "        return new_location\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "        '''\n",
    "        Helper function to grab the sequence \n",
    "        based on the Uniprot ID\n",
    "        '''\n",
    "        fields = \"sequence\"\n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        seq = \"\"\n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "            i += 1\n",
    "        return seq\n",
    "\n",
    "    def _get_conservation(self, path_to_msa:str):    \n",
    "        '''\n",
    "        Helper function to compute 3 different types of conservation.\n",
    "        \n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "        canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "        result_cons = canal.analysis(method=\"all\")\n",
    "        return result_cons\n",
    "\n",
    "    def _get_url(self, url):\n",
    "        '''Helper function that uses requests for Downloads.'''\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit() \n",
    "        return response\n",
    "\n",
    "    def _get_cosmic_mutations(self, uniprot_id:str, path_to_genome_screen_tsv:str, **kwargs)->pd.DataFrame:\n",
    "\n",
    "        #we convert the uniprot id to gene name for fetch and store it in self.gene_name so we dont need to update it for other databases.\n",
    "        self.gene_name = self.__get_gene_name(uniprot_id)\n",
    "        \n",
    "        \n",
    "        #default columns we want to retrieve. can be changed / added through kwargs later\n",
    "        usecols=['GENE_SYMBOL',\n",
    "         'MUTATION_AA', 'MUTATION_DESCRIPTION',\n",
    "       'MUTATION_ZYGOSITY', 'LOH', 'CHROMOSOME', \n",
    "                 'GENOME_START', 'GENOME_STOP']\n",
    "\n",
    "        df = dd.read_csv(path, sep=\"\\t\", dtype={'CHROMOSOME': 'object',\n",
    "       'MUTATION_ZYGOSITY': 'object', 'GENOME_START': 'float64',\n",
    "       'GENOME_STOP': 'float64',\n",
    "       'LOH': 'object'}, usecols=usecols)  #specify dtype / usecols to minimize memory usage required through load in.\n",
    "\n",
    "\n",
    "        #we need to switch these tuples and then map the 1letter aa code to 3letter aa \n",
    "        #for later compatibility.\n",
    "        lst =  [('Val',\"V\"), ('Ile',\"I\"), ('Leu',\"L\"), ('Glu',\"E\"), ('Gln',\"Q\"),\n",
    "                    ('Asp',\"D\"), ('Asn',\"N\"), ('His',\"H\"), ('Trp',\"W\"), ('Phe',\"F\"), ('Tyr',\"Y\"), \n",
    "                    ('Arg',\"R\"), ('Lys',\"K\"), ('Ser',\"S\"), ('Thr',\"T\"), ('Met',\"M\"), ('Ala',\"A\"), \n",
    "                    ('Gly',\"G\"), ('Pro',\"P\"), ('Cys',\"C\")]\n",
    "        \n",
    "        lst = [(y, x) for x, y in lst] #switch y and x position for convinience\n",
    "\n",
    "        canonical_aas = defaultdict(lambda: \"X\", lst) #default if key not found = \"X\"\n",
    "\n",
    "        #filtering based on \"missense\" mutation. this can be tricky and sometimes messy but lets stick with that\n",
    "        df_re = df[df[\"MUTATION_DESCRIPTION\"].str.contains(\"missense\")]\n",
    "\n",
    "        #now lets filter our uniprot gene name\n",
    "        df_re = df_re[df_re[\"GENE_SYMBOL\"] == f\"{gene_name}\"]\n",
    "\n",
    "        #retrieve relevant information\n",
    "        meta = ('Gene name', 'str') \n",
    "        df_re['CHROMOSOME'] = df_re['CHROMOSOME'].astype('object')\n",
    "        df_re['WT_AA'] = df_re['MUTATION_AA'].str[2].apply(lambda x: canonical_aas[x], meta=meta)\n",
    "        df_re['MUTATION_POSITION'] = df_re['MUTATION_AA'].str[3:-1]\n",
    "        df_re['MUTATED_AA'] = df_re['MUTATION_AA'].str[-1].apply(lambda x: canonical_aas[x], meta=meta)\n",
    "\n",
    "        #redundant so we drop it\n",
    "        df_re = df_re.drop(\"MUTATION_AA\", axis=1)\n",
    "\n",
    "        #now we use compute() which finally does the computation (before all actions were \"lazy\" computations\n",
    "        # so we dont actually need the RAM. now we do it though.)\n",
    "        cosmic_df = df_re.compute()\n",
    "    \n",
    "        cosmic_df[\"GENOME_START\"] = cosmic_df[\"GENOME_START\"].astype(int)\n",
    "        cosmic_df[\"GENOME_STOP\"] = cosmic_df[\"GENOME_STOP\"].astype(int)\n",
    "\n",
    "        #return df\n",
    "        return cosmic_df\n",
    "\n",
    "\n",
    "    def _get_gnomad_mutations(self, gene_name:str, gnomad_data_table_path:str, **kwargs)-> pd.Dataframe:\n",
    "        \"\"\"Documentation.\n",
    "        Currently this part does not convert the result to a df.\n",
    "        I will implement it and return a pandas DF\n",
    "        \"\"\"\n",
    "        mt = hl.read_matrix_table(path)  #matrix table because df would not work with such large data.\n",
    "\n",
    "\n",
    "        #string based search because there is NO API for gnomAD.\n",
    "        substring1 = Gene_name\n",
    "        substring2 = \"missense\"\n",
    "        \n",
    "        mt = mt.annotate_rows(Gene_names=mt.info.vep.map(\n",
    "            lambda x: x.split(\"\\|\")[3]) ,\n",
    "                          type_of_change = mt.info.vep.map(\n",
    "            lambda x: x.split(\"\\|\")[1]) , \n",
    "                          AA_change = mt.info.vep.map(\n",
    "            lambda x: x.split(\"\\|\")[11]) , \n",
    "                          ENST_identifier= mt.info.vep.map(\n",
    "            lambda x: x.split(\"\\|\")[6])\n",
    "    \n",
    "        ) \n",
    "                 \n",
    "        filtered_mt_2 = mt.filter_rows(\n",
    "        #hl.any(lambda x: hl.str(x).contains(substring3), mt.AA_change)\n",
    "        hl.any(lambda x: hl.str(x).contains(substring1), mt.info.vep) &\n",
    "        hl.any(lambda x: hl.str(x).contains(substring2), mt.info.vep)\n",
    "        \n",
    "        )\n",
    "                         \n",
    "        filtered_mt_3 = filtered_mt_2.annotate_rows(\n",
    "            Allele_count_int = filtered_mt_2.info.AC,\n",
    "            Allele_frequency_float = filtered_mt_2.info.AF,\n",
    "            Allele_number_int = filtered_mt_2.info.AN,\n",
    "            Gene_name_str = _replace_empty(filtered_mt_2.Gene_names), \n",
    "            Mutation_change_str = _replace_empty(filtered_mt_2.AA_change),\n",
    "            Type_of_change_str = _replace_empty(filtered_mt_2.type_of_change))\n",
    "        \n",
    "        #this can be again regulated later trough kwargs**\n",
    "        rows_to_keep = [\"Gene_name_str\", \"Mutation_change_str\", \"Type_of_change_str\", \"Allele_count_int\",\n",
    "                    \"Allele_frequency_float\", \"Allele_number_int\"]\n",
    "    \n",
    "    \n",
    "        selected_rows = filtered_mt_3.select_rows(\n",
    "            Allele_count_int=filtered_mt_3.Allele_count_int,\n",
    "            Allele_frequency_float=filtered_mt_3.Allele_frequency_float,\n",
    "            Allele_number_int=filtered_mt_3.Allele_number_int,\n",
    "            Gene_name_str=hl.str(filtered_mt_3.Gene_name_str),\n",
    "            Mutation_change_str=hl.str(filtered_mt_3.Mutation_change_str),\n",
    "            Type_of_change_str=hl.str(filtered_mt_3.Type_of_change_str)\n",
    "                )\n",
    "    \n",
    "        save_buffer = selected_rows.select_rows(*rows_to_keep)\n",
    "        select_rows_out = save_buffer.rows()\n",
    "\n",
    "        #part missing to convert to pandas DF.\n",
    "        \n",
    "        #return pd.Dataframe() \n",
    "\n",
    "    def _map_clinvar(self, gene_name:str, clinvar_variant_summary_txt:str, **kwargs)-> pd.Dataframe:\n",
    "        \"\"\"\n",
    "        Documentation missing.\n",
    "        \"\"\"\n",
    "\n",
    "        #can be regulated through kwargs\n",
    "        use_cols = [\"Type\", \"Name\", \"GeneSymbol\",\n",
    "           \"ClinicalSignificance\", \"PhenotypeList\",\n",
    "           \"Assembly\", \"ChromosomeAccession\", \n",
    "           \"Chromosome\", \"Start\", \"Stop\"]\n",
    "        \n",
    "        #this here as well / mapping needed to save memory at load in.\n",
    "        column_data_types = {\n",
    "        \"Type\": str,\n",
    "        \"Name\": str,\n",
    "        \"GeneSymbol\": str,\n",
    "        \"ClinicalSignificance\": str,\n",
    "        \"PhenotypeList\": str,\n",
    "        \"Assembly\": str,\n",
    "        \"ChromosomeAccession\": str,\n",
    "        \"Chromosome\": str,\n",
    "        \"Start\": int,\n",
    "        \"Stop\": int\n",
    "        }\n",
    "\n",
    "        #lets read in the clinvar all var file.\n",
    "        df_work = pd.read_csv(clinvar_variant_summary_txt, sep=\"\\t\", usecols=use_cols, dtype=column_data_types)\n",
    "    \n",
    "        df_work.loc[:, \"AA_change\"] = df_work[\"Name\"].str.split().str.get(-1)\n",
    "        df_work.loc[:, \"AA_change\"] = df_work[\"AA_change\"].str.replace(\"(\", \"\")\n",
    "        df_work.loc[:, \"AA_change\"] = df_work[\"AA_change\"].str.replace(\")\", \"\")\n",
    "        \n",
    "        df_work.loc[:,\"Original_AA\"] = df_work[\"AA_change\"].str[2:5]\n",
    "        df_work.loc[:,\"Modified_AA\"] = df_work[\"AA_change\"].str[-3:]\n",
    "        df_work['Position'] = pd.to_numeric(df_work['AA_change'].str[5:-3], errors='coerce')\n",
    "        \n",
    "        # Drop rows with NaN values in the 'Position' column\n",
    "        df_work.dropna(subset=['Position'], inplace=True)\n",
    "        df_work['Position'] = df_work['Position'].astype(int)\n",
    "        \n",
    "        df_work[\"Genomic_location\"] = df_work[\"Chromosome\"] + \":\" + df_work[\"Start\"].astype(str)\n",
    "        df_work[\"gnomad_aa_change\"] = \"p.\" + df_work[\"Original_AA\"] + df_work[\"Position\"].astype(str) + df_work[\"Modified_AA\"]\n",
    "        \n",
    "        df_work = df_work.drop(\"AA_change\", axis=1)\n",
    "        df_work = df_work.drop(\"Name\", axis=1)\n",
    "        df_work = df_work.drop(\"Chromosome\", axis=1)\n",
    "        df_work = df_work.drop(\"Start\", axis=1)\n",
    "        df_work = df_work.drop(\"Stop\", axis=1)\n",
    "        \n",
    "        df_work[\"Allele_count\"] = [np.nan] * len(df_work)\n",
    "        df_work[\"Allele_number\"] = [np.nan] * len(df_work)\n",
    "        df_work[\"Allele_frequency\"] = [np.nan] * len(df_work)\n",
    "        \n",
    "        \n",
    "        accepted_residues = [\"Ala\", \"Gly\", \"Ser\", \"Leu\", \"Pro\",\n",
    "                        \"Ile\", \"Val\", \"Phe\", \"Tyr\", \"Trp\",\n",
    "                         \"His\", \"Thr\", \"Asn\", \"Gln\", \"Asp\", \n",
    "                         \"Glu\",\"Cys\", \"Met\", \"Lys\", \"Arg\"]\n",
    "        \n",
    "        #filtering based on our Gene name.\n",
    "        df_clinvar = df_work[(df_work[\"Type\"] == \"single nucleotide variant\") & \n",
    "            (df_work[\"GeneSymbol\"] == Gene_name) &\n",
    "            (df_work[\"Assembly\"] == \"GRCh37\") & \n",
    "            (df_work['Original_AA'].isin(accepted_residues)) &\n",
    "            (df_work['Modified_AA'].isin(accepted_residues)) ]\n",
    "        \n",
    "        return df_clinvar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2926b-4575-4b7b-96f7-9f9866b0fc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4bb835-2b01-4aaa-a529-7d9cd69bce17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c2eda-38fa-4a04-ac41-012fa51baf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to avoid errors\n",
    "    updated_clinvar_df = cbioport_df = cosmic_df = gnomad_df = clinvar_df = gnomad_mut_dict = gnomad_mutation_dict = None\n",
    "\n",
    "    \n",
    "    # Step 1: Cosmic mutations\n",
    "    try:\n",
    "        cosmic_df = get_cosmic_mutations(gene_name=main_prot_name)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    #we save it in the folder for the protein outside of monomer / pos at the base level. \n",
    "    save_dataframe_to_csv(cosmic_df, path, \"cosmic_mutations\")\n",
    "\n",
    "    # Step 2: Map gnomad mutations\n",
    "    try:\n",
    "        gnomad_table_path = map_gnomad(Gene_name=main_prot_name, outpath=oligo_state_to_check)\n",
    "        gnomad_df, gnomad_mutation_dict = gnomad_to_pandas(Gene_name=main_prot_name, path_to_tsv=gnomad_table_path, fasta_seq=main_prot_seq)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(gnomad_df, path, \"gnomad_mutations\")\n",
    "\n",
    "    # Step 3: Gather mutations from clinvar\n",
    "    try:\n",
    "        clinvar_df = map_clinvar(Gene_name=main_prot_name)\n",
    "        clinvar_map_outpath = f\"{path}/clinvar_intermediate.csv\"\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(clinvar_df, path, \"clinvar_intermediate\")\n",
    "\n",
    "    # Step 4: Map clinvar to gnomad\n",
    "    try:\n",
    "        list_to_be_searched, clinvar_df = map_clinvar_to_gnomad_1(Gene_name=main_prot_name, clinvar_df=clinvar_df,\n",
    "                                                                  gnomad_mut_dict=gnomad_mutation_dict, clinvar_mapped_df_path=clinvar_map_outpath)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        # Update clinvar muts that were found 1 step before.\n",
    "        try:\n",
    "            updated_clinvar_df = update_clinvar_muts_based_on_gnomad(clinvar_df=clinvar_df, gnomad_dict=gnomad_mutation_dict)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    \n",
    "    save_dataframe_to_csv(updated_clinvar_df, path, \"clinvar_mutations\")\n",
    "\n",
    "    # Step 5: Fetch additional info from cbioportal\n",
    "    try:\n",
    "        gene_name = get_hugo_name(uniprot_id)\n",
    "        print(f\"This is gene name in hugo: {gene_name}\")\n",
    "        cbioport_df = get_cbioportal_info(gene_name=gene_name)\n",
    "        save_dataframe_to_csv(cbioport_df, path, \"cbioport_mutations\")\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    \n",
    "    # Print shapes (if available)\n",
    "    dataframes = [cosmic_df, updated_clinvar_df, gnomad_df, cbioport_df]\n",
    "    for df in dataframes:\n",
    "        try:\n",
    "            print(f\"This is df shape: {df.shape}\")\n",
    "        except Exception as error:\n",
    "            print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a926ab-942c-449b-abc9-fbf3c5399fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
