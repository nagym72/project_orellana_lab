{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d76882f-629a-47ec-9fca-3e42eff41150",
   "metadata": {},
   "source": [
    "### List of Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b45735-f5e8-4081-a9c8-60a0729c7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d958fc9-7d66-4fad-b288-da0b3ca7336e",
   "metadata": {},
   "source": [
    "# Class object that we utilize as framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f13819-7199-4034-82b6-ea0f04df25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutationalClusterer:\n",
    "    def __init__(self, work_dir, logging=True):\n",
    "        '''\n",
    "        General File structure should be something like:\n",
    "        https://tree.nathanfriend.io/?s=(%27opGHs!(%27fancy8~fullPath!fJse~trailingSlash8~rootDot8)~K(%27K%27work_dir5C0QD64Swissprot.DB4OthA_files_for_Mlookup_in_d6.DB--IntAmediates4F_protein_mulGple_seq2.Ji4summary_staGsGcs29quJity2_df93-Results4*Mdf9*MplotsL*4*additHJ4*5Pymol_output-Example4screenshotL35PDBsQ4D6_parsA_s7%204NeighbFhood_clustAA.py43-3-%27)~vAsiH!%271%27)*%20%20-5*0HsAvaGH2_Jignment3...%2F4-*5%5Cn*6atabase7cripts8!true9.csv4AerFourGtiHonJalKsFce!L.png4Mc0_Q-S7-%01QMLKJHGFA987654320-*\n",
    "        .\n",
    "        └── work_dir/\n",
    "            ├── Conservation/\n",
    "            │   ├── Scripts\n",
    "            │   ├── Database/\n",
    "            │   │   ├── Swissprot.DB\n",
    "            │   │   └── Other_files_for_conservation_lookup_in_database.DB\n",
    "            │   ├── Intermediates/\n",
    "            │   │   ├── our_protein_multiple_seq_alignment.ali\n",
    "            │   │   ├── summary_statistics_alignment.csv\n",
    "            │   │   ├── quality_alignment_df.csv\n",
    "            │   │   └── .../\n",
    "            │   └── Results/\n",
    "            │       ├── conservation_df.csv\n",
    "            │       ├── conservation_plots.png\n",
    "            │       └── additonal\n",
    "            ├── Pymol_output/\n",
    "            │   └── Example/\n",
    "            │       ├── screenshot.png\n",
    "            │       └── .../\n",
    "            └── PDBs/\n",
    "                ├── Scripts/\n",
    "                │   ├── Database_parser_scripts \n",
    "                │   ├── Neighbourhood_clusterer.py\n",
    "                │   └── .../\n",
    "                └── .../\n",
    "        \n",
    "        '''\n",
    "        self.work_dir = work_dir #the directory we want to work in\n",
    "        self.log_dir = logging # we want to store log results for whatever we do.\n",
    "        #Your other features of the class that we need.\n",
    "        \n",
    "        \n",
    "\n",
    "    def _visualize_clusters_pymol(self, pdb:str) -> None:\n",
    "        \"\"\"\n",
    "        Utility function to visualize cluster in pymol.\n",
    "        \"\"\"\n",
    "        \n",
    "        #from pymol import cmd\n",
    "        pass\n",
    "\n",
    "    def _plot_clusters(self, pdb:str) -> object:\n",
    "        \"\"\"\n",
    "        Helper function to plot some statistics or quick interactive plots to investigate clustering.\n",
    "        Mostly thought about pyplot or plotly interactive plots i.e alignment where we can see the conservation etc.\n",
    "        https://plotly.com/python/alignment-chart/\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def _compute_neighbours(self, pdb:str) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def surr_atoms(self, inpath, protname, cutoff=8, outpath=os.getcwd()):\n",
    "        with open(inpath, \"r\") as pdbfile1:\n",
    "\n",
    "        # first we need to make extract all atoms from our pdb file.\n",
    "        structure = parser.get_structure(protname, pdbfile1)\n",
    "\n",
    "        # Selection.unfold_entities(<structure object>, <level of information that you want>)\n",
    "        # other levels are \"C\" for chain, \"R\" for residue, \"A\" for atom and so on.\n",
    "        atom_list = Selection.unfold_entities(structure, \"A\")\n",
    "\n",
    "        # lets get the coordinates of all atoms now\n",
    "        atom_coords = [(atom.get_coord(), atom.get_parent()) for atom in atom_list]\n",
    "        # parse through atom_list and apply .get_coord() to each retrieved object.\n",
    "        # we also store for each atom the parent residue\n",
    "\n",
    "        # we provide as argument here the Selection.unfold.entities object which has all atoms.\n",
    "        ns = NeighborSearch(atom_list)  # this class object has the .search() method defined in its __init__\n",
    "\n",
    "        # we will store for each atom all residues that are within 8 A surroundings in this dictionary.\n",
    "        f\"\"\"Keys: atom coordinates\n",
    "        Values: all residues within {cutoff} A\"\"\"\n",
    "\n",
    "        # we need a counter\n",
    "        # we will go through all atoms of lets say residue 1: VAL.\n",
    "        # this has 10 atoms so we need to search 10 times for each atom\n",
    "        # within 8A cutoff radius, get all surrounding residues, and then\n",
    "        # we will merge them together into a list and take only unique residues to get\n",
    "        # rid of redundancy. Come to me Federico if you need explanation in detail given the next section is tricky.\n",
    "        i = 1\n",
    "\n",
    "        # we only want aa residue surroundings, excluding solvent and ligands\n",
    "        aa_lst = [\"VAL\", \"ALA\", \"GLY\", \"TRP\", \"ARG\", \"LYS\", \"LEU\", \"ILE\", \"ASP\", \"ASN\", \"GLN\", \"GLU\", \"PRO\", \"TYR\",\n",
    "                  \"PHE\",\n",
    "                  \"SER\", \"THR\", \"CYS\", \"MET\", \"HIS\"]\n",
    "\n",
    "        hits_per_atom_for_surr_residues = defaultdict(list)  # we store all hits in a dictionary\n",
    "        for atoms in atom_coords:\n",
    "            # if its a water atom, we are not interested in doing the neighbour search.\n",
    "            # I will make this even more robust to exclude other ligands by only allowing the 20 aa to be selected.\n",
    "            if atoms[1].get_resname() not in aa_lst:\n",
    "                i += 1\n",
    "                continue\n",
    "            # if we are no longer in the same residue, we increment by 1 the counter and reset our tmp list.\n",
    "            if atoms[1].get_id()[1] != i:\n",
    "                i += 1\n",
    "\n",
    "            f'''For each atom we will make a search for all surrounding atoms that are within {cutoff} A radius.'''\n",
    "\n",
    "            proximal_atoms = ns.search(atoms[0], 8, \"R\")\n",
    "            # I SET HERE search for atoms[0] because atoms is a tuple containing of coordinates\n",
    "            # and parent residue name see line 75 + 76 #print(atom_coords[0])\n",
    "\n",
    "            f\"\"\"Synthax: ns.search(<target object>, <Cutoff to be searched for>, \n",
    "            <type of information level that should be returned>\n",
    "            R means we dont want the single atoms that are within {cutoff}A \n",
    "            found but instead their corresponding residues. For all atoms we would set <A> instead of <R>\"\"\"\n",
    "\n",
    "            # this function searches through a target (in our case each atom as we loop through all available atoms)\n",
    "            # and returns a list with all atoms within specified atoms .\n",
    "\n",
    "            '''Take a look at the following print statement to see whats going on'''\n",
    "            # print(f\"The selected atom has the following coordinates:\\nX:{atoms[0]}\\nY:{atoms[1]}\\nZ:{atoms[2]}\\n \\\n",
    "            # These are all Residue ids that are within 8 A vicinity:\\n\")\n",
    "\n",
    "            tmp = []  # we store all of them in a temporary list\n",
    "            for residues in proximal_atoms:  # we go through all residues that were found within cutoff A\n",
    "                id_x = residues.get_id()[1]\n",
    "                # get_id gives us a tuple with shape (\"\", \"residue number\", \"optinal flag\").\n",
    "                # Out of this tuple we want the residue id which is [1]\n",
    "                # we only want residues that we dont have already in the list.\n",
    "                # Makes no sense to add stuff that is already in there\n",
    "                if id_x not in tmp and id_x != i:\n",
    "                    # we also exclude the residue itself to be added to its neighbours.\n",
    "                    tmp.append(id_x)\n",
    "            # if we have all we append the whole list to the dictionary. we take the atoms parent residue name as a key.\n",
    "\n",
    "            res_name = f\"{atoms[1].get_resname()}{atoms[1].get_id()[1]}\"\n",
    "            # this string concatenation is super ugly to look at\n",
    "            # and very confusing but it does the following:\n",
    "            f\"\"\"{atoms[1].get_resname()} == residue name. In this case its a\n",
    "                {atoms[1].get_id()[1]} \"\"\"\n",
    "            # so the resname would be in this case : VAL1 but you can modify the output as you wish. e.g VAL_1 or 1_VAL\n",
    "            hits_per_atom_for_surr_residues[res_name].append(tmp)\n",
    "\n",
    "        # print(hits_per_atom_for_surr_residues)\n",
    "        # this shows we captures all aa within the protein\n",
    "        # print(len(hits_per_atom_for_surr_residues))\n",
    "\n",
    "        # lets make a dictionary containing all residues within 8 A on a residue base instead for all atoms.\n",
    "        res_dict = defaultdict()\n",
    "        # we parse through the old values and only add UNIQUE residues to the new dictionary so we dont have duplicates\n",
    "        \"\"\"UGLY SOLUTION BUT DOES THE TRICK\"\"\"\n",
    "        for keys, values in hits_per_atom_for_surr_residues.items():\n",
    "            # print(values)\n",
    "            tmp_vals = []\n",
    "            for vals in values:\n",
    "                # each val is a list corresponding to 1 atom and its surrounded residues\n",
    "                # which are the list entries. values is a list of lists covering the whole residue.\n",
    "                for single_res in vals:\n",
    "                    # now we go through all atoms of the residue.\n",
    "                    if single_res not in tmp_vals:\n",
    "                        # we add all atoms that are not already counted from previous atoms\n",
    "                        # of the same target residue and add their surrounding residues to the tmp_vals list\n",
    "                        # we don't add the atom itself\n",
    "                        # print(single_res)\n",
    "                        tmp_vals.append(single_res)\n",
    "\n",
    "            # now we got 1 cycle done.\n",
    "            # This corresponds to going through all atoms of e.g VALINE which has 7 atoms.\n",
    "            # We group all residues that are neighbouring these 7 atoms and take only the unique ones.\n",
    "            # This corresponds to the surrounding residues for the whole residue.\n",
    "            res_dict[keys] = tmp_vals\n",
    "            # quick look for the results.\n",
    "            # sorry Federico for a bit mess above.\n",
    "            # I will maybe refine it but this script works and you can directly implement it\n",
    "            with open(outpath + protname, \"w\") as fh_out:\n",
    "                for keys, values in res_dict.items():\n",
    "                    full_hit = \"\"\n",
    "                    fh_out.write(keys + \",\")\n",
    "                    for single_entries in values:\n",
    "                        full_hit += str(single_entries) + \" \"\n",
    "                    fh_out.write(full_hit + \"\\n\")\n",
    "\n",
    "                    # for keys, values in hits_per_atom_for_surr_residues.items():\n",
    "                    #    #take a quick look at the result\n",
    "                    #    print(f\"here comes a list of all residues\n",
    "                    #    that are in contact with all {len(values)} atoms that {keys} has:\")\n",
    "                    #    for i, vals in enumerate(values):\n",
    "                    #        print(f\"atom {i+1}: {vals}\")\n",
    "                    #    break\n",
    "\n",
    "    \n",
    "\n",
    "    def conservation(self, uniprot_id):\n",
    "        '''Gets 3 different types of Conservation:\n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "\n",
    "        if self.log_dir and not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        \n",
    "        mmseq_fasta_result = self._mmseq_multi_fasta(uniprot_id=uniprot_id, outdir=self.work_dir)\n",
    "        #get 3 different conservation scores in a pandas df.\n",
    "        conserv_df = self._get_conservation(path_to_msa=mmseq_fasta_result)\n",
    "        self.conservation_df = conserv_df\n",
    "\n",
    "        conserv_df.to_csv(f\"{self.log_dir}/conservation_df.csv\")\n",
    "        \n",
    "    def _mmseq_multi_fasta(self, uniprot_id:str, outdir:str, \n",
    "                      sensitivity=7, filter_msa=0,\n",
    "                     query_id = 0.6):\n",
    "        \"\"\"\n",
    "        uniprot_id: The unique uniprot identifier used to fetch the corresponding fasta file that will be used as a template for mmseq2\n",
    "        outdir: location where result files will be stored.\n",
    "        sensitivity: mmseq2 specific parameter that goes from 1-7. The higher the more sensitive the search.\n",
    "        filter_msa = 0 default. if 1 hits are stricter.\n",
    "        query_id = 0.6 [0, 1]  the higher the more identity with query is retrieved. 1 means ONLY the query hits while 0 means take everything possible.\n",
    "        \"\"\"\n",
    "\n",
    "        #we blast with this fasta as query.\n",
    "        trgt_fasta_seq = self._get_gene_fasta(uniprot_id)\n",
    "        #Make outdir for all required files.\n",
    "        #we need to write it out to file.\n",
    "        with open(f\"{self.work_dir}/{uniprot_id}_fasta.fa\", \"w\") as fasta_out:\n",
    "            fasta_out.write(f\">{uniprot_id}\\n\")\n",
    "            fasta_out.write(trgt_fasta_seq)\n",
    "\n",
    "        #fetch pre downloaded database from a parent folder.\n",
    "        msa_file = None\n",
    "        new_location = None\n",
    "        try:\n",
    "            DB_storage_location = f\"{work_dir}\"\n",
    "            #shutil.copy(previous_path, savepath)\n",
    "            bash_curl_cmd = f\"mmseqs createdb {self.work_dir}/{uniprot_id}_fasta.fa {DB_storage_location}/query_fastaDB\" \n",
    "            bash_curl_cmd_rdy = bash_curl_cmd.split()\n",
    "            #run first cmd which setups query database based on our input fasta file\n",
    "            result_setup_query_db = run(bash_curl_cmd_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            bash_curl_cmd_2 = f\"mmseqs search {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/tmp -s {sensitivity}\"    \n",
    "            bash_curl_cmd_rdy_2 = bash_curl_cmd_2.split()\n",
    "            #run 2nd cmd which blasts against swiss_DB and generates the resultDB (i.e our hits that were found)\n",
    "            result_setup_blast_db = run(bash_curl_cmd_rdy_2, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #mmseqs convert2fasta DB_clu_rep DB_clu_rep.fasta\n",
    "            bash_curl_cmd_5 = f\"mmseqs result2msa {DB_storage_location}/query_fastaDB {DB_storage_location}/swiss_DB {DB_storage_location}/result_DB {DB_storage_location}/{uniprot_id}_out.fasta --msa-format-mode 3 --filter-msa {filter_msa} --qid {query_id}\" \n",
    "            bash_curl_cmd_5_rdy = bash_curl_cmd_5.split()\n",
    "            result_setup_msa_convert = run(bash_curl_cmd_5_rdy, stdout=PIPE, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            #delete last line.. required.\n",
    "            sed_cmd = f'sed -e 1,4d -e $d {DB_storage_location}/{uniprot_id}_out.fasta'        \n",
    "            bash_curl_cmd_6_rdy = sed_cmd.split()\n",
    "            #f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            with open(f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\", \"w\") as new_fasta:\n",
    "                result_truncation = run(bash_curl_cmd_6_rdy, stdout=new_fasta, stderr=PIPE, \n",
    "                                 universal_newlines=True)\n",
    "            # Specify the path to your MSA file\n",
    "            msa_file = f\"{DB_storage_location}/{uniprot_id}_new_out.fasta\"\n",
    "            #transfer the meta file to another location and delete useless files.\n",
    "            # we need to delete : all uniprot* files. \n",
    "            # all query*. All result* \n",
    "            new_location = f\"{self.work_dir}/{uniprot_id}.fasta\"\n",
    "            shutil.copy(msa_file, new_location)\n",
    "            #remove_files_and_dirs_msa(DB_storage_location, uniprot_id=uniprot_id)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        #we want the path to msa_file for downstream analysis.\n",
    "        return new_location\n",
    "\n",
    "    def _get_gene_fasta(self, uniprot_id:str):\n",
    "        '''\n",
    "        Helper function to grab the sequence \n",
    "        based on the Uniprot ID\n",
    "        '''\n",
    "        fields = \"sequence\"\n",
    "        URL = f\"https://rest.uniprot.org/uniprotkb/search?format=fasta&fields={fields}&query={uniprot_id}\"\n",
    "        resp = self._get_url(URL)\n",
    "        resp = resp.iter_lines(decode_unicode=True)\n",
    "        seq = \"\"\n",
    "        i = 0\n",
    "        for lines in resp:\n",
    "            if i > 0:\n",
    "                seq += lines\n",
    "            i += 1\n",
    "        return seq\n",
    "\n",
    "    def _get_conservation(self, path_to_msa:str):    \n",
    "        '''\n",
    "        Helper function to compute 3 different types of conservation.\n",
    "        \n",
    "        - Shannon conservation: \n",
    "        Shannon entropy. \n",
    "        Higher values indicate lower conservation and greater variability at the site.\n",
    "        \n",
    "        - Relative conservation:\n",
    "        Kullback-Leibler divergence.\n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        \n",
    "        - Lockless conservation\n",
    "        Evolutionary conservation parameter defined by Lockless and Ranganathan (1999). \n",
    "        Higher values indicate greater conservation and lower variability at the site.\n",
    "        '''\n",
    "        canal = Canal(fastafile=path_to_msa, #Multiple sequence alignment (MSA) of homologous sequences\n",
    "          ref=0, #Position of reference sequence in MSA, use first sequence always\n",
    "          startcount=0, # ALways 0 because our seqs are always from 1 - end\n",
    "          verbose=False) # no verbosity \n",
    "    \n",
    "        result_cons = canal.analysis(method=\"all\")\n",
    "        return result_cons\n",
    "\n",
    "    def _get_url(self, url):\n",
    "        '''Helper function that uses requests for Downloads.'''\n",
    "        try:\n",
    "            response = requests.get(url)  \n",
    "            if not response.ok:\n",
    "                print(response.text)\n",
    "        except:\n",
    "            response.raise_for_status()\n",
    "            #sys.exit() \n",
    "        return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
